#!/usr/bin/env python3
"""
Build project_csv split policy registry (generated artifact vs canonical dataset).

Inputs:
- registry/csv_inventory.toml
- Makefile artifact cleanup patterns
- src/verification/verify_generated_artifacts.py explicit generated checks

Outputs:
- registry/project_csv_split_policy.toml
- registry/manifests/project_csv_canonical_manifest.txt
- registry/manifests/project_csv_generated_manifest.txt
"""

from __future__ import annotations

import argparse
import fnmatch
import json
import re
import tomllib
from pathlib import Path

UPDATED_STAMP = "2026-02-09"

MAKEFILE_GENERATED_GLOBS = [
    "data/csv/cd_motif_*.csv",
    "data/csv/de_marrais_*.csv",
    "data/csv/reggiani_*.csv",
    "data/csv/m3_table.csv",
    "data/csv/dimensional_geometry_*.csv",
    "data/csv/materials_jarvis_subset.csv",
    "data/csv/materials_embedding_benchmarks.csv",
    "data/csv/modular_chaos_*.csv",
    "data/csv/sedenion_field_metrics_*.csv",
    "data/csv/spectral_flow.csv",
]


def _assert_ascii(text: str, context: str) -> None:
    bad = sorted({ch for ch in text if ord(ch) > 127})
    if bad:
        sample = "".join(bad[:20])
        raise SystemExit(f"ERROR: Non-ASCII output in {context}: {sample!r}")


def _esc(value: str) -> str:
    return json.dumps(value, ensure_ascii=True)


def _extract_generated_explicit_paths(verify_file: Path) -> set[str]:
    text = verify_file.read_text(encoding="utf-8")
    return set(re.findall(r'"(data/csv/[^"]+\.csv)"', text))


def _render_policy(
    rows: list[dict[str, object]],
    *,
    generated_count: int,
    canonical_count: int,
) -> str:
    lines: list[str] = []
    lines.append("# project_csv split policy registry (TOML-first).")
    lines.append("# Generated by src/scripts/analysis/build_project_csv_split_policy_registry.py")
    lines.append("")
    lines.append("[project_csv_split_policy]")
    lines.append(f'updated = "{UPDATED_STAMP}"')
    lines.append("authoritative = true")
    lines.append('source_inventory = "registry/csv_inventory.toml"')
    lines.append(f"dataset_count = {len(rows)}")
    lines.append(f"canonical_dataset_count = {canonical_count}")
    lines.append(f"generated_artifact_count = {generated_count}")
    lines.append("")

    lines.append("generated_evidence_refs = [")
    lines.append('  "Makefile:357",')
    lines.append('  "src/verification/verify_generated_artifacts.py:37",')
    lines.append("]")
    lines.append("")

    for row in rows:
        lines.append("[[dataset]]")
        lines.append(f"path = {_esc(str(row['path']))}")
        lines.append(f"classification = {_esc(str(row['classification']))}")
        lines.append(f"queue_for_scroll_conversion = {str(row['queue_for_scroll_conversion']).lower()}")
        lines.append(f"size_bytes = {int(row['size_bytes'])}")
        lines.append(f"source_sha256 = {_esc(str(row['source_sha256']))}")
        lines.append(f"git_status = {_esc(str(row['git_status']))}")
        lines.append(f"rationale = {_esc(str(row['rationale']))}")
        lines.append("evidence_refs = [")
        for ref in row["evidence_refs"]:
            lines.append(f"  {_esc(str(ref))},")
        lines.append("]")
        lines.append("")

    return "\n".join(lines)


def _write_manifest(path: Path, values: list[str]) -> None:
    payload = "\n".join(values) + ("\n" if values else "")
    _assert_ascii(payload, str(path))
    path.write_text(payload, encoding="utf-8")


def main() -> int:
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--repo-root",
        default=str(Path(__file__).resolve().parents[3]),
        help="Repository root.",
    )
    parser.add_argument(
        "--inventory",
        default="registry/csv_inventory.toml",
        help="Input CSV inventory TOML path.",
    )
    parser.add_argument(
        "--out",
        default="registry/project_csv_split_policy.toml",
        help="Output policy TOML path.",
    )
    parser.add_argument(
        "--canonical-manifest",
        default="registry/manifests/project_csv_canonical_manifest.txt",
        help="Output manifest for canonical dataset CSV paths.",
    )
    parser.add_argument(
        "--generated-manifest",
        default="registry/manifests/project_csv_generated_manifest.txt",
        help="Output manifest for generated artifact CSV paths.",
    )
    args = parser.parse_args()

    root = Path(args.repo_root).resolve()
    inventory = tomllib.loads((root / args.inventory).read_text(encoding="utf-8"))
    rows = [row for row in inventory.get("document", []) if str(row.get("zone")) == "project_csv"]
    rows = sorted(rows, key=lambda row: str(row.get("path", "")))

    explicit_generated = _extract_generated_explicit_paths(
        root / "src/verification/verify_generated_artifacts.py"
    )

    policy_rows: list[dict[str, object]] = []
    canonical_paths: list[str] = []
    generated_paths: list[str] = []

    for row in rows:
        path = str(row.get("path", ""))
        generated_by_glob = any(fnmatch.fnmatch(path, pat) for pat in MAKEFILE_GENERATED_GLOBS)
        generated_by_explicit = path in explicit_generated
        is_generated = generated_by_glob or generated_by_explicit

        if is_generated:
            classification = "generated_artifact"
            rationale = (
                "Matched generated artifact contract via Makefile clean-artifacts "
                "patterns and/or verify_generated_artifacts expectations."
            )
            evidence_refs = [
                "Makefile:357",
                "src/verification/verify_generated_artifacts.py:37",
            ]
            generated_paths.append(path)
        else:
            classification = "canonical_dataset"
            rationale = (
                "Not listed in generated artifact contracts; treat as canonical project dataset "
                "for TOML-first scroll conversion."
            )
            evidence_refs = [
                "Makefile:357",
                "src/verification/verify_generated_artifacts.py:37",
            ]
            canonical_paths.append(path)

        policy_rows.append(
            {
                "path": path,
                "classification": classification,
                "queue_for_scroll_conversion": True,
                "size_bytes": int(row.get("size_bytes", 0)),
                "source_sha256": str(row.get("sha256", "")),
                "git_status": str(row.get("git_status", "")),
                "rationale": rationale,
                "evidence_refs": evidence_refs,
            }
        )

    out_path = root / args.out
    out_path.parent.mkdir(parents=True, exist_ok=True)
    rendered = _render_policy(
        policy_rows,
        generated_count=len(generated_paths),
        canonical_count=len(canonical_paths),
    )
    _assert_ascii(rendered, str(out_path))
    out_path.write_text(rendered, encoding="utf-8")

    canonical_manifest = root / args.canonical_manifest
    generated_manifest = root / args.generated_manifest
    canonical_manifest.parent.mkdir(parents=True, exist_ok=True)
    _write_manifest(canonical_manifest, canonical_paths)
    _write_manifest(generated_manifest, generated_paths)

    print(
        f"Wrote {out_path} with {len(policy_rows)} project_csv records "
        f"(canonical={len(canonical_paths)}, generated={len(generated_paths)})."
    )
    print(f"Wrote canonical manifest: {canonical_manifest}")
    print(f"Wrote generated manifest: {generated_manifest}")
    return 0


if __name__ == "__main__":
    raise SystemExit(main())
