#!/usr/bin/env python3
"""
Build a unified TOML control-plane registry for CSV->TOML scroll lanes.

This consolidates lane metadata for:
- project canonical CSV datasets
- project generated CSV artifacts
- external CSV holding queue
- archive CSV holding queue
"""

from __future__ import annotations

import argparse
import json
import tomllib
from datetime import date
from pathlib import Path

LANE_SPECS: tuple[tuple[str, str, str, str], ...] = (
    (
        "project_canonical",
        "registry/project_csv_canonical_datasets.toml",
        "project_csv_canonical_datasets",
        "canonical_dataset",
    ),
    (
        "project_generated",
        "registry/project_csv_generated_artifacts.toml",
        "project_csv_generated_artifacts",
        "generated_artifact",
    ),
    (
        "external_holding",
        "registry/external_csv_holding_datasets.toml",
        "external_csv_holding_datasets",
        "holding_external_csv",
    ),
    (
        "archive_holding",
        "registry/archive_csv_holding_datasets.toml",
        "archive_csv_holding_datasets",
        "holding_archive_csv",
    ),
)


def _esc(value: str) -> str:
    return json.dumps(value, ensure_ascii=True)


def _ascii_sanitize(text: str) -> str:
    replacements = {
        "\u2018": "'",
        "\u2019": "'",
        "\u201c": '"',
        "\u201d": '"',
        "\u2013": "-",
        "\u2014": "-",
        "\u2026": "...",
        "\u00a0": " ",
    }
    out: list[str] = []
    for ch in text:
        mapped = replacements.get(ch, ch)
        for item in mapped:
            code = ord(item)
            if item in {"\n", "\r", "\t"}:
                out.append(item)
            elif code < 32:
                out.append(" ")
            elif code <= 127:
                out.append(item)
            else:
                out.append(f"<U+{code:04X}>")
    return "".join(out)


def _assert_ascii(text: str, context: str) -> None:
    bad = sorted({ch for ch in text if ord(ch) > 127})
    if bad:
        sample = "".join(bad[:20])
        raise SystemExit(f"ERROR: Non-ASCII output in {context}: {sample!r}")


def _manifest_path(source_descriptor: str) -> str:
    descriptor = source_descriptor.strip()
    if descriptor.startswith("manifest:"):
        return descriptor.split(":", 1)[1]
    return ""


def _load_toml(path: Path) -> dict:
    return tomllib.loads(path.read_text(encoding="utf-8"))


def _build(repo_root: Path) -> tuple[list[dict[str, object]], list[dict[str, object]]]:
    lanes: list[dict[str, object]] = []
    refs: list[dict[str, object]] = []

    for lane_name, registry_path, table_name, dataset_class in LANE_SPECS:
        registry_abs = repo_root / registry_path
        raw = _load_toml(registry_abs)
        section = raw.get(table_name, {})
        datasets = raw.get("dataset", [])
        source_descriptor = str(section.get("source_descriptor", ""))
        manifest = _manifest_path(source_descriptor)
        canonical_dir = str(section.get("canonical_dir", ""))

        lane = {
            "name": lane_name,
            "source_registry": registry_path,
            "source_table": table_name,
            "dataset_class": dataset_class,
            "source_descriptor": source_descriptor,
            "manifest_path": manifest,
            "canonical_dir": canonical_dir,
            "dataset_count": len(datasets),
        }
        lanes.append(lane)

        for row in datasets:
            refs.append(
                {
                    "lane_name": lane_name,
                    "dataset_id": str(row.get("id", "")),
                    "slug": str(row.get("slug", "")),
                    "dataset_class": str(row.get("dataset_class", dataset_class)),
                    "source_csv": str(row.get("source_csv", "")),
                    "canonical_toml": str(row.get("canonical_toml", "")),
                    "source_sha256": str(row.get("source_sha256", "")),
                    "row_count": int(row.get("row_count", 0)),
                    "column_count": int(row.get("column_count", 0)),
                }
            )

    lanes.sort(key=lambda row: str(row["name"]))
    refs.sort(
        key=lambda row: (
            str(row["lane_name"]),
            str(row["dataset_id"]),
            str(row["source_csv"]),
        )
    )
    return lanes, refs


def _render(lanes: list[dict[str, object]], refs: list[dict[str, object]]) -> str:
    lines: list[str] = []
    lines.append("# Unified CSV scroll pipeline control-plane registry.")
    lines.append("# Generated by src/scripts/analysis/build_csv_scroll_pipeline_registry.py")
    lines.append("")
    lines.append("[csv_scroll_pipeline]")
    lines.append(f"updated = {_esc(date.today().isoformat())}")
    lines.append("authoritative = true")
    lines.append(f"lane_count = {len(lanes)}")
    lines.append(f"dataset_total = {len(refs)}")
    lines.append(
        'policy = "All in-scope CSV corpora must flow through canonical/generated/'
        'holding TOML scroll lanes."'
    )
    lines.append("")

    for idx, lane in enumerate(lanes, start=1):
        lines.append("[[lane]]")
        lines.append(f"id = {_esc(f'CSP-LANE-{idx:04d}')}")
        lines.append(f"name = {_esc(str(lane['name']))}")
        lines.append(f"source_registry = {_esc(str(lane['source_registry']))}")
        lines.append(f"source_table = {_esc(str(lane['source_table']))}")
        lines.append(f"dataset_class = {_esc(str(lane['dataset_class']))}")
        lines.append(f"source_descriptor = {_esc(str(lane['source_descriptor']))}")
        lines.append(f"manifest_path = {_esc(str(lane['manifest_path']))}")
        lines.append(f"canonical_dir = {_esc(str(lane['canonical_dir']))}")
        lines.append(f"dataset_count = {int(lane['dataset_count'])}")
        lines.append("")

    for idx, ref in enumerate(refs, start=1):
        lines.append("[[dataset_ref]]")
        lines.append(f"id = {_esc(f'CSP-REF-{idx:05d}')}")
        lines.append(f"lane_name = {_esc(str(ref['lane_name']))}")
        lines.append(f"dataset_id = {_esc(str(ref['dataset_id']))}")
        lines.append(f"slug = {_esc(str(ref['slug']))}")
        lines.append(f"dataset_class = {_esc(str(ref['dataset_class']))}")
        lines.append(f"source_csv = {_esc(str(ref['source_csv']))}")
        lines.append(f"canonical_toml = {_esc(str(ref['canonical_toml']))}")
        lines.append(f"source_sha256 = {_esc(str(ref['source_sha256']))}")
        lines.append(f"row_count = {int(ref['row_count'])}")
        lines.append(f"column_count = {int(ref['column_count'])}")
        lines.append("")

    return "\n".join(lines)


def _write(path: Path, text: str) -> None:
    clean = _ascii_sanitize(text)
    _assert_ascii(clean, str(path))
    path.parent.mkdir(parents=True, exist_ok=True)
    path.write_text(clean, encoding="utf-8")


def main() -> int:
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--repo-root",
        default=str(Path(__file__).resolve().parents[3]),
        help="Repository root.",
    )
    parser.add_argument(
        "--out",
        default="registry/csv_scroll_pipeline.toml",
        help="Output registry path.",
    )
    args = parser.parse_args()

    repo_root = Path(args.repo_root).resolve()
    lanes, refs = _build(repo_root)
    out_text = _render(lanes, refs)
    _write(repo_root / args.out, out_text)
    print(
        "Wrote csv scroll pipeline registry: "
        f"lanes={len(lanes)} dataset_refs={len(refs)} path={repo_root / args.out}"
    )
    return 0


if __name__ == "__main__":
    raise SystemExit(main())
