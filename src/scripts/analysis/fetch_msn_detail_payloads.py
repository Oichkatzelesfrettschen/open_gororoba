#!/usr/bin/env python3
"""
Fetch MSN structured detail payloads for registry entries.

This converts brittle dynamic page scraping into deterministic JSON captures:
- data/external/intake/.../msn_detail/*.json
- data/external/intake/.../msn_detail_manifest.tsv
- data/external/intake/.../msn_detail_provenance.toml
"""

from __future__ import annotations

import argparse
import hashlib
import re
import subprocess
from dataclasses import dataclass
from datetime import UTC, datetime
from pathlib import Path
import tomllib
from urllib.parse import urlparse


MSN_ID_RE = re.compile(r"/(?:ar|vi)-([A-Za-z0-9]+)")
MSN_DETAIL_TMPL = "https://assets.msn.com/content/view/v2/Detail/en-us/{content_id}"
USER_AGENT = (
    "open_gororoba-intake-bot/2026-02-14 "
    "(msn detail capture; +https://github.com/eirikr/open_gororoba)"
)


@dataclass
class Row:
    entry_id: str
    content_id: str
    topic: str
    url: str
    detail_url: str
    status: str
    http_status: str
    output_path: str
    sha256: str
    size_bytes: int
    error: str


def _utc_now() -> str:
    return datetime.now(UTC).isoformat()


def _assert_ascii(text: str, context: str) -> None:
    bad = sorted({ch for ch in text if ord(ch) > 127})
    if bad:
        sample = "".join(bad[:20])
        raise SystemExit(f"ERROR: Non-ASCII output in {context}: {sample!r}")


def _escape(text: str) -> str:
    escaped = (
        text.replace("\\", "\\\\")
        .replace('"', '\\"')
        .replace("\n", "\\n")
        .replace("\r", "\\r")
        .replace("\t", "\\t")
    )
    return f'"{escaped}"'


def _sha256(path: Path) -> str:
    digest = hashlib.sha256()
    with path.open("rb") as handle:
        while True:
            chunk = handle.read(1024 * 1024)
            if not chunk:
                break
            digest.update(chunk)
    return digest.hexdigest()


def _run(cmd: list[str]) -> subprocess.CompletedProcess[str]:
    return subprocess.run(cmd, capture_output=True, text=True, check=False)


def _fetch_json(url: str, output: Path, timeout_s: int) -> tuple[bool, str, str]:
    cmd = [
        "curl",
        "-L",
        "--silent",
        "--show-error",
        "--max-time",
        str(timeout_s),
        "-A",
        USER_AGENT,
        "-o",
        str(output),
        "-w",
        "%{http_code}",
        url,
    ]
    proc = _run(cmd)
    code = proc.stdout.strip() or "000"
    if proc.returncode == 0 and code.startswith("2") and output.exists() and output.stat().st_size > 0:
        return True, code, ""
    detail = proc.stderr.strip() or proc.stdout.strip() or "curl_failed"
    return False, code, detail


def _extract_content_id(url: str) -> str:
    match = MSN_ID_RE.search(url)
    if not match:
        return ""
    return match.group(1)


def _is_msn_url(url: str) -> bool:
    host = (urlparse(url).hostname or "").lower()
    return host.endswith("msn.com")


def _render_manifest(rows: list[Row]) -> str:
    header = (
        "id\tcontent_id\ttopic\tstatus\thttp_status\tsha256\tsize_bytes\toutput_path\tdetail_url\terror"
    )
    lines = [header]
    for row in rows:
        lines.append(
            "\t".join(
                [
                    row.entry_id,
                    row.content_id,
                    row.topic,
                    row.status,
                    row.http_status,
                    row.sha256,
                    str(row.size_bytes),
                    row.output_path,
                    row.detail_url,
                    row.error.replace("\t", " ").replace("\n", " "),
                ]
            )
        )
    rendered = "\n".join(lines) + "\n"
    _assert_ascii(rendered, "msn_detail_manifest.tsv")
    return rendered


def _render_provenance(
    intake_id: str,
    registry_rel: str,
    out_root_rel: str,
    rows: list[Row],
) -> str:
    ok_count = sum(1 for row in rows if row.status == "ok")
    fail_count = len(rows) - ok_count
    lines: list[str] = []
    lines.append("# Generated by src/scripts/analysis/fetch_msn_detail_payloads.py")
    lines.append("[batch]")
    lines.append(f"id = {_escape(f'{intake_id}-msn-detail')}")
    lines.append(f"generated_at_utc = {_escape(_utc_now())}")
    lines.append(f"registry_path = {_escape(registry_rel)}")
    lines.append(f"output_root = {_escape(out_root_rel)}")
    lines.append(f"entry_count = {len(rows)}")
    lines.append(f"success_count = {ok_count}")
    lines.append(f"failure_count = {fail_count}")
    lines.append("")
    for row in rows:
        lines.append("[[artifact]]")
        lines.append(f"id = {_escape(row.entry_id)}")
        lines.append(f"content_id = {_escape(row.content_id)}")
        lines.append(f"topic = {_escape(row.topic)}")
        lines.append(f"source_url = {_escape(row.url)}")
        lines.append(f"detail_url = {_escape(row.detail_url)}")
        lines.append(f"status = {_escape(row.status)}")
        lines.append(f"http_status = {_escape(row.http_status)}")
        lines.append(f"output_path = {_escape(row.output_path)}")
        lines.append(f"sha256 = {_escape(row.sha256)}")
        lines.append(f"size_bytes = {row.size_bytes}")
        lines.append(f"error = {_escape(row.error)}")
        lines.append("")
    rendered = "\n".join(lines)
    _assert_ascii(rendered, "msn_detail_provenance.toml")
    return rendered


def main() -> int:
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--repo-root",
        default=str(Path(__file__).resolve().parents[3]),
        help="Repository root.",
    )
    parser.add_argument(
        "--registry",
        default="registry/research_intake_2026_02_14.toml",
        help="Input registry path.",
    )
    parser.add_argument(
        "--output-root",
        default="data/external/intake/2026_02_14_hypercomplex_news/msn_detail",
        help="Output root for MSN detail payloads.",
    )
    parser.add_argument(
        "--manifest-out",
        default="data/external/intake/2026_02_14_hypercomplex_news/msn_detail_manifest.tsv",
        help="Manifest TSV output path.",
    )
    parser.add_argument(
        "--provenance-out",
        default="data/external/intake/2026_02_14_hypercomplex_news/msn_detail_provenance.toml",
        help="Provenance TOML output path.",
    )
    parser.add_argument(
        "--timeout-seconds",
        type=int,
        default=45,
        help="Network timeout in seconds per payload fetch.",
    )
    args = parser.parse_args()

    root = Path(args.repo_root).resolve()
    registry_path = root / args.registry
    if not registry_path.exists():
        raise SystemExit(f"ERROR: missing registry path: {registry_path}")
    data = tomllib.loads(registry_path.read_text(encoding="utf-8"))
    intake_meta = data.get("research_intake", {})
    intake_id = str(intake_meta.get("id", "unknown-intake"))
    entries = list(data.get("entry", []))

    output_root = root / args.output_root
    output_root.mkdir(parents=True, exist_ok=True)

    rows: list[Row] = []
    for item in entries:
        entry_id = str(item.get("id", "")).strip()
        topic = str(item.get("topic", "")).strip()
        normalized_url = str(item.get("normalized_url", "")).strip()
        if not entry_id or not normalized_url:
            continue
        if not _is_msn_url(normalized_url):
            continue
        content_id = _extract_content_id(normalized_url)
        if not content_id:
            continue
        detail_url = MSN_DETAIL_TMPL.format(content_id=content_id)
        out_file = output_root / f"{entry_id}_{content_id}.json"
        ok, http_status, error = _fetch_json(detail_url, out_file, args.timeout_seconds)
        if ok:
            digest = _sha256(out_file)
            size_bytes = out_file.stat().st_size
            rel_path = out_file.relative_to(root).as_posix()
            status = "ok"
            err = ""
        else:
            if out_file.exists():
                out_file.unlink()
            digest = ""
            size_bytes = 0
            rel_path = ""
            status = "failed"
            err = error

        rows.append(
            Row(
                entry_id=entry_id,
                content_id=content_id,
                topic=topic,
                url=normalized_url,
                detail_url=detail_url,
                status=status,
                http_status=http_status,
                output_path=rel_path,
                sha256=digest,
                size_bytes=size_bytes,
                error=err,
            )
        )

    manifest_text = _render_manifest(rows)
    provenance_text = _render_provenance(
        intake_id=intake_id,
        registry_rel=registry_path.relative_to(root).as_posix(),
        out_root_rel=output_root.relative_to(root).as_posix(),
        rows=rows,
    )
    (root / args.manifest_out).write_text(manifest_text, encoding="utf-8")
    (root / args.provenance_out).write_text(provenance_text, encoding="utf-8")

    print(
        "Fetched MSN detail payloads: "
        f"entries={len(rows)} "
        f"ok={sum(1 for row in rows if row.status == 'ok')} "
        f"failed={sum(1 for row in rows if row.status != 'ok')}"
    )
    return 0


if __name__ == "__main__":
    raise SystemExit(main())
