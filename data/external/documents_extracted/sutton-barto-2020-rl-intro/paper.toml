full_text = """
ii

Adaptive Computation and Machine Learning\r
Francis Bach, series editor\r
A complete list of books published in the Adaptive Computation and Machine Learning\r
series appears at the back of this book.

Reinforcement Learning:\r
An Introduction\r
second edition\r
Richard S. Sutton and Andrew G. Barto\r
The MIT Press\r
Cambridge, Massachusetts\r
London, England

© 2018, 2020 Richard S. Sutton and Andrew G. Barto\r
All rights reserved. No part of this book may be reproduced in any form by any electronic\r
or mechanical means (including photocopying, recording, or information storage and retrieval)\r
without permission in writing from the copyright holder. This work is licensed under the\r
Creative Commons Attribution-NonCommercial-NoDerivs 2.0 Generic License. To view a copy\r
of this license, visit http://creativecommons.org/licenses/by-nc-nd/2.0/ or send a letter\r
to Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.\r
This book was set in 10/12, CMR by Westchester Publishing Services. Printed and bound in\r
the United States of America.\r
Library of Congress Cataloging-in-Publication Data\r
Names: Sutton, Richard S., author. | Barto, Andrew G., author.\r
Title: Reinforcement learning : an introduction / Richard S. Sutton and Andrew G. Barto.\r
Description: Second edition. | Cambridge, MA : The MIT Press, [2018] | Series: Adaptive\r
computation and machine learning series | Includes bibliographical references and index.\r
Identifiers: LCCN 2018023826 | ISBN 9780262039246 (hardcover : alk. paper)\r
Subjects: LCSH: Reinforcement learning.\r
Classification: LCC Q325.6 .R45 2018 | DDC 006.3/1--dc23 LC record available\r
at https://lccn.loc.gov/2018023826\r
10 9 8 7 6 5 4 3 2 1

In memory of A. Harry Klopf

Contents\r
Preface to the Second Edition xiii\r
Preface to the First Edition xvii\r
Summary of Notation xix\r
1 Introduction 1\r
1.1 Reinforcement Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1\r
1.2 Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\r
1.3 Elements of Reinforcement Learning . . . . . . . . . . . . . . . . . . . . . 6\r
1.4 Limitations and Scope . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\r
1.5 An Extended Example: Tic-Tac-Toe . . . . . . . . . . . . . . . . . . . . . 8\r
1.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\r
1.7 Early History of Reinforcement Learning . . . . . . . . . . . . . . . . . . . 13\r
I Tabular Solution Methods 23\r
2 Multi-armed Bandits 25\r
2.1 A k-armed Bandit Problem . . . . . . . . . . . . . . . . . . . . . . . . . . 25\r
2.2 Action-value Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\r
2.3 The 10-armed Testbed . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\r
2.4 Incremental Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . 30\r
2.5 Tracking a Nonstationary Problem . . . . . . . . . . . . . . . . . . . . . . 32\r
2.6 Optimistic Initial Values . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34\r
2.7 Upper-Confidence-Bound Action Selection . . . . . . . . . . . . . . . . . . 35\r
2.8 Gradient Bandit Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . 37\r
2.9 Associative Search (Contextual Bandits) . . . . . . . . . . . . . . . . . . . 41\r
2.10 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42

viii Contents\r
3 Finite Markov Decision Processes 47\r
3.1 The Agent–Environment Interface . . . . . . . . . . . . . . . . . . . . . . 47\r
3.2 Goals and Rewards . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53\r
3.3 Returns and Episodes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54\r
3.4 Unified Notation for Episodic and Continuing Tasks . . . . . . . . . . . . 57\r
3.5 Policies and Value Functions . . . . . . . . . . . . . . . . . . . . . . . . . 58\r
3.6 Optimal Policies and Optimal Value Functions . . . . . . . . . . . . . . . 62\r
3.7 Optimality and Approximation . . . . . . . . . . . . . . . . . . . . . . . . 67\r
3.8 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68\r
4 Dynamic Programming 73\r
4.1 Policy Evaluation (Prediction) . . . . . . . . . . . . . . . . . . . . . . . . 74\r
4.2 Policy Improvement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76\r
4.3 Policy Iteration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80\r
4.4 Value Iteration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82\r
4.5 Asynchronous Dynamic Programming . . . . . . . . . . . . . . . . . . . . 85\r
4.6 Generalized Policy Iteration . . . . . . . . . . . . . . . . . . . . . . . . . . 86\r
4.7 Eciency of Dynamic Programming . . . . . . . . . . . . . . . . . . . . . 87\r
4.8 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88\r
5 Monte Carlo Methods 91\r
5.1 Monte Carlo Prediction . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92\r
5.2 Monte Carlo Estimation of Action Values . . . . . . . . . . . . . . . . . . 96\r
5.3 Monte Carlo Control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97\r
5.4 Monte Carlo Control without Exploring Starts . . . . . . . . . . . . . . . 100\r
5.5 O↵-policy Prediction via Importance Sampling . . . . . . . . . . . . . . . 103\r
5.6 Incremental Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . 109\r
5.7 O↵-policy Monte Carlo Control . . . . . . . . . . . . . . . . . . . . . . . . 110\r
5.8 *Discounting-aware Importance Sampling . . . . . . . . . . . . . . . . . . 112\r
5.9 *Per-decision Importance Sampling . . . . . . . . . . . . . . . . . . . . . . 114\r
5.10 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115\r
6 Temporal-Di↵erence Learning 119\r
6.1 TD Prediction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 119\r
6.2 Advantages of TD Prediction Methods . . . . . . . . . . . . . . . . . . . . 124\r
6.3 Optimality of TD(0) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 126\r
6.4 Sarsa: On-policy TD Control . . . . . . . . . . . . . . . . . . . . . . . . . 129\r
6.5 Q-learning: O↵-policy TD Control . . . . . . . . . . . . . . . . . . . . . . 131\r
6.6 Expected Sarsa . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133\r
6.7 Maximization Bias and Double Learning . . . . . . . . . . . . . . . . . . . 134\r
6.8 Games, Afterstates, and Other Special Cases . . . . . . . . . . . . . . . . 136\r
6.9 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 138

Contents ix\r
7 n-step Bootstrapping 141\r
7.1 n-step TD Prediction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 142\r
7.2 n-step Sarsa . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 145\r
7.3 n-step O↵-policy Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . 148\r
7.4 *Per-decision Methods with Control Variates . . . . . . . . . . . . . . . . 150\r
7.5 O↵-policy Learning Without Importance Sampling:\r
The n-step Tree Backup Algorithm . . . . . . . . . . . . . . . . . . . . . . 152\r
7.6 *A Unifying Algorithm: n-step Q()..................... 154\r
7.7 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 157\r
8 Planning and Learning with Tabular Methods 159\r
8.1 Models and Planning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 159\r
8.2 Dyna: Integrated Planning, Acting, and Learning . . . . . . . . . . . . . . 161\r
8.3 When the Model Is Wrong . . . . . . . . . . . . . . . . . . . . . . . . . . . 166\r
8.4 Prioritized Sweeping . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 168\r
8.5 Expected vs. Sample Updates . . . . . . . . . . . . . . . . . . . . . . . . . 172\r
8.6 Trajectory Sampling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 174\r
8.7 Real-time Dynamic Programming . . . . . . . . . . . . . . . . . . . . . . . 177\r
8.8 Planning at Decision Time . . . . . . . . . . . . . . . . . . . . . . . . . . . 180\r
8.9 Heuristic Search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 181\r
8.10 Rollout Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 183\r
8.11 Monte Carlo Tree Search . . . . . . . . . . . . . . . . . . . . . . . . . . . . 185\r
8.12 Summary of the Chapter . . . . . . . . . . . . . . . . . . . . . . . . . . . . 188\r
8.13 Summary of Part I: Dimensions . . . . . . . . . . . . . . . . . . . . . . . . 189\r
II Approximate Solution Methods 195\r
9 On-policy Prediction with Approximation 197\r
9.1 Value-function Approximation . . . . . . . . . . . . . . . . . . . . . . . . . 198\r
9.2 The Prediction Objective (VE) . . . . . . . . . . . . . . . . . . . . . . . . 199\r
9.3 Stochastic-gradient and Semi-gradient Methods . . . . . . . . . . . . . . . 200\r
9.4 Linear Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 204\r
9.5 Feature Construction for Linear Methods . . . . . . . . . . . . . . . . . . 210\r
9.5.1 Polynomials . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 210\r
9.5.2 Fourier Basis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 211\r
9.5.3 Coarse Coding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 215\r
9.5.4 Tile Coding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 217\r
9.5.5 Radial Basis Functions . . . . . . . . . . . . . . . . . . . . . . . . . 221\r
9.6 Selecting Step-Size Parameters Manually . . . . . . . . . . . . . . . . . . . 222\r
9.7 Nonlinear Function Approximation: Artificial Neural Networks . . . . . . 223\r
9.8 Least-Squares TD . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 228

x Contents\r
9.9 Memory-based Function Approximation . . . . . . . . . . . . . . . . . . . 230\r
9.10 Kernel-based Function Approximation . . . . . . . . . . . . . . . . . . . . 232\r
9.11 Looking Deeper at On-policy Learning: Interest and Emphasis . . . . . . 234\r
9.12 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 236\r
10 On-policy Control with Approximation 243\r
10.1 Episodic Semi-gradient Control . . . . . . . . . . . . . . . . . . . . . . . . 243\r
10.2 Semi-gradient n-step Sarsa . . . . . . . . . . . . . . . . . . . . . . . . . . 247\r
10.3 Average Reward: A New Problem Setting for Continuing Tasks . . . . . . 249\r
10.4 Deprecating the Discounted Setting . . . . . . . . . . . . . . . . . . . . . . 253\r
10.5 Di↵erential Semi-gradient n-step Sarsa . . . . . . . . . . . . . . . . . . . . 255\r
10.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 256\r
11 *O↵-policy Methods with Approximation 257\r
11.1 Semi-gradient Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 258\r
11.2 Examples of O↵-policy Divergence . . . . . . . . . . . . . . . . . . . . . . 260\r
11.3 The Deadly Triad . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 264\r
11.4 Linear Value-function Geometry . . . . . . . . . . . . . . . . . . . . . . . 266\r
11.5 Gradient Descent in the Bellman Error . . . . . . . . . . . . . . . . . . . . 269\r
11.6 The Bellman Error is Not Learnable . . . . . . . . . . . . . . . . . . . . . 274\r
11.7 Gradient-TD Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 278\r
11.8 Emphatic-TD Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 281\r
11.9 Reducing Variance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 283\r
11.10 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 284\r
12 Eligibility Traces 287\r
12.1 The -return . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 288\r
12.2 TD()...................................... 292\r
12.3 n-step Truncated -return Methods . . . . . . . . . . . . . . . . . . . . . 295\r
12.4 Redoing Updates: Online -return Algorithm . . . . . . . . . . . . . . . . 297\r
12.5 True Online TD()............................... 299\r
12.6 *Dutch Traces in Monte Carlo Learning . . . . . . . . . . . . . . . . . . . 301\r
12.7 Sarsa()..................................... 303\r
12.8 Variable  and  ................................ 307\r
12.9 O↵-policy Traces with Control Variates . . . . . . . . . . . . . . . . . . . 309\r
12.10 Watkins’s Q() to Tree-Backup()...................... 312\r
12.11 Stable O↵-policy Methods with Traces . . . . . . . . . . . . . . . . . . . 314\r
12.12 Implementation Issues . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 316\r
12.13 Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 317

Contents xi\r
13 Policy Gradient Methods 321\r
13.1 Policy Approximation and its Advantages . . . . . . . . . . . . . . . . . . 322\r
13.2 The Policy Gradient Theorem . . . . . . . . . . . . . . . . . . . . . . . . . 324\r
13.3 REINFORCE: Monte Carlo Policy Gradient . . . . . . . . . . . . . . . . . 326\r
13.4 REINFORCE with Baseline . . . . . . . . . . . . . . . . . . . . . . . . . . 329\r
13.5 Actor–Critic Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 331\r
13.6 Policy Gradient for Continuing Problems . . . . . . . . . . . . . . . . . . 333\r
13.7 Policy Parameterization for Continuous Actions . . . . . . . . . . . . . . . 335\r
13.8 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 337\r
III Looking Deeper 339\r
14 Psychology 341\r
14.1 Prediction and Control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 342\r
14.2 Classical Conditioning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 343\r
14.2.1 Blocking and Higher-order Conditioning . . . . . . . . . . . . . . . 345\r
14.2.2 The Rescorla–Wagner Model . . . . . . . . . . . . . . . . . . . . . 346\r
14.2.3 The TD Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 349\r
14.2.4 TD Model Simulations . . . . . . . . . . . . . . . . . . . . . . . . . 350\r
14.3 Instrumental Conditioning . . . . . . . . . . . . . . . . . . . . . . . . . . . 357\r
14.4 Delayed Reinforcement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 361\r
14.5 Cognitive Maps . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 363\r
14.6 Habitual and Goal-directed Behavior . . . . . . . . . . . . . . . . . . . . . 364\r
14.7 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 368\r
15 Neuroscience 377\r
15.1 Neuroscience Basics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 378\r
15.2 Reward Signals, Reinforcement Signals, Values, and Prediction Errors . . 380\r
15.3 The Reward Prediction Error Hypothesis . . . . . . . . . . . . . . . . . . 381\r
15.4 Dopamine . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 383\r
15.5 Experimental Support for the Reward Prediction Error Hypothesis . . . . 387\r
15.6 TD Error/Dopamine Correspondence . . . . . . . . . . . . . . . . . . . . . 390\r
15.7 Neural Actor–Critic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 395\r
15.8 Actor and Critic Learning Rules . . . . . . . . . . . . . . . . . . . . . . . 398\r
15.9 Hedonistic Neurons . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 402\r
15.10 Collective Reinforcement Learning . . . . . . . . . . . . . . . . . . . . . . 404\r
15.11 Model-based Methods in the Brain . . . . . . . . . . . . . . . . . . . . . . 407\r
15.12 Addiction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 409\r
15.13 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 410

xii Contents\r
16 Applications and Case Studies 421\r
16.1 TD-Gammon . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 421\r
16.2 Samuel’s Checkers Player . . . . . . . . . . . . . . . . . . . . . . . . . . . 426\r
16.3 Watson’s Daily-Double Wagering . . . . . . . . . . . . . . . . . . . . . . . 429\r
16.4 Optimizing Memory Control . . . . . . . . . . . . . . . . . . . . . . . . . . 432\r
16.5 Human-level Video Game Play . . . . . . . . . . . . . . . . . . . . . . . . 436\r
16.6 Mastering the Game of Go . . . . . . . . . . . . . . . . . . . . . . . . . . . 441\r
16.6.1 AlphaGo . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 444\r
16.6.2 AlphaGo Zero . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 447\r
16.7 Personalized Web Services . . . . . . . . . . . . . . . . . . . . . . . . . . . 450\r
16.8 Thermal Soaring . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 453\r
17 Frontiers 459\r
17.1 General Value Functions and Auxiliary Tasks . . . . . . . . . . . . . . . . 459\r
17.2 Temporal Abstraction via Options . . . . . . . . . . . . . . . . . . . . . . 461\r
17.3 Observations and State . . . . . . . . . . . . . . . . . . . . . . . . . . . . 464\r
17.4 Designing Reward Signals . . . . . . . . . . . . . . . . . . . . . . . . . . . 469\r
17.5 Remaining Issues . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 472\r
17.6 Reinforcement Learning and the Future of Artificial Intelligence . . . . . . 475\r
References 481\r
Index 519

Preface to the Second Edition\r
The twenty years since the publication of the first edition of this book have seen tremendous\r
progress in artificial intelligence, propelled in large part by advances in machine learning,\r
including advances in reinforcement learning. Although the impressive computational\r
power that became available is responsible for some of these advances, new developments\r
in theory and algorithms have been driving forces as well. In the face of this progress, a\r
second edition of our 1998 book was long overdue, and we finally began the project in\r
2012. Our goal for the second edition was the same as our goal for the first: to provide a\r
clear and simple account of the key ideas and algorithms of reinforcement learning that\r
is accessible to readers in all the related disciplines. The edition remains an introduction,\r
and we retain a focus on core, online learning algorithms. This edition includes some new\r
topics that rose to importance over the intervening years, and we expanded coverage of\r
topics that we now understand better. But we made no attempt to provide comprehensive\r
coverage of the field, which has exploded in many di↵erent directions. We apologize for\r
having to leave out all but a handful of these contributions.\r
As in the first edition, we chose not to produce a rigorous formal treatment of\r
reinforcement learning, or to formulate it in the most general terms. However, our deeper\r
understanding of some topics since the first edition required a bit more mathematics\r
to explain; we have set o↵ the more mathematical parts in shaded boxes that the non\u0002mathematically-inclined may choose to skip. We also use a slightly di↵erent notation\r
than was used in the first edition. In teaching, we have found that the new notation\r
helps to address some common points of confusion. It emphasizes the di↵erence between\r
random variables, denoted with capital letters, and their instantiations, denoted in lower\r
case. For example, the state, action, and reward at time step t are denoted St, At,\r
and Rt, while their possible values might be denoted s, a, and r. Along with this, it is\r
natural to use lower case for value functions (e.g., v⇡) and restrict capitals to their tabular\r
estimates (e.g., Qt(s, a)). Approximate value functions are deterministic functions of\r
random parameters and are thus also in lower case (e.g., vˆ(s,wt) ⇡ v⇡(s)). Vectors, such\r
as the weight vector wt (formerly ✓t) and the feature vector xt (formerly t), are bold\r
and written in lowercase even if they are random variables. Uppercase bold is reserved for\r
matrices. In the first edition we used special notations, Pa\r
ss0 and Ra\r
ss0 , for the transition\r
probabilities and expected rewards. One weakness of that notation is that it still did not\r
fully characterize the dynamics of the rewards, giving only their expectations, which is\r
sucient for dynamic programming but not for reinforcement learning. Another weakness

xiv Preface to the Second Edition\r
is the excess of subscripts and superscripts. In this edition we use the explicit notation of\r
p(s0, r|s, a) for the joint probability for the next state and reward given the current state\r
and action. All the changes in notation are summarized in a table on page xix.\r
The second edition is significantly expanded, and its top-level organization has been\r
changed. After the introductory first chapter, the second edition is divided into three new\r
parts. The first part (Chapters 2–8) treats as much of reinforcement learning as possible\r
without going beyond the tabular case for which exact solutions can be found. We cover\r
both learning and planning methods for the tabular case, as well as their unification\r
in n-step methods and in Dyna. Many algorithms presented in this part are new to\r
the second edition, including UCB, Expected Sarsa, Double learning, tree-backup, Q(),\r
RTDP, and MCTS. Doing the tabular case first, and thoroughly, enables core ideas to be\r
developed in the simplest possible setting. The second part of the book (Chapters 9–13)\r
is then devoted to extending the ideas to function approximation. It has new sections on\r
artificial neural networks, the fourier basis, LSTD, kernel-based methods, Gradient-TD\r
and Emphatic-TD methods, average-reward methods, true online TD(), and policy\u0002gradient methods. The second edition significantly expands the treatment of o↵-policy\r
learning, first for the tabular case in Chapters 5–7, then with function approximation in\r
Chapters 11 and 12. Another change is that the second edition separates the forward-view\r
idea of n-step bootstrapping (now treated more fully in Chapter 7) from the backward\u0002view idea of eligibility traces (now treated independently in Chapter 12). The third part\r
of the book has large new chapters on reinforcement learning’s relationships to psychology\r
(Chapter 14) and neuroscience (Chapter 15), as well as an updated case-studies chapter\r
including Atari game playing, Watson’s wagering strategy, and the Go playing programs\r
AlphaGo and AlphaGo Zero (Chapter 16). Still, out of necessity we have included only a\r
small subset of all that has been done in the field. Our choices reflect our long-standing\r
interests in inexpensive model-free methods that should scale well to large applications.\r
The final chapter now includes a discussion of the future societal impacts of reinforcement\r
learning. For better or worse, the second edition is about twice as large as the first.\r
This book is designed to be used as the primary text for a one- or two-semester\r
course on reinforcement learning. For a one-semester course, the first ten chapters should\r
be covered in order and form a good core, to which can be added material from the\r
other chapters, from other books such as Bertsekas and Tsitsiklis (1996), Wiering and\r
van Otterlo (2012), and Szepesv´ari (2010), or from the literature, according to taste.\r
Depending of the students’ background, some additional material on online supervised\r
learning may be helpful. The ideas of options and option models are a natural addition\r
(Sutton, Precup and Singh, 1999). A two-semester course can cover all the chapters as\r
well as supplementary material. The book can also be used as part of broader courses\r
on machine learning, artificial intelligence, or neural networks. In this case, it may be\r
desirable to cover only a subset of the material. We recommend covering Chapter 1 for a\r
brief overview, Chapter 2 through Section 2.4, Chapter 3, and then selecting sections\r
from the remaining chapters according to time and interests. Chapter 6 is the most\r
important for the subject and for the rest of the book. A course focusing on machine\r
learning or neural networks should cover Chapters 9 and 10, and a course focusing on\r
artificial intelligence or planning should cover Chapter 8. Throughout the book, sections\r
and chapters that are more dicult and not essential to the rest of the book are marked

Preface to the Second Edition xv\r
with a ⇤. These can be omitted on first reading without creating problems later on. Some\r
exercises are also marked with a ⇤ to indicate that they are more advanced and not\r
essential to understanding the basic material of the chapter.\r
Most chapters end with a section entitled “Bibliographical and Historical Remarks,”\r
wherein we credit the sources of the ideas presented in that chapter, provide pointers to\r
further reading and ongoing research, and describe relevant historical background. Despite\r
our attempts to make these sections authoritative and complete, we have undoubtedly left\r
out some important prior work. For that we again apologize, and we welcome corrections\r
and extensions for incorporation into the electronic version of the book.\r
Like the first edition, this edition of the book is dedicated to the memory of A. Harry\r
Klopf. It was Harry who introduced us to each other, and it was his ideas about the brain\r
and artificial intelligence that launched our long excursion into reinforcement learning.\r
Trained in neurophysiology and long interested in machine intelligence, Harry was a\r
senior scientist aliated with the Avionics Directorate of the Air Force Oce of Scientific\r
Research (AFOSR) at Wright-Patterson Air Force Base, Ohio. He was dissatisfied with\r
the great importance attributed to equilibrium-seeking processes, including homeostasis\r
and error-correcting pattern classification methods, in explaining natural intelligence\r
and in providing a basis for machine intelligence. He noted that systems that try to\r
maximize something (whatever that might be) are qualitatively di↵erent from equilibrium\u0002seeking systems, and he argued that maximizing systems hold the key to understanding\r
important aspects of natural intelligence and for building artificial intelligences. Harry was\r
instrumental in obtaining funding from AFOSR for a project to assess the scientific merit\r
of these and related ideas. This project was conducted in the late 1970s at the University\r
of Massachusetts Amherst (UMass Amherst), initially under the direction of Michael\r
Arbib, William Kilmer, and Nico Spinelli, professors in the Department of Computer\r
and Information Science at UMass Amherst, and founding members of the Cybernetics\r
Center for Systems Neuroscience at the University, a farsighted group focusing on the\r
intersection of neuroscience and artificial intelligence. Barto, a recent PhD from the\r
University of Michigan, was hired as post doctoral researcher on the project. Meanwhile,\r
Sutton, an undergraduate studying computer science and psychology at Stanford, had\r
been corresponding with Harry regarding their mutual interest in the role of stimulus\r
timing in classical conditioning. Harry suggested to the UMass group that Sutton would\r
be a great addition to the project. Thus, Sutton became a UMass graduate student,\r
whose PhD was directed by Barto, who had become an Associate Professor. The study\r
of reinforcement learning as presented in this book is rightfully an outcome of that\r
project instigated by Harry and inspired by his ideas. Further, Harry was responsible\r
for bringing us, the authors, together in what has been a long and enjoyable interaction.\r
By dedicating this book to Harry we honor his essential contributions, not only to the\r
field of reinforcement learning, but also to our collaboration. We also thank Professors\r
Arbib, Kilmer, and Spinelli for the opportunity they provided to us to begin exploring\r
these ideas. Finally, we thank AFOSR for generous support over the early years of our\r
research, and the NSF for its generous support over many of the following years.\r
We have very many people to thank for their inspiration and help with this second\r
edition. Everyone we acknowledged for their inspiration and help with the first edition

xvi Preface to the Second Edition\r
deserve our deepest gratitude for this edition as well, which would not exist were it not\r
for their contributions to edition number one. To that long list we must add many others\r
who contributed specifically to the second edition. Our students over the many years that\r
we have taught this material contributed in countless ways: exposing errors, o↵ering fixes,\r
and—not the least—being confused in places where we could have explained things better.\r
We especially thank Martha Steenstrup for reading and providing detailed comments\r
throughout. The chapters on psychology and neuroscience could not have been written\r
without the help of many experts in those fields. We thank John Moore for his patient\r
tutoring over many many years on animal learning experiments, theory, and neuroscience,\r
and for his careful reading of multiple drafts of Chapters 14 and 15. We also thank Matt\r
Botvinick, Nathaniel Daw, Peter Dayan, and Yael Niv for their penetrating comments on\r
drafts of these chapters, their essential guidance through the massive literature, and their\r
interception of many of our errors in early drafts. Of course, the remaining errors in these\r
chapters—and there must still be some—are totally our own. We thank Phil Thomas for\r
helping us make these chapters accessible to non-psychologists and non-neuroscientists,\r
and we thank Peter Sterling for helping us improve the exposition. We are grateful to Jim\r
Houk for introducing us to the subject of information processing in the basal ganglia and\r
for alerting us to other relevant aspects of neuroscience. Jos´e Mart´ınez, Terry Sejnowski,\r
David Silver, Gerry Tesauro, Georgios Theocharous, and Phil Thomas generously helped\r
us understand details of their reinforcement learning applications for inclusion in the\r
case-studies chapter, and they provided helpful comments on drafts of these sections.\r
Special thanks are owed to David Silver for helping us better understand Monte Carlo\r
Tree Search and the DeepMind Go-playing programs. We thank George Konidaris for his\r
help with the section on the Fourier basis. Emilio Cartoni, Thomas Cederborg, Stefan\r
Dernbach, Clemens Rosenbaum, Patrick Taylor, Thomas Colin, and Pierre-Luc Bacon\r
helped us in a number important ways for which we are most grateful.\r
Sutton would also like to thank the members of the Reinforcement Learning and\r
Artificial Intelligence laboratory at the University of Alberta for contributions to the\r
second edition. He owes a particular debt to Rupam Mahmood for essential contributions\r
to the treatment of o↵-policy Monte Carlo methods in Chapter 5, to Hamid Maei for\r
helping develop the perspective on o↵-policy learning presented in Chapter 11, to Eric\r
Graves for conducting the experiments in Chapter 13, to Shangtong Zhang for replicating\r
and thus verifying almost all the experimental results, to Kris De Asis for improving\r
the new technical content of Chapters 7 and 12, and to Harm van Seijen for insights\r
that led to the separation of n-step methods from eligibility traces and (along with Hado\r
van Hasselt) for the ideas involving exact equivalence of forward and backward views of\r
eligibility traces presented in Chapter 12. Sutton also gratefully acknowledges the support\r
and freedom he was granted by the Government of Alberta and the National Science and\r
Engineering Research Council of Canada throughout the period during which the second\r
edition was conceived and written. In particular, he would like to thank Randy Goebel\r
for creating a supportive and far-sighted environment for research in Alberta. He would\r
also like to thank DeepMind their support in the last six months of writing the book.\r
Finally, we owe thanks to the many careful readers of drafts of the second edition that\r
we posted on the internet. They found many errors that we had missed and alerted us to\r
potential points of confusion.

Preface to the First Edition\r
We first came to focus on what is now known as reinforcement learning in late 1979. We\r
were both at the University of Massachusetts, working on one of the earliest projects to\r
revive the idea that networks of neuronlike adaptive elements might prove to be a promising\r
approach to artificial adaptive intelligence. The project explored the “heterostatic theory\r
of adaptive systems” developed by A. Harry Klopf. Harry’s work was a rich source of\r
ideas, and we were permitted to explore them critically and compare them with the long\r
history of prior work in adaptive systems. Our task became one of teasing the ideas apart\r
and understanding their relationships and relative importance. This continues today,\r
but in 1979 we came to realize that perhaps the simplest of the ideas, which had long\r
been taken for granted, had received surprisingly little attention from a computational\r
perspective. This was simply the idea of a learning system that wants something, that\r
adapts its behavior in order to maximize a special signal from its environment. This\r
was the idea of a “hedonistic” learning system, or, as we would say now, the idea of\r
reinforcement learning.\r
Like others, we had a sense that reinforcement learning had been thoroughly explored\r
in the early days of cybernetics and artificial intelligence. On closer inspection, though,\r
we found that it had been explored only slightly. While reinforcement learning had clearly\r
motivated some of the earliest computational studies of learning, most of these researchers\r
had gone on to other things, such as pattern classification, supervised learning, and\r
adaptive control, or they had abandoned the study of learning altogether. As a result, the\r
special issues involved in learning how to get something from the environment received\r
relatively little attention. In retrospect, focusing on this idea was the critical step that\r
set this branch of research in motion. Little progress could be made in the computational\r
study of reinforcement learning until it was recognized that such a fundamental idea had\r
not yet been thoroughly explored.\r
The field has come a long way since then, evolving and maturing in several directions.\r
Reinforcement learning has gradually become one of the most active research areas in ma\u0002chine learning, artificial intelligence, and neural network research. The field has developed\r
strong mathematical foundations and impressive applications. The computational study\r
of reinforcement learning is now a large field, with hundreds of active researchers around\r
the world in diverse disciplines such as psychology, control theory, artificial intelligence,\r
and neuroscience. Particularly important have been the contributions establishing and\r
developing the relationships to the theory of optimal control and dynamic programming.

xviii Preface to the First Edition\r
The overall problem of learning from interaction to achieve goals is still far from being\r
solved, but our understanding of it has improved significantly. We can now place compo\u0002nent ideas, such as temporal-di↵erence learning, dynamic programming, and function\r
approximation, within a coherent perspective with respect to the overall problem.\r
Our goal in writing this book was to provide a clear and simple account of the key\r
ideas and algorithms of reinforcement learning. We wanted our treatment to be accessible\r
to readers in all of the related disciplines, but we could not cover all of these perspectives\r
in detail. For the most part, our treatment takes the point of view of artificial intelligence\r
and engineering. Coverage of connections to other fields we leave to others or to another\r
time. We also chose not to produce a rigorous formal treatment of reinforcement learning.\r
We did not reach for the highest possible level of mathematical abstraction and did not\r
rely on a theorem–proof format. We tried to choose a level of mathematical detail that\r
points the mathematically inclined in the right directions without distracting from the\r
simplicity and potential generality of the underlying ideas.\r
In some sense we have been working toward this book for thirty years, and we have lots\r
of people to thank. First, we thank those who have personally helped us develop the overall\r
view presented in this book: Harry Klopf, for helping us recognize that reinforcement\r
learning needed to be revived; Chris Watkins, Dimitri Bertsekas, John Tsitsiklis, and\r
Paul Werbos, for helping us see the value of the relationships to dynamic programming;\r
John Moore and Jim Kehoe, for insights and inspirations from animal learning theory;\r
Oliver Selfridge, for emphasizing the breadth and importance of adaptation; and, more\r
generally, our colleagues and students who have contributed in countless ways: Ron\r
Williams, Charles Anderson, Satinder Singh, Sridhar Mahadevan, Steve Bradtke, Bob\r
Crites, Peter Dayan, and Leemon Baird. Our view of reinforcement learning has been\r
significantly enriched by discussions with Paul Cohen, Paul Utgo↵, Martha Steenstrup,\r
Gerry Tesauro, Mike Jordan, Leslie Kaelbling, Andrew Moore, Chris Atkeson, Tom\r
Mitchell, Nils Nilsson, Stuart Russell, Tom Dietterich, Tom Dean, and Bob Narendra.\r
We thank Michael Littman, Gerry Tesauro, Bob Crites, Satinder Singh, and Wei Zhang\r
for providing specifics of Sections 4.7, 15.1, 15.4, 15.5, and 15.6 respectively. We thank\r
the Air Force Oce of Scientific Research, the National Science Foundation, and GTE\r
Laboratories for their long and farsighted support.\r
We also wish to thank the many people who have read drafts of this book and\r
provided valuable comments, including Tom Kalt, John Tsitsiklis, Pawel Cichosz, Olle\r
G¨allmo, Chuck Anderson, Stuart Russell, Ben Van Roy, Paul Steenstrup, Paul Cohen,\r
Sridhar Mahadevan, Jette Randlov, Brian Sheppard, Thomas O’Connell, Richard Coggins,\r
Cristina Versino, John H. Hiett, Andreas Badelt, Jay Ponte, Joe Beck, Justus Piater,\r
Martha Steenstrup, Satinder Singh, Tommi Jaakkola, Dimitri Bertsekas, Torbj¨orn Ekman,\r
Christina Bj¨orkman, Jakob Carlstr¨om, and Olle Palmgren. Finally, we thank Gwyn\r
Mitchell for helping in many ways, and Harry Stanton and Bob Prior for being our\r
champions at MIT Press.

Summary of Notation\r
Capital letters are used for random variables, whereas lower case letters are used for\r
the values of random variables and for scalar functions. Quantities that are required to\r
be real-valued vectors are written in bold and in lower case (even if random variables).\r
Matrices are bold capitals.\r
.\r
= equality relationship that is true by definition\r
⇡ approximately equal\r
/ proportional to\r
Pr{X =x} probability that a random variable X takes on the value x\r
X ⇠ p random variable X selected from distribution p(x) .= Pr{X =x}\r
E[X] expectation of a random variable X, i.e., E[X] .= P\r
x p(x)x\r
argmaxa f(a) a value of a at which f(a) takes its maximal value\r
ln x natural logarithm of x\r
ex, exp(x) the base of the natural logarithm, e ⇡ 2.71828, carried to power x; eln x = x\r
R set of real numbers\r
f : X ! Y function f from elements of set X to elements of set Y\r
 assignment\r
(a, b] the real interval between a and b including b but not including a\r
" probability of taking a random action in an "-greedy policy\r
↵,  step-size parameters\r
 discount-rate parameter\r
 decay-rate parameter for eligibility traces\r
predicate indicator function ( predicate\r
.\r
= 1 if the predicate is true, else 0)\r
In a multi-arm bandit problem:\r
k number of actions (arms)\r
t discrete time step or play number\r
q⇤(a) true value (expected reward) of action a\r
Qt(a) estimate at time t of q⇤(a)\r
Nt(a) number of times action a has been selected up prior to time t\r
Ht(a) learned preference for selecting action a at time t\r
⇡t(a) probability of selecting action a at time t\r
R¯t estimate at time t of the expected reward given ⇡t

xx Summary of Notation\r
In a Markov Decision Process:\r
s, s0 states\r
a an action\r
r a reward\r
S set of all nonterminal states\r
S+ set of all states, including the terminal state\r
A(s) set of all actions available in state s\r
R set of all possible rewards, a finite subset of R\r
⇢ subset of (e.g., R ⇢ R)\r
2 is an element of; e.g. (s 2 S, r 2 R)\r
|S| number of elements in set S\r
t discrete time step\r
T,T(t) final time step of an episode, or of the episode including time step t\r
At action at time t\r
St state at time t, typically due, stochastically, to St1 and At1\r
Rt reward at time t, typically due, stochastically, to St1 and At1\r
⇡ policy (decision-making rule)\r
⇡(s) action taken in state s under deterministic policy ⇡\r
⇡(a|s) probability of taking action a in state s under stochastic policy ⇡\r
Gt return following time t\r
h horizon, the time step one looks up to in a forward view\r
Gt:t+n, Gt:h n-step return from t + 1 to t + n, or to h (discounted and corrected)\r
G¯t:h flat return (undiscounted and uncorrected) from t + 1 to h (Section 5.8)\r
G\r
t -return (Section 12.1)\r
G\r
t:h truncated, corrected -return (Section 12.3)\r
Gs\r
t , Gat -return, corrected by estimated state, or action, values (Section 12.8)\r
p(s0, r|s, a) probability of transition to state s0 with reward r, from state s and action a\r
p(s0 |s, a) probability of transition to state s0, from state s taking action a\r
r(s, a) expected immediate reward from state s after action a\r
r(s, a, s0) expected immediate reward on transition from s to s0 under action a\r
v⇡(s) value of state s under policy ⇡ (expected return)\r
v⇤(s) value of state s under the optimal policy\r
q⇡(s, a) value of taking action a in state s under policy ⇡\r
q⇤(s, a) value of taking action a in state s under the optimal policy\r
V,Vt array estimates of state-value function v⇡ or v⇤\r
Q, Qt array estimates of action-value function q⇡ or q⇤\r
V¯t(s) expected approximate action value; for example, V¯t(s) .= P\r
a ⇡(a|s)Qt(s, a)\r
Ut target for estimate at time t

Summary of Notation xxi\r
t temporal-di↵erence (TD) error at t (a random variable) (Section 6.1)\r
s\r
t , at state- and action-specific forms of the TD error (Section 12.9)\r
n in n-step methods, n is the number of steps of bootstrapping\r
d dimensionality—the number of components of w\r
d0 alternate dimensionality—the number of components of ✓\r
w, wt d-vector of weights underlying an approximate value function\r
wi, wt,i ith component of learnable weight vector\r
vˆ(s,w) approximate value of state s given weight vector w\r
vw(s) alternate notation for ˆv(s,w)\r
qˆ(s, a, w) approximate value of state–action pair s, a given weight vector w\r
rvˆ(s,w) column vector of partial derivatives of ˆv(s,w) with respect to w\r
rqˆ(s, a, w) column vector of partial derivatives of ˆq(s, a, w) with respect to w\r
x(s) vector of features visible when in state s\r
x(s, a) vector of features visible when in state s taking action a\r
xi(s), xi(s, a) ith component of vector x(s) or x(s, a)\r
xt shorthand for x(St) or x(St, At)\r
w>x inner product of vectors, w>x .= P\r
i wixi; for example, ˆv(s,w) .\r
= w>x(s)\r
v, vt secondary d-vector of weights, used to learn w (Chapter 11)\r
zt d-vector of eligibility traces at time t (Chapter 12)\r
✓, ✓t parameter vector of target policy (Chapter 13)\r
⇡(a|s, ✓) probability of taking action a in state s given parameter vector ✓\r
⇡✓ policy corresponding to parameter ✓\r
r⇡(a|s, ✓) column vector of partial derivatives of ⇡(a|s, ✓) with respect to ✓\r
J(✓) performance measure for the policy ⇡✓\r
rJ(✓) column vector of partial derivatives of J(✓) with respect to ✓\r
h(s, a, ✓) preference for selecting action a in state s based on ✓\r
b(a|s) behavior policy used to select actions while learning about target policy ⇡\r
b(s) a baseline function b : S 7! R for policy-gradient methods\r
b branching factor for an MDP or search tree\r
⇢t:h importance sampling ratio for time t through time h (Section 5.5)\r
⇢t importance sampling ratio for time t alone, ⇢t\r
.\r
= ⇢t:t\r
r(⇡) average reward (reward rate) for policy ⇡ (Section 10.3)\r
R¯t estimate of r(⇡) at time t\r
µ(s) on-policy distribution over states (Section 9.2)\r
µ |S|-vector of the µ(s) for all s 2 S\r
kvk\r
2\r
µ µ-weighted squared norm of value function v, i.e., kvk\r
2\r
µ\r
.\r
= P\r
s2S µ(s)v(s)2\r
⌘(s) expected number of visits to state s per episode (page 199)\r
⇧ projection operator for value functions (page 268)\r
B⇡ Bellman operator for value functions (Section 11.4)

xxii Summary of Notation\r
A d ⇥ d matrix A .= E\r
h\r
xt\r
\r
xt  xt+1>i\r
b d-dimensional vector b .= E[Rt+1xt]\r
wTD TD fixed point wTD\r
.\r
= A1b (a d-vector, Section 9.4)\r
I identity matrix\r
P |S| ⇥ |S| matrix of state-transition probabilities under ⇡\r
D |S| ⇥ |S| diagonal matrix with µ on its diagonal\r
X |S| ⇥ d matrix with the x(s) as its rows\r
¯w(s) Bellman error (expected TD error) for vw at state s (Section 11.4) ¯w, BE Bellman error vector, with components ¯w(s)\r
VE(w) mean square value error VE(w) .= kvw  v⇡k\r
2\r
µ (Section 9.2)\r
BE(w) mean square Bellman error BE(w) .= \r
¯w\r
\r
\r
2\r
µ\r
PBE(w) mean square projected Bellman error PBE(w) .= \r
⇧¯w\r
\r
\r
2\r
µ\r
TDE(w) mean square temporal-di↵erence error TDE(w) .= Eb\r
⇥\r
⇢t2\r
t\r
⇤\r
(Section 11.5)\r
RE(w) mean square return error (Section 11.6)

Chapter 1\r
Introduction\r
The idea that we learn by interacting with our environment is probably the first to occur\r
to us when we think about the nature of learning. When an infant plays, waves its arms,\r
or looks about, it has no explicit teacher, but it does have a direct sensorimotor connection\r
to its environment. Exercising this connection produces a wealth of information about\r
cause and e↵ect, about the consequences of actions, and about what to do in order to\r
achieve goals. Throughout our lives, such interactions are undoubtedly a major source\r
of knowledge about our environment and ourselves. Whether we are learning to drive\r
a car or to hold a conversation, we are acutely aware of how our environment responds\r
to what we do, and we seek to influence what happens through our behavior. Learning\r
from interaction is a foundational idea underlying nearly all theories of learning and\r
intelligence.\r
In this book we explore a computational approach to learning from interaction. Rather\r
than directly theorizing about how people or animals learn, we primarily explore idealized\r
learning situations and evaluate the e↵ectiveness of various learning methods. That\r
is, we adopt the perspective of an artificial intelligence researcher or engineer. We\r
explore designs for machines that are e↵ective in solving learning problems of scientific or\r
economic interest, evaluating the designs through mathematical analysis or computational\r
experiments. The approach we explore, called reinforcement learning, is much more\r
focused on goal-directed learning from interaction than are other approaches to machine\r
learning.\r
1.1 Reinforcement Learning\r
Reinforcement learning is learning what to do—how to map situations to actions—so\r
as to maximize a numerical reward signal. The learner is not told which actions to\r
take, but instead must discover which actions yield the most reward by trying them. In\r
the most interesting and challenging cases, actions may a↵ect not only the immediate\r
reward but also the next situation and, through that, all subsequent rewards. These two\r
characteristics—trial-and-error search and delayed reward—are the two most important\r
distinguishing features of reinforcement learning.

2 Chapter 1: Introduction\r
Reinforcement learning, like many topics whose names end with “ing,” such as machine\r
learning and mountaineering, is simultaneously a problem, a class of solution methods\r
that work well on the problem, and the field that studies this problem and its solution\r
methods. It is convenient to use a single name for all three things, but at the same time\r
essential to keep the three conceptually separate. In particular, the distinction between\r
problems and solution methods is very important in reinforcement learning; failing to\r
make this distinction is the source of many confusions.\r
We formalize the problem of reinforcement learning using ideas from dynamical sys\u0002tems theory, specifically, as the optimal control of incompletely-known Markov decision\r
processes. The details of this formalization must wait until Chapter 3, but the basic idea\r
is simply to capture the most important aspects of the real problem facing a learning\r
agent interacting over time with its environment to achieve a goal. A learning agent\r
must be able to sense the state of its environment to some extent and must be able to\r
take actions that a↵ect the state. The agent also must have a goal or goals relating to\r
the state of the environment. Markov decision processes are intended to include just\r
these three aspects—sensation, action, and goal—in their simplest possible forms without\r
trivializing any of them. Any method that is well suited to solving such problems we\r
consider to be a reinforcement learning method.\r
Reinforcement learning is di↵erent from supervised learning, the kind of learning studied\r
in most current research in the field of machine learning. Supervised learning is learning\r
from a training set of labeled examples provided by a knowledgable external supervisor.\r
Each example is a description of a situation together with a specification—the label—of\r
the correct action the system should take in that situation, which is often to identify a\r
category to which the situation belongs. The object of this kind of learning is for the\r
system to extrapolate, or generalize, its responses so that it acts correctly in situations\r
not present in the training set. This is an important kind of learning, but alone it is not\r
adequate for learning from interaction. In interactive problems it is often impractical to\r
obtain examples of desired behavior that are both correct and representative of all the\r
situations in which the agent has to act. In uncharted territory—where one would expect\r
learning to be most beneficial—an agent must be able to learn from its own experience.\r
Reinforcement learning is also di↵erent from what machine learning researchers call\r
unsupervised learning, which is typically about finding structure hidden in collections of\r
unlabeled data. The terms supervised learning and unsupervised learning would seem\r
to exhaustively classify machine learning paradigms, but they do not. Although one\r
might be tempted to think of reinforcement learning as a kind of unsupervised learning\r
because it does not rely on examples of correct behavior, reinforcement learning is trying\r
to maximize a reward signal instead of trying to find hidden structure. Uncovering\r
structure in an agent’s experience can certainly be useful in reinforcement learning, but by\r
itself does not address the reinforcement learning problem of maximizing a reward signal.\r
We therefore consider reinforcement learning to be a third machine learning paradigm,\r
alongside supervised learning and unsupervised learning and perhaps other paradigms.

1.1. Reinforcement Learning 3\r
One of the challenges that arise in reinforcement learning, and not in other kinds\r
of learning, is the trade-o↵ between exploration and exploitation. To obtain a lot of\r
reward, a reinforcement learning agent must prefer actions that it has tried in the past\r
and found to be e↵ective in producing reward. But to discover such actions, it has to\r
try actions that it has not selected before. The agent has to exploit what it has already\r
experienced in order to obtain reward, but it also has to explore in order to make better\r
action selections in the future. The dilemma is that neither exploration nor exploitation\r
can be pursued exclusively without failing at the task. The agent must try a variety of\r
actions and progressively favor those that appear to be best. On a stochastic task, each\r
action must be tried many times to gain a reliable estimate of its expected reward. The\r
exploration–exploitation dilemma has been intensively studied by mathematicians for\r
many decades, yet remains unresolved. For now, we simply note that the entire issue of\r
balancing exploration and exploitation does not even arise in supervised and unsupervised\r
learning, at least in the purest forms of these paradigms.\r
Another key feature of reinforcement learning is that it explicitly considers the whole\r
problem of a goal-directed agent interacting with an uncertain environment. This is in\r
contrast to many approaches that consider subproblems without addressing how they\r
might fit into a larger picture. For example, we have mentioned that many machine\r
learning researchers have studied supervised learning without specifying how such an\r
ability would ultimately be useful. Other researchers have developed theories of planning\r
with general goals, but without considering planning’s role in real-time decision making,\r
or the question of where the predictive models necessary for planning would come from.\r
Although these approaches have yielded many useful results, their focus on isolated\r
subproblems is a significant limitation.\r
Reinforcement learning takes the opposite tack, starting with a complete, interactive,\r
goal-seeking agent. All reinforcement learning agents have explicit goals, can sense\r
aspects of their environments, and can choose actions to influence their environments.\r
Moreover, it is usually assumed from the beginning that the agent has to operate despite\r
significant uncertainty about the environment it faces. When reinforcement learning\r
involves planning, it has to address the interplay between planning and real-time action\r
selection, as well as the question of how environment models are acquired and improved.\r
When reinforcement learning involves supervised learning, it does so for specific reasons\r
that determine which capabilities are critical and which are not. For learning research to\r
make progress, important subproblems have to be isolated and studied, but they should\r
be subproblems that play clear roles in complete, interactive, goal-seeking agents, even if\r
all the details of the complete agent cannot yet be filled in.\r
By a complete, interactive, goal-seeking agent we do not always mean something like\r
a complete organism or robot. These are clearly examples, but a complete, interactive,\r
goal-seeking agent can also be a component of a larger behaving system. In this case, the\r
agent directly interacts with the rest of the larger system and indirectly interacts with\r
the larger system’s environment. A simple example is an agent that monitors the charge\r
level of robot’s battery and sends commands to the robot’s control architecture. This\r
agent’s environment is the rest of the robot together with the robot’s environment. It is

4 Chapter 1: Introduction\r
important to look beyond the most obvious examples of agents and their environments\r
to appreciate the generality of the reinforcement learning framework.\r
One of the most exciting aspects of modern reinforcement learning is its substantive\r
and fruitful interactions with other engineering and scientific disciplines. Reinforcement\r
learning is part of a decades-long trend within artificial intelligence and machine learning\r
toward greater integration with statistics, optimization, and other mathematical subjects.\r
For example, the ability of some reinforcement learning methods to learn with parameter\u0002ized approximators addresses the classical “curse of dimensionality” in operations research\r
and control theory. More distinctively, reinforcement learning has also interacted strongly\r
with psychology and neuroscience, with substantial benefits going both ways. Of all the\r
forms of machine learning, reinforcement learning is the closest to the kind of learning\r
that humans and other animals do, and many of the core algorithms of reinforcement\r
learning were originally inspired by biological learning systems. Reinforcement learning\r
has also given back, both through a psychological model of animal learning that better\r
matches some of the empirical data, and through an influential model of parts of the\r
brain’s reward system. The body of this book develops the ideas of reinforcement learning\r
that pertain to engineering and artificial intelligence, with connections to psychology and\r
neuroscience summarized in Chapters 14 and 15.\r
Finally, reinforcement learning is also part of a larger trend in artificial intelligence\r
back toward simple general principles. Since the late 1960s, many artificial intelligence re\u0002searchers presumed that there are no general principles to be discovered, that intelligence is\r
instead due to the possession of a vast number of special purpose tricks, procedures, and\r
heuristics. It was sometimes said that if we could just get enough relevant facts into a\r
machine, say one million, or one billion, then it would become intelligent. Methods based\r
on general principles, such as search or learning, were characterized as “weak methods,”\r
whereas those based on specific knowledge were called “strong methods.” This view is\r
uncommon today. From our point of view, it was premature: too little e↵ort had been\r
put into the search for general principles to conclude that there were none. Modern\r
artificial intelligence now includes much research looking for general principles of learning,\r
search, and decision making. It is not clear how far back the pendulum will swing, but\r
reinforcement learning research is certainly part of the swing back toward simpler and\r
fewer general principles of artificial intelligence.\r
1.2 Examples\r
A good way to understand reinforcement learning is to consider some of the examples\r
and possible applications that have guided its development.\r
• A master chess player makes a move. The choice is informed both by planning—\r
anticipating possible replies and counterreplies—and by immediate, intuitive judg\u0002ments of the desirability of particular positions and moves.\r
• An adaptive controller adjusts parameters of a petroleum refinery’s operation in\r
real time. The controller optimizes the yield/cost/quality trade-o↵ on the basis

1.2. Examples 5\r
of specified marginal costs without sticking strictly to the set points originally\r
suggested by engineers.\r
• A gazelle calf struggles to its feet minutes after being born. Half an hour later it is\r
running at 20 miles per hour.\r
• A mobile robot decides whether it should enter a new room in search of more trash\r
to collect or start trying to find its way back to its battery recharging station. It\r
makes its decision based on the current charge level of its battery and how quickly\r
and easily it has been able to find the recharger in the past.\r
• Phil prepares his breakfast. Closely examined, even this apparently mundane\r
activity reveals a complex web of conditional behavior and interlocking goal–subgoal\r
relationships: walking to the cupboard, opening it, selecting a cereal box, then\r
reaching for, grasping, and retrieving the box. Other complex, tuned, interactive\r
sequences of behavior are required to obtain a bowl, spoon, and milk carton. Each\r
step involves a series of eye movements to obtain information and to guide reaching\r
and locomotion. Rapid judgments are continually made about how to carry the\r
objects or whether it is better to ferry some of them to the dining table before\r
obtaining others. Each step is guided by goals, such as grasping a spoon or getting\r
to the refrigerator, and is in service of other goals, such as having the spoon to eat\r
with once the cereal is prepared and ultimately obtaining nourishment. Whether\r
he is aware of it or not, Phil is accessing information about the state of his body\r
that determines his nutritional needs, level of hunger, and food preferences.\r
These examples share features that are so basic that they are easy to overlook. All\r
involve interaction between an active decision-making agent and its environment, within\r
which the agent seeks to achieve a goal despite uncertainty about its environment. The\r
agent’s actions are permitted to a↵ect the future state of the environment (e.g., the\r
next chess position, the level of reservoirs of the refinery, the robot’s next location and\r
the future charge level of its battery), thereby a↵ecting the actions and opportunities\r
available to the agent at later times. Correct choice requires taking into account indirect,\r
delayed consequences of actions, and thus may require foresight or planning.\r
At the same time, in all of these examples the e↵ects of actions cannot be fully predicted;\r
thus the agent must monitor its environment frequently and react appropriately. For\r
example, Phil must watch the milk he pours into his cereal bowl to keep it from overflowing.\r
All these examples involve goals that are explicit in the sense that the agent can judge\r
progress toward its goal based on what it can sense directly. The chess player knows\r
whether or not he wins, the refinery controller knows how much petroleum is being\r
produced, the gazelle calf knows when it falls, the mobile robot knows when its batteries\r
run down, and Phil knows whether or not he is enjoying his breakfast.\r
In all of these examples the agent can use its experience to improve its performance\r
over time. The chess player refines the intuition he uses to evaluate positions, thereby\r
improving his play; the gazelle calf improves the eciency with which it can run; Phil\r
learns to streamline making his breakfast. The knowledge the agent brings to the task at\r
the start—either from previous experience with related tasks or built into it by design or

6 Chapter 1: Introduction\r
evolution—influences what is useful or easy to learn, but interaction with the environment\r
is essential for adjusting behavior to exploit specific features of the task.\r
1.3 Elements of Reinforcement Learning\r
Beyond the agent and the environment, one can identify four main subelements of a\r
reinforcement learning system: a policy, a reward signal, a value function, and, optionally,\r
a model of the environment.\r
A policy defines the learning agent’s way of behaving at a given time. Roughly speaking,\r
a policy is a mapping from perceived states of the environment to actions to be taken\r
when in those states. It corresponds to what in psychology would be called a set of\r
stimulus–response rules or associations. In some cases the policy may be a simple function\r
or lookup table, whereas in others it may involve extensive computation such as a search\r
process. The policy is the core of a reinforcement learning agent in the sense that it alone\r
is sucient to determine behavior. In general, policies may be stochastic, specifying\r
probabilities for each action.\r
A reward signal defines the goal of a reinforcement learning problem. On each time\r
step, the environment sends to the reinforcement learning agent a single number called\r
the reward. The agent’s sole objective is to maximize the total reward it receives over\r
the long run. The reward signal thus defines what are the good and bad events for the\r
agent. In a biological system, we might think of rewards as analogous to the experiences\r
of pleasure or pain. They are the immediate and defining features of the problem faced\r
by the agent. The reward signal is the primary basis for altering the policy; if an action\r
selected by the policy is followed by low reward, then the policy may be changed to\r
select some other action in that situation in the future. In general, reward signals may\r
be stochastic functions of the state of the environment and the actions taken.\r
Whereas the reward signal indicates what is good in an immediate sense, a value\r
function specifies what is good in the long run. Roughly speaking, the value of a state is\r
the total amount of reward an agent can expect to accumulate over the future, starting\r
from that state. Whereas rewards determine the immediate, intrinsic desirability of\r
environmental states, values indicate the long-term desirability of states after taking into\r
account the states that are likely to follow and the rewards available in those states. For\r
example, a state might always yield a low immediate reward but still have a high value\r
because it is regularly followed by other states that yield high rewards. Or the reverse\r
could be true. To make a human analogy, rewards are somewhat like pleasure (if high)\r
and pain (if low), whereas values correspond to a more refined and farsighted judgment\r
of how pleased or displeased we are that our environment is in a particular state.\r
Rewards are in a sense primary, whereas values, as predictions of rewards, are secondary.\r
Without rewards there could be no values, and the only purpose of estimating values is to\r
achieve more reward. Nevertheless, it is values with which we are most concerned when\r
making and evaluating decisions. Action choices are made based on value judgments. We\r
seek actions that bring about states of highest value, not highest reward, because these\r
actions obtain the greatest amount of reward for us over the long run. Unfortunately, it\r
is much harder to determine values than it is to determine rewards. Rewards are basically\r
given directly by the environment, but values must be estimated and re-estimated from

1.4. Limitations and Scope 7\r
the sequences of observations an agent makes over its entire lifetime. In fact, the most\r
important component of almost all reinforcement learning algorithms we consider is a\r
method for eciently estimating values. The central role of value estimation is arguably\r
the most important thing that has been learned about reinforcement learning over the\r
last six decades.\r
The fourth and final element of some reinforcement learning systems is a model of\r
the environment. This is something that mimics the behavior of the environment, or\r
more generally, that allows inferences to be made about how the environment will behave.\r
For example, given a state and action, the model might predict the resultant next state\r
and next reward. Models are used for planning, by which we mean any way of deciding\r
on a course of action by considering possible future situations before they are actually\r
experienced. Methods for solving reinforcement learning problems that use models and\r
planning are called model-based methods, as opposed to simpler model-free methods that\r
are explicitly trial-and-error learners—viewed as almost the opposite of planning. In\r
Chapter 8 we explore reinforcement learning systems that simultaneously learn by trial\r
and error, learn a model of the environment, and use the model for planning. Modern\r
reinforcement learning spans the spectrum from low-level, trial-and-error learning to\r
high-level, deliberative planning.\r
1.4 Limitations and Scope\r
Reinforcement learning relies heavily on the concept of state—as input to the policy and\r
value function, and as both input to and output from the model. Informally, we can\r
think of the state as a signal conveying to the agent some sense of “how the environment\r
is” at a particular time. The formal definition of state as we use it here is given by\r
the framework of Markov decision processes presented in Chapter 3. More generally,\r
however, we encourage the reader to follow the informal meaning and think of the state\r
as whatever information is available to the agent about its environment. In e↵ect, we\r
assume that the state signal is produced by some preprocessing system that is nominally\r
part of the agent’s environment. We do not address the issues of constructing, changing,\r
or learning the state signal in this book (other than briefly in Section 17.3). We take this\r
approach not because we consider state representation to be unimportant, but in order\r
to focus fully on the decision-making issues. In other words, our concern in this book is\r
not with designing the state signal, but with deciding what action to take as a function\r
of whatever state signal is available.\r
Most of the reinforcement learning methods we consider in this book are structured\r
around estimating value functions, but it is not strictly necessary to do this to solve\r
reinforcement learning problems. For example, solution methods such as genetic algo\u0002rithms, genetic programming, simulated annealing, and other optimization methods never\r
estimate value functions. These methods apply multiple static policies each interacting\r
over an extended period of time with a separate instance of the environment. The policies\r
that obtain the most reward, and random variations of them, are carried over to the\r
next generation of policies, and the process repeats. We call these evolutionary methods\r
because their operation is analogous to the way biological evolution produces organisms

8 Chapter 1: Introduction\r
with skilled behavior even if they do not learn during their individual lifetimes. If the\r
space of policies is suciently small, or can be structured so that good policies are\r
common or easy to find—or if a lot of time is available for the search—then evolutionary\r
methods can be e↵ective. In addition, evolutionary methods have advantages on problems\r
in which the learning agent cannot sense the complete state of its environment.\r
Our focus is on reinforcement learning methods that learn while interacting with the\r
environment, which evolutionary methods do not do. Methods able to take advantage\r
of the details of individual behavioral interactions can be much more ecient than\r
evolutionary methods in many cases. Evolutionary methods ignore much of the useful\r
structure of the reinforcement learning problem: they do not use the fact that the policy\r
they are searching for is a function from states to actions; they do not notice which states\r
an individual passes through during its lifetime, or which actions it selects. In some cases\r
such information can be misleading (e.g., when states are misperceived), but more often it\r
should enable more ecient search. Although evolution and learning share many features\r
and naturally work together, we do not consider evolutionary methods by themselves to\r
be especially well suited to reinforcement learning problems and, accordingly, we do not\r
cover them in this book.\r
1.5 An Extended Example: Tic-Tac-Toe\r
To illustrate the general idea of reinforcement learning and contrast it with other ap\u0002proaches, we next consider a single example in more detail.\r
X\r
X\r
X\r
O O\r
O X\r
Consider the familiar child’s game of tic-tac-toe. Two players\r
take turns playing on a three-by-three board. One player plays\r
Xs and the other Os until one player wins by placing three marks\r
in a row, horizontally, vertically, or diagonally, as the X player\r
has in the game shown to the right. If the board fills up with\r
neither player getting three in a row, then the game is a draw.\r
Because a skilled player can play so as never to lose, let us assume\r
that we are playing against an imperfect player, one whose play\r
is sometimes incorrect and allows us to win. For the moment, in\r
fact, let us consider draws and losses to be equally bad for us. How might we construct a\r
player that will find the imperfections in its opponent’s play and learn to maximize its\r
chances of winning?\r
Although this is a simple problem, it cannot readily be solved in a satisfactory way\r
through classical techniques. For example, the classical “minimax” solution from game\r
theory is not correct here because it assumes a particular way of playing by the opponent.\r
For example, a minimax player would never reach a game state from which it could\r
lose, even if in fact it always won from that state because of incorrect play by the\r
opponent. Classical optimization methods for sequential decision problems, such as\r
dynamic programming, can compute an optimal solution for any opponent, but require\r
as input a complete specification of that opponent, including the probabilities with which\r
the opponent makes each move in each board state. Let us assume that this information\r
is not available a priori for this problem, as it is not for the vast majority of problems of

1.5. An Extended Example: Tic-Tac-Toe 9\r
practical interest. On the other hand, such information can be estimated from experience,\r
in this case by playing many games against the opponent. About the best one can do\r
on this problem is first to learn a model of the opponent’s behavior, up to some level of\r
confidence, and then apply dynamic programming to compute an optimal solution given\r
the approximate opponent model. In the end, this is not that di↵erent from some of the\r
reinforcement learning methods we examine later in this book.\r
An evolutionary method applied to this problem would directly search the space\r
of possible policies for one with a high probability of winning against the opponent.\r
Here, a policy is a rule that tells the player what move to make for every state of the\r
game—every possible configuration of Xs and Os on the three-by-three board. For each\r
policy considered, an estimate of its winning probability would be obtained by playing\r
some number of games against the opponent. This evaluation would then direct which\r
policy or policies were considered next. A typical evolutionary method would hill-climb\r
in policy space, successively generating and evaluating policies in an attempt to obtain\r
incremental improvements. Or, perhaps, a genetic-style algorithm could be used that\r
would maintain and evaluate a population of policies. Literally hundreds of di↵erent\r
optimization methods could be applied.\r
Here is how the tic-tac-toe problem would be approached with a method making use\r
of a value function. First we would set up a table of numbers, one for each possible state\r
of the game. Each number will be the latest estimate of the probability of our winning\r
from that state. We treat this estimate as the state’s value, and the whole table is the\r
learned value function. State A has higher value than state B, or is considered “better”\r
than state B, if the current estimate of the probability of our winning from A is higher\r
than it is from B. Assuming we always play Xs, then for all states with three Xs in a row\r
the probability of winning is 1, because we have already won. Similarly, for all states\r
with three Os in a row, or that are filled up, the correct probability is 0, as we cannot\r
win from them. We set the initial values of all the other states to 0.5, representing a\r
guess that we have a 50% chance of winning.\r
We then play many games against the opponent. To select our moves we examine the\r
states that would result from each of our possible moves (one for each blank space on the\r
board) and look up their current values in the table. Most of the time we move greedily,\r
selecting the move that leads to the state with greatest value, that is, with the highest\r
estimated probability of winning. Occasionally, however, we select randomly from among\r
the other moves instead. These are called exploratory moves because they cause us to\r
experience states that we might otherwise never see. A sequence of moves made and\r
considered during a game can be diagrammed as in Figure 1.1.\r
While we are playing, we change the values of the states in which we find ourselves\r
during the game. We attempt to make them more accurate estimates of the probabilities\r
of winning. To do this, we “back up” the value of the state after each greedy move to\r
the state before the move, as suggested by the arrows in Figure 1.1. More precisely, the\r
current value of the earlier state is updated to be closer to the value of the later state.\r
This can be done by moving the earlier state’s value a fraction of the way toward the\r
value of the later state. If we let St denote the state before the greedy move, and St+1\r
the state after that move, then the update to the estimated value of St, denoted V (St),

10 Chapter 1: Introduction\r
.\r
.\r
•\r
our move {\r
opponent's move { our move {\r
starting position\r
•\r
•\r
•\r
a\r
b\r
c*\r
d\r
e e*\r
opponent's move {\r
c\r
•f\r
•\r
g* g\r
opponent's move { our move { .\r
•\r
a\r
g*\r
c\r
starting position\r
b\r
c*\r
d\r
e* e\r
f\r
g\r
…\r
Figure 1.1: A sequence of tic-tac-toe moves. The solid black lines represent the moves taken\r
during a game; the dashed lines represent moves that we (our reinforcement learning player)\r
considered but did not make. The * indicates the move currently estimated to be the best. Our\r
second move was an exploratory move, meaning that it was taken even though another sibling\r
move, the one leading to e⇤, was ranked higher. Exploratory moves do not result in any learning,\r
but each of our other moves does, causing updates as suggested by the red arrows in which\r
estimated values are moved up the tree from later nodes to earlier nodes as detailed in the text.\r
can be written as\r
V (St) V (St) + ↵\r
h\r
V (St+1)  V (St)\r
i\r
,\r
where ↵ is a small positive fraction called the step-size parameter, which influences\r
the rate of learning. This update rule is an example of a temporal-di↵erence learning\r
method, so called because its changes are based on a di↵erence, V (St+1)V (St), between\r
estimates at two successive times.\r
The method described above performs quite well on this task. For example, if the\r
step-size parameter is reduced properly over time, then this method converges, for any\r
fixed opponent, to the true probabilities of winning from each state given optimal play\r
by our player. Furthermore, the moves then taken (except on exploratory moves) are in\r
fact the optimal moves against this (imperfect) opponent. In other words, the method\r
converges to an optimal policy for playing the game against this opponent. If the step-size\r
parameter is not reduced all the way to zero over time, then this player also plays well\r
against opponents that slowly change their way of playing.

1.5. An Extended Example: Tic-Tac-Toe 11\r
This example illustrates the di↵erences between evolutionary methods and methods\r
that learn value functions. To evaluate a policy, an evolutionary method holds the policy\r
fixed and plays many games against the opponent or simulates many games using a model\r
of the opponent. The frequency of wins gives an unbiased estimate of the probability\r
of winning with that policy, and can be used to direct the next policy selection. But\r
each policy change is made only after many games, and only the final outcome of each\r
game is used: what happens during the games is ignored. For example, if the player wins,\r
then all of its behavior in the game is given credit, independently of how specific moves\r
might have been critical to the win. Credit is even given to moves that never occurred!\r
Value function methods, in contrast, allow individual states to be evaluated. In the end,\r
evolutionary and value function methods both search the space of policies, but learning a\r
value function takes advantage of information available during the course of play.\r
This simple example illustrates some of the key features of reinforcement learning\r
methods. First, there is the emphasis on learning while interacting with an environment,\r
in this case with an opponent player. Second, there is a clear goal, and correct behavior\r
requires planning or foresight that takes into account delayed e↵ects of one’s choices. For\r
example, the simple reinforcement learning player would learn to set up multi-move traps\r
for a shortsighted opponent. It is a striking feature of the reinforcement learning solution\r
that it can achieve the e↵ects of planning and lookahead without using a model of the\r
opponent and without conducting an explicit search over possible sequences of future\r
states and actions.\r
While this example illustrates some of the key features of reinforcement learning, it is\r
so simple that it might give the impression that reinforcement learning is more limited\r
than it really is. Although tic-tac-toe is a two-person game, reinforcement learning\r
also applies in the case in which there is no external adversary, that is, in the case of\r
a “game against nature.” Reinforcement learning also is not restricted to problems in\r
which behavior breaks down into separate episodes, like the separate games of tic-tac-toe,\r
with reward only at the end of each episode. It is just as applicable when behavior\r
continues indefinitely and when rewards of various magnitudes can be received at any\r
time. Reinforcement learning is also applicable to problems that do not even break down\r
into discrete time steps like the plays of tic-tac-toe. The general principles apply to\r
continuous-time problems as well, although the theory gets more complicated and we\r
omit it from this introductory treatment.\r
Tic-tac-toe has a relatively small, finite state set, whereas reinforcement learning can\r
be used when the state set is very large, or even infinite. For example, Gerry Tesauro\r
(1992, 1995) combined the algorithm described above with an artificial neural network to\r
learn to play backgammon, which has approximately 1020 states. With this many states\r
it is impossible ever to experience more than a small fraction of them. Tesauro’s program\r
learned to play far better than any previous program and eventually better than the\r
world’s best human players (see Section 16.1). The artificial neural network provides the\r
program with the ability to generalize from its experience, so that in new states it selects\r
moves based on information saved from similar states faced in the past, as determined\r
by the network. How well a reinforcement learning system can work in problems with\r
such large state sets is intimately tied to how appropriately it can generalize from past

12 Chapter 1: Introduction\r
experience. It is in this role that we have the greatest need for supervised learning\r
methods within reinforcement learning. Artificial neural networks and deep learning\r
(Section 9.7) are not the only, or necessarily the best, way to do this.\r
In this tic-tac-toe example, learning started with no prior knowledge beyond the\r
rules of the game, but reinforcement learning by no means entails a tabula rasa view of\r
learning and intelligence. On the contrary, prior information can be incorporated into\r
reinforcement learning in a variety of ways that can be critical for ecient learning (e.g.,\r
see Sections 9.5, 17.4, and 13.1). We also have access to the true state in the tic-tac-toe\r
example, whereas reinforcement learning can also be applied when part of the state is\r
hidden, or when di↵erent states appear to the learner to be the same.\r
Finally, the tic-tac-toe player was able to look ahead and know the states that would\r
result from each of its possible moves. To do this, it had to have a model of the game\r
that allowed it to foresee how its environment would change in response to moves that it\r
might never make. Many problems are like this, but in others even a short-term model\r
of the e↵ects of actions is lacking. Reinforcement learning can be applied in either case.\r
A model is not required, but models can easily be used if they are available or can be\r
learned (Chapter 8).\r
On the other hand, there are reinforcement learning methods that do not need any\r
kind of environment model at all. Model-free systems cannot even think about how\r
their environments will change in response to a single action. The tic-tac-toe player is\r
model-free in this sense with respect to its opponent: it has no model of its opponent\r
of any kind. Because models have to be reasonably accurate to be useful, model-free\r
methods can have advantages over more complex methods when the real bottleneck in\r
solving a problem is the diculty of constructing a suciently accurate environment\r
model. Model-free methods are also important building blocks for model-based methods.\r
In this book we devote several chapters to model-free methods before we discuss how\r
they can be used as components of more complex model-based methods.\r
Reinforcement learning can be used at both high and low levels in a system. Although\r
the tic-tac-toe player learned only about the basic moves of the game, nothing prevents\r
reinforcement learning from working at higher levels where each of the “actions” may\r
itself be the application of a possibly elaborate problem-solving method. In hierarchical\r
learning systems, reinforcement learning can work simultaneously on several levels.\r
Exercise 1.1: Self-Play Suppose, instead of playing against a random opponent, the\r
reinforcement learning algorithm described above played against itself, with both sides\r
learning. What do you think would happen in this case? Would it learn a di↵erent policy\r
for selecting moves? ⇤\r
Exercise 1.2: Symmetries Many tic-tac-toe positions appear di↵erent but are really\r
the same because of symmetries. How might we amend the learning process described\r
above to take advantage of this? In what ways would this change improve the learning\r
process? Now think again. Suppose the opponent did not take advantage of symmetries.\r
In that case, should we? Is it true, then, that symmetrically equivalent positions should\r
necessarily have the same value? ⇤\r
Exercise 1.3: Greedy Play Suppose the reinforcement learning player was greedy, that is,\r
it always played the move that brought it to the position that it rated the best. Might it

1.7. Early History of Reinforcement Learning 13\r
learn to play better, or worse, than a nongreedy player? What problems might occur? ⇤\r
Exercise 1.4: Learning from Exploration Suppose learning updates occurred after all\r
moves, including exploratory moves. If the step-size parameter is appropriately reduced\r
over time (but not the tendency to explore), then the state values would converge to\r
a di↵erent set of probabilities. What (conceptually) are the two sets of probabilities\r
computed when we do, and when we do not, learn from exploratory moves? Assuming\r
that we do continue to make exploratory moves, which set of probabilities might be better\r
to learn? Which would result in more wins? ⇤\r
Exercise 1.5: Other Improvements Can you think of other ways to improve the reinforce\u0002ment learning player? Can you think of any better way to solve the tic-tac-toe problem\r
as posed? ⇤\r
1.6 Summary\r
Reinforcement learning is a computational approach to understanding and automating\r
goal-directed learning and decision making. It is distinguished from other computational\r
approaches by its emphasis on learning by an agent from direct interaction with its\r
environment, without requiring exemplary supervision or complete models of the envi\u0002ronment. In our opinion, reinforcement learning is the first field to seriously address the\r
computational issues that arise when learning from interaction with an environment in\r
order to achieve long-term goals.\r
Reinforcement learning uses the formal framework of Markov decision processes to\r
define the interaction between a learning agent and its environment in terms of states,\r
actions, and rewards. This framework is intended to be a simple way of representing\r
essential features of the artificial intelligence problem. These features include a sense of\r
cause and e↵ect, a sense of uncertainty and nondeterminism, and the existence of explicit\r
goals.\r
The concepts of value and value function are key to most of the reinforcement learning\r
methods that we consider in this book. We take the position that value functions\r
are important for ecient search in the space of policies. The use of value functions\r
distinguishes reinforcement learning methods from evolutionary methods that search\r
directly in policy space guided by evaluations of entire policies.\r
1.7 Early History of Reinforcement Learning\r
The early history of reinforcement learning has two main threads, both long and rich, that\r
were pursued independently before intertwining in modern reinforcement learning. One\r
thread concerns learning by trial and error, and originated in the psychology of animal\r
learning. This thread runs through some of the earliest work in artificial intelligence\r
and led to the revival of reinforcement learning in the early 1980s. The second thread\r
concerns the problem of optimal control and its solution using value functions and\r
dynamic programming. For the most part, this thread did not involve learning. The\r
two threads were mostly independent, but became interrelated to some extent around a

14 Chapter 1: Introduction\r
third, less distinct thread concerning temporal-di↵erence methods such as that used in\r
the tic-tac-toe example in this chapter. All three threads came together in the late 1980s\r
to produce the modern field of reinforcement learning as we present it in this book.\r
The thread focusing on trial-and-error learning is the one with which we are most\r
familiar and about which we have the most to say in this brief history. Before doing that,\r
however, we briefly discuss the optimal control thread.\r
The term “optimal control” came into use in the late 1950s to describe the problem of\r
designing a controller to minimize or maximize a measure of a dynamical system’s behavior\r
over time. One of the approaches to this problem was developed in the mid-1950s by\r
Richard Bellman and others through extending a nineteenth century theory of Hamilton\r
and Jacobi. This approach uses the concepts of a dynamical system’s state and of a\r
value function, or “optimal return function,” to define a functional equation, now often\r
called the Bellman equation. The class of methods for solving optimal control problems\r
by solving this equation came to be known as dynamic programming (Bellman, 1957a).\r
Bellman (1957b) also introduced the discrete stochastic version of the optimal control\r
problem known as Markov decision processes (MDPs). Ronald Howard (1960) devised\r
the policy iteration method for MDPs. All of these are essential elements underlying the\r
theory and algorithms of modern reinforcement learning.\r
Dynamic programming is widely considered the only feasible way of solving general\r
stochastic optimal control problems. It su↵ers from what Bellman called “the curse of\r
dimensionality,” meaning that its computational requirements grow exponentially with\r
the number of state variables, but it is still far more ecient and more widely applicable\r
than any other general method. Dynamic programming has been extensively developed\r
since the late 1950s, including extensions to partially observable MDPs (surveyed by\r
Lovejoy, 1991), many applications (surveyed by White, 1985, 1988, 1993), approximation\r
methods (surveyed by Rust, 1996), and asynchronous methods (Bertsekas, 1982, 1983).\r
Many excellent modern treatments of dynamic programming are available (e.g., Bertsekas,\r
2005, 2012; Puterman, 1994; Ross, 1983; Whittle, 1982, 1983). Bryson (1996) provides\r
an authoritative history of optimal control.\r
Connections between optimal control and dynamic programming, on the one hand,\r
and learning, on the other, were slow to be recognized. We cannot be sure about what\r
accounted for this separation, but its main cause was likely the separation between\r
the disciplines involved and their di↵erent goals. Also contributing may have been the\r
prevalent view of dynamic programming as an o↵-line computation depending essentially\r
on accurate system models and analytic solutions to the Bellman equation. Further,\r
the simplest form of dynamic programming is a computation that proceeds backwards\r
in time, making it dicult to see how it could be involved in a learning process that\r
must proceed in a forward direction. Some of the earliest work in dynamic programming,\r
such as that by Bellman and Dreyfus (1959), might now be classified as following\r
a learning approach. Witten’s (1977) work (discussed below) certainly qualifies as a\r
combination of learning and dynamic-programming ideas. Werbos (1987) argued explicitly\r
for greater interrelation of dynamic programming and learning methods and for dynamic\r
programming’s relevance to understanding neural and cognitive mechanisms. For us the\r
full integration of dynamic programming methods with online learning did not occur

1.7. Early History of Reinforcement Learning 15\r
until the work of Chris Watkins in 1989, whose treatment of reinforcement learning using\r
the MDP formalism has been widely adopted. Since then these relationships have been\r
extensively developed by many researchers, most particularly by Dimitri Bertsekas and\r
John Tsitsiklis (1996), who coined the term “neurodynamic programming” to refer to\r
the combination of dynamic programming and artificial neural networks. Another term\r
currently in use is “approximate dynamic programming.” These various approaches\r
emphasize di↵erent aspects of the subject, but they all share with reinforcement learning\r
an interest in circumventing the classical shortcomings of dynamic programming.\r
We consider all of the work in optimal control also to be, in a sense, work in reinforce\u0002ment learning. We define a reinforcement learning method as any e↵ective way of solving\r
reinforcement learning problems, and it is now clear that these problems are closely\r
related to optimal control problems, particularly stochastic optimal control problems\r
such as those formulated as MDPs. Accordingly, we must consider the solution methods\r
of optimal control, such as dynamic programming, also to be reinforcement learning\r
methods. Because almost all of the conventional methods require complete knowledge\r
of the system to be controlled, it feels a little unnatural to say that they are part of\r
reinforcement learning. On the other hand, many dynamic programming algorithms are\r
incremental and iterative. Like learning methods, they gradually reach the correct answer\r
through successive approximations. As we show in the rest of this book, these similarities\r
are far more than superficial. The theories and solution methods for the cases of complete\r
and incomplete knowledge are so closely related that we feel they must be considered\r
together as part of the same subject matter.\r
Let us return now to the other major thread leading to the modern field of reinforcement\r
learning, the thread centered on the idea of trial-and-error learning. We only touch on\r
the major points of contact here, taking up this topic in more detail in Section 14.3.\r
According to American psychologist R. S. Woodworth (1938) the idea of trial-and-error\r
learning goes as far back as the 1850s to Alexander Bain’s discussion of learning by\r
“groping and experiment” and more explicitly to the British ethologist and psychologist\r
Conway Lloyd Morgan’s 1894 use of the term to describe his observations of animal\r
behavior. Perhaps the first to succinctly express the essence of trial-and-error learning as\r
a principle of learning was Edward Thorndike:\r
Of several responses made to the same situation, those which are accompanied\r
or closely followed by satisfaction to the animal will, other things being\r
equal, be more firmly connected with the situation, so that, when it recurs,\r
they will be more likely to recur; those which are accompanied or closely\r
followed by discomfort to the animal will, other things being equal, have their\r
connections with that situation weakened, so that, when it recurs, they will\r
be less likely to occur. The greater the satisfaction or discomfort, the greater\r
the strengthening or weakening of the bond. (Thorndike, 1911, p. 244)\r
Thorndike called this the “Law of E↵ect” because it describes the e↵ect of reinforcing\r
events on the tendency to select actions. Thorndike later modified the law to better\r
account for subsequent data on animal learning (such as di↵erences between the e↵ects\r
of reward and punishment), and the law in its various forms has generated considerable\r
controversy among learning theorists (e.g., see Gallistel, 2005; Herrnstein, 1970; Kimble,

16 Chapter 1: Introduction\r
1961, 1967; Mazur, 1994). Despite this, the Law of E↵ect—in one form or another—is\r
widely regarded as a basic principle underlying much behavior (e.g., Hilgard and Bower,\r
1975; Dennett, 1978; Campbell, 1960; Cziko, 1995). It is the basis of the influential\r
learning theories of Clark Hull (1943, 1952) and the influential experimental methods of\r
B. F. Skinner (1938).\r
The term “reinforcement” in the context of animal learning came into use well after\r
Thorndike’s expression of the Law of E↵ect, first appearing in this context (to the best of\r
our knowledge) in the 1927 English translation of Pavlov’s monograph on conditioned\r
reflexes. Pavlov described reinforcement as the strengthening of a pattern of behavior due\r
to an animal receiving a stimulus—a reinforcer—in an appropriate temporal relationship\r
with another stimulus or with a response. Some psychologists extended the idea of\r
reinforcement to include weakening as well as strengthening of behavior, and extended\r
the idea of a reinforcer to include possibly the omission or termination of stimulus. To be\r
considered a reinforcer, the strengthening or weakening must persist after the reinforcer\r
is withdrawn; a stimulus that merely attracts an animal’s attention or that energizes its\r
behavior without producing lasting changes would not be considered a reinforcer.\r
The idea of implementing trial-and-error learning in a computer appeared among the\r
earliest thoughts about the possibility of artificial intelligence. In a 1948 report, Alan\r
Turing described a design for a “pleasure-pain system” that worked along the lines of the\r
Law of E↵ect:\r
When a configuration is reached for which the action is undetermined, a\r
random choice for the missing data is made and the appropriate entry is made\r
in the description, tentatively, and is applied. When a pain stimulus occurs\r
all tentative entries are cancelled, and when a pleasure stimulus occurs they\r
are all made permanent. (Turing, 1948)\r
Many ingenious electro-mechanical machines were constructed that demonstrated trial\u0002and-error learning. The earliest may have been a machine built by Thomas Ross (1933)\r
that was able to find its way through a simple maze and remember the path through\r
the settings of switches. In 1951 W. Grey Walter built a version of his “mechanical\r
tortoise” (Walter, 1950) capable of a simple form of learning. In 1952 Claude Shannon\r
demonstrated a maze-running mouse named Theseus that used trial and error to find\r
its way through a maze, with the maze itself remembering the successful directions\r
via magnets and relays under its floor (see also Shannon, 1951). J. A. Deutsch (1954)\r
described a maze-solving machine based on his behavior theory (Deutsch, 1953) that\r
has some properties in common with model-based reinforcement learning (Chapter 8).\r
In his PhD dissertation, Marvin Minsky (1954) discussed computational models of\r
reinforcement learning and described his construction of an analog machine composed of\r
components he called SNARCs (Stochastic Neural-Analog Reinforcement Calculators)\r
meant to resemble modifiable synaptic connections in the brain (Chapter 15). The\r
web site cyberneticzoo.com contains a wealth of information on these and many other\r
electro-mechanical learning machines.\r
Building electro-mechanical learning machines gave way to programming digital com\u0002puters to perform various types of learning, some of which implemented trial-and-error\r
learning. Farley and Clark (1954) described a digital simulation of a neural-network

1.7. Early History of Reinforcement Learning 17\r
learning machine that learned by trial and error. But their interests soon shifted from\r
trial-and-error learning to generalization and pattern recognition, that is, from reinforce\u0002ment learning to supervised learning (Clark and Farley, 1955). This began a pattern\r
of confusion about the relationship between these types of learning. Many researchers\r
seemed to believe that they were studying reinforcement learning when they were actually\r
studying supervised learning. For example, artificial neural network pioneers such as\r
Rosenblatt (1962) and Widrow and Ho↵ (1960) were clearly motivated by reinforcement\r
learning—they used the language of rewards and punishments—but the systems they\r
studied were supervised learning systems suitable for pattern recognition and perceptual\r
learning. Even today, some researchers and textbooks minimize or blur the distinction\r
between these types of learning. For example, some textbooks have used the term “trial\u0002and-error” to describe artificial neural networks that learn from training examples. This\r
is an understandable confusion because these networks use error information to update\r
connection weights, but this misses the essential character of trial-and-error learning as\r
selecting actions on the basis of evaluative feedback that does not rely on knowledge of\r
what the correct action should be.\r
Partly as a result of these confusions, research into genuine trial-and-error learning\r
became rare in the 1960s and 1970s, although there were notable exceptions. In the 1960s\r
the terms “reinforcement” and “reinforcement learning” were used in the engineering\r
literature for the first time to describe engineering uses of trial-and-error learning (e.g.,\r
Waltz and Fu, 1965; Mendel, 1966; Fu, 1970; Mendel and McClaren, 1970). Particularly\r
influential was Minsky’s paper “Steps Toward Artificial Intelligence” (Minsky, 1961),\r
which discussed several issues relevant to trial-and-error learning, including prediction,\r
expectation, and what he called the basic credit-assignment problem for complex rein\u0002forcement learning systems: How do you distribute credit for success among the many\r
decisions that may have been involved in producing it? All of the methods we discuss in\r
this book are, in a sense, directed toward solving this problem. Minsky’s paper is well\r
worth reading today.\r
In the next few paragraphs we discuss some of the other exceptions and partial\r
exceptions to the relative neglect of computational and theoretical study of genuine\r
trial-and-error learning in the 1960s and 1970s.\r
One exception was the work of the New Zealand researcher John Andreae, who\r
developed a system called STeLLA that learned by trial and error in interaction with\r
its environment. This system included an internal model of the world and, later, an\r
“internal monologue” to deal with problems of hidden state (Andreae, 1963, 1969; Andreae\r
and Cashin, 1969). Andreae’s later work (1977) placed more emphasis on learning\r
from a teacher, but still included learning by trial and error, with the generation of\r
novel events being one of the system’s goals. A feature of this work was a “leakback\r
process,” elaborated more fully in Andreae (1998), that implemented a credit-assignment\r
mechanism similar to the backing-up update operations that we describe. Unfortunately,\r
his pioneering research was not well known and did not greatly impact subsequent\r
reinforcement learning research. Recent summaries are available (Andreae, 2017a,b).\r
More influential was the work of Donald Michie. In 1961 and 1963 he described a\r
simple trial-and-error learning system for learning how to play tic-tac-toe (or naughts

18 Chapter 1: Introduction\r
and crosses) called MENACE (for Matchbox Educable Naughts and Crosses Engine). It\r
consisted of a matchbox for each possible game position, each matchbox containing a\r
number of colored beads, a di↵erent color for each possible move from that position. By\r
drawing a bead at random from the matchbox corresponding to the current game position,\r
one could determine MENACE’s move. When a game was over, beads were added to\r
or removed from the boxes used during play to reward or punish MENACE’s decisions.\r
Michie and Chambers (1968) described another tic-tac-toe reinforcement learner called\r
GLEE (Game Learning Expectimaxing Engine) and a reinforcement learning controller\r
called BOXES. They applied BOXES to the task of learning to balance a pole hinged to\r
a movable cart on the basis of a failure signal occurring only when the pole fell or the\r
cart reached the end of a track. This task was adapted from the earlier work of Widrow\r
and Smith (1964), who used supervised learning methods, assuming instruction from a\r
teacher already able to balance the pole. Michie and Chambers’s version of pole-balancing\r
is one of the best early examples of a reinforcement learning task under conditions of\r
incomplete knowledge. It influenced much later work in reinforcement learning, beginning\r
with some of our own studies (Barto, Sutton, and Anderson, 1983; Sutton, 1984). Michie\r
(1974) consistently emphasized trial and error and learning as essential aspects of artificial\r
intelligence.\r
Widrow, Gupta, and Maitra (1973) modified the Least-Mean-Square (LMS) algorithm\r
of Widrow and Ho↵ (1960) to produce a reinforcement learning rule that could learn\r
from success and failure signals instead of from training examples. They called this form\r
of learning “selective bootstrap adaptation” and described it as “learning with a critic”\r
instead of “learning with a teacher.” They analyzed this rule and showed how it could\r
learn to play blackjack. This was an isolated foray into reinforcement learning by Widrow,\r
whose contributions to supervised learning were much more influential. Our use of the\r
term “critic” is derived from Widrow, Gupta, and Maitra’s paper. Buchanan, Mitchell,\r
Smith, and Johnson (1978) independently used the term critic in the context of machine\r
learning (see also Dietterich and Buchanan, 1984), but for them a critic was an expert\r
system able to do more than evaluate performance.\r
Research on learning automata had a more direct influence on the trial-and-error\r
thread leading to modern reinforcement learning research. These are methods for solving\r
a nonassociative, purely selectional learning problem known as the k-armed bandit by\r
analogy to a slot machine, or “one-armed bandit,” except with k levers (see Chapter 2).\r
Learning automata are simple, low-memory machines for improving the probability\r
of reward in these problems. Learning automata originated with work in the 1960s\r
of the Russian mathematician and physicist M. L. Tsetlin and colleagues (published\r
posthumously in Tsetlin, 1973) and has been extensively developed since then within\r
engineering (see Narendra and Thathachar, 1974, 1989). These developments included the\r
study of stochastic learning automata, which are methods for updating action probabilities\r
on the basis of reward signals. Although not developed in the tradition of stochastic\r
learning automata, Harth and Tzanakou’s (1974) Alopex algorithm (for Algorithm of\r
pattern extraction) is a stochastic method for detecting correlations between actions and\r
reinforcement that influenced some of our early research (Barto, Sutton, and Brouwer,\r
1981). Stochastic learning automata were foreshadowed by earlier work in psychology,\r
beginning with William Estes’ (1950) e↵ort toward a statistical theory of learning and\r
further developed by others (e.g., Bush and Mosteller, 1955; Sternberg, 1963).

1.7. Early History of Reinforcement Learning 19\r
The statistical learning theories developed in psychology were adopted by researchers in\r
economics, leading to a thread of research in that field devoted to reinforcement learning.\r
This work began in 1973 with the application of Bush and Mosteller’s learning theory to\r
a collection of classical economic models (Cross, 1973). One goal of this research was to\r
study artificial agents that act more like real people than do traditional idealized economic\r
agents (Arthur, 1991). This approach expanded to the study of reinforcement learning\r
in the context of game theory. Reinforcement learning in economics developed largely\r
independently of the early work in reinforcement learning in artificial intelligence, though\r
game theory remains a topic of interest in both fields (beyond the scope of this book).\r
Camerer (2011) discusses the reinforcement learning tradition in economics, and Now´e,\r
Vrancx, and De Hauwere (2012) provide an overview of the subject from the point of view\r
of multi-agent extensions to the approach that we introduce in this book. Reinforcement\r
learning in the context of game theory is a much di↵erent subject than reinforcement\r
learning used in programs to play tic-tac-toe, checkers, and other recreational games. See,\r
for example, Szita (2012) for an overview of this aspect of reinforcement learning and\r
games.\r
John Holland (1975) outlined a general theory of adaptive systems based on selectional\r
principles. His early work concerned trial and error primarily in its nonassociative\r
form, as in evolutionary methods and the k-armed bandit. In 1976 and more fully in\r
1986, he introduced classifier systems, true reinforcement learning systems including\r
association and value functions. A key component of Holland’s classifier systems was\r
the “bucket-brigade algorithm” for credit assignment, which is closely related to the\r
temporal di↵erence algorithm used in our tic-tac-toe example and discussed in Chapter 6.\r
Another key component was a genetic algorithm, an evolutionary method whose role was\r
to evolve useful representations. Classifier systems have been extensively developed by\r
many researchers to form a major branch of reinforcement learning research (reviewed by\r
Urbanowicz and Moore, 2009), but genetic algorithms—which we do not consider to be\r
reinforcement learning systems by themselves—have received much more attention, as\r
have other approaches to evolutionary computation (e.g., Fogel, Owens and Walsh, 1966;\r
Koza, 1992).\r
The individual most responsible for reviving the trial-and-error thread of reinforcement\r
learning within artificial intelligence was Harry Klopf (1972, 1975, 1982). Klopf recognized\r
that essential aspects of adaptive behavior were being lost as learning researchers came\r
to focus almost exclusively on supervised learning. What was missing, according to\r
Klopf, were the hedonic aspects of behavior: the drive to achieve some result from the\r
environment, to control the environment toward desired ends and away from undesired\r
ends (see Section 15.9). This is the essential idea of trial-and-error learning. Klopf’s\r
ideas were especially influential on the authors because our assessment of them (Barto\r
and Sutton, 1981a) led to our appreciation of the distinction between supervised and\r
reinforcement learning, and to our eventual focus on reinforcement learning. Much of\r
the early work that we and colleagues accomplished was directed toward showing that\r
reinforcement learning and supervised learning were indeed di↵erent (Barto, Sutton, and\r
Brouwer, 1981; Barto and Sutton, 1981b; Barto and Anandan, 1985). Other studies\r
showed how reinforcement learning could address important problems in artificial neural

20 Chapter 1: Introduction\r
network learning, in particular, how it could produce learning algorithms for multilayer\r
networks (Barto, Anderson, and Sutton, 1982; Barto and Anderson, 1985; Barto, 1985,\r
1986; Barto and Jordan, 1987; see Section 15.10).\r
We turn now to the third thread to the history of reinforcement learning, that concerning\r
temporal-di↵erence learning. Temporal-di↵erence learning methods are distinctive in\r
being driven by the di↵erence between temporally successive estimates of the same\r
quantity—for example, of the probability of winning in the tic-tac-toe example. This\r
thread is smaller and less distinct than the other two, but it has played a particularly\r
important role in the field, in part because temporal-di↵erence methods seem to be new\r
and unique to reinforcement learning.\r
The origins of temporal-di↵erence learning are in part in animal learning psychology,\r
in particular, in the notion of secondary reinforcers. A secondary reinforcer is a stimulus\r
that has been paired with a primary reinforcer such as food or pain and, as a result, has\r
come to take on similar reinforcing properties. Minsky (1954) may have been the first to\r
realize that this psychological principle could be important for artificial learning systems.\r
Arthur Samuel (1959) was the first to propose and implement a learning method that\r
included temporal-di↵erence ideas, as part of his celebrated checkers-playing program\r
(Section 16.2).\r
Samuel made no reference to Minsky’s work or to possible connections to animal\r
learning. His inspiration apparently came from Claude Shannon’s (1950) suggestion that\r
a computer could be programmed to use an evaluation function to play chess, and that it\r
might be able to improve its play by modifying this function online. (It is possible that\r
these ideas of Shannon’s also influenced Bellman, but we know of no evidence for this.)\r
Minsky (1961) extensively discussed Samuel’s work in his “Steps” paper, suggesting the\r
connection to secondary reinforcement theories, both natural and artificial.\r
As we have discussed, in the decade following the work of Minsky and Samuel, little\r
computational work was done on trial-and-error learning, and apparently no computational\r
work at all was done on temporal-di↵erence learning. In 1972, Klopf brought trial-and\u0002error learning together with an important component of temporal-di↵erence learning.\r
Klopf was interested in principles that would scale to learning in large systems, and thus\r
was intrigued by notions of local reinforcement, whereby subcomponents of an overall\r
learning system could reinforce one another. He developed the idea of “generalized\r
reinforcement,” whereby every component (nominally, every neuron) views all of its\r
inputs in reinforcement terms: excitatory inputs as rewards and inhibitory inputs as\r
punishments. This is not the same idea as what we now know as temporal-di↵erence\r
learning, and in retrospect it is farther from it than was Samuel’s work. On the other\r
hand, Klopf linked the idea with trial-and-error learning and related it to the massive\r
empirical database of animal learning psychology.\r
Sutton (1978a,b,c) developed Klopf’s ideas further, particularly the links to animal\r
learning theories, describing learning rules driven by changes in temporally successive\r
predictions. He and Barto refined these ideas and developed a psychological model of\r
classical conditioning based on temporal-di↵erence learning (Sutton and Barto, 1981a;\r
Barto and Sutton, 1982). There followed several other influential psychological models of\r
classical conditioning based on temporal-di↵erence learning (e.g., Klopf, 1988; Moore et al.,

1.7. Early History of Reinforcement Learning 21\r
1986; Sutton and Barto, 1987, 1990). Some neuroscience models developed at this time\r
are well interpreted in terms of temporal-di↵erence learning (Hawkins and Kandel, 1984;\r
Byrne, Gingrich, and Baxter, 1990; Gelperin, Hopfield, and Tank, 1985; Tesauro, 1986;\r
Friston et al., 1994), although in most cases there was no historical connection.\r
Our early work on temporal-di↵erence learning was strongly influenced by animal\r
learning theories and by Klopf’s work. Relationships to Minsky’s “Steps” paper and to\r
Samuel’s checkers players were recognized only afterward. By 1981, however, we were\r
fully aware of all the prior work mentioned above as part of the temporal-di↵erence and\r
trial-and-error threads. At this time we developed a method for using temporal-di↵erence\r
learning combined with trial-and-error learning, known as the actor–critic architecture,\r
and applied this method to Michie and Chambers’s pole-balancing problem (Barto,\r
Sutton, and Anderson, 1983). This method was extensively studied in Sutton’s (1984)\r
PhD dissertation and extended to use backpropagation neural networks in Anderson’s\r
(1986) PhD dissertation. Around this time, Holland (1986) incorporated temporal\u0002di↵erence ideas explicitly into his classifier systems in the form of his bucket-brigade\r
algorithm. A key step was taken by Sutton (1988) by separating temporal-di↵erence\r
learning from control, treating it as a general prediction method. That paper also\r
introduced the TD() algorithm and proved some of its convergence properties.\r
As we were finalizing our work on the actor–critic architecture in 1981, we discovered\r
a paper by Ian Witten (1977, 1976a) which appears to be the earliest publication of a\r
temporal-di↵erence learning rule. He proposed the method that we now call tabular TD(0)\r
for use as part of an adaptive controller for solving MDPs. This work was first submitted\r
for journal publication in 1974 and also appeared in Witten’s 1976 PhD dissertation.\r
Witten’s work was a descendant of Andreae’s early experiments with STeLLA and other\r
trial-and-error learning systems. Thus, Witten’s 1977 paper spanned both major threads\r
of reinforcement learning research—trial-and-error learning and optimal control—while\r
making a distinct early contribution to temporal-di↵erence learning.\r
The temporal-di↵erence and optimal control threads were fully brought together\r
in 1989 with Chris Watkins’s development of Q-learning. This work extended and\r
integrated prior work in all three threads of reinforcement learning research. Paul Werbos\r
(1987) contributed to this integration by arguing for the convergence of trial-and-error\r
learning and dynamic programming since 1977. By the time of Watkins’s work there had\r
been tremendous growth in reinforcement learning research, primarily in the machine\r
learning subfield of artificial intelligence, but also in artificial neural networks and artificial\r
intelligence more broadly. In 1992, the remarkable success of Gerry Tesauro’s backgammon\r
playing program, TD-Gammon, brought additional attention to the field.\r
In the time since publication of the first edition of this book, a flourishing subfield of\r
neuroscience developed that focuses on the relationship between reinforcement learning\r
algorithms and reinforcement learning in the nervous system. Most responsible for this is\r
an uncanny similarity between the behavior of temporal-di↵erence algorithms and the\r
activity of dopamine producing neurons in the brain, as pointed out by a number of\r
researchers (Friston et al., 1994; Barto, 1995a; Houk, Adams, and Barto, 1995; Montague,\r
Dayan, and Sejnowski, 1996; and Schultz, Dayan, and Montague, 1997). Chapter 15\r
provides an introduction to this exciting aspect of reinforcement learning. Other important

22 Chapter 1: Introduction\r
contributions made in the recent history of reinforcement learning are too numerous to\r
mention in this brief account; we cite many of these at the end of the individual chapters\r
in which they arise.\r
Bibliographical Remarks\r
For additional general coverage of reinforcement learning, we refer the reader to the\r
books by Szepesv´ari (2010), Bertsekas and Tsitsiklis (1996), Kaelbling (1993a), and\r
Sugiyama, Hachiya, and Morimura (2013). Books that take a control or operations research\r
perspective include those of Si, Barto, Powell, and Wunsch (2004), Powell (2011), Lewis\r
and Liu (2012), and Bertsekas (2012). Cao’s (2009) review places reinforcement learning\r
in the context of other approaches to learning and optimization of stochastic dynamic\r
systems. Three special issues of the journal Machine Learning focus on reinforcement\r
learning: Sutton (1992a), Kaelbling (1996), and Singh (2002). Useful surveys are provided\r
by Barto (1995b); Kaelbling, Littman, and Moore (1996); and Keerthi and Ravindran\r
(1997). The volume edited by Weiring and van Otterlo (2012) provides an excellent\r
overview of recent developments.\r
1.2 The example of Phil’s breakfast in this chapter was inspired by Agre (1988).\r
1.5 The temporal-di↵erence method used in the tic-tac-toe example is developed in\r
Chapter 6.

Part I: Tabular Solution Methods\r
In this part of the book we describe almost all the core ideas of reinforcement learning\r
algorithms in their simplest forms: that in which the state and action spaces are small\r
enough for the approximate value functions to be represented as arrays, or tables. In\r
this case, the methods can often find exact solutions, that is, they can often find exactly\r
the optimal value function and the optimal policy. This contrasts with the approximate\r
methods described in the next part of the book, which only find approximate solutions,\r
but which in return can be applied e↵ectively to much larger problems.\r
The first chapter of this part of the book describes solution methods for the special\r
case of the reinforcement learning problem in which there is only a single state, called\r
bandit problems. The second chapter describes the general problem formulation that we\r
treat throughout the rest of the book—finite Markov decision processes—and its main\r
ideas including Bellman equations and value functions.\r
The next three chapters describe three fundamental classes of methods for solving finite\r
Markov decision problems: dynamic programming, Monte Carlo methods, and temporal\u0002di↵erence learning. Each class of methods has its strengths and weaknesses. Dynamic\r
programming methods are well developed mathematically, but require a complete and\r
accurate model of the environment. Monte Carlo methods don’t require a model and are\r
conceptually simple, but are not well suited for step-by-step incremental computation.\r
Finally, temporal-di↵erence methods require no model and are fully incremental, but are\r
more complex to analyze. The methods also di↵er in several ways with respect to their\r
eciency and speed of convergence.\r
The remaining two chapters describe how these three classes of methods can be\r
combined to obtain the best features of each of them. In one chapter we describe how\r
the strengths of Monte Carlo methods can be combined with the strengths of temporal\u0002di↵erence methods via multi-step bootstrapping methods. In the final chapter of this part\r
of the book we show how temporal-di↵erence learning methods can be combined with\r
model learning and planning methods (such as dynamic programming) for a complete\r
and unified solution to the tabular reinforcement learning problem.

Chapter 2\r
Multi-armed Bandits\r
The most important feature distinguishing reinforcement learning from other types of\r
learning is that it uses training information that evaluates the actions taken rather\r
than instructs by giving correct actions. This is what creates the need for active\r
exploration, for an explicit search for good behavior. Purely evaluative feedback indicates\r
how good the action taken was, but not whether it was the best or the worst action\r
possible. Purely instructive feedback, on the other hand, indicates the correct action to\r
take, independently of the action actually taken. This kind of feedback is the basis of\r
supervised learning, which includes large parts of pattern classification, artificial neural\r
networks, and system identification. In their pure forms, these two kinds of feedback\r
are quite distinct: evaluative feedback depends entirely on the action taken, whereas\r
instructive feedback is independent of the action taken.\r
In this chapter we study the evaluative aspect of reinforcement learning in a simplified\r
setting, one that does not involve learning to act in more than one situation. This\r
nonassociative setting is the one in which most prior work involving evaluative feedback\r
has been done, and it avoids much of the complexity of the full reinforcement learning\r
problem. Studying this case enables us to see most clearly how evaluative feedback di↵ers\r
from, and yet can be combined with, instructive feedback.\r
The particular nonassociative, evaluative feedback problem that we explore is a simple\r
version of the k-armed bandit problem. We use this problem to introduce a number\r
of basic learning methods which we extend in later chapters to apply to the full rein\u0002forcement learning problem. At the end of this chapter, we take a step closer to the full\r
reinforcement learning problem by discussing what happens when the bandit problem\r
becomes associative, that is, when the best action depends on the situation.\r
2.1 A k-armed Bandit Problem\r
Consider the following learning problem. You are faced repeatedly with a choice among\r
k di↵erent options, or actions. After each choice you receive a numerical reward chosen\r
from a stationary probability distribution that depends on the action you selected. Your

26 Chapter 2: Multi-armed Bandits\r
objective is to maximize the expected total reward over some time period, for example,\r
over 1000 action selections, or time steps.\r
This is the original form of the k-armed bandit problem, so named by analogy to a slot\r
machine, or “one-armed bandit,” except that it has k levers instead of one. Each action\r
selection is like a play of one of the slot machine’s levers, and the rewards are the payo↵s\r
for hitting the jackpot. Through repeated action selections you are to maximize your\r
winnings by concentrating your actions on the best levers. Another analogy is that of\r
a doctor choosing between experimental treatments for a series of seriously ill patients.\r
Each action is the selection of a treatment, and each reward is the survival or well-being\r
of the patient. Today the term “bandit problem” is sometimes used for a generalization\r
of the problem described above, but in this book we use it to refer just to this simple\r
case.\r
In our k-armed bandit problem, each of the k actions has an expected or mean reward\r
given that that action is selected; let us call this the value of that action. We denote the\r
action selected on time step t as At, and the corresponding reward as Rt. The value then\r
of an arbitrary action a, denoted q⇤(a), is the expected reward given that a is selected:\r
q⇤(a) .= E[Rt | At =a] .\r
If you knew the value of each action, then it would be trivial to solve the k-armed bandit\r
problem: you would always select the action with highest value. We assume that you do\r
not know the action values with certainty, although you may have estimates. We denote\r
the estimated value of action a at time step t as Qt(a). We would like Qt(a) to be close\r
to q⇤(a).\r
If you maintain estimates of the action values, then at any time step there is at least\r
one action whose estimated value is greatest. We call these the greedy actions. When you\r
select one of these actions, we say that you are exploiting your current knowledge of the\r
values of the actions. If instead you select one of the nongreedy actions, then we say you\r
are exploring, because this enables you to improve your estimate of the nongreedy action’s\r
value. Exploitation is the right thing to do to maximize the expected reward on the one\r
step, but exploration may produce the greater total reward in the long run. For example,\r
suppose a greedy action’s value is known with certainty, while several other actions are\r
estimated to be nearly as good but with substantial uncertainty. The uncertainty is\r
such that at least one of these other actions probably is actually better than the greedy\r
action, but you don’t know which one. If you have many time steps ahead on which\r
to make action selections, then it may be better to explore the nongreedy actions and\r
discover which of them are better than the greedy action. Reward is lower in the short\r
run, during exploration, but higher in the long run because after you have discovered\r
the better actions, you can exploit them many times. Because it is not possible both to\r
explore and to exploit with any single action selection, one often refers to the “conflict”\r
between exploration and exploitation.\r
In any specific case, whether it is better to explore or exploit depends in a complex\r
way on the precise values of the estimates, uncertainties, and the number of remaining\r
steps. There are many sophisticated methods for balancing exploration and exploitation\r
for particular mathematical formulations of the k-armed bandit and related problems.

2.2. Action-value Methods 27\r
However, most of these methods make strong assumptions about stationarity and prior\r
knowledge that are either violated or impossible to verify in most applications and in\r
the full reinforcement learning problem that we consider in subsequent chapters. The\r
guarantees of optimality or bounded loss for these methods are of little comfort when the\r
assumptions of their theory do not apply.\r
In this book we do not worry about balancing exploration and exploitation in a\r
sophisticated way; we worry only about balancing them at all. In this chapter we present\r
several simple balancing methods for the k-armed bandit problem and show that they\r
work much better than methods that always exploit. The need to balance exploration\r
and exploitation is a distinctive challenge that arises in reinforcement learning; the\r
simplicity of our version of the k-armed bandit problem enables us to show this in a\r
particularly clear form.\r
2.2 Action-value Methods\r
We begin by looking more closely at methods for estimating the values of actions and\r
for using the estimates to make action selection decisions, which we collectively call\r
action-value methods. Recall that the true value of an action is the mean reward when\r
that action is selected. One natural way to estimate this is by averaging the rewards\r
actually received:\r
Qt(a) .= sum of rewards when a taken prior to t\r
number of times a taken prior to t =\r
Pt1\r
i=1 Ri · Ai=a Pt1\r
i=1 Ai=a\r
, (2.1)\r
where predicate denotes the random variable that is 1 if predicate is true and 0 if it is not.\r
If the denominator is zero, then we instead define Qt(a) as some default value, such as\r
0. As the denominator goes to infinity, by the law of large numbers, Qt(a) converges to\r
q⇤(a). We call this the sample-average method for estimating action values because each\r
estimate is an average of the sample of relevant rewards. Of course this is just one way\r
to estimate action values, and not necessarily the best one. Nevertheless, for now let us\r
stay with this simple estimation method and turn to the question of how the estimates\r
might be used to select actions.\r
The simplest action selection rule is to select one of the actions with the highest\r
estimated value, that is, one of the greedy actions as defined in the previous section.\r
If there is more than one greedy action, then a selection is made among them in some\r
arbitrary way, perhaps randomly. We write this greedy action selection method as\r
At\r
.\r
= argmax aQt(a), (2.2)\r
where argmaxa denotes the action a for which the expression that follows is maximized\r
(with ties broken arbitrarily). Greedy action selection always exploits current knowledge to\r
maximize immediate reward; it spends no time at all sampling apparently inferior actions\r
to see if they might really be better. A simple alternative is to behave greedily most of\r
the time, but every once in a while, say with small probability ", instead select randomly

28 Chapter 2: Multi-armed Bandits\r
from among all the actions with equal probability, independently of the action-value\r
estimates. We call methods using this near-greedy action selection rule "-greedy methods.\r
An advantage of these methods is that, in the limit as the number of steps increases,\r
every action will be sampled an infinite number of times, thus ensuring that all the Qt(a)\r
converge to their respective q⇤(a). This of course implies that the probability of selecting\r
the optimal action converges to greater than 1  ", that is, to near certainty. These are\r
just asymptotic guarantees, however, and say little about the practical e↵ectiveness of\r
the methods.\r
Exercise 2.1 In "-greedy action selection, for the case of two actions and " = 0.5, what is\r
the probability that the greedy action is selected? ⇤\r
2.3 The 10-armed Testbed\r
To roughly assess the relative e↵ectiveness of the greedy and "-greedy action-value\r
methods, we compared them numerically on a suite of test problems. This was a set\r
of 2000 randomly generated k-armed bandit problems with k = 10. For each bandit\r
problem, such as the one shown in Figure 2.1, the action values, q⇤(a), a = 1,..., 10,\r
0\r
1\r
2\r
3\r
-3\r
-2\r
-1\r
q⇤(1)\r
q⇤(2)\r
q⇤(3)\r
q⇤(4)\r
q⇤(5)\r
q⇤(6)\r
q⇤(7)\r
q⇤(8)\r
q⇤(9)\r
q⇤(10)\r
Reward\r
distribution\r
1 2 3 4 5 6 7 8 9 10\r
Action\r
Figure 2.1: An example bandit problem from the 10-armed testbed. The true value q⇤(a) of\r
each of the ten actions was selected according to a normal distribution with mean zero and unit\r
variance, and then the actual rewards were selected according to a mean q⇤(a), unit-variance\r
normal distribution, as suggested by these gray distributions.

2.3. The 10-armed Testbed 29\r
were selected according to a normal (Gaussian) distribution with mean 0 and variance 1.\r
Then, when a learning method applied to that problem selected action At at time step t,\r
the actual reward, Rt, was selected from a normal distribution with mean q⇤(At) and\r
variance 1. These distributions are shown in gray in Figure 2.1. We call this suite of test\r
tasks the 10-armed testbed. For any learning method, we can measure its performance\r
and behavior as it improves with experience over 1000 time steps when applied to one of\r
the bandit problems. This makes up one run. Repeating this for 2000 independent runs,\r
each with a di↵erent bandit problem, we obtained measures of the learning algorithm’s\r
average behavior.\r
Figure 2.2 compares a greedy method with two "-greedy methods ("= 0.01 and "= 0.1),\r
as described above, on the 10-armed testbed. All the methods formed their action-value\r
estimates using the sample-average technique (with an initial estimate of 0). The upper\r
graph shows the increase in expected reward with experience. The greedy method\r
improved slightly faster than the other methods at the very beginning, but then leveled\r
o↵ at a lower level. It achieved a reward-per-step of only about 1, compared with the best\r
possible of about 1.54 on this testbed. The greedy method performed significantly worse\r
in the long run because it often got stuck performing suboptimal actions. The lower graph\r
 (greedy)\r
0\r
0.5\r
1\r
1.5\r
Average\r
reward\r
0 250 500 750 1000\r
Steps\r
0%\r
20%\r
40%\r
60%\r
80%\r
100%\r
%\r
Optimal\r
action\r
0 250 500 750 1000\r
Steps\r
1\r
1\r
"= 0.1\r
"= 0.01\r
"= 0.1\r
"= 0.01\r
"= 0\r
"= 0 (greedy)\r
Figure 2.2: Average performance of "-greedy action-value methods on the 10-armed testbed.\r
These data are averages over 2000 runs with di↵erent bandit problems. All methods used sample\r
averages as their action-value estimates.

30 Chapter 2: Multi-armed Bandits\r
shows that the greedy method found the optimal action in only approximately one-third\r
of the tasks. In the other two-thirds, its initial samples of the optimal action were\r
disappointing, and it never returned to it. The "-greedy methods eventually performed\r
better because they continued to explore and to improve their chances of recognizing\r
the optimal action. The " = 0.1 method explored more, and usually found the optimal\r
action earlier, but it never selected that action more than 91% of the time. The " = 0.01\r
method improved more slowly, but eventually would perform better than the " = 0.1\r
method on both performance measures shown in the figure. It is also possible to reduce "\r
over time to try to get the best of both high and low values.\r
The advantage of "-greedy over greedy methods depends on the task. For example,\r
suppose the reward variance had been larger, say 10 instead of 1. With noisier rewards\r
it takes more exploration to find the optimal action, and "-greedy methods should fare\r
even better relative to the greedy method. On the other hand, if the reward variances\r
were zero, then the greedy method would know the true value of each action after trying\r
it once. In this case the greedy method might actually perform best because it would\r
soon find the optimal action and then never explore. But even in the deterministic case\r
there is a large advantage to exploring if we weaken some of the other assumptions. For\r
example, suppose the bandit task were nonstationary, that is, the true values of the\r
actions changed over time. In this case exploration is needed even in the deterministic\r
case to make sure one of the nongreedy actions has not changed to become better than\r
the greedy one. As we shall see in the next few chapters, nonstationarity is the case\r
most commonly encountered in reinforcement learning. Even if the underlying task is\r
stationary and deterministic, the learner faces a set of banditlike decision tasks each of\r
which changes over time as learning proceeds and the agent’s decision-making policy\r
changes. Reinforcement learning requires a balance between exploration and exploitation.\r
Exercise 2.2: Bandit example Consider a k-armed bandit problem with k = 4 actions,\r
denoted 1, 2, 3, and 4. Consider applying to this problem a bandit algorithm using\r
"-greedy action selection, sample-average action-value estimates, and initial estimates\r
of Q1(a) = 0, for all a. Suppose the initial sequence of actions and rewards is A1 = 1,\r
R1 = 1, A2 = 2, R2 = 1, A3 = 2, R3 = 2, A4 = 2, R4 = 2, A5 = 3, R5 = 0. On some\r
of these time steps the " case may have occurred, causing an action to be selected at\r
random. On which time steps did this definitely occur? On which time steps could this\r
possibly have occurred? ⇤\r
Exercise 2.3 In the comparison shown in Figure 2.2, which method will perform best in\r
the long run in terms of cumulative reward and probability of selecting the best action?\r
How much better will it be? Express your answer quantitatively. ⇤\r
2.4 Incremental Implementation\r
The action-value methods we have discussed so far all estimate action values as sample\r
averages of observed rewards. We now turn to the question of how these averages can be\r
computed in a computationally ecient manner, in particular, with constant memory\r
and constant per-time-step computation.

2.4. Incremental Implementation 31\r
To simplify notation we concentrate on a single action. Let Ri now denote the reward\r
received after the ith selection of this action, and let Qn denote the estimate of its action\r
value after it has been selected n  1 times, which we can now write simply as\r
Qn\r
.\r
= R1 + R2 + ··· + Rn1\r
n  1 .\r
The obvious implementation would be to maintain a record of all the rewards and then\r
perform this computation whenever the estimated value was needed. However, if this is\r
done, then the memory and computational requirements would grow over time as more\r
rewards are seen. Each additional reward would require additional memory to store it\r
and additional computation to compute the sum in the numerator.\r
As you might suspect, this is not really necessary. It is easy to devise incremental\r
formulas for updating averages with small, constant computation required to process\r
each new reward. Given Qn and the nth reward, Rn, the new average of all n rewards\r
can be computed by\r
Qn+1 = 1\r
n\r
Xn\r
i=1\r
Ri\r
= 1\r
n\r
 \r
Rn +\r
n\r
X1\r
i=1\r
Ri\r
!\r
= 1\r
n\r
 \r
Rn + (n  1) 1\r
n  1\r
n\r
X1\r
i=1\r
Ri\r
!\r
= 1\r
n\r
⇣\r
Rn + (n  1)Qn\r
⌘\r
= 1\r
n\r
⇣\r
Rn + nQn  Qn\r
⌘\r
= Qn +\r
1\r
n\r
h\r
Rn  Qn\r
i\r
, (2.3)\r
which holds even for n = 1, obtaining Q2 = R1 for arbitrary Q1. This implementation\r
requires memory only for Qn and n, and only the small computation (2.3) for each new\r
reward.\r
This update rule (2.3) is of a form that occurs frequently throughout this book. The\r
general form is\r
NewEstimate OldEstimate + StepSize hTarget  OldEstimatei. (2.4)\r
The expression ⇥TargetOldEstimate⇤is an error in the estimate. It is reduced by taking\r
a step toward the “Target.” The target is presumed to indicate a desirable direction in\r
which to move, though it may be noisy. In the case above, for example, the target is the\r
nth reward.\r
Note that the step-size parameter (StepSize) used in the incremental method (2.3)\r
changes from time step to time step. In processing the nth reward for action a, the

32 Chapter 2: Multi-armed Bandits\r
method uses the step-size parameter 1\r
n . In this book we denote the step-size parameter\r
by ↵ or, more generally, by ↵t(a).\r
Pseudocode for a complete bandit algorithm using incrementally computed sample\r
averages and "-greedy action selection is shown in the box below. The function bandit(a)\r
is assumed to take an action and return a corresponding reward.\r
A simple bandit algorithm\r
Initialize, for a = 1 to k:\r
Q(a) 0\r
N(a) 0\r
Loop forever:\r
A \r
⇢ argmaxa Q(a) with probability 1  " (breaking ties randomly)\r
a random action with probability "\r
R bandit(A)\r
N(A) N(A)+1\r
Q(A) Q(A) + 1\r
N(A)\r
⇥\r
R  Q(A)\r
⇤\r
2.5 Tracking a Nonstationary Problem\r
The averaging methods discussed so far are appropriate for stationary bandit problems,\r
that is, for bandit problems in which the reward probabilities do not change over time.\r
As noted earlier, we often encounter reinforcement learning problems that are e↵ectively\r
nonstationary. In such cases it makes sense to give more weight to recent rewards than\r
to long-past rewards. One of the most popular ways of doing this is to use a constant\r
step-size parameter. For example, the incremental update rule (2.3) for updating an\r
average Qn of the n  1 past rewards is modified to be\r
Qn+1\r
.\r
= Qn + ↵\r
h\r
Rn  Qn\r
i\r
, (2.5)\r
where the step-size parameter ↵ 2 (0, 1] is constant. This results in Qn+1 being a weighted\r
average of past rewards and the initial estimate Q1:\r
Qn+1 = Qn + ↵\r
h\r
Rn  Qn\r
i\r
= ↵Rn + (1  ↵)Qn\r
= ↵Rn + (1  ↵) [↵Rn1 + (1  ↵)Qn1]\r
= ↵Rn + (1  ↵)↵Rn1 + (1  ↵)\r
2Qn1\r
= ↵Rn + (1  ↵)↵Rn1 + (1  ↵)\r
2↵Rn2 +\r
··· + (1  ↵)\r
n1↵R1 + (1  ↵)nQ1\r
= (1  ↵)\r
nQ1 +Xn\r
i=1\r
↵(1  ↵)\r
ni\r
Ri. (2.6)

2.5. Tracking a Nonstationary Problem 33\r
We call this a weighted average because the sum of the weights is (1  ↵)n + Pn\r
i=1 ↵(1 \r
↵)ni = 1, as you can check for yourself. Note that the weight, ↵(1  ↵)ni, given to the\r
reward Ri depends on how many rewards ago, n  i, it was observed. The quantity 1  ↵\r
is less than 1, and thus the weight given to Ri decreases as the number of intervening\r
rewards increases. In fact, the weight decays exponentially according to the exponent\r
on 1  ↵. (If 1  ↵ = 0, then all the weight goes on the very last reward, Rn, because\r
of the convention that 00 = 1.) Accordingly, this is sometimes called an exponential\r
recency-weighted average.\r
Sometimes it is convenient to vary the step-size parameter from step to step. Let ↵n(a)\r
denote the step-size parameter used to process the reward received after the nth selection\r
of action a. As we have noted, the choice ↵n(a) = 1\r
n results in the sample-average method,\r
which is guaranteed to converge to the true action values by the law of large numbers.\r
But of course convergence is not guaranteed for all choices of the sequence {↵n(a)}. A\r
well-known result in stochastic approximation theory gives us the conditions required to\r
assure convergence with probability 1:\r
X1\r
n=1\r
↵n(a) = 1 and X1\r
n=1\r
↵2\r
n(a) < 1. (2.7)\r
The first condition is required to guarantee that the steps are large enough to eventually\r
overcome any initial conditions or random fluctuations. The second condition guarantees\r
that eventually the steps become small enough to assure convergence.\r
Note that both convergence conditions are met for the sample-average case, ↵n(a) = 1\r
n ,\r
but not for the case of constant step-size parameter, ↵n(a) = ↵. In the latter case, the\r
second condition is not met, indicating that the estimates never completely converge but\r
continue to vary in response to the most recently received rewards. As we mentioned\r
above, this is actually desirable in a nonstationary environment, and problems that are\r
e↵ectively nonstationary are the most common in reinforcement learning. In addition,\r
sequences of step-size parameters that meet the conditions (2.7) often converge very slowly\r
or need considerable tuning in order to obtain a satisfactory convergence rate. Although\r
sequences of step-size parameters that meet these convergence conditions are often used\r
in theoretical work, they are seldom used in applications and empirical research.\r
Exercise 2.4 If the step-size parameters, ↵n, are not constant, then the estimate Qn is\r
a weighted average of previously received rewards with a weighting di↵erent from that\r
given by (2.6). What is the weighting on each prior reward for the general case, analogous\r
to (2.6), in terms of the sequence of step-size parameters? ⇤\r
Exercise 2.5 (programming) Design and conduct an experiment to demonstrate the\r
diculties that sample-average methods have for nonstationary problems. Use a modified\r
version of the 10-armed testbed in which all the q⇤(a) start out equal and then take\r
independent random walks (say by adding a normally distributed increment with mean 0\r
and standard deviation 0.01 to all the q⇤(a) on each step). Prepare plots like Figure 2.2\r
for an action-value method using sample averages, incrementally computed, and another\r
action-value method using a constant step-size parameter, ↵ = 0.1. Use " = 0.1 and\r
longer runs, say of 10,000 steps. ⇤

34 Chapter 2: Multi-armed Bandits\r
2.6 Optimistic Initial Values\r
All the methods we have discussed so far are dependent to some extent on the initial\r
action-value estimates, Q1(a). In the language of statistics, these methods are biased\r
by their initial estimates. For the sample-average methods, the bias disappears once all\r
actions have been selected at least once, but for methods with constant ↵, the bias is\r
permanent, though decreasing over time as given by (2.6). In practice, this kind of bias\r
is usually not a problem and can sometimes be very helpful. The downside is that the\r
initial estimates become, in e↵ect, a set of parameters that must be picked by the user, if\r
only to set them all to zero. The upside is that they provide an easy way to supply some\r
prior knowledge about what level of rewards can be expected.\r
Initial action values can also be used as a simple way to encourage exploration. Suppose\r
that instead of setting the initial action values to zero, as we did in the 10-armed testbed,\r
we set them all to +5. Recall that the q⇤(a) in this problem are selected from a normal\r
distribution with mean 0 and variance 1. An initial estimate of +5 is thus wildly optimistic.\r
But this optimism encourages action-value methods to explore. Whichever actions are\r
initially selected, the reward is less than the starting estimates; the learner switches to\r
other actions, being “disappointed” with the rewards it is receiving. The result is that all\r
actions are tried several times before the value estimates converge. The system does a\r
fair amount of exploration even if greedy actions are selected all the time.\r
Figure 2.3 shows the performance on the 10-armed bandit testbed of a greedy method\r
using Q1(a) = +5, for all a. For comparison, also shown is an "-greedy method with\r
Q1(a) = 0. Initially, the optimistic method performs worse because it explores more,\r
but eventually it performs better because its exploration decreases with time. We call\r
this technique for encouraging exploration optimistic initial values. We regard it as\r
a simple trick that can be quite e↵ective on stationary problems, but it is far from\r
being a generally useful approach to encouraging exploration. For example, it is not\r
well suited to nonstationary problems because its drive for exploration is inherently\r
0%\r
20%\r
40%\r
60%\r
80%\r
100%\r
%\r
Optimal\r
action\r
0 200 400 600 800 1000\r
Plays\r
optimistic, greedy\r
Q0 = 5, \u0004\u0004= 0\r
realistic, \u0004-greedy\r
Q0 = 0, \u0004\u0004= 0.1 1\r
1\r
Steps\r
1\r
Optimistic, greedy\r
Q1 = 5, "= 0\r
Realistic, -greedy "\r
Q1 = 0, "= 0.1\r
Figure 2.3: The e↵ect of optimistic initial action-value estimates on the 10-armed testbed.\r
Both methods used a constant step-size parameter, ↵ = 0.1.

2.7. Upper-Confidence-Bound Action Selection 35\r
temporary. If the task changes, creating a renewed need for exploration, this method\r
cannot help. Indeed, any method that focuses on the initial conditions in any special way\r
is unlikely to help with the general nonstationary case. The beginning of time occurs\r
only once, and thus we should not focus on it too much. This criticism applies as well to\r
the sample-average methods, which also treat the beginning of time as a special event,\r
averaging all subsequent rewards with equal weights. Nevertheless, all of these methods\r
are very simple, and one of them—or some simple combination of them—is often adequate\r
in practice. In the rest of this book we make frequent use of several of these simple\r
exploration techniques.\r
Exercise 2.6: Mysterious Spikes The results shown in Figure 2.3 should be quite reliable\r
because they are averages over 2000 individual, randomly chosen 10-armed bandit tasks.\r
Why, then, are there oscillations and spikes in the early part of the curve for the optimistic\r
method? In other words, what might make this method perform particularly better or\r
worse, on average, on particular early steps? ⇤\r
Exercise 2.7: Unbiased Constant-Step-Size Trick In most of this chapter we have used\r
sample averages to estimate action values because sample averages do not produce the\r
initial bias that constant step sizes do (see the analysis leading to (2.6)). However, sample\r
averages are not a completely satisfactory solution because they may perform poorly\r
on nonstationary problems. Is it possible to avoid the bias of constant step sizes while\r
retaining their advantages on nonstationary problems? One way is to use a step size of\r
n\r
.\r
= ↵/o¯n, (2.8)\r
to process the nth reward for a particular action, where ↵ > 0 is a conventional constant\r
step size, and ¯on is a trace of one that starts at 0:\r
o¯n\r
.\r
= ¯on1 + ↵(1  o¯n1), for n > 0, with ¯o0\r
.\r
= 0. (2.9)\r
Carry out an analysis like that in (2.6) to show that Qn is an exponential recency-weighted\r
average without initial bias. ⇤\r
2.7 Upper-Confidence-Bound Action Selection\r
Exploration is needed because there is always uncertainty about the accuracy of the\r
action-value estimates. The greedy actions are those that look best at present, but some of\r
the other actions may actually be better. "-greedy action selection forces the non-greedy\r
actions to be tried, but indiscriminately, with no preference for those that are nearly\r
greedy or particularly uncertain. It would be better to select among the non-greedy\r
actions according to their potential for actually being optimal, taking into account both\r
how close their estimates are to being maximal and the uncertainties in those estimates.\r
One e↵ective way of doing this is to select actions according to\r
At\r
.\r
= argmax a\r
"\r
Qt(a) + c\r
s\r
ln t\r
Nt(a)\r
#\r
, (2.10)\r
where ln t denotes the natural logarithm of t (the number that e ⇡ 2.71828 would have\r
to be raised to in order to equal t), Nt(a) denotes the number of times that action a has

36 Chapter 2: Multi-armed Bandits\r
been selected prior to time t (the denominator in (2.1)), and the number c > 0 controls\r
the degree of exploration. If Nt(a) = 0, then a is considered to be a maximizing action.\r
The idea of this upper confidence bound (UCB) action selection is that the square-root\r
term is a measure of the uncertainty or variance in the estimate of a’s value. The quantity\r
being max’ed over is thus a sort of upper bound on the possible true value of action a, with\r
c determining the confidence level. Each time a is selected the uncertainty is presumably\r
reduced: Nt(a) increments, and, as it appears in the denominator, the uncertainty term\r
decreases. On the other hand, each time an action other than a is selected, t increases but\r
Nt(a) does not; because t appears in the numerator, the uncertainty estimate increases.\r
The use of the natural logarithm means that the increases get smaller over time, but are\r
unbounded; all actions will eventually be selected, but actions with lower value estimates,\r
or that have already been selected frequently, will be selected with decreasing frequency\r
over time.\r
Results with UCB on the 10-armed testbed are shown in Figure 2.4. UCB often\r
performs well, as shown here, but is more dicult than "-greedy to extend beyond bandits\r
to the more general reinforcement learning settings considered in the rest of this book.\r
One diculty is in dealing with nonstationary problems; methods more complex than\r
those presented in Section 2.5 would be needed. Another diculty is dealing with large\r
state spaces, particularly when using function approximation as developed in Part II of\r
this book. In these more advanced settings the idea of UCB action selection is usually\r
not practical.\r
1 250 500 750 1000\r
0\r
0.5\r
1\r
1.5\r
-greedy  = 0.1\r
UCB c = 2\r
Average\r
reward\r
Steps\r
Figure 2.4: Average performance of UCB action selection on the 10-armed testbed. As shown,\r
UCB generally performs better than "-greedy action selection, except in the first k steps, when\r
it selects randomly among the as-yet-untried actions.\r
Exercise 2.8: UCB Spikes In Figure 2.4 the UCB algorithm shows a distinct spike\r
in performance on the 11th step. Why is this? Note that for your answer to be fully\r
satisfactory it must explain both why the reward increases on the 11th step and why it\r
decreases on the subsequent steps. Hint: If c = 1, then the spike is less prominent. ⇤

2.8. Gradient Bandit Algorithms 37\r
2.8 Gradient Bandit Algorithms\r
So far in this chapter we have considered methods that estimate action values and use\r
those estimates to select actions. This is often a good approach, but it is not the only\r
one possible. In this section we consider learning a numerical preference for each action\r
a, which we denote Ht(a) 2 R. The larger the preference, the more often that action is\r
taken, but the preference has no interpretation in terms of reward. Only the relative\r
preference of one action over another is important; if we add 1000 to all the action\r
preferences there is no e↵ect on the action probabilities, which are determined according\r
to a soft-max distribution (i.e., Gibbs or Boltzmann distribution) as follows:\r
Pr{At =a} .= eHt(a)\r
Pk\r
b=1 eHt(b)\r
.\r
= ⇡t(a), (2.11)\r
where here we have also introduced a useful new notation, ⇡t(a), for the probability of\r
taking action a at time t. Initially all action preferences are the same (e.g., H1(a) = 0,\r
for all a) so that all actions have an equal probability of being selected.\r
Exercise 2.9 Show that in the case of two actions, the soft-max distribution is the same\r
as that given by the logistic, or sigmoid, function often used in statistics and artificial\r
neural networks. ⇤\r
There is a natural learning algorithm for soft-max action preferences based on the idea\r
of stochastic gradient ascent. On each step, after selecting action At and receiving the\r
reward Rt, the action preferences are updated by:\r
Ht+1(At) .= Ht(At) + ↵\r
\r
Rt  R¯t\r
1  ⇡t(At)\r
, and\r
Ht+1(a) .= Ht(a)  ↵\r
\r
Rt  R¯t\r
\r
⇡t(a), for all a 6= At, (2.12)\r
where ↵ > 0 is a step-size parameter, and R¯t 2 R is the average of the rewards up to but\r
not including time t (with R¯1\r
.\r
= R1), which can be computed incrementally as described\r
in Section 2.4 (or Section 2.5 if the problem is nonstationary).1 The R¯t term serves as a\r
baseline with which the reward is compared. If the reward is higher than the baseline,\r
then the probability of taking At in the future is increased, and if the reward is below\r
baseline, then the probability is decreased. The non-selected actions move in the opposite\r
direction.\r
Figure 2.5 shows results with the gradient bandit algorithm on a variant of the 10-\r
armed testbed in which the true expected rewards were selected according to a normal\r
distribution with a mean of +4 instead of zero (and with unit variance as before). This\r
shifting up of all the rewards has absolutely no e↵ect on the gradient bandit algorithm\r
because of the reward baseline term, which instantaneously adapts to the new level. But\r
if the baseline were omitted (that is, if R¯t was taken to be constant zero in (2.12)), then\r
performance would be significantly degraded, as shown in the figure.\r
1In the empirical results in this chapter, the baseline R¯t also included Rt.

38 Chapter 2: Multi-armed Bandits\r
%\r
Optimal\r
action\r
Steps\r
α = 0.1\r
100%\r
80%\r
60%\r
40%\r
20%\r
0%\r
α = 0.4\r
α = 0.1\r
α = 0.4\r
without baseline\r
with baseline\r
1 250 500 750 1000\r
Figure 2.5: Average performance of the gradient bandit algorithm with and without a reward\r
baseline on the 10-armed testbed when the q⇤(a) are chosen to be near +4 rather than near zero.\r
The Bandit Gradient Algorithm as Stochastic Gradient Ascent\r
One can gain a deeper insight into the gradient bandit algorithm by understanding\r
it as a stochastic approximation to gradient ascent. In exact gradient ascent, each\r
action preference Ht(a) would be incremented in proportion to the increment’s\r
e↵ect on performance:\r
Ht+1(a) .= Ht(a) + ↵\r
@E[Rt]\r
@Ht(a)\r
, (2.13)\r
where the measure of performance here is the expected reward:\r
E[Rt] = X\r
x\r
⇡t(x)q⇤(x),\r
and the measure of the increment’s e↵ect is the partial derivative of this performance\r
measure with respect to the action preference. Of course, it is not possible to\r
implement gradient ascent exactly in our case because by assumption we do not\r
know the q⇤(x), but in fact the updates of our algorithm (2.12) are equal to (2.13)\r
in expected value, making the algorithm an instance of stochastic gradient ascent.\r
The calculations showing this require only beginning calculus, but take several

2.8. Gradient Bandit Algorithms 39\r
steps. First we take a closer look at the exact performance gradient:\r
@E[Rt]\r
@Ht(a) = @@Ht(a)\r
"\r
X\r
x\r
⇡t(x)q⇤(x)\r
#\r
= X\r
x\r
q⇤(x)\r
@ ⇡t(x)\r
@Ht(a)\r
= X\r
x\r
\r
q⇤(x)  Bt\r
 @ ⇡t(x)\r
@Ht(a)\r
,\r
where Bt, called the baseline, can be any scalar that does not depend on x. We can\r
include a baseline here without changing the equality because the gradient sums\r
to zero over all the actions, P\r
x\r
@ ⇡t(x)\r
@Ht(a) = 0. As Ht(a) is changed, some actions’\r
probabilities go up and some go down, but the sum of the changes must be zero\r
because the sum of the probabilities is always one.\r
Next we multiply each term of the sum by ⇡t(x)/⇡t(x):\r
@E[Rt]\r
@Ht(a) = X\r
x\r
⇡t(x)\r
\r
q⇤(x)  Bt\r
 @ ⇡t(x)\r
@Ht(a)\r
/⇡t(x).\r
The equation is now in the form of an expectation, summing over all possible values\r
x of the random variable At, then multiplying by the probability of taking those\r
values. Thus:\r
= E\r
\r
\r
q⇤(At)  Bt\r
@ ⇡t(At)\r
@Ht(a) /⇡t(At)\r
\r
= E\r
\r
\r
Rt  R¯t\r
@ ⇡t(At)\r
@Ht(a) /⇡t(At)\r
\r
,\r
where here we have chosen the baseline Bt = R¯t and substituted Rt for q⇤(At),\r
which is permitted because E[Rt|At] = q⇤(At). Shortly we will establish that\r
@ ⇡t(x)\r
@Ht(a) = ⇡t(x)\r
 a=x  ⇡t(a)\r
, where a=x is defined to be 1 if a = x, else 0.\r
Assuming that for now, we have\r
= E\r
⇥Rt  R¯t\r
⇡t(At)\r
 a=At  ⇡t(a)\r
/⇡t(At)\r
⇤\r
= E\r
⇥Rt  R¯t a=At  ⇡t(a)⇤ .\r
Recall that our plan has been to write the performance gradient as an expectation\r
of something that we can sample on each step, as we have just done, and then\r
update on each step in proportion to the sample. Substituting a sample of the\r
expectation above for the performance gradient in (2.13) yields:\r
Ht+1(a) = Ht(a) + ↵\r
\r
Rt  R¯t\r
 a=At  ⇡t(a)\r
, for all a,\r
which you may recognize as being equivalent to our original algorithm (2.12).

40 Chapter 2: Multi-armed Bandits\r
Thus it remains only to show that @ ⇡t(x)\r
@Ht(a) = ⇡t(x)\r
 a=x  ⇡t(a)\r
, as we assumed.\r
Recall the standard quotient rule for derivatives:\r
@\r
@x\r
\r
f(x)\r
g(x)\r
\r
=\r
@f(x)\r
@x g(x)  f(x) @g(x)@x\r
g(x)2 .\r
Using this, we can write\r
@ ⇡t(x)\r
@Ht(a) = @@Ht(a)\r
⇡t(x)\r
= @\r
@Ht(a)\r
"\r
eHt(x)\r
Pk\r
y=1 eHt(y)\r
#\r
=\r
@eHt(x)\r
@Ht(a)\r
Pk\r
y=1 eHt(y)  eHt(x) @ Pk\r
y=1 eHt(y)\r
@Ht(a)\r
⇣Pk\r
y=1 eHt(y)\r
⌘2 (by the quotient rule)\r
= a=xeHt(x) Pk\r
y=1 eHt(y)  eHt(x)\r
eHt(a)\r
⇣Pk\r
y=1 eHt(y)\r
⌘2 (because @ex\r
@x = ex)\r
= a=xeHt(x)\r
Pk\r
y=1 eHt(y)  eHt(x)\r
eHt(a)\r
⇣Pk\r
y=1 eHt(y)\r
⌘2\r
= a=x⇡t(x)  ⇡t(x)⇡t(a)\r
= ⇡t(x)\r
 a=x  ⇡t(a)\r
. Q.E.D.\r
We have just shown that the expected update of the gradient bandit algorithm\r
is equal to the gradient of expected reward, and thus that the algorithm is an\r
instance of stochastic gradient ascent. This assures us that the algorithm has robust\r
convergence properties.\r
Note that we did not require any properties of the reward baseline other than\r
that it does not depend on the selected action. For example, we could have set\r
it to zero, or to 1000, and the algorithm would still be an instance of stochastic\r
gradient ascent. The choice of the baseline does not a↵ect the expected update\r
of the algorithm, but it does a↵ect the variance of the update and thus the rate of\r
convergence (as shown, for example, in Figure 2.5). Choosing it as the average of\r
the rewards may not be the very best, but it is simple and works well in practice.

2.9. Associative Search (Contextual Bandits) 41\r
2.9 Associative Search (Contextual Bandits)\r
So far in this chapter we have considered only nonassociative tasks, that is, tasks in which\r
there is no need to associate di↵erent actions with di↵erent situations. In these tasks\r
the learner either tries to find a single best action when the task is stationary, or tries to\r
track the best action as it changes over time when the task is nonstationary. However,\r
in a general reinforcement learning task there is more than one situation, and the goal\r
is to learn a policy: a mapping from situations to the actions that are best in those\r
situations. To set the stage for the full problem, we briefly discuss the simplest way in\r
which nonassociative tasks extend to the associative setting.\r
As an example, suppose there are several di↵erent k-armed bandit tasks, and that on\r
each step you confront one of these chosen at random. Thus, the bandit task changes\r
randomly from step to step. If the probabilities with which each task is selected for you\r
do not change over time, this would appear as a single stationary k-armed bandit task,\r
and you could use one of the methods described in this chapter. Now suppose, however,\r
that when a bandit task is selected for you, you are given some distinctive clue about its\r
identity (but not its action values). Maybe you are facing an actual slot machine that\r
changes the color of its display as it changes its action values. Now you can learn a policy\r
associating each task, signaled by the color you see, with the best action to take when\r
facing that task—for instance, if red, select arm 1; if green, select arm 2. With the right\r
policy you can usually do much better than you could in the absence of any information\r
distinguishing one bandit task from another.\r
This is an example of an associative search task, so called because it involves both\r
trial-and-error learning to search for the best actions, and association of these actions\r
with the situations in which they are best. Associative search tasks are often now called\r
contextual bandits in the literature. Associative search tasks are intermediate between\r
the k-armed bandit problem and the full reinforcement learning problem. They are like\r
the full reinforcement learning problem in that they involve learning a policy, but they\r
are also like our version of the k-armed bandit problem in that each action a↵ects only\r
the immediate reward. If actions are allowed to a↵ect the next situation as well as the\r
reward, then we have the full reinforcement learning problem. We present this problem\r
in the next chapter and consider its ramifications throughout the rest of the book.\r
Exercise 2.10 Suppose you face a 2-armed bandit task whose true action values change\r
randomly from time step to time step. Specifically, suppose that, for any time step,\r
the true values of actions 1 and 2 are respectively 10 and 20 with probability 0.5 (case\r
A), and 90 and 80 with probability 0.5 (case B). If you are not able to tell which case\r
you face at any step, what is the best expected reward you can achieve and how should\r
you behave to achieve it? Now suppose that on each step you are told whether you are\r
facing case A or case B (although you still don’t know the true action values). This is an\r
associative search task. What is the best expected reward you can achieve in this task,\r
and how should you behave to achieve it? ⇤

42 Chapter 2: Multi-armed Bandits\r
2.10 Summary\r
We have presented in this chapter several simple ways of balancing exploration and\r
exploitation. The "-greedy methods choose randomly a small fraction of the time, whereas\r
UCB methods choose deterministically but achieve exploration by subtly favoring at each\r
step the actions that have so far received fewer samples. Gradient bandit algorithms\r
estimate not action values, but action preferences, and favor the more preferred actions\r
in a graded, probabilistic manner using a soft-max distribution. The simple expedient of\r
initializing estimates optimistically causes even greedy methods to explore significantly.\r
It is natural to ask which of these methods is best. Although this is a dicult question\r
to answer in general, we can certainly run them all on the 10-armed testbed that we\r
have used throughout this chapter and compare their performances. A complication is\r
that they all have a parameter; to get a meaningful comparison we have to consider\r
their performance as a function of their parameter. Our graphs so far have shown the\r
course of learning over time for each algorithm and parameter setting, to produce a\r
learning curve for that algorithm and parameter setting. If we plotted learning curves\r
for all algorithms and all parameter settings, then the graph would be too complex and\r
crowded to make clear comparisons. Instead we summarize a complete learning curve\r
by its average value over the 1000 steps; this value is proportional to the area under the\r
learning curve. Figure 2.6 shows this measure for the various bandit algorithms from\r
this chapter, each as a function of its own parameter shown on a single scale on the\r
x-axis. This kind of graph is called a parameter study. Note that the parameter values\r
are varied by factors of two and presented on a log scale. Note also the characteristic\r
inverted-U shapes of each algorithm’s performance; all the algorithms perform best at\r
an intermediate value of their parameter, neither too large nor too small. In assessing\r
Average\r
reward\r
over first \r
1000 steps\r
1.5\r
1.4\r
1.3\r
1.2\r
1.1\r
1\r
-greedy\r
UCB\r
gradient\r
bandit\r
greedy with\r
optimistic\r
initialization\r
α = 0.1\r
1/128 1/64 1/32 1/16 1/8 1/4 1/2 1 2 4\r
" ↵ c Q0\r
Figure 2.6: A parameter study of the various bandit algorithms presented in this chapter.\r
Each point is the average reward obtained over 1000 steps with a particular algorithm at a\r
particular setting of its parameter.

2.10. Summary 43\r
a method, we should attend not just to how well it does at its best parameter setting,\r
but also to how sensitive it is to its parameter value. All of these algorithms are fairly\r
insensitive, performing well over a range of parameter values varying by about an order\r
of magnitude. Overall, on this problem, UCB seems to perform best.\r
Despite their simplicity, in our opinion the methods presented in this chapter can\r
fairly be considered the state of the art. There are more sophisticated methods, but their\r
complexity and assumptions make them impractical for the full reinforcement learning\r
problem that is our real focus. Starting in Chapter 5 we present learning methods for\r
solving the full reinforcement learning problem that use in part the simple methods\r
explored in this chapter.\r
Although the simple methods explored in this chapter may be the best we can do\r
at present, they are far from a fully satisfactory solution to the problem of balancing\r
exploration and exploitation.\r
One well-studied approach to balancing exploration and exploitation in k-armed bandit\r
problems is to compute a special kind of action value called a Gittins index. In certain\r
important special cases, this computation is tractable and leads directly to optimal\r
solutions, although it does require complete knowledge of the prior distribution of possible\r
problems, which we generally assume is not available. In addition, neither the theory\r
nor the computational tractability of this approach appear to generalize to the full\r
reinforcement learning problem that we consider in the rest of the book.\r
The Gittins-index approach is an instance of Bayesian methods, which assume a known\r
initial distribution over the action values and then update the distribution exactly after\r
each step (assuming that the true action values are stationary). In general, the update\r
computations can be very complex, but for certain special distributions (called conjugate\r
priors) they are easy. One possibility is to then select actions at each step according\r
to their posterior probability of being the best action. This method, sometimes called\r
posterior sampling or Thompson sampling, often performs similarly to the best of the\r
distribution-free methods we have presented in this chapter.\r
In the Bayesian setting it is even conceivable to compute the optimal balance between\r
exploration and exploitation. One can compute for any possible action the probability\r
of each possible immediate reward and the resultant posterior distributions over action\r
values. This evolving distribution becomes the information state of the problem. Given\r
a horizon, say of 1000 steps, one can consider all possible actions, all possible resulting\r
rewards, all possible next actions, all next rewards, and so on for all 1000 steps. Given\r
the assumptions, the rewards and probabilities of each possible chain of events can be\r
determined; one need only pick the best. But the tree of possibilities grows extremely\r
rapidly; even if there were only two actions and two rewards, the tree would have 22000\r
leaves. It is generally not feasible to perform this immense computation exactly, but\r
perhaps it could be approximated eciently. This approach would e↵ectively turn the\r
bandit problem into an instance of the full reinforcement learning problem. In the end, we\r
may be able to use approximate reinforcement learning methods such as those presented\r
in Part II of this book to approach this optimal solution. But that is a topic for research\r
and beyond the scope of this introductory book.

44 Chapter 2: Multi-armed Bandits\r
Exercise 2.11 (programming) Make a figure analogous to Figure 2.6 for the nonstationary\r
case outlined in Exercise 2.5. Include the constant-step-size "-greedy algorithm with\r
↵= 0.1. Use runs of 200,000 steps and, as a performance measure for each algorithm and\r
parameter setting, use the average reward over the last 100,000 steps. ⇤\r
Bibliographical and Historical Remarks\r
2.1 Bandit problems have been studied in statistics, engineering, and psychology. In\r
statistics, bandit problems fall under the heading “sequential design of experi\u0002ments,” introduced by Thompson (1933, 1934) and Robbins (1952), and studied\r
by Bellman (1956). Berry and Fristedt (1985) provide an extensive treatment of\r
bandit problems from the perspective of statistics. Narendra and Thathachar\r
(1989) treat bandit problems from the engineering perspective, providing a good\r
discussion of the various theoretical traditions that have focused on them. In\r
psychology, bandit problems have played roles in statistical learning theory (e.g.,\r
Bush and Mosteller, 1955; Estes, 1950).\r
The term greedy is often used in the heuristic search literature (e.g., Pearl, 1984).\r
The conflict between exploration and exploitation is known in control engineering\r
as the conflict between identification (or estimation) and control (e.g., Witten,\r
1976b). Feldbaum (1965) called it the dual control problem, referring to the\r
need to solve the two problems of identification and control simultaneously when\r
trying to control a system under uncertainty. In discussing aspects of genetic\r
algorithms, Holland (1975) emphasized the importance of this conflict, referring\r
to it as the conflict between the need to exploit and the need for new information.\r
2.2 Action-value methods for our k-armed bandit problem were first proposed by\r
Thathachar and Sastry (1985). These are often called estimator algorithms in the\r
learning automata literature. The term action value is due to Watkins (1989).\r
The first to use "-greedy methods may also have been Watkins (1989, p. 187),\r
but the idea is so simple that some earlier use seems likely.\r
2.4–5 This material falls under the general heading of stochastic iterative algorithms,\r
which is well covered by Bertsekas and Tsitsiklis (1996).\r
2.6 Optimistic initialization was used in reinforcement learning by Sutton (1996).\r
2.7 Early work on using estimates of the upper confidence bound to select actions\r
was done by Lai and Robbins (1985), Kaelbling (1993b), and Agrawal (1995).\r
The UCB algorithm we present here is called UCB1 in the literature and was\r
first developed by Auer, Cesa-Bianchi and Fischer (2002).\r
2.8 Gradient bandit algorithms are a special case of the gradient-based reinforcement\r
learning algorithms introduced by Williams (1992) that later developed into the\r
actor–critic and policy-gradient algorithms that we treat later in this book. Our\r
development here was influenced by that by Balaraman Ravindran (personal

2.10. Summary 45\r
communication). Further discussion of the choice of baseline is provided by\r
Greensmith, Bartlett, and Baxter (2002, 2004) and by Dick (2015). Early\r
systematic studies of algorithms like this were done by Sutton (1984).\r
The term soft-max for the action selection rule (2.11) is due to Bridle (1990).\r
This rule appears to have been first proposed by Luce (1959).\r
2.9 The term associative search and the corresponding problem were introduced by\r
Barto, Sutton, and Brouwer (1981). The term associative reinforcement learning\r
has also been used for associative search (Barto and Anandan, 1985), but we\r
prefer to reserve that term as a synonym for the full reinforcement learning\r
problem (as in Sutton, 1984). (And, as we noted, the modern literature also\r
uses the term “contextual bandits” for this problem.) We note that Thorndike’s\r
Law of E↵ect (quoted in Chapter 1) describes associative search by referring\r
to the formation of associative links between situations (states) and actions.\r
According to the terminology of operant, or instrumental, conditioning (e.g.,\r
Skinner, 1938), a discriminative stimulus is a stimulus that signals the presence\r
of a particular reinforcement contingency. In our terms, di↵erent discriminative\r
stimuli correspond to di↵erent states.\r
2.10 Bellman (1956) was the first to show how dynamic programming could be used\r
to compute the optimal balance between exploration and exploitation within a\r
Bayesian formulation of the problem. The Gittins index approach is due to Gittins\r
and Jones (1974). Du↵ (1995) showed how it is possible to learn Gittins indices\r
for bandit problems through reinforcement learning. The survey by Kumar (1985)\r
provides a good discussion of Bayesian and non-Bayesian approaches to these\r
problems. The term information state comes from the literature on partially\r
observable MDPs; see, for example, Lovejoy (1991).\r
Other theoretical research focuses on the eciency of exploration, usually ex\u0002pressed as how quickly an algorithm can approach an optimal decision-making\r
policy. One way to formalize exploration eciency is by adapting to reinforcement\r
learning the notion of sample complexity for a supervised learning algorithm,\r
which is the number of training examples the algorithm needs to attain a desired\r
degree of accuracy in learning the target function. A definition of the sample\r
complexity of exploration for a reinforcement learning algorithm is the number of\r
time steps in which the algorithm does not select near-optimal actions (Kakade,\r
2003). Li (2012) discusses this and several other approaches in a survey of theo\u0002retical approaches to exploration eciency in reinforcement learning. A thorough\r
modern treatment of Thompson sampling is provided by Russo et al. (2018).

Chapter 3\r
Finite Markov Decision\r
Processes\r
In this chapter we introduce the formal problem of finite Markov decision processes, or\r
finite MDPs, which we try to solve in the rest of the book. This problem involves evaluative\r
feedback, as in bandits, but also an associative aspect—choosing di↵erent actions in\r
di↵erent situations. MDPs are a classical formalization of sequential decision making,\r
where actions influence not just immediate rewards, but also subsequent situations,\r
or states, and through those future rewards. Thus MDPs involve delayed reward and\r
the need to trade o↵ immediate and delayed reward. Whereas in bandit problems we\r
estimated the value q⇤(a) of each action a, in MDPs we estimate the value q⇤(s, a) of\r
each action a in each state s, or we estimate the value v⇤(s) of each state given optimal\r
action selections. These state-dependent quantities are essential to accurately assigning\r
credit for long-term consequences to individual action selections.\r
MDPs are a mathematically idealized form of the reinforcement learning problem\r
for which precise theoretical statements can be made. We introduce key elements of\r
the problem’s mathematical structure, such as returns, value functions, and Bellman\r
equations. We try to convey the wide range of applications that can be formulated as\r
finite MDPs. As in all of artificial intelligence, there is a tension between breadth of\r
applicability and mathematical tractability. In this chapter we introduce this tension\r
and discuss some of the trade-o↵s and challenges that it implies. Some ways in which\r
reinforcement learning can be taken beyond MDPs are treated in Chapter 17.\r
3.1 The Agent–Environment Interface\r
MDPs are meant to be a straightforward framing of the problem of learning from\r
interaction to achieve a goal. The learner and decision maker is called the agent. The\r
thing it interacts with, comprising everything outside the agent, is called the environment.\r
These interact continually, the agent selecting actions and the environment responding to

48 Chapter 3: Finite Markov Decision Processes\r
these actions and presenting new situations to the agent.1 The environment also gives\r
rise to rewards, special numerical values that the agent seeks to maximize over time\r
through its choice of actions.\r
Agent\r
Environment\r
action\r
At\r
reward\r
Rt\r
state\r
St\r
Rt+1\r
St+1\r
Figure 3.1: The agent–environment interaction in a Markov decision process.\r
More specifically, the agent and environment interact at each of a sequence of discrete\r
time steps, t = 0, 1, 2, 3,....\r
2 At each time step t, the agent receives some representation\r
of the environment’s state, St 2 S, and on that basis selects an action, At 2 A(s).3 One\r
time step later, in part as a consequence of its action, the agent receives a numerical\r
reward, Rt+1 2 R ⇢ R, and finds itself in a new state, St+1.\r
4 The MDP and agent\r
together thereby give rise to a sequence or trajectory that begins like this:\r
S0, A0, R1, S1, A1, R2, S2, A2, R3,... (3.1)\r
In a finite MDP, the sets of states, actions, and rewards (S, A, and R) all have a finite\r
number of elements. In this case, the random variables Rt and St have well defined\r
discrete probability distributions dependent only on the preceding state and action. That\r
is, for particular values of these random variables, s0 2 S and r 2 R, there is a probability\r
of those values occurring at time t, given particular values of the preceding state and\r
action:\r
p(s0, r|s, a) .= Pr{St =s0, Rt =r | St1 =s, At1 =a}, (3.2)\r
for all s0, s 2 S, r 2 R, and a 2 A(s). The function p defines the dynamics of the MDP.\r
The dot over the equals sign in the equation reminds us that it is a definition (in this\r
case of the function p) rather than a fact that follows from previous definitions. The\r
dynamics function p : S ⇥ R ⇥ S ⇥ A ! [0, 1] is an ordinary deterministic function of four\r
arguments. The ‘|’ in the middle of it comes from the notation for conditional probability,\r
1We use the terms agent, environment, and action instead of the engineers’ terms controller, controlled\r
system (or plant), and control signal because they are meaningful to a wider audience.\r
2We restrict attention to discrete time to keep things as simple as possible, even though many of the\r
ideas can be extended to the continuous-time case (e.g., see Bertsekas and Tsitsiklis, 1996; Doya, 1996).\r
3To simplify notation, we sometimes assume the special case in which the action set is the same in all\r
states and write it simply as A.\r
4We use Rt+1 instead of Rt to denote the reward due to At because it emphasizes that the next\r
reward and next state, Rt+1 and St+1, are jointly determined. Unfortunately, both conventions are\r
widely used in the literature.

3.1. The Agent–Environment Interface 49\r
but here it just reminds us that p specifies a probability distribution for each choice of s\r
and a, that is, that\r
X\r
s02S\r
X\r
r2R\r
p(s0, r|s, a)=1, for all s 2 S, a 2 A(s). (3.3)\r
In a Markov decision process, the probabilities given by p completely characterize the\r
environment’s dynamics. That is, the probability of each possible value for St and Rt\r
depends on the immediately preceding state and action, St1 and At1, and, given them,\r
not at all on earlier states and actions. This is best viewed as a restriction not on the\r
decision process, but on the state. The state must include information about all aspects\r
of the past agent–environment interaction that make a di↵erence for the future. If it\r
does, then the state is said to have the Markov property. We will assume the Markov\r
property throughout this book, though starting in Part II we will consider approximation\r
methods that do not rely on it, and in Chapter 17 we consider how a Markov state can\r
be eciently learned and constructed from non-Markov observations.\r
From the four-argument dynamics function, p, one can compute anything else one might\r
want to know about the environment, such as the state-transition probabilities (which we\r
denote, with a slight abuse of notation, as a three-argument function p : S⇥S⇥A ! [0, 1]),\r
p(s0|s, a) .= Pr{St =s0 | St1 =s, At1 =a} = X\r
r2R\r
p(s0, r|s, a). (3.4)\r
We can also compute the expected rewards for state–action pairs as a two-argument\r
function r : S ⇥ A ! R:\r
r(s, a) .= E[Rt | St1 =s, At1 =a] = X\r
r2R\r
r\r
X\r
s02S\r
p(s0, r|s, a), (3.5)\r
and the expected rewards for state–action–next-state triples as a three-argument function\r
r : S ⇥ A ⇥ S ! R,\r
r(s, a, s0) .= E[Rt | St1 =s, At1 =a, St = s0] = X\r
r2R\r
r\r
p(s0, r|s, a)\r
p(s0 |s, a) . (3.6)\r
In this book, we usually use the four-argument p function (3.2), but each of these other\r
notations are also occasionally convenient.\r
The MDP framework is abstract and flexible and can be applied to many di↵erent\r
problems in many di↵erent ways. For example, the time steps need not refer to fixed\r
intervals of real time; they can refer to arbitrary successive stages of decision making\r
and acting. The actions can be low-level controls, such as the voltages applied to the\r
motors of a robot arm, or high-level decisions, such as whether or not to have lunch or\r
to go to graduate school. Similarly, the states can take a wide variety of forms. They\r
can be completely determined by low-level sensations, such as direct sensor readings, or\r
they can be more high-level and abstract, such as symbolic descriptions of objects in a\r
room. Some of what makes up a state could be based on memory of past sensations or

50 Chapter 3: Finite Markov Decision Processes\r
even be entirely mental or subjective. For example, an agent could be in the state of not\r
being sure where an object is, or of having just been surprised in some clearly defined\r
sense. Similarly, some actions might be totally mental or computational. For example,\r
some actions might control what an agent chooses to think about, or where it focuses its\r
attention. In general, actions can be any decisions we want to learn how to make, and\r
states can be anything we can know that might be useful in making them.\r
In particular, the boundary between agent and environment is typically not the same\r
as the physical boundary of a robot’s or an animal’s body. Usually, the boundary is\r
drawn closer to the agent than that. For example, the motors and mechanical linkages of\r
a robot and its sensing hardware should usually be considered parts of the environment\r
rather than parts of the agent. Similarly, if we apply the MDP framework to a person\r
or animal, the muscles, skeleton, and sensory organs should be considered part of the\r
environment. Rewards, too, presumably are computed inside the physical bodies of\r
natural and artificial learning systems, but are considered external to the agent.\r
The general rule we follow is that anything that cannot be changed arbitrarily by\r
the agent is considered to be outside of it and thus part of its environment. We do\r
not assume that everything in the environment is unknown to the agent. For example,\r
the agent often knows quite a bit about how its rewards are computed as a function of\r
its actions and the states in which they are taken. But we always consider the reward\r
computation to be external to the agent because it defines the task facing the agent and\r
thus must be beyond its ability to change arbitrarily. In fact, in some cases the agent may\r
know everything about how its environment works and still face a dicult reinforcement\r
learning task, just as we may know exactly how a puzzle like Rubik’s cube works, but\r
still be unable to solve it. The agent–environment boundary represents the limit of the\r
agent’s absolute control, not of its knowledge.\r
The agent–environment boundary can be located at di↵erent places for di↵erent\r
purposes. In a complicated robot, many di↵erent agents may be operating at once, each\r
with its own boundary. For example, one agent may make high-level decisions which form\r
part of the states faced by a lower-level agent that implements the high-level decisions. In\r
practice, the agent–environment boundary is determined once one has selected particular\r
states, actions, and rewards, and thus has identified a specific decision-making task of\r
interest.\r
The MDP framework is a considerable abstraction of the problem of goal-directed\r
learning from interaction. It proposes that whatever the details of the sensory, memory,\r
and control apparatus, and whatever objective one is trying to achieve, any problem of\r
learning goal-directed behavior can be reduced to three signals passing back and forth\r
between an agent and its environment: one signal to represent the choices made by the\r
agent (the actions), one signal to represent the basis on which the choices are made (the\r
states), and one signal to define the agent’s goal (the rewards). This framework may not\r
be sucient to represent all decision-learning problems usefully, but it has proved to be\r
widely useful and applicable.\r
Of course, the particular states and actions vary greatly from task to task, and how\r
they are represented can strongly a↵ect performance. In reinforcement learning, as in\r
other kinds of learning, such representational choices are at present more art than science.

3.1. The Agent–Environment Interface 51\r
In this book we o↵er some advice and examples regarding good ways of representing\r
states and actions, but our primary focus is on general principles for learning how to\r
behave once the representations have been selected.\r
Example 3.1: Bioreactor Suppose reinforcement learning is being applied to determine\r
moment-by-moment temperatures and stirring rates for a bioreactor (a large vat of\r
nutrients and bacteria used to produce useful chemicals). The actions in such an\r
application might be target temperatures and target stirring rates that are passed to\r
lower-level control systems that, in turn, directly activate heating elements and motors to\r
attain the targets. The states are likely to be thermocouple and other sensory readings,\r
perhaps filtered and delayed, plus symbolic inputs representing the ingredients in the\r
vat and the target chemical. The rewards might be moment-by-moment measures of the\r
rate at which the useful chemical is produced by the bioreactor. Notice that here each\r
state is a list, or vector, of sensor readings and symbolic inputs, and each action is a\r
vector consisting of a target temperature and a stirring rate. It is typical of reinforcement\r
learning tasks to have states and actions with such structured representations. Rewards,\r
on the other hand, are always single numbers.\r
Example 3.2: Pick-and-Place Robot Consider using reinforcement learning to\r
control the motion of a robot arm in a repetitive pick-and-place task. If we want to learn\r
movements that are fast and smooth, the learning agent will have to control the motors\r
directly and have low-latency information about the current positions and velocities\r
of the mechanical linkages. The actions in this case might be the voltages applied to\r
each motor at each joint, and the states might be the latest readings of joint angles and\r
velocities. The reward might be +1 for each object successfully picked up and placed. To\r
encourage smooth movements, on each time step a small, negative reward could be given\r
as a function of the moment-to-moment jerkiness of the motion.\r
Exercise 3.1 Devise three example tasks of your own that fit into the MDP framework,\r
identifying for each its states, actions, and rewards. Make the three examples as di↵erent\r
from each other as possible. The framework is abstract and flexible and can be applied in\r
many di↵erent ways. Stretch its limits in some way in at least one of your examples. ⇤\r
Exercise 3.2 Is the MDP framework adequate to usefully represent all goal-directed\r
learning tasks? Can you think of any clear exceptions? ⇤\r
Exercise 3.3 Consider the problem of driving. You could define the actions in terms of\r
the accelerator, steering wheel, and brake, that is, where your body meets the machine.\r
Or you could define them farther out—say, where the rubber meets the road, considering\r
your actions to be tire torques. Or you could define them farther in—say, where your\r
brain meets your body, the actions being muscle twitches to control your limbs. Or you\r
could go to a really high level and say that your actions are your choices of where to drive.\r
What is the right level, the right place to draw the line between agent and environment?\r
On what basis is one location of the line to be preferred over another? Is there any\r
fundamental reason for preferring one location over another, or is it a free choice? ⇤

52 Chapter 3: Finite Markov Decision Processes\r
Example 3.3 Recycling Robot\r
A mobile robot has the job of collecting empty soda cans in an oce environment. It\r
has sensors for detecting cans, and an arm and gripper that can pick them up and place\r
them in an onboard bin; it runs on a rechargeable battery. The robot’s control system\r
has components for interpreting sensory information, for navigating, and for controlling\r
the arm and gripper. High-level decisions about how to search for cans are made by a\r
reinforcement learning agent based on the current charge level of the battery. To make a\r
simple example, we assume that only two charge levels can be distinguished, comprising\r
a small state set S = {high, low}. In each state, the agent can decide whether to (1)\r
actively search for a can for a certain period of time, (2) remain stationary and wait\r
for someone to bring it a can, or (3) head back to its home base to recharge its battery.\r
When the energy level is high, recharging would always be foolish, so we do not include it\r
in the action set for this state. The action sets are then A(high) = {search, wait} and\r
A(low) = {search, wait, recharge}.\r
The rewards are zero most of the time, but become positive when the robot secures an\r
empty can, or large and negative if the battery runs all the way down. The best way to\r
find cans is to actively search for them, but this runs down the robot’s battery, whereas\r
waiting does not. Whenever the robot is searching, the possibility exists that its battery\r
will become depleted. In this case the robot must shut down and wait to be rescued\r
(producing a low reward). If the energy level is high, then a period of active search can\r
always be completed without risk of depleting the battery. A period of searching that\r
begins with a high energy level leaves the energy level high with probability ↵ and reduces\r
it to low with probability 1  ↵. On the other hand, a period of searching undertaken\r
when the energy level is low leaves it low with probability  and depletes the battery with\r
probability 1  . In the latter case, the robot must be rescued, and the battery is then\r
recharged back to high. Each can collected by the robot counts as a unit reward, whereas\r
a reward of 3 results whenever the robot has to be rescued. Let rsearch and rwait, with\r
rsearch > rwait, denote the expected number of cans the robot will collect (and hence the\r
expected reward) while searching and while waiting respectively. Finally, suppose that no\r
cans can be collected during a run home for recharging, and that no cans can be collected\r
on a step in which the battery is depleted. This system is then a finite MDP, and we\r
can write down the transition probabilities and the expected rewards, with dynamics as\r
indicated in the table on the left:\r
sa s0 p(s0 |s, a) r(s, a, s0)\r
high search high ↵ rsearch high search low 1  ↵ rsearch low search high 1   3\r
low search low  rsearch high wait high 1 rwait high wait low 0 -\r
low wait high 0 -\r
low wait low 1 rwait low recharge high 1 0\r
low recharge low 0 -\r
search\r
high low 1, 0\r
search\r
recharge\r
wait\r
wait\r
, rsearch\r
↵, rsearch 1↵, rsearch\r
1, 3\r
1, rwait\r
1, rwait\r
Note that there is a row in the table for each possible combination of current state, s,\r
action, a 2 A(s), and next state, s0. Some transitions have zero probability of occurring,\r
so no expected reward is specified for them. Shown on the right is another useful way of

3.2. Goals and Rewards 53\r
summarizing the dynamics of a finite MDP, as a transition graph. There are two kinds of\r
nodes: state nodes and action nodes. There is a state node for each possible state (a large\r
open circle labeled by the name of the state), and an action node for each state–action\r
pair (a small solid circle labeled by the name of the action and connected by a line to the\r
state node). Starting in state s and taking action a moves you along the line from state\r
node s to action node (s, a). Then the environment responds with a transition to the next\r
state’s node via one of the arrows leaving action node (s, a). Each arrow corresponds to\r
a triple (s, s0, a), where s0 is the next state, and we label the arrow with the transition\r
probability, p(s0|s, a), and the expected reward for that transition, r(s, a, s0). Note that\r
the transition probabilities labeling the arrows leaving an action node always sum to 1.\r
Exercise 3.4 Give a table analogous to that in Example 3.3, but for p(s0, r|s, a). It\r
should have columns for s, a, s0, r, and p(s0, r|s, a), and a row for every 4-tuple for which\r
p(s0, r|s, a) > 0. ⇤\r
3.2 Goals and Rewards\r
In reinforcement learning, the purpose or goal of the agent is formalized in terms of a\r
special signal, called the reward, passing from the environment to the agent. At each time\r
step, the reward is a simple number, Rt 2 R. Informally, the agent’s goal is to maximize\r
the total amount of reward it receives. This means maximizing not immediate reward,\r
but cumulative reward in the long run. We can clearly state this informal idea as the\r
reward hypothesis:\r
That all of what we mean by goals and purposes can be well thought of as\r
the maximization of the expected value of the cumulative sum of a received\r
scalar signal (called reward).\r
The use of a reward signal to formalize the idea of a goal is one of the most distinctive\r
features of reinforcement learning.\r
Although formulating goals in terms of reward signals might at first appear limiting,\r
in practice it has proved to be flexible and widely applicable. The best way to see this is\r
to consider examples of how it has been, or could be, used. For example, to make a robot\r
learn to walk, researchers have provided reward on each time step proportional to the\r
robot’s forward motion. In making a robot learn how to escape from a maze, the reward\r
is often 1 for every time step that passes prior to escape; this encourages the agent to\r
escape as quickly as possible. To make a robot learn to find and collect empty soda cans\r
for recycling, one might give it a reward of zero most of the time, and then a reward of\r
+1 for each can collected. One might also want to give the robot negative rewards when\r
it bumps into things or when somebody yells at it. For an agent to learn to play checkers\r
or chess, the natural rewards are +1 for winning, 1 for losing, and 0 for drawing and\r
for all nonterminal positions.\r
You can see what is happening in all of these examples. The agent always learns to\r
maximize its reward. If we want it to do something for us, we must provide rewards\r
to it in such a way that in maximizing them the agent will also achieve our goals. It

54 Chapter 3: Finite Markov Decision Processes\r
is thus critical that the rewards we set up truly indicate what we want accomplished.\r
In particular, the reward signal is not the place to impart to the agent prior knowledge\r
about how to achieve what we want it to do.5 For example, a chess-playing agent should\r
be rewarded only for actually winning, not for achieving subgoals such as taking its\r
opponent’s pieces or aining control of the center of the board. If achieving these sorts\r
of subgoals were rewarded, then the agent might find a way to achieve them without\r
achieving the real goal. For example, it might find a way to take the opponent’s pieces\r
even at the cost of losing the game. The reward signal is your way of communicating to\r
the agent what you want achieved, not how you want it achieved.6\r
3.3 Returns and Episodes\r
So far we have discussed informally the objective of learning. We have said that the\r
agent’s goal is to maximize the cumulative reward it receives in the long run. How might\r
this be defined formally? If the sequence of rewards received after time step t is denoted\r
Rt+1, Rt+2, Rt+3,..., then what precise aspect of this sequence do we wish to maximize?\r
In general, we seek to maximize the expected return, where the return, denoted Gt, is\r
defined as some specific function of the reward sequence. In the simplest case the return\r
is the sum of the rewards:\r
Gt\r
.\r
= Rt+1 + Rt+2 + Rt+3 + ··· + RT , (3.7)\r
where T is a final time step. This approach makes sense in applications in which there\r
is a natural notion of final time step, that is, when the agent–environment interaction\r
breaks naturally into subsequences, which we call episodes,\r
7 such as plays of a game,\r
trips through a maze, or any sort of repeated interaction. Each episode ends in a special\r
state called the terminal state, followed by a reset to a standard starting state or to a\r
sample from a standard distribution of starting states. Even if you think of episodes as\r
ending in di↵erent ways, such as winning and losing a game, the next episode begins\r
independently of how the previous one ended. Thus the episodes can all be considered to\r
end in the same terminal state, with di↵erent rewards for the di↵erent outcomes. Tasks\r
with episodes of this kind are called episodic tasks. In episodic tasks we sometimes need\r
to distinguish the set of all nonterminal states, denoted S, from the set of all states plus\r
the terminal state, denoted S+. The time of termination, T, is a random variable that\r
normally varies from episode to episode.\r
On the other hand, in many cases the agent–environment interaction does not break\r
naturally into identifiable episodes, but goes on continually without limit. For example,\r
this would be the natural way to formulate an on-going process-control task, or an\r
application to a robot with a long life span. We call these continuing tasks. The return\r
formulation (3.7) is problematic for continuing tasks because the final time step would be\r
T = 1, and the return, which is what we are trying to maximize, could easily be infinite.\r
5Better places for imparting this kind of prior knowledge are the initial policy or initial value function.\r
6Section 17.4 delves further into the issue of designing e↵ective reward signals.\r
7Episodes are sometimes called “trials” in the literature.

3.3. Returns and Episodes 55\r
(For example, suppose the agent receives a reward of +1 at each time step.) Thus, in this\r
book we usually use a definition of return that is slightly more complex conceptually but\r
much simpler mathematically.\r
The additional concept that we need is that of discounting. According to this approach,\r
the agent tries to select actions so that the sum of the discounted rewards it receives over\r
the future is maximized. In particular, it chooses At to maximize the expected discounted\r
return:\r
Gt\r
.\r
= Rt+1 + Rt+2 + 2Rt+3 + ··· = X1\r
k=0\r
kRt+k+1, (3.8)\r
where  is a parameter, 0    1, called the discount rate.\r
The discount rate determines the present value of future rewards: a reward received\r
k time steps in the future is worth only k1 times what it would be worth if it were\r
received immediately. If  < 1, the infinite sum in (3.8) has a finite value as long as the\r
reward sequence {Rk} is bounded. If  = 0, the agent is “myopic” in being concerned\r
only with maximizing immediate rewards: its objective in this case is to learn how to\r
choose At so as to maximize only Rt+1. If each of the agent’s actions happened to\r
influence only the immediate reward, not future rewards as well, then a myopic agent\r
could maximize (3.8) by separately maximizing each immediate reward. But in general,\r
acting to maximize immediate reward can reduce access to future rewards so that the\r
return is reduced. As  approaches 1, the return objective takes future rewards into\r
account more strongly; the agent becomes more farsighted.\r
Returns at successive time steps are related to each other in a way that is important\r
for the theory and algorithms of reinforcement learning:\r
Gt\r
.\r
= Rt+1 + Rt+2 + 2Rt+3 + 3Rt+4 + ···\r
= Rt+1 + \r
\r
Rt+2 + Rt+3 + 2Rt+4 + ··· \r
= Rt+1 + Gt+1 (3.9)\r
Note that this works for all time steps t<T, even if termination occurs at t + 1, provided\r
we define GT = 0. This often makes it easy to compute returns from reward sequences.\r
Note that although the return (3.8) is a sum of an infinite number of terms, it is still\r
finite if the reward is nonzero and constant—if  < 1. For example, if the reward is a\r
constant +1, then the return is\r
Gt = X1\r
k=0\r
k = 1\r
1   . (3.10)\r
Exercise 3.5 The equations in Section 3.1 are for the continuing case and need to be\r
modified (very slightly) to apply to episodic tasks. Show that you know the modifications\r
needed by giving the modified version of (3.3). ⇤

56 Chapter 3: Finite Markov Decision Processes\r
Example 3.4: Pole-Balancing\r
The objective in this task is to apply\r
forces to a cart moving along a track\r
so as to keep a pole hinged to the cart\r
from falling over: A failure is said to\r
occur if the pole falls past a given angle\r
from vertical or if the cart runs o↵ the\r
track. The pole is reset to vertical\r
after each failure. This task could be\r
treated as episodic, where the natural\r
episodes are the repeated attempts to balance the pole. The reward in this case could be\r
+1 for every time step on which failure did not occur, so that the return at each time\r
would be the number of steps until failure. In this case, successful balancing forever would\r
mean a return of infinity. Alternatively, we could treat pole-balancing as a continuing\r
task, using discounting. In this case the reward would be 1 on each failure and zero at\r
all other times. The return at each time would then be related to K1, where K is\r
the number of time steps before failure (as well as to the times of later failures). In either\r
case, the return is maximized by keeping the pole balanced for as long as possible.\r
Exercise 3.6 Suppose you treated pole-balancing as an episodic task but also used\r
discounting, with all rewards zero except for 1 upon failure. What then would the\r
return be at each time? How does this return di↵er from that in the discounted, continuing\r
formulation of this task? ⇤\r
Exercise 3.7 Imagine that you are designing a robot to run a maze. You decide to give it a\r
reward of +1 for escaping from the maze and a reward of zero at all other times. The task\r
seems to break down naturally into episodes—the successive runs through the maze—so\r
you decide to treat it as an episodic task, where the goal is to maximize expected total\r
reward (3.7). After running the learning agent for a while, you find that it is showing\r
no improvement in escaping from the maze. What is going wrong? Have you e↵ectively\r
communicated to the agent what you want it to achieve? ⇤\r
Exercise 3.8 Suppose  = 0.5 and the following sequence of rewards is received R1 = 1,\r
R2 = 2, R3 = 6, R4 = 3, and R5 = 2, with T = 5. What are G0, G1, ..., G5? Hint:\r
Work backwards. ⇤\r
Exercise 3.9 Suppose  = 0.9 and the reward sequence is R1 = 2 followed by an infinite\r
sequence of 7s. What are G1 and G0? ⇤\r
Exercise 3.10 Prove the second equality in (3.10). ⇤

3.4. Unified Notation for Episodic and Continuing Tasks 57\r
3.4 Unified Notation for Episodic and Continuing Tasks\r
In the preceding section we described two kinds of reinforcement learning tasks, one\r
in which the agent–environment interaction naturally breaks down into a sequence of\r
separate episodes (episodic tasks), and one in which it does not (continuing tasks). The\r
former case is mathematically easier because each action a↵ects only the finite number of\r
rewards subsequently received during the episode. In this book we consider sometimes\r
one kind of problem and sometimes the other, but often both. It is therefore useful to\r
establish one notation that enables us to talk precisely about both cases simultaneously.\r
To be precise about episodic tasks requires some additional notation. Rather than one\r
long sequence of time steps, we need to consider a series of episodes, each of which consists\r
of a finite sequence of time steps. We number the time steps of each episode starting\r
anew from zero. Therefore, we have to refer not just to St, the state representation at\r
time t, but to St,i, the state representation at time t of episode i (and similarly for At,i,\r
Rt,i, ⇡t,i, Ti, etc.). However, it turns out that when we discuss episodic tasks we almost\r
never have to distinguish between di↵erent episodes. We are almost always considering\r
a particular episode, or stating something that is true for all episodes. Accordingly, in\r
practice we almost always abuse notation slightly by dropping the explicit reference to\r
episode number. That is, we write St to refer to St,i, and so on.\r
We need one other convention to obtain a single notation that covers both episodic\r
and continuing tasks. We have defined the return as a sum over a finite number of terms\r
in one case (3.7) and as a sum over an infinite number of terms in the other (3.8). These\r
two can be unified by considering episode termination to be the entering of a special\r
absorbing state that transitions only to itself and that generates only rewards of zero. For\r
example, consider the state transition diagram:\r
R1 = +1 S0 S1R2 = +1 S2\r
R3 = +1 R4 = 0\r
R5 = 0. . .\r
Here the solid square represents the special absorbing state corresponding to the end of an\r
episode. Starting from S0, we get the reward sequence +1, +1, +1, 0, 0, 0,.... Summing\r
these, we get the same return whether we sum over the first T rewards (here T = 3) or\r
over the full infinite sequence. This remains true even if we introduce discounting. Thus,\r
we can define the return, in general, according to (3.8), using the convention of omitting\r
episode numbers when they are not needed, and including the possibility that  = 1 if the\r
sum remains defined (e.g., because all episodes terminate). Alternatively, we can write\r
Gt\r
.\r
= X\r
T\r
k=t+1\r
kt1Rk, (3.11)\r
including the possibility that T = 1 or  = 1 (but not both). We use these conventions\r
throughout the rest of the book to simplify notation and to express the close parallels

58 Chapter 3: Finite Markov Decision Processes\r
between episodic and continuing tasks. (Later, in Chapter 10, we will introduce a\r
formulation that is both continuing and undiscounted.)\r
3.5 Policies and Value Functions\r
Almost all reinforcement learning algorithms involve estimating value functions—functions\r
of states (or of state–action pairs) that estimate how good it is for the agent to be in a\r
given state (or how good it is to perform a given action in a given state). The notion\r
of “how good” here is defined in terms of future rewards that can be expected, or, to\r
be precise, in terms of expected return. Of course the rewards the agent can expect to\r
receive in the future depend on what actions it will take. Accordingly, value functions\r
are defined with respect to particular ways of acting, called policies.\r
Formally, a policy is a mapping from states to probabilities of selecting each possible\r
action. If the agent is following policy ⇡ at time t, then ⇡(a|s) is the probability that\r
At = a if St = s. Like p, ⇡ is an ordinary function; the “|” in the middle of ⇡(a|s)\r
merely reminds us that it defines a probability distribution over a 2 A(s) for each s 2 S.\r
Reinforcement learning methods specify how the agent’s policy is changed as a result of\r
its experience.\r
Exercise 3.11 If the current state is St, and actions are selected according to a stochastic\r
policy ⇡, then what is the expectation of Rt+1 in terms of ⇡ and the four-argument\r
function p (3.2)? ⇤\r
The value function of a state s under a policy ⇡, denoted v⇡(s), is the expected return\r
when starting in s and following ⇡ thereafter. For MDPs, we can define v⇡ formally by\r
v⇡(s) .= E⇡[Gt | St =s] = E⇡\r
"\r
X1\r
k=0\r
kRt+k+1\r
\r
\r
\r
\r
\r
St =s\r
#\r
, for all s 2 S, (3.12)\r
where E⇡[·] denotes the expected value of a random variable given that the agent follows\r
policy ⇡, and t is any time step. Note that the value of the terminal state, if any, is\r
always zero. We call the function v⇡ the state-value function for policy ⇡.\r
Similarly, we define the value of taking action a in state s under a policy ⇡, denoted\r
q⇡(s, a), as the expected return starting from s, taking the action a, and thereafter\r
following policy ⇡:\r
q⇡(s, a) .= E⇡[Gt | St =s, At = a] = E⇡\r
"\r
X1\r
k=0\r
kRt+k+1\r
\r
\r
\r
\r
\r
St =s, At =a\r
#\r
. (3.13)\r
We call q⇡ the action-value function for policy ⇡.\r
Exercise 3.12 Give an equation for v⇡ in terms of q⇡ and ⇡. ⇤\r
Exercise 3.13 Give an equation for q⇡ in terms of v⇡ and the four-argument p. ⇤\r
The value functions v⇡ and q⇡ can be estimated from experience. For example, if an\r
agent follows policy ⇡ and maintains an average, for each state encountered, of the actual\r
returns that have followed that state, then the average will converge to the state’s value,\r
v⇡(s), as the number of times that state is encountered approaches infinity. If separate

3.5. Policies and Value Functions 59\r
averages are kept for each action taken in each state, then these averages will similarly\r
converge to the action values, q⇡(s, a). We call estimation methods of this kind Monte\r
Carlo methods because they involve averaging over many random samples of actual returns.\r
These kinds of methods are presented in Chapter 5. Of course, if there are very many\r
states, then it may not be practical to keep separate averages for each state individually.\r
Instead, the agent would have to maintain v⇡ and q⇡ as parameterized functions (with\r
fewer parameters than states) and adjust the parameters to better match the observed\r
returns. This can also produce accurate estimates, although much depends on the nature\r
of the parameterized function approximator. These possibilities are discussed in Part II\r
of the book.\r
A fundamental property of value functions used throughout reinforcement learning and\r
dynamic programming is that they satisfy recursive relationships similar to that which\r
we have already established for the return (3.9). For any policy ⇡ and any state s, the\r
following consistency condition holds between the value of s and the value of its possible\r
successor states:\r
v⇡(s) .= E⇡[Gt | St =s]\r
= E⇡[Rt+1 + Gt+1 | St =s] (by (3.9))\r
= X\r
a\r
⇡(a|s)\r
X\r
s0\r
X\r
r\r
p(s0, r|s, a)\r
h\r
r + E⇡[Gt+1|St+1 =s0]\r
i\r
= X\r
a\r
⇡(a|s)\r
X\r
s0,r\r
p(s0, r|s, a)\r
h\r
r + v⇡(s0)\r
i\r
, for all s 2 S, (3.14)\r
where it is implicit that the actions, a, are taken from the set A(s), that the next states,\r
s0, are taken from the set S (or from S+ in the case of an episodic problem), and that\r
the rewards, r, are taken from the set R. Note also how in the last equation we have\r
merged the two sums, one over all the values of s0 and the other over all the values of r,\r
into one sum over all the possible values of both. We use this kind of merged sum often\r
to simplify formulas. Note how the final expression can be read easily as an expected\r
value. It is really a sum over all values of the three variables, a, s0, and r. For each triple,\r
we compute its probability, ⇡(a|s)p(s0, r|s, a), weight the quantity in brackets by that\r
probability, then sum over all possibilities to get an expected value.\r
⇡\r
s\r
s0\r
⇡\r
r p\r
a\r
Backup diagram for v⇡\r
Equation (3.14) is the Bellman equation for v⇡. It expresses\r
a relationship between the value of a state and the values of\r
its successor states. Think of looking ahead from a state to its\r
possible successor states, as suggested by the diagram to the\r
right. Each open circle represents a state and each solid circle\r
represents a state–action pair. Starting from state s, the root\r
node at the top, the agent could take any of some set of actions—\r
three are shown in the diagram—based on its policy ⇡. From\r
each of these, the environment could respond with one of several next states, s0 (two are\r
shown in the figure), along with a reward, r, depending on its dynamics given by the\r
function p. The Bellman equation (3.14) averages over all the possibilities, weighting each\r
by its probability of occurring. It states that the value of the start state must equal the\r
(discounted) value of the expected next state, plus the reward expected along the way.

60 Chapter 3: Finite Markov Decision Processes\r
The value function v⇡ is the unique solution to its Bellman equation. We show in\r
subsequent chapters how this Bellman equation forms the basis of a number of ways to\r
compute, approximate, and learn v⇡. We call diagrams like that above backup diagrams\r
because they diagram relationships that form the basis of the update or backup operations\r
that are at the heart of reinforcement learning methods. These operations transfer\r
value information back to a state (or a state–action pair) from its successor states (or\r
state–action pairs). We use backup diagrams throughout the book to provide graphical\r
summaries of the algorithms we discuss. (Note that, unlike transition graphs, the state\r
nodes of backup diagrams do not necessarily represent distinct states; for example, a\r
state might be its own successor.)\r
Example 3.5: Gridworld Figure 3.2 (left) shows a rectangular gridworld representation\r
of a simple finite MDP. The cells of the grid correspond to the states of the environment. At\r
each cell, four actions are possible: north, south, east, and west, which deterministically\r
cause the agent to move one cell in the respective direction on the grid. Actions that\r
would take the agent o↵ the grid leave its location unchanged, but also result in a reward\r
of 1. Other actions result in a reward of 0, except those that move the agent out of the\r
special states A and B. From state A, all four actions yield a reward of +10 and take the\r
agent to A0. From state B, all actions yield a reward of +5 and take the agent to B0.\r
the states of the environment. At each cell, four actions are possible: north,\r
south, east, and west, which deterministically cause the agent to move one\r
cell in the respective direction on the grid. Actions that would take the agent\r
o the grid leave its location unchanged, but also result in a reward of 1.\r
Other actions result in a reward of 0, except those that move the agent out\r
of the special states A and B. From state A, all four actions yield a reward of\r
+10 and take the agent to A. From state B, all actions yield a reward of +5\r
and take the agent to B.\r
Suppose the agent selects all four actions with equal probability in all\r
states. Figure 3.5b shows the value function, v⇡, for this policy, for the dis\u0002counted reward case with  = 0.9. This value function was computed by solv\u0002ing the system of equations (3.10). Notice the negative values near the lower\r
edge; these are the result of the high probability of hitting the edge of the grid\r
there under the random policy. State A is the best state to be in under this pol\u0002icy, but its expected return is less than 10, its immediate reward, because from\r
A the agent is taken to A, from which it is likely to run into the edge of the\r
grid. State B, on the other hand, is valued more than 5, its immediate reward,\r
because from B the agent is taken to B, which has a positive value. From B the\r
expected penalty (negative reward) for possibly running into an edge is more\r
3.3 8.8 4.4 5.3 1.5\r
1.5 3.0 2.3 1.9 0.5\r
0.1 0.7 0.7 0.4 -0.4\r
-1.0 -0.4 -0.4 -0.6 -1.2\r
-1.9 -1.3 -1.2 -1.4 -2.0\r
A B\r
A'\r
+10 B'\r
+5\r
Actions\r
(a) (b)\r
Figure 3.5: Grid example: (a) exceptional reward dynamics; (b) state-value\r
function for the equiprobable random policy.\r
Figure 3.2: Gridworld example: exceptional reward dynamics (left) and state-value function\r
for the equiprobable random policy (right).\r
Suppose the agent selects all four actions with equal probability in all states. Figure 3.2\r
(right) shows the value function, v⇡, for this policy, for the discounted reward case with\r
 = 0.9. This value function was computed by solving the system of linear equations\r
(3.14). Notice the negative values near the lower edge; these are the result of the high\r
probability of hitting the edge of the grid there under the random policy. State A is\r
the best state to be in under this policy. Note that A’s expected return is less than its\r
immediate reward of 10, because from A the agent is taken to state A0 from which it is\r
likely to run into the edge of the grid. State B, on the other hand, is valued more than\r
its immediate reward of 5, because from B the agent is taken to B0 which has a positive\r
value. From B0 the expected penalty (negative reward) for possibly running into an edge\r
is more than compensated for by the expected gain for possibly stumbling onto A or B.\r
Exercise 3.14 The Bellman equation (3.14) must hold for each state for the value function\r
v⇡ shown in Figure 3.2 (right) of Example 3.5. Show numerically that this equation holds\r
for the center state, valued at +0.7, with respect to its four neighboring states, valued at\r
+2.3, +0.4, 0.4, and +0.7. (These numbers are accurate only to one decimal place.) ⇤

3.5. Policies and Value Functions 61\r
Exercise 3.15 In the gridworld example, rewards are positive for goals, negative for\r
running into the edge of the world, and zero the rest of the time. Are the signs of these\r
rewards important, or only the intervals between them? Prove, using (3.8), that adding a\r
constant c to all the rewards adds a constant, vc, to the values of all states, and thus\r
does not a↵ect the relative values of any states under any policies. What is vc in terms\r
of c and ? ⇤\r
Exercise 3.16 Now consider adding a constant c to all the rewards in an episodic task,\r
such as maze running. Would this have any e↵ect, or would it leave the task unchanged\r
as in the continuing task above? Why or why not? Give an example. ⇤\r
Example 3.6: Golf To formulate playing a hole of golf as a reinforcement learning\r
task, we count a penalty (negative reward) of 1 for each stroke until we hit the ball\r
into the hole. The state is the location of the ball. The value of a state is the negative of\r
the number of strokes to the hole from that location. Our actions are how we aim and\r
swing at the ball, of course, and which club we select. Let us take the former as given\r
and consider just the choice of club, which we assume is either a putter or a driver. The\r
upper part of Figure 3.3 shows a possible state-value function, vputt(s), for the policy that\r
Q*(s,driver)\r
Vputt\r
s a n d\r
green\r
!1\r
s a\r
n\r
d\r
!2 !2 !3\r
!4\r
!1\r
!5\r
!6\r
!4\r
!3\r
!3 !2\r
!4\r
s a n d\r
green\r
!1\r
s a\r
n\r
d\r
!2\r
!3\r
!2\r
0\r
0\r
!"\r
!"\r
vputt\r
q⇤(s, driver)\r
Figure 3.3: A golf example: the state-value func\u0002tion for putting (upper) and the optimal action\u0002value function for using the driver (lower).\r
always uses the putter. The terminal\r
state in-the-hole has a value of 0. From\r
anywhere on the green we assume we can\r
make a putt; these states have value 1.\r
O↵ the green we cannot reach the hole by\r
putting, and the value is lower. If we can\r
reach the green from a state by putting,\r
then that state must have value one less\r
than the green’s value, that is, 2. For\r
simplicity, let us assume we can putt very\r
precisely and deterministically, but with\r
a limited range. This gives us the sharp\r
contour line labeled 2 in the figure; all\r
locations between that line and the green\r
require exactly two strokes to complete\r
the hole. Similarly, any location within\r
putting range of the 2 contour line must\r
have a value of 3, and so on to get all the\r
contour lines shown in the figure. Putting\r
doesn’t get us out of sand traps, so they\r
have a value of 1. Overall, it takes us\r
six strokes to get from the tee to the hole\r
by putting.\r
r\r
s0\r
s, a\r
a0\r
⇡\r
p\r
q⇡ backup diagram\r
Exercise 3.17 What is the Bellman equation for action values, that\r
is, for q⇡? It must give the action value q⇡(s, a) in terms of the action\r
values, q⇡(s0, a0), of possible successors to the state–action pair (s, a).\r
Hint: The backup diagram to the right corresponds to this equation.\r
Show the sequence of equations analogous to (3.14), but for action\r
values. ⇤

62 Chapter 3: Finite Markov Decision Processes\r
Exercise 3.18 The value of a state depends on the values of the actions possible in that\r
state and on how likely each action is to be taken under the current policy. We can\r
think of this in terms of a small backup diagram rooted at the state and considering each\r
possible action:\r
s\r
taken with\r
probability ⇡(a|s)\r
v⇡(s)\r
q⇡(s, a)\r
a1 a2 a3\r
Give the equation corresponding to this intuition and diagram for the value at the root\r
node, v⇡(s), in terms of the value at the expected leaf node, q⇡(s, a), given St = s. This\r
equation should include an expectation conditioned on following the policy, ⇡. Then give\r
a second equation in which the expected value is written out explicitly in terms of ⇡(a|s)\r
such that no expected value notation appears in the equation. ⇤\r
Exercise 3.19 The value of an action, q⇡(s, a), depends on the expected next reward and\r
the expected sum of the remaining rewards. Again we can think of this in terms of a\r
small backup diagram, this one rooted at an action (state–action pair) and branching to\r
the possible next states:\r
s, a q⇡(s, a)\r
s0\r
3 s0\r
2 s01\r
r1 r2 r3 v⇡(s0\r
)\r
expected\r
rewards\r
Give the equation corresponding to this intuition and diagram for the action value,\r
q⇡(s, a), in terms of the expected next reward, Rt+1, and the expected next state value,\r
v⇡(St+1), given that St =s and At =a. This equation should include an expectation but\r
not one conditioned on following the policy. Then give a second equation, writing out the\r
expected value explicitly in terms of p(s0, r|s, a) defined by (3.2), such that no expected\r
value notation appears in the equation. ⇤\r
3.6 Optimal Policies and Optimal Value Functions\r
Solving a reinforcement learning task means, roughly, finding a policy that achieves a lot\r
of reward over the long run. For finite MDPs, we can precisely define an optimal policy\r
in the following way. Value functions define a partial ordering over policies. A policy ⇡ is\r
defined to be better than or equal to a policy ⇡0 if its expected return is greater than\r
or equal to that of ⇡0 for all states. In other words, ⇡  ⇡0 if and only if v⇡(s)  v⇡0 (s)\r
for all s 2 S. There is always at least one policy that is better than or equal to all other\r
policies. This is an optimal policy. Although there may be more than one, we denote all\r
the optimal policies by ⇡⇤. They share the same state-value function, called the optimal\r
state-value function, denoted v⇤, and defined as\r
v⇤(s) .= max ⇡ v⇡(s), (3.15)\r
for all s 2 S.

3.6. Optimal Policies and Optimal Value Functions 63\r
Optimal policies also share the same optimal action-value function, denoted q⇤, and\r
defined as\r
q⇤(s, a) .= max ⇡ q⇡(s, a), (3.16)\r
for all s 2 S and a 2 A(s). For the state–action pair (s, a), this function gives the\r
expected return for taking action a in state s and thereafter following an optimal policy.\r
Thus, we can write q⇤ in terms of v⇤ as follows:\r
q⇤(s, a) = E[Rt+1 + v⇤(St+1) | St =s, At =a] . (3.17)\r
Example 3.7: Optimal Value Functions for Golf The lower part of Figure 3.3\r
shows the contours of a possible optimal action-value function q⇤(s, driver). These are\r
the values of each state if we first play a stroke with the driver and afterward select either\r
the driver or the putter, whichever is better. The driver enables us to hit the ball farther,\r
but with less accuracy. We can reach the hole in one shot using the driver only if we\r
are already very close; thus the 1 contour for q⇤(s, driver) covers only a small portion\r
of the green. If we have two strokes, however, then we can reach the hole from much\r
farther away, as shown by the 2 contour. In this case we don’t have to drive all the way\r
to within the small 1 contour, but only to anywhere on the green; from there we can\r
use the putter. The optimal action-value function gives the values after committing to a\r
particular first action, in this case, to the driver, but afterward using whichever actions\r
are best. The 3 contour is still farther out and includes the starting tee. From the tee,\r
the best sequence of actions is two drives and one putt, sinking the ball in three strokes.\r
Because v⇤ is the value function for a policy, it must satisfy the self-consistency\r
condition given by the Bellman equation for state values (3.14). Because it is the optimal\r
value function, however, v⇤’s consistency condition can be written in a special form\r
without reference to any specific policy. This is the Bellman equation for v⇤, or the\r
Bellman optimality equation. Intuitively, the Bellman optimality equation expresses the\r
fact that the value of a state under an optimal policy must equal the expected return for\r
the best action from that state:\r
v⇤(s) = max a2A(s)q⇡⇤ (s, a)\r
= maxa E⇡⇤[Gt | St =s, At =a]\r
= maxa E⇡⇤[Rt+1 + Gt+1 | St =s, At =a] (by (3.9))\r
= maxa E[Rt+1 + v⇤(St+1) | St =s, At =a] (3.18)\r
= maxa\r
X\r
s0,r\r
p(s0, r|s, a)\r
⇥\r
r + v⇤(s0)\r
⇤\r
. (3.19)

64 Chapter 3: Finite Markov Decision Processes\r
The last two equations are two forms of the Bellman optimality equation for v⇤. The\r
Bellman optimality equation for q⇤ is\r
q⇤(s, a) = E\r
h\r
Rt+1 +  max\r
a0 q⇤(St+1, a0\r
)\r
\r
\r
\r
St = s, At = a\r
i\r
= X\r
s0,r\r
p(s0, r|s, a)\r
h\r
r +  max\r
a0 q⇤(s0\r
, a0)\r
i\r
. (3.20)\r
The backup diagrams in the figure below show graphically the spans of future states\r
and actions considered in the Bellman optimality equations for v⇤ and q⇤. These are the\r
same as the backup diagrams for v⇡ and q⇡ presented earlier except that arcs have been\r
added at the agent’s choice points to represent that the maximum over that choice is\r
taken rather than the expected value given some policy. The backup diagram on the left\r
graphically represents the Bellman optimality equation (3.19) and the backup diagram\r
on the right graphically represents (3.20).\r
s\r
s0\r
a\r
r\r
r\r
s0\r
s, a\r
a0\r
max\r
max\r
(v⇤) (q⇤)\r
Figure 3.4: Backup diagrams for v⇤ and q⇤\r
For finite MDPs, the Bellman optimality equation for v⇤ (3.19) has a unique solution.\r
The Bellman optimality equation is actually a system of equations, one for each state, so\r
if there are n states, then there are n equations in n unknowns. If the dynamics p of the\r
environment are known, then in principle one can solve this system of equations for v⇤\r
using any one of a variety of methods for solving systems of nonlinear equations. One\r
can solve a related set of equations for q⇤.\r
Once one has v⇤, it is relatively easy to determine an optimal policy. For each state\r
s, there will be one or more actions at which the maximum is obtained in the Bellman\r
optimality equation. Any policy that assigns nonzero probability only to these actions is\r
an optimal policy. You can think of this as a one-step search. If you have the optimal\r
value function, v⇤, then the actions that appear best after a one-step search will be optimal\r
actions. Another way of saying this is that any policy that is greedy with respect to the\r
optimal evaluation function v⇤ is an optimal policy. The term greedy is used in computer\r
science to describe any search or decision procedure that selects alternatives based only\r
on local or immediate considerations, without considering the possibility that such a\r
selection may prevent future access to even better alternatives. Consequently, it describes\r
policies that select actions based only on their short-term consequences. The beauty of v⇤\r
is that if one uses it to evaluate the short-term consequences of actions—specifically, the\r
one-step consequences—then a greedy policy is actually optimal in the long-term sense in\r
which we are interested because v⇤ already takes into account the reward consequences of\r
all possible future behavior. By means of v⇤, the optimal expected long-term return is

3.6. Optimal Policies and Optimal Value Functions 65\r
turned into a quantity that is locally and immediately available for each state. Hence, a\r
one-step-ahead search yields the long-term optimal actions.\r
Having q⇤ makes choosing optimal actions even easier. With q⇤, the agent does not\r
even have to do a one-step-ahead search: for any state s, it can simply find any action\r
that maximizes q⇤(s, a). The action-value function e↵ectively caches the results of all\r
one-step-ahead searches. It provides the optimal expected long-term return as a value\r
that is locally and immediately available for each state–action pair. Hence, at the cost of\r
representing a function of state–action pairs, instead of just of states, the optimal action\u0002value function allows optimal actions to be selected without having to know anything\r
about possible successor states and their values, that is, without having to know anything\r
about the environment’s dynamics.\r
Example 3.8: Solving the Gridworld Suppose we solve the Bellman equation for v⇤\r
for the simple grid task introduced in Example 3.5 and shown again in Figure 3.5 (left).\r
Recall that state A is followed by a reward of +10 and transition to state A0, while state\r
B is followed by a reward of +5 and transition to state B0. Figure 3.5 (middle) shows the\r
optimal value function, and Figure 3.5 (right) shows the corresponding optimal policies.\r
Where there are multiple arrows in a cell, all of the corresponding actions are optimal.\r
a) gridworld b) V* c) !*\r
22.0 24.4 22.0 19.4 17.5\r
19.8 22.0 19.8 17.8 16.0\r
17.8 19.8 17.8 16.0 14.4\r
16.0 17.8 16.0 14.4 13.0\r
14.4 16.0 14.4 13.0 11.7\r
A B\r
A'\r
+10 B'\r
+5\r
v* π* Gridworld v⇤ ⇡⇤\r
Figure 3.5: Optimal solutions to the gridworld example.\r
Example 3.9: Bellman Optimality Equations for the Recycling Robot Using\r
(3.19), we can explicitly give the Bellman optimality equation for the recycling robot\r
example. To make things more compact, we abbreviate the states high and low, and the\r
actions search, wait, and recharge respectively by h, l, s, w, and re. Because there are\r
only two states, the Bellman optimality equation consists of two equations. The equation\r
for v⇤(h) can be written as follows:\r
v⇤(h) = max ⇢ p(h|h, s)[r(h, s, h) + v⇤(h)] + p(l|h, s)[r(h, s, l) + v⇤(l)],\r
p(h|h, w)[r(h, w, h) + v⇤(h)] + p(l|h, w)[r(h, w, l) + v⇤(l)] \r
= max ⇢ ↵[rs + v⇤(h)] + (1  ↵)[rs + v⇤(l)],\r
1[rw + v⇤(h)] + 0[rw + v⇤(l)] \r
= max ⇢ rs + [↵v⇤(h) + (1  ↵)v⇤(l)],\r
rw + v⇤(h)\r
\r
.

66 Chapter 3: Finite Markov Decision Processes\r
Following the same procedure for v⇤(l) yields the equation\r
v⇤(l) = max\r
8\r
<\r
:\r
rs  3(1  ) + [(1  )v⇤(h) + v⇤(l)],\r
rw + v⇤(l),\r
v⇤(h)\r
9\r
=\r
; .\r
For any choice of rs, rw, ↵, , and , with 0   < 1, 0  ↵,   1, there is exactly\r
one pair of numbers, v⇤(h) and v⇤(l), that simultaneously satisfy these two nonlinear\r
equations.\r
Explicitly solving the Bellman optimality equation provides one route to finding an\r
optimal policy, and thus to solving the reinforcement learning problem. However, this\r
solution is rarely directly useful. It is akin to an exhaustive search, looking ahead at\r
all possibilities, computing their probabilities of occurrence and their desirabilities in\r
terms of expected rewards. This solution relies on at least three assumptions that are\r
rarely true in practice: (1) the dynamics of the environment are accurately known; (2)\r
computational resources are sucient to complete the calculation; and (3) the states\r
have the Markov property. For the kinds of tasks in which we are interested, one is\r
generally not able to implement this solution exactly because various combinations of\r
these assumptions are violated. For example, although the first and third assumptions\r
present no problems for the game of backgammon, the second is a major impediment.\r
Because the game has about 1020 states, it would take thousands of years on today’s\r
fastest computers to solve the Bellman equation for v⇤, and the same is true for finding\r
q⇤. In reinforcement learning one typically has to settle for approximate solutions.\r
Many di↵erent decision-making methods can be viewed as ways of approximately\r
solving the Bellman optimality equation. For example, heuristic search methods can be\r
viewed as expanding the right-hand side of (3.19) several times, up to some depth, forming\r
a “tree” of possibilities, and then using a heuristic evaluation function to approximate\r
v⇤ at the “leaf” nodes. (Heuristic search methods such as A⇤ are almost always based\r
on the episodic case.) The methods of dynamic programming can be related even more\r
closely to the Bellman optimality equation. Many reinforcement learning methods can\r
be clearly understood as approximately solving the Bellman optimality equation, using\r
actual experienced transitions in place of knowledge of the expected transitions. We\r
consider a variety of such methods in the following chapters.\r
Exercise 3.20 Draw or describe the optimal state-value function for the golf example. ⇤\r
Exercise 3.21 Draw or describe the contours of the optimal action-value function for\r
putting, q⇤(s, putter), for the golf example. ⇤\r
0 +2 +1 0\r
left right\r
Exercise 3.22 Consider the continuing MDP shown to the\r
right. The only decision to be made is that in the top state,\r
where two actions are available, left and right. The numbers\r
show the rewards that are received deterministically after\r
each action. There are exactly two deterministic policies,\r
⇡left and ⇡right. What policy is optimal if  = 0? If  = 0.9?\r
If  = 0.5? ⇤

3.7. Optimality and Approximation 67\r
Exercise 3.23 Give the Bellman equation for q⇤ for the recycling robot. ⇤\r
Exercise 3.24 Figure 3.5 gives the optimal value of the best state of the gridworld as\r
24.4, to one decimal place. Use your knowledge of the optimal policy and (3.8) to express\r
this value symbolically, and then to compute it to three decimal places. ⇤\r
Exercise 3.25 Give an equation for v⇤ in terms of q⇤. ⇤\r
Exercise 3.26 Give an equation for q⇤ in terms of v⇤ and the four-argument p. ⇤\r
Exercise 3.27 Give an equation for ⇡⇤ in terms of q⇤. ⇤\r
Exercise 3.28 Give an equation for ⇡⇤ in terms of v⇤ and the four-argument p. ⇤\r
Exercise 3.29 Rewrite the four Bellman equations for the four value functions (v⇡, v⇤, q⇡,\r
and q⇤) in terms of the three argument function p (3.4) and the two-argument function r\r
(3.5). ⇤\r
3.7 Optimality and Approximation\r
We have defined optimal value functions and optimal policies. Clearly, an agent that\r
learns an optimal policy has done very well, but in practice this rarely happens. For\r
the kinds of tasks in which we are interested, optimal policies can be generated only\r
with extreme computational cost. A well-defined notion of optimality organizes the\r
approach to learning we describe in this book and provides a way to understand the\r
theoretical properties of various learning algorithms, but it is an ideal that agents can\r
only approximate. As we discussed above, even if we have a complete and accurate model\r
of the environment’s dynamics, it is usually not possible to simply compute an optimal\r
policy by solving the Bellman optimality equation. For example, board games such as\r
chess are a tiny fraction of human experience, yet large, custom-designed computers still\r
cannot compute the optimal moves. A critical aspect of the problem facing the agent is\r
always the computational power available to it, in particular, the amount of computation\r
it can perform in a single time step.\r
The memory available is also an important constraint. A large amount of memory\r
is often required to build up approximations of value functions, policies, and models.\r
In tasks with small, finite state sets, it is possible to form these approximations using\r
arrays or tables with one entry for each state (or state–action pair). This we call the\r
tabular case, and the corresponding methods we call tabular methods. In many cases\r
of practical interest, however, there are far more states than could possibly be entries\r
in a table. In these cases the functions must be approximated, using some sort of more\r
compact parameterized function representation.\r
Our framing of the reinforcement learning problem forces us to settle for approxi\u0002mations. However, it also presents us with some unique opportunities for achieving\r
useful approximations. For example, in approximating optimal behavior, there may be\r
many states that the agent faces with such a low probability that selecting suboptimal\r
actions for them has little impact on the amount of reward the agent receives. Tesauro’s\r
backgammon player, for example, plays with exceptional skill even though it might make

68 Chapter 3: Finite Markov Decision Processes\r
very bad decisions on board configurations that never occur in games against experts. In\r
fact, it is possible that TD-Gammon makes bad decisions for a large fraction of the game’s\r
state set. The online nature of reinforcement learning makes it possible to approximate\r
optimal policies in ways that put more e↵ort into learning to make good decisions for\r
frequently encountered states, at the expense of less e↵ort for infrequently encountered\r
states. This is one key property that distinguishes reinforcement learning from other\r
approaches to approximately solving MDPs.\r
3.8 Summary\r
Let us summarize the elements of the reinforcement learning problem that we have\r
presented in this chapter. Reinforcement learning is about learning from interaction\r
how to behave in order to achieve a goal. The reinforcement learning agent and its\r
environment interact over a sequence of discrete time steps. The specification of their\r
interface defines a particular task: the actions are the choices made by the agent; the\r
states are the basis for making the choices; and the rewards are the basis for evaluating\r
the choices. Everything inside the agent is known and controllable. Its environment, on\r
the other hand, is incompletely controllable and may or may not be completely known.\r
A policy is a stochastic rule by which the agent selects actions as a function of states.\r
The agent’s objective is to maximize the amount of reward it receives over time.\r
When the reinforcement learning setup described above is formulated with well defined\r
transition probabilities it constitutes a Markov decision process (MDP). A finite MDP is\r
an MDP with finite state, action, and (as we formulate it here) reward sets. Much of the\r
current theory of reinforcement learning is restricted to finite MDPs, but the methods\r
and ideas apply more generally.\r
The return is the function of future rewards that the agent seeks to maximize (in\r
expected value). It has several di↵erent definitions depending upon the nature of the\r
task and whether one wishes to discount delayed reward. The undiscounted formulation\r
is appropriate for episodic tasks, in which the agent–environment interaction breaks\r
naturally into episodes; the discounted formulation is appropriate for tabular continuing\r
tasks, in which the interaction does not naturally break into episodes but continues\r
without limit (but see Sections 10.3–4). We try to define the returns for the two kinds of\r
tasks such that one set of equations can apply to both the episodic and continuing cases.\r
A policy’s value functions (v⇡ and q⇡) assign to each state, or state–action pair, the\r
expected return from that state, or state–action pair, given that the agent uses the\r
policy. The optimal value functions (v⇤ and q⇤) assign to each state, or state–action pair,\r
the largest expected return achievable by any policy. A policy whose value functions\r
are optimal is an optimal policy. Whereas the optimal value functions for states and\r
state–action pairs are unique for a given MDP, there can be many optimal policies. Any\r
policy that is greedy with respect to the optimal value functions must be an optimal\r
policy. The Bellman optimality equations are special consistency conditions that the\r
optimal value functions must satisfy and that can, in principle, be solved for the optimal\r
value functions, from which an optimal policy can be determined with relative ease.

3.8. Summary 69\r
A reinforcement learning problem can be posed in a variety of di↵erent ways depending\r
on assumptions about the level of knowledge initially available to the agent. In problems\r
of complete knowledge, the agent has a complete and accurate model of the environment’s\r
dynamics. If the environment is an MDP, then such a model consists of the complete four\u0002argument dynamics function p (3.2). In problems of incomplete knowledge, a complete\r
and perfect model of the environment is not available.\r
Even if the agent had a complete and accurate environment model, the agent would\r
typically be unable to fully use it because of limitations on its memory and computation\r
per time step. In particular, extensive memory may be required to build up accurate\r
approximations of value functions, policies, and models. In most cases of practical interest\r
there are far more states than could possibly be entries in a table, and approximations\r
must be made.\r
A well-defined notion of optimality organizes the approach to learning we describe in\r
this book and provides a way to understand the theoretical properties of various learning\r
algorithms, but it is an ideal that reinforcement learning agents can only approximate\r
to varying degrees. In reinforcement learning we are very much concerned with cases in\r
which optimal solutions cannot be found but must be approximated in some way.\r
Bibliographical and Historical Remarks\r
The reinforcement learning problem is deeply indebted to the idea of Markov decision\r
processes (MDPs) from the field of optimal control. These historical influences and other\r
major influences from psychology are described in the brief history given in Chapter 1.\r
Reinforcement learning adds to MDPs a focus on approximation and incomplete infor\u0002mation for realistically large problems. MDPs and the reinforcement learning problem\r
are only weakly linked to traditional learning and decision-making problems in artificial\r
intelligence. However, artificial intelligence is now vigorously exploring MDP formulations\r
for planning and decision making from a variety of perspectives. MDPs are more general\r
than previous formulations used in artificial intelligence in that they permit more general\r
kinds of goals and uncertainty.\r
The theory of MDPs is treated by, for example, Bertsekas (2005), White (1969), Whittle\r
(1982, 1983), and Puterman (1994). A particularly compact treatment of the finite case\r
is given by Ross (1983). MDPs are also studied under the heading of stochastic optimal\r
control, where adaptive optimal control methods are most closely related to reinforcement\r
learning (e.g., Kumar, 1985; Kumar and Varaiya, 1986).\r
The theory of MDPs evolved from e↵orts to understand the problem of making sequences\r
of decisions under uncertainty, where each decision can depend on the previous decisions\r
and their outcomes. It is sometimes called the theory of multistage decision processes,\r
or sequential decision processes, and has roots in the statistical literature on sequential\r
sampling beginning with the papers by Thompson (1933, 1934) and Robbins (1952) that\r
we cited in Chapter 2 in connection with bandit problems (which are prototypical MDPs\r
if formulated as multiple-situation problems).\r
The earliest instance (that we are aware of) in which reinforcement learning was\r
discussed using the MDP formalism is Andreae’s (1969) description of a unified view of

70 Chapter 3: Finite Markov Decision Processes\r
learning machines. Witten and Corbin (1973) experimented with a reinforcement learning\r
system later analyzed by Witten (1977, 1976a) using the MDP formalism. Although\r
he did not explicitly mention MDPs, Werbos (1977) suggested approximate solution\r
methods for stochastic optimal control problems that are related to modern reinforcement\r
learning methods (see also Werbos, 1982, 1987, 1988, 1989, 1992). Although Werbos’s\r
ideas were not widely recognized at the time, they were prescient in emphasizing the\r
importance of approximately solving optimal control problems in a variety of domains,\r
including artificial intelligence. The most influential integration of reinforcement learning\r
and MDPs is due to Watkins (1989).\r
3.1 Our characterization of the dynamics of an MDP in terms of p(s0, r|s, a) is\r
slightly unusual. It is more common in the MDP literature to describe the\r
dynamics in terms of the state transition probabilities p(s0 |s, a) and expected\r
next rewards r(s, a). In reinforcement learning, however, we more often have\r
to refer to individual actual or sample rewards (rather than just their expected\r
values). Our notation also makes it plainer that St and Rt are in general jointly\r
determined, and thus must have the same time index. In teaching reinforcement\r
learning, we have found our notation to be more straightforward conceptually\r
and easier to understand.\r
For a good intuitive discussion of the system-theoretic concept of state, see\r
Minsky (1967).\r
The bioreactor example is based on the work of Ungar (1990) and Miller and\r
Williams (1992). The recycling robot example was inspired by the can-collecting\r
robot built by Jonathan Connell (1989). Kober and Peters (2012) present a\r
collection of robotics applications of reinforcement learning.\r
3.2 An explicit statement of the reward hypothesis was suggested by Michael Littman\r
(personal communication).\r
3.3–4 The terminology of episodic and continuing tasks is di↵erent from that usually\r
used in the MDP literature. In that literature it is common to distinguish\r
three types of tasks: (1) finite-horizon tasks, in which interaction terminates\r
after a particular fixed number of time steps; (2) indefinite-horizon tasks, in\r
which interaction can last arbitrarily long but must eventually terminate; and\r
(3) infinite-horizon tasks, in which interaction does not terminate. Our episodic\r
and continuing tasks are similar to indefinite-horizon and infinite-horizon tasks,\r
respectively, but we prefer to emphasize the di↵erence in the nature of the\r
interaction. This di↵erence seems more fundamental than the di↵erence in the\r
objective functions emphasized by the usual terms. Often episodic tasks use\r
an indefinite-horizon objective function and continuing tasks an infinite-horizon\r
objective function, but we see this as a common coincidence rather than a\r
fundamental di↵erence.\r
The pole-balancing example is from Michie and Chambers (1968) and Barto,\r
Sutton, and Anderson (1983).

3.8. Summary 71\r
3.5–6 Assigning value on the basis of what is good or bad in the long run has ancient\r
roots. In control theory, mapping states to numerical values representing the\r
long-term consequences of control decisions is a key part of optimal control theory,\r
which was developed in the 1950s by extending nineteenth century state-function\r
theories of classical mechanics (see, for example, Schultz and Melsa, 1967). In\r
describing how a computer could be programmed to play chess, Shannon (1950)\r
suggested using an evaluation function that took into account the long-term\r
advantages and disadvantages of chess positions.\r
Watkins’s (1989) Q-learning algorithm for estimating q⇤ (Chapter 6) made action\u0002value functions an important part of reinforcement learning, and consequently\r
these functions are often called “Q-functions.” But the idea of an action-value\r
function is much older than this. Shannon (1950) suggested that a function\r
h(P,M) could be used by a chess-playing program to decide whether a move M\r
in position P is worth exploring. Michie’s (1961, 1963) MENACE system and\r
Michie and Chambers’s (1968) BOXES system can be understood as estimating\r
action-value functions. In classical physics, Hamilton’s principal function is\r
an action-value function; Newtonian dynamics are greedy with respect to this\r
function (e.g., Goldstein, 1957). Action-value functions also played a central role\r
in Denardo’s (1967) theoretical treatment of dynamic programming in terms of\r
contraction mappings.\r
The Bellman optimality equation (for v⇤) was popularized by Richard Bellman\r
(1957a), who called it the “basic functional equation.” The counterpart of the\r
Bellman optimality equation for continuous time and state problems is known\r
as the Hamilton–Jacobi–Bellman equation (or often just the Hamilton–Jacobi\r
equation), indicating its roots in classical physics (e.g., Schultz and Melsa, 1967).\r
The golf example was suggested by Chris Watkins.

Chapter 4\r
Dynamic Programming\r
The term dynamic programming (DP) refers to a collection of algorithms that can be\r
used to compute optimal policies given a perfect model of the environment as a Markov\r
decision process (MDP). Classical DP algorithms are of limited utility in reinforcement\r
learning both because of their assumption of a perfect model and because of their great\r
computational expense, but they are still important theoretically. DP provides an essential\r
foundation for the understanding of the methods presented in the rest of this book. In\r
fact, all of these methods can be viewed as attempts to achieve much the same e↵ect as\r
DP, only with less computation and without assuming a perfect model of the environment.\r
Starting with this chapter, we usually assume that the environment is a finite MDP.\r
That is, we assume that its state, action, and reward sets, S, A, and R, are finite, and\r
that its dynamics are given by a set of probabilities p(s0, r|s, a), for all s 2 S, a 2 A(s),\r
r 2 R, and s0 2 S+ (S+ is S plus a terminal state if the problem is episodic). Although\r
DP ideas can be applied to problems with continuous state and action spaces, exact\r
solutions are possible only in special cases. A common way of obtaining approximate\r
solutions for tasks with continuous states and actions is to quantize the state and action\r
spaces and then apply finite-state DP methods. The methods we explore in Part II are\r
applicable to continuous problems and are a significant extension of that approach.\r
The key idea of DP, and of reinforcement learning generally, is the use of value functions\r
to organize and structure the search for good policies. In this chapter we show how DP\r
can be used to compute the value functions defined in Chapter 3. As discussed there, we\r
can easily obtain optimal policies once we have found the optimal value functions, v⇤ or\r
q⇤, which satisfy the Bellman optimality equations:\r
v⇤(s) = maxa E[Rt+1 + v⇤(St+1) | St =s, At =a]\r
= maxa\r
X\r
s0,r\r
p(s0, r|s, a)\r
h\r
r + v⇤(s0)\r
i\r
, (4.1)\r
or\r
q⇤(s, a) = E\r
h\r
Rt+1 +  max\r
a0 q⇤(St+1, a0\r
)\r
\r
\r
\r
St =s, At =a\r
i\r
= X\r
s0,r\r
p(s0, r|s, a)\r
h\r
r +  max\r
a0 q⇤(s0\r
, a0)\r
i\r
, (4.2)

74 Chapter 4: Dynamic Programming\r
for all s 2 S, a 2 A(s), and s0 2 S+. As we shall see, DP algorithms are obtained by\r
turning Bellman equations such as these into assignments, that is, into update rules for\r
improving approximations of the desired value functions.\r
4.1 Policy Evaluation (Prediction)\r
First we consider how to compute the state-value function v⇡ for an arbitrary policy ⇡.\r
This is called policy evaluation in the DP literature. We also refer to it as the prediction\r
problem. Recall from Chapter 3 that, for all s 2 S,\r
v⇡(s) .= E⇡[Gt | St =s]\r
= E⇡[Rt+1 + Gt+1 | St =s] (from (3.9))\r
= E⇡[Rt+1 + v⇡(St+1) | St =s] (4.3)\r
= X\r
a\r
⇡(a|s)\r
X\r
s0,r\r
p(s0, r|s, a)\r
h\r
r + v⇡(s0)\r
i\r
, (4.4)\r
where ⇡(a|s) is the probability of taking action a in state s under policy ⇡, and the\r
expectations are subscripted by ⇡ to indicate that they are conditional on ⇡ being followed.\r
The existence and uniqueness of v⇡ are guaranteed as long as either  < 1 or eventual\r
termination is guaranteed from all states under the policy ⇡.\r
If the environment’s dynamics are completely known, then (4.4) is a system of |S|\r
simultaneous linear equations in |S| unknowns (the v⇡(s), s 2 S). In principle, its solution\r
is a straightforward, if tedious, computation. For our purposes, iterative solution methods\r
are most suitable. Consider a sequence of approximate value functions v0, v1, v2,...,\r
each mapping S+ to R (the real numbers). The initial approximation, v0, is chosen\r
arbitrarily (except that the terminal state, if any, must be given value 0), and each\r
successive approximation is obtained by using the Bellman equation for v⇡ (4.4) as an\r
update rule:\r
vk+1(s) .= E⇡[Rt+1 + vk(St+1) | St =s]\r
= X\r
a\r
⇡(a|s)\r
X\r
s0,r\r
p(s0, r|s, a)\r
h\r
r + vk(s0)\r
i\r
, (4.5)\r
for all s 2 S. Clearly, vk = v⇡ is a fixed point for this update rule because the Bellman\r
equation for v⇡ assures us of equality in this case. Indeed, the sequence {vk} can be\r
shown in general to converge to v⇡ as k ! 1 under the same conditions that guarantee\r
the existence of v⇡. This algorithm is called iterative policy evaluation.\r
To produce each successive approximation, vk+1 from vk, iterative policy evaluation\r
applies the same operation to each state s: it replaces the old value of s with a new value\r
obtained from the old values of the successor states of s, and the expected immediate\r
rewards, along all the one-step transitions possible under the policy being evaluated. We\r
call this kind of operation an expected update. Each iteration of iterative policy evaluation\r
updates the value of every state once to produce the new approximate value function

4.1. Policy Evaluation (Prediction) 75\r
vk+1. There are several di↵erent kinds of expected updates, depending on whether a\r
state (as here) or a state–action pair is being updated, and depending on the precise way\r
the estimated values of the successor states are combined. All the updates done in DP\r
algorithms are called expected updates because they are based on an expectation over all\r
possible next states rather than on a sample next state. The nature of an update can\r
be expressed in an equation, as above, or in a backup diagram like those introduced in\r
Chapter 3. For example, the backup diagram corresponding to the expected update used\r
in iterative policy evaluation is shown on page 59.\r
To write a sequential computer program to implement iterative policy evaluation as\r
given by (4.5) you would have to use two arrays, one for the old values, vk(s), and one\r
for the new values, vk+1(s). With two arrays, the new values can be computed one by\r
one from the old values without the old values being changed. Alternatively, you could\r
use one array and update the values “in place,” that is, with each new value immediately\r
overwriting the old one. Then, depending on the order in which the states are updated,\r
sometimes new values are used instead of old ones on the right-hand side of (4.5). This\r
in-place algorithm also converges to v⇡; in fact, it usually converges faster than the\r
two-array version, as you might expect, because it uses new data as soon as they are\r
available. We think of the updates as being done in a sweep through the state space. For\r
the in-place algorithm, the order in which states have their values updated during the\r
sweep has a significant influence on the rate of convergence. We usually have the in-place\r
version in mind when we think of DP algorithms.\r
A complete in-place version of iterative policy evaluation is shown in pseudocode in\r
the box below. Note how it handles termination. Formally, iterative policy evaluation\r
converges only in the limit, but in practice it must be halted short of this. The pseudocode\r
tests the quantity maxs2S |vk+1(s)vk(s)| after each sweep and stops when it is suciently\r
small.\r
Iterative Policy Evaluation, for estimating V ⇡ v⇡\r
Input ⇡, the policy to be evaluated\r
Algorithm parameter: a small threshold ✓ > 0 determining accuracy of estimation\r
Initialize V (s) arbitrarily, for s 2 S, and V (terminal) to 0\r
Loop:\r
 0\r
Loop for each s 2 S:\r
v V (s)\r
V (s) P\r
a ⇡(a|s)\r
P\r
s0,r p(s0\r
, r|s, a)\r
⇥\r
r + V (s0)\r
⇤\r
 max(, |v  V (s)|)\r
until  < ✓

76 Chapter 4: Dynamic Programming\r
Example 4.1 Consider the 4⇥4 gridworld shown below.\r
actions\r
r = !1\r
on all transitions\r
1 2 3\r
4 5 6 7\r
8 9 10 11\r
12 13 14\r
Rt = 1\r
The nonterminal states are S = {1, 2,..., 14}. There are four actions possible in each\r
state, A = {up, down, right, left}, which deterministically cause the corresponding\r
state transitions, except that actions that would take the agent o↵ the grid in fact leave\r
the state unchanged. Thus, for instance, p(6, 1|5, right) = 1, p(7, 1|7, right) = 1,\r
and p(10, r|5, right) = 0 for all r 2 R. This is an undiscounted, episodic task. The\r
reward is 1 on all transitions until the terminal state is reached. The terminal state is\r
shaded in the figure (although it is shown in two places, it is formally one state). The\r
expected reward function is thus r(s, a, s0) = 1 for all states s, s0 and actions a. Suppose\r
the agent follows the equiprobable random policy (all actions equally likely). The left side\r
of Figure 4.1 shows the sequence of value functions {vk} computed by iterative policy\r
evaluation. The final estimate is in fact v⇡, which in this case gives for each state the\r
negation of the expected number of steps from that state until termination.\r
Exercise 4.1 In Example 4.1, if ⇡ is the equiprobable random policy, what is q⇡(11, down)?\r
What is q⇡(7, down)? ⇤\r
Exercise 4.2 In Example 4.1, suppose a new state 15 is added to the gridworld just below\r
state 13, and its actions, left, up, right, and down, take the agent to states 12, 13, 14,\r
and 15, respectively. Assume that the transitions from the original states are unchanged.\r
What, then, is v⇡(15) for the equiprobable random policy? Now suppose the dynamics of\r
state 13 are also changed, such that action down from state 13 takes the agent to the new\r
state 15. What is v⇡(15) for the equiprobable random policy in this case? ⇤\r
Exercise 4.3 What are the equations analogous to (4.3), (4.4), and (4.5), but for action\u0002value functions instead of state-value functions? ⇤\r
4.2 Policy Improvement\r
Our reason for computing the value function for a policy is to help find better policies.\r
Suppose we have determined the value function v⇡ for an arbitrary deterministic policy\r
⇡. For some state s we would like to know whether or not we should change the policy\r
to deterministically choose an action a 6= ⇡(s). We know how good it is to follow the\r
current policy from s—that is v⇡(s)—but would it be better or worse to change to the\r
new policy? One way to answer this question is to consider selecting a in s and thereafter

4.2. Policy Improvement 77\r
0.0 0.0 0.0\r
0.0 0.0 0.0 0.0\r
0.0 0.0 0.0 0.0\r
0.0 0.0 0.0\r
-1.0 -1.0 -1.0\r
-1.0 -1.0 -1.0 -1.0\r
-1.0 -1.0 -1.0 -1.0\r
-1.0 -1.0 -1.0\r
-1.7 -2.0 -2.0\r
-1.7 -2.0 -2.0 -2.0\r
-2.0 -2.0 -2.0 -1.7\r
-2.0 -2.0 -1.7\r
-2.4 -2.9 -3.0\r
-2.4 -2.9 -3.0 -2.9\r
-2.9 -3.0 -2.9 -2.4\r
-3.0 -2.9 -2.4\r
-6.1 -8.4 -9.0\r
-6.1 -7.7 -8.4 -8.4\r
-8.4 -8.4 -7.7 -6.1\r
-9.0 -8.4 -6.1\r
-14. -20. -22.\r
-14. -18. -20. -20.\r
-20. -20. -18. -14.\r
-22. -20. -14.\r
Vk for the\r
Random Policy\r
Greedy Policy\r
w.r.t. Vk\r
k = 0\r
k = 1\r
k = 2\r
k = 10\r
k = !\r
k = 3\r
optimal \r
policy\r
random\r
policy\r
0.0\r
0.0\r
0.0\r
0.0\r
0.0\r
0.0\r
0.0\r
0.0\r
0.0\r
0.0\r
0.0\r
0.0\r
vk\r
 for the\r
random policy\r
vk greedy policy\r
 w.r.t. vk\r
Figure 4.1: Convergence of iterative policy evaluation on a small gridworld. The left column is\r
the sequence of approximations of the state-value function for the random policy (all actions\r
equally likely). The right column is the sequence of greedy policies corresponding to the value\r
function estimates (arrows are shown for all actions achieving the maximum, and the numbers\r
shown are rounded to two significant digits). The last policy is guaranteed only to be an\r
improvement over the random policy, but in this case it, and all policies after the third iteration,\r
are optimal.

78 Chapter 4: Dynamic Programming\r
following the existing policy, ⇡. The value of this way of behaving is\r
q⇡(s, a) .= E[Rt+1 + v⇡(St+1) | St =s, At =a] (4.6)\r
= X\r
s0,r\r
p(s0, r|s, a)\r
h\r
r + v⇡(s0)\r
i\r
.\r
The key criterion is whether this is greater than or less than v⇡(s). If it is greater—that\r
is, if it is better to select a once in s and thereafter follow ⇡ than it would be to follow\r
⇡ all the time—then one would expect it to be better still to select a every time s is\r
encountered, and that the new policy would in fact be a better one overall.\r
That this is true is a special case of a general result called the policy improvement\r
theorem. Let ⇡ and ⇡0 be any pair of deterministic policies such that, for all s 2 S,\r
q⇡(s, ⇡0(s))  v⇡(s). (4.7)\r
Then the policy ⇡0 must be as good as, or better than, ⇡. That is, it must obtain greater\r
or equal expected return from all states s 2 S:\r
v⇡0 (s)  v⇡(s). (4.8)\r
Moreover, if there is strict inequality of (4.7) at any state, then there must be strict\r
inequality of (4.8) at that state.\r
The policy improvement theorem applies to the two policies that we considered at the\r
beginning of this section: an original deterministic policy, ⇡, and a changed policy, ⇡0,\r
that is identical to ⇡ except that ⇡0(s) = a 6= ⇡(s). For states other than s, (4.7) holds\r
because the two sides are equal. Thus, if q⇡(s, a) > v⇡(s), then the changed policy is\r
indeed better than ⇡.\r
The idea behind the proof of the policy improvement theorem is easy to understand.\r
Starting from (4.7), we keep expanding the q⇡ side with (4.6) and reapplying (4.7) until\r
we get v⇡0 (s):\r
v⇡(s)  q⇡(s, ⇡0(s))\r
= E[Rt+1 + v⇡(St+1) | St =s, At =⇡0(s)] (by (4.6))\r
= E⇡0[Rt+1 + v⇡(St+1) | St =s]\r
 E⇡0[Rt+1 + q⇡(St+1, ⇡0(St+1)) | St =s] (by (4.7))\r
= E⇡0[Rt+1 + E[Rt+2 + v⇡(St+2)|St+1, At+1 =⇡0(St+1)] | St =s]\r
= E⇡0\r
⇥\r
Rt+1 + Rt+2 + 2v⇡(St+2)\r
\r
 St =s\r
⇤\r
 E⇡0\r
⇥\r
Rt+1 + Rt+2 + 2Rt+3 + 3v⇡(St+3)\r
\r
 St =s\r
⇤\r
.\r
.\r
.\r
 E⇡0\r
⇥\r
Rt+1 + Rt+2 + 2Rt+3 + 3Rt+4 + ··· \r
 St =s\r
⇤\r
= v⇡0 (s).\r
So far we have seen how, given a policy and its value function, we can easily evaluate\r
a change in the policy at a single state. It is a natural extension to consider changes at

4.2. Policy Improvement 79\r
all states, selecting at each state the action that appears best according to q⇡(s, a). In\r
other words, to consider the new greedy policy, ⇡0, given by\r
⇡0(s) .= argmax aq⇡(s, a)\r
= argmax aE[Rt+1 + v⇡(St+1) | St =s, At =a] (4.9)\r
= argmax a\r
X\r
s0,r\r
p(s0, r|s, a)\r
h\r
r + v⇡(s0)\r
i\r
,\r
where argmaxa denotes the value of a at which the expression that follows is maximized\r
(with ties broken arbitrarily). The greedy policy takes the action that looks best in the\r
short term—after one step of lookahead—according to v⇡. By construction, the greedy\r
policy meets the conditions of the policy improvement theorem (4.7), so we know that it\r
is as good as, or better than, the original policy. The process of making a new policy that\r
improves on an original policy, by making it greedy with respect to the value function of\r
the original policy, is called policy improvement.\r
Suppose the new greedy policy, ⇡0, is as good as, but not better than, the old policy ⇡.\r
Then v⇡ = v⇡0 , and from (4.9) it follows that for all s 2 S:\r
v⇡0 (s) = maxa E[Rt+1 + v⇡0 (St+1) | St =s, At =a]\r
= maxa\r
X\r
s0,r\r
p(s0, r|s, a)\r
h\r
r + v⇡0 (s0)\r
i\r
.\r
But this is the same as the Bellman optimality equation (4.1), and therefore, v⇡0 must be\r
v⇤, and both ⇡ and ⇡0 must be optimal policies. Policy improvement thus must give us a\r
strictly better policy except when the original policy is already optimal.\r
So far in this section we have considered the special case of deterministic policies.\r
In the general case, a stochastic policy ⇡ specifies probabilities, ⇡(a|s), for taking each\r
action, a, in each state, s. We will not go through the details, but in fact all the ideas of\r
this section extend easily to stochastic policies. In particular, the policy improvement\r
theorem carries through as stated for the stochastic case. In addition, if there are ties in\r
policy improvement steps such as (4.9)—that is, if there are several actions at which the\r
maximum is achieved—then in the stochastic case we need not select a single action from\r
among them. Instead, each maximizing action can be given a portion of the probability\r
of being selected in the new greedy policy. Any apportioning scheme is allowed as long\r
as all submaximal actions are given zero probability.\r
The last row of Figure 4.1 shows an example of policy improvement for stochastic\r
policies. Here the original policy, ⇡, is the equiprobable random policy, and the new\r
policy, ⇡0, is greedy with respect to v⇡. The value function v⇡ is shown in the bottom-left\r
diagram and the set of possible ⇡0 is shown in the bottom-right diagram. The states\r
with multiple arrows in the ⇡0 diagram are those in which several actions achieve the\r
maximum in (4.9); any apportionment of probability among these actions is permitted.\r
For any such policy, its state values v⇡0 (s) can be seen by inspection to be either 1, 2,\r
or 3, for all states s 2 S, whereas v⇡(s) is at most 14. Thus, v⇡0 (s)  v⇡(s), for all

80 Chapter 4: Dynamic Programming\r
s 2 S, illustrating policy improvement. Although in this case the new policy ⇡0 happens\r
to be optimal, in general only an improvement is guaranteed.\r
4.3 Policy Iteration\r
Once a policy, ⇡, has been improved using v⇡ to yield a better policy, ⇡0, we can then\r
compute v⇡0 and improve it again to yield an even better ⇡00. We can thus obtain a\r
sequence of monotonically improving policies and value functions:\r
⇡0\r
E\r
! v⇡0\r
I\r
! ⇡1\r
E\r
! v⇡1\r
I\r
! ⇡2\r
E\r
! · · · I! ⇡⇤\r
E\r
! v⇤,\r
where E! denotes a policy evaluation and I! denotes a policy improvement. Each\r
policy is guaranteed to be a strict improvement over the previous one (unless it is already\r
optimal). Because a finite MDP has only a finite number of deterministic policies, this\r
process must converge to an optimal policy and the optimal value function in a finite\r
number of iterations.\r
This way of finding an optimal policy is called policy iteration. A complete algorithm is\r
given in the box below. Note that each policy evaluation, itself an iterative computation,\r
is started with the value function for the previous policy. This typically results in a great\r
increase in the speed of convergence of policy evaluation (presumably because the value\r
function changes little from one policy to the next).\r
Policy Iteration (using iterative policy evaluation) for estimating ⇡ ⇡ ⇡⇤\r
1. Initialization\r
V (s) 2 R and ⇡(s) 2 A(s) arbitrarily for all s 2 S; V (terminal) .= 0\r
2. Policy Evaluation\r
Loop:\r
 0\r
Loop for each s 2 S:\r
v V (s)\r
V (s) P\r
s0,r p(s0\r
, r|s, ⇡(s))⇥r + V (s0)\r
⇤\r
 max(, |v  V (s)|)\r
until  < ✓ (a small positive number determining the accuracy of estimation)\r
3. Policy Improvement\r
policy-stable true\r
For each s 2 S:\r
old-action ⇡(s)\r
⇡(s) argmaxa\r
P\r
s0,r p(s0\r
, r|s, a)\r
⇥\r
r + V (s0)\r
⇤\r
If old-action 6= ⇡(s), then policy-stable f alse\r
If policy-stable, then stop and return V ⇡ v⇤ and ⇡ ⇡ ⇡⇤; else go to 2

4.3. Policy Iteration 81\r
Example 4.2: Jack’s Car Rental Jack manages two locations for a nationwide car\r
rental company. Each day, some number of customers arrive at each location to rent cars.\r
If Jack has a car available, he rents it out and is credited $10 by the national company.\r
If he is out of cars at that location, then the business is lost. Cars become available for\r
renting the day after they are returned. To help ensure that cars are available where\r
they are needed, Jack can move them between the two locations overnight, at a cost of\r
$2 per car moved. We assume that the number of cars requested and returned at each\r
location are Poisson random variables, meaning that the probability that the number is\r
n is n\r
n! e, where  is the expected number. Suppose  is 3 and 4 for rental requests at\r
the first and second locations and 3 and 2 for returns. To simplify the problem slightly,\r
we assume that there can be no more than 20 cars at each location (any additional cars\r
are returned to the nationwide company, and thus disappear from the problem) and a\r
maximum of five cars can be moved from one location to the other in one night. We take\r
the discount rate to be  = 0.9 and formulate this as a continuing finite MDP, where\r
the time steps are days, the state is the number of cars at each location at the end of\r
the day, and the actions are the net numbers of cars moved between the two locations\r
overnight. Figure 4.2 shows the sequence of policies found by policy iteration starting\r
from the policy that never moves any cars.\r
V4\r
612\r
#Cars at second location\r
0 420\r
20 0\r
20\r
#Cars at first location\r
1\r
1\r
5\r
\u00021\r
\u00022\r
-4\r
4 3 2\r
4 3 2\r
\u00023\r
0\r
0\r
5\r
\u00021\r
\u00022\r
\u00023 \u00024\r
1 2 3 4\r
0\r
\u00010 \u00011 \u00012\r
\u00023 \u00024\r
\u00022\r
0\r
1 2\r
3 4\r
\u00021\r
\u00013\r
2\r
\u00023 \u00024\r
\u00022\r
0\r
1\r
3 4\r
5\r
\u00021\r
\u00014\r
#Cars at second location\r
#Cars at first location\r
5\r
0 20\r
0 20\r
v⇡4\r
⇡0 ⇡1 ⇡2\r
⇡3 ⇡4\r
Figure 4.2: The sequence of policies found by policy iteration on Jack’s car rental problem,\r
and the final state-value function. The first five diagrams show, for each number of cars at\r
each location at the end of the day, the number of cars to be moved from the first location to\r
the second (negative numbers indicate transfers from the second location to the first). Each\r
successive policy is a strict improvement over the previous policy, and the last policy is optimal.

82 Chapter 4: Dynamic Programming\r
Policy iteration often converges in surprisingly few iterations, as illustrated in the\r
example of Jack’s car rental and in the example in Figure 4.1. The bottom-left diagram of\r
Figure 4.1 shows the value function for the equiprobable random policy, and the bottom\u0002right diagram shows a greedy policy for this value function. The policy improvement\r
theorem assures us that these policies are better than the original random policy. In this\r
case, however, these policies are not just better, but optimal, proceeding to the terminal\r
states in the minimum number of steps. In this example, policy iteration would find the\r
optimal policy after just one iteration.\r
Exercise 4.4 The policy iteration algorithm on page 80 has a subtle bug in that it may\r
never terminate if the policy continually switches between two or more policies that are\r
equally good. This is okay for pedagogy, but not for actual use. Modify the pseudocode\r
so that convergence is guaranteed. ⇤\r
Exercise 4.5 How would policy iteration be defined for action values? Give a complete\r
algorithm for computing q⇤, analogous to that on page 80 for computing v⇤. Please pay\r
special attention to this exercise, because the ideas involved will be used throughout the\r
rest of the book. ⇤\r
Exercise 4.6 Suppose you are restricted to considering only policies that are "-soft,\r
meaning that the probability of selecting each action in each state, s, is at least "/|A(s)|.\r
Describe qualitatively the changes that would be required in each of the steps 3, 2, and 1,\r
in that order, of the policy iteration algorithm for v⇤ on page 80. ⇤\r
Exercise 4.7 (programming) Write a program for policy iteration and re-solve Jack’s car\r
rental problem with the following changes. One of Jack’s employees at the first location\r
rides a bus home each night and lives near the second location. She is happy to shuttle\r
one car to the second location for free. Each additional car still costs $2, as do all cars\r
moved in the other direction. In addition, Jack has limited parking space at each location.\r
If more than 10 cars are kept overnight at a location (after any moving of cars), then an\r
additional cost of $4 must be incurred to use a second parking lot (independent of how\r
many cars are kept there). These sorts of nonlinearities and arbitrary dynamics often\r
occur in real problems and cannot easily be handled by optimization methods other than\r
dynamic programming. To check your program, first replicate the results given for the\r
original problem. ⇤\r
4.4 Value Iteration\r
One drawback to policy iteration is that each of its iterations involves policy evaluation,\r
which may itself be a protracted iterative computation requiring multiple sweeps through\r
the state set. If policy evaluation is done iteratively, then convergence exactly to v⇡\r
occurs only in the limit. Must we wait for exact convergence, or can we stop short of\r
that? The example in Figure 4.1 certainly suggests that it may be possible to truncate\r
policy evaluation. In that example, policy evaluation iterations beyond the first three\r
have no e↵ect on the corresponding greedy policy.\r
In fact, the policy evaluation step of policy iteration can be truncated in several ways\r
without losing the convergence guarantees of policy iteration. One important special

4.4. Value Iteration 83\r
case is when policy evaluation is stopped after just one sweep (one update of each state).\r
This algorithm is called value iteration. It can be written as a particularly simple update\r
operation that combines the policy improvement and truncated policy evaluation steps:\r
vk+1(s) .= maxa E[Rt+1 + vk(St+1) | St =s, At =a]\r
= maxa\r
X\r
s0,r\r
p(s0, r|s, a)\r
h\r
r + vk(s0)\r
i\r
, (4.10)\r
for all s 2 S. For arbitrary v0, the sequence {vk} can be shown to converge to v⇤ under\r
the same conditions that guarantee the existence of v⇤.\r
Another way of understanding value iteration is by reference to the Bellman optimality\r
equation (4.1). Note that value iteration is obtained simply by turning the Bellman\r
optimality equation into an update rule. Also note how the value iteration update is\r
identical to the policy evaluation update (4.5) except that it requires the maximum to be\r
taken over all actions. Another way of seeing this close relationship is to compare the\r
backup diagrams for these algorithms on page 59 (policy evaluation) and on the left of\r
Figure 3.4 (value iteration). These two are the natural backup operations for computing\r
v⇡ and v⇤.\r
Finally, let us consider how value iteration terminates. Like policy evaluation, value\r
iteration formally requires an infinite number of iterations to converge exactly to v⇤. In\r
practice, we stop once the value function changes by only a small amount in a sweep.\r
The box below shows a complete algorithm with this kind of termination condition.\r
Value Iteration, for estimating ⇡ ⇡ ⇡⇤\r
Algorithm parameter: a small threshold ✓ > 0 determining accuracy of estimation\r
Initialize V (s), for all s 2 S+, arbitrarily except that V (terminal)=0\r
Loop:\r
|  0\r
| Loop for each s 2 S:\r
| v V (s)\r
| V (s) maxa\r
P\r
s0,r p(s0\r
, r|s, a)\r
⇥\r
r + V (s0)\r
⇤\r
|  max(, |v  V (s)|)\r
until  < ✓\r
Output a deterministic policy, ⇡ ⇡ ⇡⇤, such that\r
⇡(s) = argmaxa\r
P\r
s0,r p(s0\r
, r|s, a)\r
⇥\r
r + V (s0)\r
⇤\r
Value iteration e↵ectively combines, in each of its sweeps, one sweep of policy evaluation\r
and one sweep of policy improvement. Faster convergence is often achieved by interposing\r
multiple policy evaluation sweeps between each policy improvement sweep. In general,\r
the entire class of truncated policy iteration algorithms can be thought of as sequences\r
of sweeps, some of which use policy evaluation updates and some of which use value\r
iteration updates. Because the max operation in (4.10) is the only di↵erence between

84 Chapter 4: Dynamic Programming\r
these updates, this just means that the max operation is added to some sweeps of policy\r
evaluation. All of these algorithms converge to an optimal policy for discounted finite\r
MDPs.\r
Example 4.3: Gambler’s Problem A gambler has the opportunity to make bets on the\r
outcomes of a sequence of coin flips. If the coin comes up heads, he wins as many dollars as\r
he has staked on that flip; if it is tails, he loses his stake. The game ends when the gambler\r
wins by reaching his goal of $100, or loses by running out of money. On each flip, the gam\u0002bler must decide what portion of his capital to stake, in integer numbers of dollars. This\r
problem can be formulated as an undiscounted, episodic, finite MDP. The state is the gam\u0002bler’s capital, s 2 {1, 2,..., 99} and the actions are stakes, a 2 {0, 1,..., min(s, 100  s)}.\r
1 25 50 75 99\r
1\r
10\r
20\r
30\r
40\r
50\r
1\r
0\r
0.2\r
0.4\r
0.6\r
0.8\r
1\r
25 50 75 99\r
Capital\r
Capital\r
Value\r
estimates\r
Final\r
policy\r
(stake)\r
sweep 1\r
sweep 2\r
sweep 3\r
sweep 32\r
Final value\r
function\r
Figure 4.3: The solution to the gambler’s problem for\r
ph = 0.4. The upper graph shows the value function\r
found by successive sweeps of value iteration. The\r
lower graph shows the final policy.\r
The reward is zero on all transi\u0002tions except those on which the gam\u0002bler reaches his goal, when it is +1.\r
The state-value function then gives\r
the probability of winning from each\r
state. A policy is a mapping from\r
levels of capital to stakes. The opti\u0002mal policy maximizes the probability\r
of reaching the goal. Let ph denote\r
the probability of the coin coming\r
up heads. If ph is known, then the\r
entire problem is known and it can\r
be solved, for instance, by value iter\u0002ation. Figure 4.3 shows the change\r
in the value function over successive\r
sweeps of value iteration, and the\r
final policy found, for the case of\r
ph = 0.4. This policy is optimal, but\r
not unique. In fact, there is a whole\r
family of optimal policies, all corre\u0002sponding to ties for the argmax ac\u0002tion selection with respect to the op\u0002timal value function. Can you guess\r
what the entire family looks like?\r
Exercise 4.8 Why does the optimal\r
policy for the gambler’s problem have such a curious form? In particular, for capital of 50\r
it bets it all on one flip, but for capital of 51 it does not. Why is this a good policy? ⇤\r
Exercise 4.9 (programming) Implement value iteration for the gambler’s problem and\r
solve it for ph = 0.25 and ph = 0.55. In programming, you may find it convenient to\r
introduce two dummy states corresponding to termination with capital of 0 and 100,\r
giving them values of 0 and 1 respectively. Show your results graphically, as in Figure 4.3.\r
Are your results stable as ✓ ! 0? ⇤\r
Exercise 4.10 What is the analog of the value iteration update (4.10) for action values,\r
qk+1(s, a)? ⇤

4.5. Asynchronous Dynamic Programming 85\r
4.5 Asynchronous Dynamic Programming\r
A major drawback to the DP methods that we have discussed so far is that they involve\r
operations over the entire state set of the MDP, that is, they require sweeps of the state\r
set. If the state set is very large, then even a single sweep can be prohibitively expensive.\r
For example, the game of backgammon has over 1020 states. Even if we could perform\r
the value iteration update on a million states per second, it would take over a thousand\r
years to complete a single sweep.\r
Asynchronous DP algorithms are in-place iterative DP algorithms that are not organized\r
in terms of systematic sweeps of the state set. These algorithms update the values of\r
states in any order whatsoever, using whatever values of other states happen to be\r
available. The values of some states may be updated several times before the values of\r
others are updated once. To converge correctly, however, an asynchronous algorithm\r
must continue to update the values of all the states: it can’t ignore any state after some\r
point in the computation. Asynchronous DP algorithms allow great flexibility in selecting\r
states to update.\r
For example, one version of asynchronous value iteration updates the value, in place, of\r
only one state, sk, on each step, k, using the value iteration update (4.10). If 0   < 1,\r
asymptotic convergence to v⇤ is guaranteed given only that all states occur in the sequence\r
{sk} an infinite number of times (the sequence could even be random).1 Similarly, it is\r
possible to intermix policy evaluation and value iteration updates to produce a kind of\r
asynchronous truncated policy iteration. Although the details of this and other more\r
unusual DP algorithms are beyond the scope of this book, it is clear that a few di↵erent\r
updates form building blocks that can be used flexibly in a wide variety of sweepless DP\r
algorithms.\r
Of course, avoiding sweeps does not necessarily mean that we can get away with less\r
computation. It just means that an algorithm does not need to get locked into any\r
hopelessly long sweep before it can make progress improving a policy. We can try to\r
take advantage of this flexibility by selecting the states to which we apply updates so\r
as to improve the algorithm’s rate of progress. We can try to order the updates to let\r
value information propagate from state to state in an ecient way. Some states may not\r
need their values updated as often as others. We might even try to skip updating some\r
states entirely if they are not relevant to optimal behavior. Some ideas for doing this are\r
discussed in Chapter 8.\r
Asynchronous algorithms also make it easier to intermix computation with real-time\r
interaction. To solve a given MDP, we can run an iterative DP algorithm at the same\r
time that an agent is actually experiencing the MDP. The agent’s experience can be used\r
to determine the states to which the DP algorithm applies its updates. At the same time,\r
the latest value and policy information from the DP algorithm can guide the agent’s\r
decision making. For example, we can apply updates to states as the agent visits them.\r
This makes it possible to focus the DP algorithm’s updates onto parts of the state set\r
1In the undiscounted episodic case, it is possible that there are some orderings of updates that do not\r
result in convergence, but it is relatively easy to avoid these.

86 Chapter 4: Dynamic Programming\r
that are most relevant to the agent. This kind of focusing is a repeated theme in\r
reinforcement learning.\r
4.6 Generalized Policy Iteration\r
Policy iteration consists of two simultaneous, interacting processes, one making the value\r
function consistent with the current policy (policy evaluation), and the other making\r
the policy greedy with respect to the current value function (policy improvement). In\r
policy iteration, these two processes alternate, each completing before the other begins,\r
but this is not really necessary. In value iteration, for example, only a single iteration\r
of policy evaluation is performed in between each policy improvement. In asynchronous\r
DP methods, the evaluation and improvement processes are interleaved at an even\r
finer grain. In some cases a single state is updated in one process before returning\r
to the other. As long as both processes continue to update all states, the ultimate\r
result is typically the same—convergence to the optimal value function and an optimal\r
policy.\r
evaluation\r
improvement\r
⇡  greedy(V )\r
⇡ V\r
V  v⇡\r
⇡⇤ v⇤\r
We use the term generalized policy iteration (GPI) to refer\r
to the general idea of letting policy-evaluation and policy\u0002improvement processes interact, independent of the granularity\r
and other details of the two processes. Almost all reinforcement\r
learning methods are well described as GPI. That is, all have\r
identifiable policies and value functions, with the policy always\r
being improved with respect to the value function and the value\r
function always being driven toward the value function for the\r
policy, as suggested by the diagram to the right. If both the\r
evaluation process and the improvement process stabilize, that\r
is, no longer produce changes, then the value function and policy\r
must be optimal. The value function stabilizes only when it\r
is consistent with the current policy, and the policy stabilizes\r
only when it is greedy with respect to the current value function.\r
Thus, both processes stabilize only when a policy has been found that is greedy with\r
respect to its own evaluation function. This implies that the Bellman optimality equation\r
(4.1) holds, and thus that the policy and the value function are optimal.\r
The evaluation and improvement processes in GPI can be viewed as both competing\r
and cooperating. They compete in the sense that they pull in opposing directions. Making\r
the policy greedy with respect to the value function typically makes the value function\r
incorrect for the changed policy, and making the value function consistent with the policy\r
typically causes that policy no longer to be greedy. In the long run, however, these\r
two processes interact to find a single joint solution: the optimal value function and an\r
optimal policy.\r
One might also think of the interaction between the evaluation and improvement\r
processes in GPI in terms of two constraints or goals—for example, as two lines in

4.7. Eciency of Dynamic Programming 87\r
v⇤, ⇡⇤\r
⇡ = greedy(v)\r
v, ⇡\r
v = v⇡\r
a two-dimensional space as suggested by the dia\u0002gram to the right. Although the real geometry is\r
much more complicated than this, the diagram sug\u0002gests what happens in the real case. Each process\r
drives the value function or policy toward one of\r
the lines representing a solution to one of the two\r
goals. The goals interact because the two lines are\r
not orthogonal. Driving directly toward one goal\r
causes some movement away from the other goal.\r
Inevitably, however, the joint process is brought closer to the overall goal of optimality.\r
The arrows in this diagram correspond to the behavior of policy iteration in that each\r
takes the system all the way to achieving one of the two goals completely. In GPI\r
one could also take smaller, incomplete steps toward each goal. In either case, the two\r
processes together achieve the overall goal of optimality even though neither is attempting\r
to achieve it directly.\r
4.7 Eciency of Dynamic Programming\r
DP may not be practical for very large problems, but compared with other methods\r
for solving MDPs, DP methods are actually quite ecient. If we ignore a few technical\r
details, then, in the worst case, the time that DP methods take to find an optimal policy\r
is polynomial in the number of states and actions. If n and k denote the number of states\r
and actions, this means that a DP method takes a number of computational operations\r
that is less than some polynomial function of n and k. A DP method is guaranteed to\r
find an optimal policy in polynomial time even though the total number of (deterministic)\r
policies is kn. In this sense, DP is exponentially faster than any direct search in policy\r
space could be, because direct search would have to exhaustively examine each policy\r
to provide the same guarantee. Linear programming methods can also be used to solve\r
MDPs, and in some cases their worst-case convergence guarantees are better than those\r
of DP methods. But linear programming methods become impractical at a much smaller\r
number of states than do DP methods (by a factor of about 100). For the largest problems,\r
only DP methods are feasible.\r
DP is sometimes thought to be of limited applicability because of the curse of dimen\u0002sionality, the fact that the number of states often grows exponentially with the number\r
of state variables. Large state sets do create diculties, but these are inherent diculties\r
of the problem, not of DP as a solution method. In fact, DP is comparatively better\r
suited to handling large state spaces than competing methods such as direct search and\r
linear programming.\r
In practice, DP methods can be used with today’s computers to solve MDPs with\r
millions of states. Both policy iteration and value iteration are widely used, and it is not\r
clear which, if either, is better in general. In practice, these methods usually converge\r
much faster than their theoretical worst-case run times, particularly if they are started\r
with good initial value functions or policies.

88 Chapter 4: Dynamic Programming\r
On problems with large state spaces, asynchronous DP methods are often preferred. To\r
complete even one sweep of a synchronous method requires computation and memory for\r
every state. For some problems, even this much memory and computation is impractical,\r
yet the problem is still potentially solvable because relatively few states occur along\r
optimal solution trajectories. Asynchronous methods and other variations of GPI can be\r
applied in such cases and may find good or optimal policies much faster than synchronous\r
methods can.\r
4.8 Summary\r
In this chapter we have become familiar with the basic ideas and algorithms of dynamic\r
programming as they relate to solving finite MDPs. Policy evaluation refers to the (typi\u0002cally) iterative computation of the value function for a given policy. Policy improvement\r
refers to the computation of an improved policy given the value function for that policy.\r
Putting these two computations together, we obtain policy iteration and value iteration,\r
the two most popular DP methods. Either of these can be used to reliably compute\r
optimal policies and value functions for finite MDPs given complete knowledge of the\r
MDP.\r
Classical DP methods operate in sweeps through the state set, performing an expected\r
update operation on each state. Each such operation updates the value of one state\r
based on the values of all possible successor states and their probabilities of occurring.\r
Expected updates are closely related to Bellman equations: they are little more than\r
these equations turned into assignment statements. When the updates no longer result in\r
any changes in value, convergence has occurred to values that satisfy the corresponding\r
Bellman equation. Just as there are four primary value functions (v⇡, v⇤, q⇡, and q⇤),\r
there are four corresponding Bellman equations and four corresponding expected updates.\r
An intuitive view of the operation of DP updates is given by their backup diagrams.\r
Insight into DP methods and, in fact, into almost all reinforcement learning methods,\r
can be gained by viewing them as generalized policy iteration (GPI). GPI is the general idea\r
of two interacting processes revolving around an approximate policy and an approximate\r
value function. One process takes the policy as given and performs some form of policy\r
evaluation, changing the value function to be more like the true value function for the\r
policy. The other process takes the value function as given and performs some form\r
of policy improvement, changing the policy to make it better, assuming that the value\r
function is its value function. Although each process changes the basis for the other,\r
overall they work together to find a joint solution: a policy and value function that are\r
unchanged by either process and, consequently, are optimal. In some cases, GPI can be\r
proved to converge, most notably for the classical DP methods that we have presented in\r
this chapter. In other cases convergence has not been proved, but still the idea of GPI\r
improves our understanding of the methods.\r
It is not necessary to perform DP methods in complete sweeps through the state\r
set. Asynchronous DP methods are in-place iterative methods that update states in an\r
arbitrary order, perhaps stochastically determined and using out-of-date information.\r
Many of these methods can be viewed as fine-grained forms of GPI.

4.8. Summary 89\r
Finally, we note one last special property of DP methods. All of them update estimates\r
of the values of states based on estimates of the values of successor states. That is, they\r
update estimates on the basis of other estimates. We call this general idea bootstrapping.\r
Many reinforcement learning methods perform bootstrapping, even those that do not\r
require, as DP requires, a complete and accurate model of the environment. In the next\r
chapter we explore reinforcement learning methods that do not require a model and do\r
not bootstrap. In the chapter after that we explore methods that do not require a model\r
but do bootstrap. These key features and properties are separable, yet can be mixed in\r
interesting combinations.\r
Bibliographical and Historical Remarks\r
The term “dynamic programming” is due to Bellman (1957a), who showed how these\r
methods could be applied to a wide range of problems. Extensive treatments of DP can\r
be found in many texts, including Bertsekas (2005, 2012), Bertsekas and Tsitsiklis (1996),\r
Dreyfus and Law (1977), Ross (1983), White (1969), and Whittle (1982, 1983). Our\r
interest in DP is restricted to its use in solving MDPs, but DP also applies to other types\r
of problems. Kumar and Kanal (1988) provide a more general look at DP.\r
To the best of our knowledge, the first connection between DP and reinforcement\r
learning was made by Minsky (1961) in commenting on Samuel’s checkers player. In\r
a footnote, Minsky mentioned that it is possible to apply DP to problems in which\r
Samuel’s backing-up process can be handled in closed analytic form. This remark may\r
have misled artificial intelligence researchers into believing that DP was restricted to\r
analytically tractable problems and therefore largely irrelevant to artificial intelligence.\r
Andreae (1969) mentioned DP in the context of reinforcement learning. Werbos (1977)\r
suggested an approach to approximating DP called “heuristic dynamic programming”\r
that emphasizes gradient-descent methods for continuous-state problems (Werbos, 1982,\r
1987, 1988, 1989, 1992). These methods are closely related to the reinforcement learning\r
algorithms that we discuss in this book. Watkins (1989) was explicit in connecting\r
reinforcement learning to DP, characterizing a class of reinforcement learning methods as\r
“incremental dynamic programming.”\r
4.1–4 These sections describe well-established DP algorithms that are covered in any of\r
the general DP references cited above. The policy improvement theorem and the\r
policy iteration algorithm are due to Bellman (1957a) and Howard (1960). Our\r
presentation was influenced by the local view of policy improvement taken by\r
Watkins (1989). Our discussion of value iteration as a form of truncated policy\r
iteration is based on the approach of Puterman and Shin (1978), who presented a\r
class of algorithms called modified policy iteration, which includes policy iteration\r
and value iteration as special cases. An analysis showing how value iteration can\r
be made to find an optimal policy in finite time is given by Bertsekas (1987).\r
Iterative policy evaluation is an example of a classical successive approximation\r
algorithm for solving a system of linear equations. The version of the algorithm

90 Chapter 4: Dynamic Programming\r
that uses two arrays, one holding the old values while the other is updated, is\r
often called a Jacobi-style algorithm, after Jacobi’s classical use of this method.\r
It is also sometimes called a synchronous algorithm because the e↵ect is as if all\r
the values are updated at the same time. The second array is needed to simulate\r
this parallel computation sequentially. The in-place version of the algorithm\r
is often called a Gauss–Seidel-style algorithm after the classical Gauss–Seidel\r
algorithm for solving systems of linear equations. In addition to iterative policy\r
evaluation, other DP algorithms can be implemented in these di↵erent versions.\r
Bertsekas and Tsitsiklis (1989) provide excellent coverage of these variations and\r
their performance di↵erences.\r
4.5 Asynchronous DP algorithms are due to Bertsekas (1982, 1983), who also called\r
them distributed DP algorithms. The original motivation for asynchronous\r
DP was its implementation on a multiprocessor system with communication\r
delays between processors and no global synchronizing clock. These algorithms\r
are extensively discussed by Bertsekas and Tsitsiklis (1989). Jacobi-style and\r
Gauss–Seidel-style DP algorithms are special cases of the asynchronous version.\r
Williams and Baird (1990) presented DP algorithms that are asynchronous at a\r
finer grain than the ones we have discussed: the update operations themselves\r
are broken into steps that can be performed asynchronously.\r
4.7 This section, written with the help of Michael Littman, is based on Littman,\r
Dean, and Kaelbling (1995). The phrase “curse of dimensionality” is due to\r
Bellman (1957a).\r
Foundational work on the linear programming approach to reinforcement learning\r
was done by Daniela de Farias (de Farias, 2002; de Farias and Van Roy, 2003).

Chapter 5\r
Monte Carlo Methods\r
In this chapter we consider our first learning methods for estimating value functions and\r
discovering optimal policies. Unlike the previous chapter, here we do not assume complete\r
knowledge of the environment. Monte Carlo methods require only experience—sample\r
sequences of states, actions, and rewards from actual or simulated interaction with an\r
environment. Learning from actual experience is striking because it requires no prior\r
knowledge of the environment’s dynamics, yet can still attain optimal behavior. Learning\r
from simulated experience is also powerful. Although a model is required, the model need\r
only generate sample transitions, not the complete probability distributions of all possible\r
transitions that is required for dynamic programming (DP). In surprisingly many cases it\r
is easy to generate experience sampled according to the desired probability distributions,\r
but infeasible to obtain the distributions in explicit form.\r
Monte Carlo methods are ways of solving the reinforcement learning problem based on\r
averaging sample returns. To ensure that well-defined returns are available, here we define\r
Monte Carlo methods only for episodic tasks. That is, we assume experience is divided\r
into episodes, and that all episodes eventually terminate no matter what actions are\r
selected. Only on the completion of an episode are value estimates and policies changed.\r
Monte Carlo methods can thus be incremental in an episode-by-episode sense, but not in\r
a step-by-step (online) sense. The term “Monte Carlo” is often used more broadly for\r
any estimation method whose operation involves a significant random component. Here\r
we use it specifically for methods based on averaging complete returns (as opposed to\r
methods that learn from partial returns, considered in the next chapter).\r
Monte Carlo methods sample and average returns for each state–action pair much like\r
the bandit methods we explored in Chapter 2 sample and average rewards for each action.\r
The main di↵erence is that now there are multiple states, each acting like a di↵erent\r
bandit problem (like an associative-search or contextual bandit) and the di↵erent bandit\r
problems are interrelated. That is, the return after taking an action in one state depends\r
on the actions taken in later states in the same episode. Because all the action selections\r
are undergoing learning, the problem becomes nonstationary from the point of view of\r
the earlier state.

92 Chapter 5: Monte Carlo Methods\r
To handle the nonstationarity, we adapt the idea of general policy iteration (GPI)\r
developed in Chapter 4 for DP. Whereas there we computed value functions from knowledge\r
of the MDP, here we learn value functions from sample returns with the MDP. The value\r
functions and corresponding policies still interact to attain optimality in essentially the\r
same way (GPI). As in the DP chapter, first we consider the prediction problem (the\r
computation of v⇡ and q⇡ for a fixed arbitrary policy ⇡) then policy improvement, and,\r
finally, the control problem and its solution by GPI. Each of these ideas taken from DP\r
is extended to the Monte Carlo case in which only sample experience is available.\r
5.1 Monte Carlo Prediction\r
We begin by considering Monte Carlo methods for learning the state-value function for a\r
given policy. Recall that the value of a state is the expected return—expected cumulative\r
future discounted reward—starting from that state. An obvious way to estimate it from\r
experience, then, is simply to average the returns observed after visits to that state. As\r
more returns are observed, the average should converge to the expected value. This idea\r
underlies all Monte Carlo methods.\r
In particular, suppose we wish to estimate v⇡(s), the value of a state s under policy ⇡,\r
given a set of episodes obtained by following ⇡ and passing through s. Each occurrence\r
of state s in an episode is called a visit to s. Of course, s may be visited multiple times\r
in the same episode; let us call the first time it is visited in an episode the first visit\r
to s. The first-visit MC method estimates v⇡(s) as the average of the returns following\r
first visits to s, whereas the every-visit MC method averages the returns following all\r
visits to s. These two Monte Carlo (MC) methods are very similar but have slightly\r
di↵erent theoretical properties. First-visit MC has been most widely studied, dating back\r
to the 1940s, and is the one we focus on in this chapter. Every-visit MC extends more\r
naturally to function approximation and eligibility traces, as discussed in Chapters 9 and\r
12. First-visit MC is shown in procedural form in the box. Every-visit MC would be the\r
same except without the check for St having occurred earlier in the episode.\r
First-visit MC prediction, for estimating V ⇡ v⇡\r
Input: a policy ⇡ to be evaluated\r
Initialize:\r
V (s) 2 R, arbitrarily, for all s 2 S\r
Returns(s) an empty list, for all s 2 S\r
Loop forever (for each episode):\r
Generate an episode following ⇡: S0, A0, R1, S1, A1, R2,...,ST 1, AT 1, RT\r
G 0\r
Loop for each step of episode, t = T 1, T 2,..., 0:\r
G G + Rt+1\r
Unless St appears in S0, S1,...,St1:\r
Append G to Returns(St)\r
V (St) average(Returns(St))

5.1. Monte Carlo Prediction 93\r
Both first-visit MC and every-visit MC converge to v⇡(s) as the number of visits (or\r
first visits) to s goes to infinity. This is easy to see for the case of first-visit MC. In\r
this case each return is an independent, identically distributed estimate of v⇡(s) with\r
finite variance. By the law of large numbers the sequence of averages of these estimates\r
converges to their expected value. Each average is itself an unbiased estimate, and the\r
standard deviation of its error falls as 1/\r
pn, where n is the number of returns averaged.\r
Every-visit MC is less straightforward, but its estimates also converge quadratically to\r
v⇡(s) (Singh and Sutton, 1996).\r
The use of Monte Carlo methods is best illustrated through an example.\r
Example 5.1: Blackjack The object of the popular casino card game of blackjack is to\r
obtain cards the sum of whose numerical values is as great as possible without exceeding\r
21. All face cards count as 10, and an ace can count either as 1 or as 11. We consider\r
the version in which each player competes independently against the dealer. The game\r
begins with two cards dealt to both dealer and player. One of the dealer’s cards is face\r
up and the other is face down. If the player has 21 immediately (an ace and a 10-card),\r
it is called a natural. He then wins unless the dealer also has a natural, in which case the\r
game is a draw. If the player does not have a natural, then he can request additional\r
cards, one by one (hits), until he either stops (sticks) or exceeds 21 (goes bust). If he goes\r
bust, he loses; if he sticks, then it becomes the dealer’s turn. The dealer hits or sticks\r
according to a fixed strategy without choice: he sticks on any sum of 17 or greater, and\r
hits otherwise. If the dealer goes bust, then the player wins; otherwise, the outcome—win,\r
lose, or draw—is determined by whose final sum is closer to 21.\r
Playing blackjack is naturally formulated as an episodic finite MDP. Each game of\r
blackjack is an episode. Rewards of +1, 1, and 0 are given for winning, losing, and\r
drawing, respectively. All rewards within a game are zero, and we do not discount ( = 1);\r
therefore these terminal rewards are also the returns. The player’s actions are to hit or\r
to stick. The states depend on the player’s cards and the dealer’s showing card. We\r
assume that cards are dealt from an infinite deck (i.e., with replacement) so that there is\r
no advantage to keeping track of the cards already dealt. If the player holds an ace that\r
he could count as 11 without going bust, then the ace is said to be usable. In this case\r
it is always counted as 11 because counting it as 1 would make the sum 11 or less, in\r
which case there is no decision to be made because, obviously, the player should always\r
hit. Thus, the player makes decisions on the basis of three variables: his current sum\r
(12–21), the dealer’s one showing card (ace–10), and whether or not he holds a usable\r
ace. This makes for a total of 200 states.\r
Consider the policy that sticks if the player’s sum is 20 or 21, and otherwise hits. To\r
find the state-value function for this policy by a Monte Carlo approach, one simulates\r
many blackjack games using the policy and averages the returns following each state.\r
In this way, we obtained the estimates of the state-value function shown in Figure 5.1.\r
The estimates for states with a usable ace are less certain and less regular because these\r
states are less common. In any event, after 500,000 games the value function is very well\r
approximated.

94 Chapter 5: Monte Carlo Methods\r
+1\r
!1\r
A\r
Dealer showing 10 12\r
Player sum\r
21\r
After 10,000 episodes After 500,000 episodes\r
Usable\r
ace\r
No\r
usable\r
ace\r
Figure 5.1: Approximate state-value functions for the blackjack policy that sticks only on 20\r
or 21, computed by Monte Carlo policy evaluation.\r
Exercise 5.1 Consider the diagrams on the right in Figure 5.1. Why does the estimated\r
value function jump up for the last two rows in the rear? Why does it drop o↵ for the\r
whole last row on the left? Why are the frontmost values higher in the upper diagrams\r
than in the lower? ⇤\r
Exercise 5.2 Suppose every-visit MC was used instead of first-visit MC on the blackjack\r
task. Would you expect the results to be very di↵erent? Why or why not? ⇤\r
Although we have complete knowledge of the environment in the blackjack task, it\r
would not be easy to apply DP methods to compute the value function. DP methods\r
require the distribution of next events—in particular, they require the environments\r
dynamics as given by the four-argument function p—and it is not easy to determine\r
this for blackjack. For example, suppose the player’s sum is 14 and he chooses to stick.\r
What is his probability of terminating with a reward of +1 as a function of the dealer’s\r
showing card? All of the probabilities must be computed before DP can be applied, and\r
such computations are often complex and error-prone. In contrast, generating the sample\r
games required by Monte Carlo methods is easy. This is the case surprisingly often; the\r
ability of Monte Carlo methods to work with sample episodes alone can be a significant\r
advantage even when one has complete knowledge of the environment’s dynamics.\r
Can we generalize the idea of backup diagrams to Monte Carlo algorithms? The\r
general idea of a backup diagram is to show at the top the root node to be updated and\r
to show below all the transitions and leaf nodes whose rewards and estimated values\r
contribute to the update. For Monte Carlo estimation of v⇡, the root is a state node, and\r
below it is the entire trajectory of transitions along a particular single episode, ending

5.1. Monte Carlo Prediction 95\r
at the terminal state, as shown to the right. Whereas the DP diagram (page 59)\r
shows all possible transitions, the Monte Carlo diagram shows only those sampled\r
on the one episode. Whereas the DP diagram includes only one-step transitions,\r
the Monte Carlo diagram goes all the way to the end of the episode. These\r
di↵erences in the diagrams accurately reflect the fundamental di↵erences between\r
the algorithms.\r
An important fact about Monte Carlo methods is that the estimates for each\r
state are independent. The estimate for one state does not build upon the estimate\r
of any other state, as is the case in DP. In other words, Monte Carlo methods do\r
not bootstrap as we defined it in the previous chapter.\r
In particular, note that the computational expense of estimating the value of\r
a single state is independent of the number of states. This can make Monte Carlo\r
methods particularly attractive when one requires the value of only one or a subset\r
of states. One can generate many sample episodes starting from the states of interest,\r
averaging returns from only these states, ignoring all others. This is a third advantage\r
Monte Carlo methods can have over DP methods (after the ability to learn from actual\r
experience and from simulated experience).\r
A bubble on a wire loop.\r
From Hersh and Griego (1969). Reproduced with\r
permission. ©1969 Scientific American, a divi\u0002sion of Nature America, Inc. All rights reserved.\r
Example 5.2: Soap Bubble Suppose a wire\r
frame forming a closed loop is dunked in soapy\r
water to form a soap surface or bubble conform\u0002ing at its edges to the wire frame. If the geom\u0002etry of the wire frame is irregular but known,\r
how can you compute the shape of the surface?\r
The shape has the property that the total force\r
on each point exerted by neighboring points is\r
zero (or else the shape would change). This\r
means that the surface’s height at any point is\r
the average of its heights at points in a small\r
circle around that point. In addition, the sur\u0002face must meet at its boundaries with the wire\r
frame. The usual approach to problems of this\r
kind is to put a grid over the area covered by\r
the surface and solve for its height at the grid points by an iterative computation. Grid\r
points at the boundary are forced to the wire frame, and all others are adjusted toward\r
the average of the heights of their four nearest neighbors. This process then iterates, much\r
like DP’s iterative policy evaluation, and ultimately converges to a close approximation\r
to the desired surface.\r
This is similar to the kind of problem for which Monte Carlo methods were originally\r
designed. Instead of the iterative computation described above, imagine standing on the\r
surface and taking a random walk, stepping randomly from grid point to neighboring\r
grid point, with equal probability, until you reach the boundary. It turns out that the\r
expected value of the height at the boundary is a close approximation to the height of\r
the desired surface at the starting point (in fact, it is exactly the value computed by the\r
iterative method described above). Thus, one can closely approximate the height of the

96 Chapter 5: Monte Carlo Methods\r
surface at a point by simply averaging the boundary heights of many walks started at\r
the point. If one is interested in only the value at one point, or any fixed small set of\r
points, then this Monte Carlo method can be far more ecient than the iterative method\r
based on local consistency.\r
5.2 Monte Carlo Estimation of Action Values\r
If a model is not available, then it is particularly useful to estimate action values (the\r
values of state–action pairs) rather than state values. With a model, state values alone are\r
sucient to determine a policy; one simply looks ahead one step and chooses whichever\r
action leads to the best combination of reward and next state, as we did in the chapter on\r
DP. Without a model, however, state values alone are not sucient. One must explicitly\r
estimate the value of each action in order for the values to be useful in suggesting a policy.\r
Thus, one of our primary goals for Monte Carlo methods is to estimate q⇤. To achieve\r
this, we first consider the policy evaluation problem for action values.\r
The policy evaluation problem for action values is to estimate q⇡(s, a), the expected\r
return when starting in state s, taking action a, and thereafter following policy ⇡. The\r
Monte Carlo methods for this are essentially the same as just presented for state values,\r
except now we talk about visits to a state–action pair rather than to a state. A state–\r
action pair s, a is said to be visited in an episode if ever the state s is visited and action\r
a is taken in it. The every-visit MC method estimates the value of a state–action pair\r
as the average of the returns that have followed all the visits to it. The first-visit MC\r
method averages the returns following the first time in each episode that the state was\r
visited and the action was selected. These methods converge quadratically, as before, to\r
the true expected values as the number of visits to each state–action pair approaches\r
infinity.\r
The only complication is that many state–action pairs may never be visited. If ⇡ is\r
a deterministic policy, then in following ⇡ one will observe returns only for one of the\r
actions from each state. With no returns to average, the Monte Carlo estimates of the\r
other actions will not improve with experience. This is a serious problem because the\r
purpose of learning action values is to help in choosing among the actions available in\r
each state. To compare alternatives we need to estimate the value of all the actions from\r
each state, not just the one we currently favor.\r
This is the general problem of maintaining exploration, as discussed in the context\r
of the k-armed bandit problem in Chapter 2. For policy evaluation to work for action\r
values, we must assure continual exploration. One way to do this is by specifying that\r
the episodes start in a state–action pair, and that every pair has a nonzero probability of\r
being selected as the start. This guarantees that all state–action pairs will be visited an\r
infinite number of times in the limit of an infinite number of episodes. We call this the\r
assumption of exploring starts.\r
The assumption of exploring starts is sometimes useful, but of course it cannot be\r
relied upon in general, particularly when learning directly from actual interaction with an\r
environment. In that case the starting conditions are unlikely to be so helpful. The most\r
common alternative approach to assuring that all state–action pairs are encountered is

5.3. Monte Carlo Control 97\r
to consider only policies that are stochastic with a nonzero probability of selecting all\r
actions in each state. We discuss two important variants of this approach in later sections.\r
For now, we retain the assumption of exploring starts and complete the presentation of a\r
full Monte Carlo control method.\r
Exercise 5.3 What is the backup diagram for Monte Carlo estimation of q⇡? ⇤\r
5.3 Monte Carlo Control\r
We are now ready to consider how Monte Carlo estimation can be used in control, that\r
is, to approximate optimal policies. The overall idea is to proceed according to the same\r
pattern as in the DP chapter, that is, according to the idea of generalized policy iteration\r
evaluation\r
improvement\r
⇡ Q\r
⇡  greedy(Q)\r
Q  q⇡\r
(GPI). In GPI one maintains both an approximate policy and\r
an approximate value function. The value function is repeatedly\r
altered to more closely approximate the value function for the\r
current policy, and the policy is repeatedly improved with respect\r
to the current value function, as suggested by the diagram to\r
the right. These two kinds of changes work against each other to\r
some extent, as each creates a moving target for the other, but\r
together they cause both policy and value function to approach\r
optimality.\r
To begin, let us consider a Monte Carlo version of classical policy iteration. In\r
this method, we perform alternating complete steps of policy evaluation and policy\r
improvement, beginning with an arbitrary policy ⇡0 and ending with the optimal policy\r
and optimal action-value function:\r
⇡0\r
E\r
! q⇡0\r
I\r
! ⇡1\r
E\r
! q⇡1\r
I\r
! ⇡2\r
E\r
! · · · I! ⇡⇤\r
E\r
! q⇤,\r
where E! denotes a complete policy evaluation and I! denotes a complete policy\r
improvement. Policy evaluation is done exactly as described in the preceding section.\r
Many episodes are experienced, with the approximate action-value function approaching\r
the true function asymptotically. For the moment, let us assume that we do indeed\r
observe an infinite number of episodes and that, in addition, the episodes are generated\r
with exploring starts. Under these assumptions, the Monte Carlo methods will compute\r
each q⇡k exactly, for arbitrary ⇡k.\r
Policy improvement is done by making the policy greedy with respect to the current\r
value function. In this case we have an action-value function, and therefore no model is\r
needed to construct the greedy policy. For any action-value function q, the corresponding\r
greedy policy is the one that, for each s 2 S, deterministically chooses an action with\r
maximal action-value:\r
⇡(s) .= arg maxa q(s, a). (5.1)\r
Policy improvement then can be done by constructing each ⇡k+1 as the greedy policy\r
with respect to q⇡k . The policy improvement theorem (Section 4.2) then applies to ⇡k

98 Chapter 5: Monte Carlo Methods\r
and ⇡k+1 because, for all s 2 S,\r
q⇡k (s, ⇡k+1(s)) = q⇡k (s, argmax aq⇡k (s, a))\r
= maxa q⇡k (s, a)\r
 q⇡k (s, ⇡k(s))\r
 v⇡k (s).\r
As we discussed in the previous chapter, the theorem assures us that each ⇡k+1 is uniformly\r
better than ⇡k, or just as good as ⇡k, in which case they are both optimal policies. This\r
in turn assures us that the overall process converges to the optimal policy and optimal\r
value function. In this way Monte Carlo methods can be used to find optimal policies\r
given only sample episodes and no other knowledge of the environment’s dynamics.\r
We made two unlikely assumptions above in order to easily obtain this guarantee of\r
convergence for the Monte Carlo method. One was that the episodes have exploring\r
starts, and the other was that policy evaluation could be done with an infinite number of\r
episodes. To obtain a practical algorithm we will have to remove both assumptions. We\r
postpone consideration of the first assumption until later in this chapter.\r
For now we focus on the assumption that policy evaluation operates on an infinite\r
number of episodes. This assumption is relatively easy to remove. In fact, the same issue\r
arises even in classical DP methods such as iterative policy evaluation, which also converge\r
only asymptotically to the true value function. In both DP and Monte Carlo cases there\r
are two ways to solve the problem. One is to hold firm to the idea of approximating q⇡k\r
in each policy evaluation. Measurements and assumptions are made to obtain bounds\r
on the magnitude and probability of error in the estimates, and then sucient steps are\r
taken during each policy evaluation to assure that these bounds are suciently small.\r
This approach can probably be made completely satisfactory in the sense of guaranteeing\r
correct convergence up to some level of approximation. However, it is also likely to require\r
far too many episodes to be useful in practice on any but the smallest problems.\r
There is a second approach to avoiding the infinite number of episodes nominally\r
required for policy evaluation, in which we give up trying to complete policy evaluation\r
before returning to policy improvement. On each evaluation step we move the value\r
function toward q⇡k , but we do not expect to actually get close except over many steps.\r
We used this idea when we first introduced the idea of GPI in Section 4.6. One extreme\r
form of the idea is value iteration, in which only one iteration of iterative policy evaluation\r
is performed between each step of policy improvement. The in-place version of value\r
iteration is even more extreme; there we alternate between improvement and evaluation\r
steps for single states.\r
For Monte Carlo policy iteration it is natural to alternate between evaluation and\r
improvement on an episode-by-episode basis. After each episode, the observed returns\r
are used for policy evaluation, and then the policy is improved at all the states visited in\r
the episode. A complete simple algorithm along these lines, which we call Monte Carlo\r
ES, for Monte Carlo with Exploring Starts, is given in pseudocode in the box on the next\r
page.

5.3. Monte Carlo Control 99\r
Monte Carlo ES (Exploring Starts), for estimating ⇡ ⇡ ⇡⇤\r
Initialize:\r
⇡(s) 2 A(s) (arbitrarily), for all s 2 S\r
Q(s, a) 2 R (arbitrarily), for all s 2 S, a 2 A(s)\r
Returns(s, a) empty list, for all s 2 S, a 2 A(s)\r
Loop forever (for each episode):\r
Choose S0 2 S, A0 2 A(S0) randomly such that all pairs have probability > 0\r
Generate an episode from S0, A0, following ⇡: S0, A0, R1,...,ST 1, AT 1, RT\r
G 0\r
Loop for each step of episode, t = T 1, T 2,..., 0:\r
G G + Rt+1\r
Unless the pair St, At appears in S0, A0, S1, A1 ...,St1, At1:\r
Append G to Returns(St, At)\r
Q(St, At) average(Returns(St, At))\r
⇡(St) argmaxa Q(St, a)\r
Exercise 5.4 The pseudocode for Monte Carlo ES is inecient because, for each state–\r
action pair, it maintains a list of all returns and repeatedly calculates their mean. It would\r
be more ecient to use techniques similar to those explained in Section 2.4 to maintain\r
just the mean and a count (for each state–action pair) and update them incrementally.\r
Describe how the pseudocode would be altered to achieve this. ⇤\r
In Monte Carlo ES, all the returns for each state–action pair are accumulated and\r
averaged, irrespective of what policy was in force when they were observed. It is easy\r
to see that Monte Carlo ES cannot converge to any suboptimal policy. If it did, then\r
the value function would eventually converge to the value function for that policy, and\r
that in turn would cause the policy to change. Stability is achieved only when both\r
the policy and the value function are optimal. Convergence to this optimal fixed point\r
seems inevitable as the changes to the action-value function decrease over time, but has\r
not yet been formally proved. In our opinion, this is one of the most fundamental open\r
theoretical questions in reinforcement learning (for a partial solution, see Tsitsiklis, 2002).\r
Example 5.3: Solving Blackjack It is straightforward to apply Monte Carlo ES to\r
blackjack. Because the episodes are all simulated games, it is easy to arrange for exploring\r
starts that include all possibilities. In this case one simply picks the dealer’s cards, the\r
player’s sum, and whether or not the player has a usable ace, all at random with equal\r
probability. As the initial policy we use the policy evaluated in the previous blackjack\r
example, that which sticks only on 20 or 21. The initial action-value function can be zero\r
for all state–action pairs. Figure 5.2 shows the optimal policy for blackjack found by\r
Monte Carlo ES. This policy is the same as the “basic” strategy of Thorp (1966) with the\r
sole exception of the leftmost notch in the policy for a usable ace, which is not present\r
in Thorp’s strategy. We are uncertain of the reason for this discrepancy, but confident\r
that what is shown here is indeed the optimal policy for the version of blackjack we have\r
described.

100 Chapter 5: Monte Carlo Methods\r
Usable\r
ace\r
No\r
usable\r
ace\r
20\r
A 2 3 4 5 6 7 8 9 10\r
Dealer showing\r
Player sum\r
HIT\r
STICK 19\r
21\r
11\r
12\r
13\r
14\r
15\r
16\r
17\r
18\r
!\r
*\r
A 2 3 4 5 6 7 8 9 10\r
HIT\r
STICK 20\r
19\r
21\r
11\r
12\r
13\r
14\r
15\r
16\r
17\r
18\r
V*\r
21\r
10 12\r
A\r
Dealer showing\r
Player sum\r
10\r
A 12\r
21\r
+1\r
"1\r
v*\r
Usable\r
ace\r
No\r
usable\r
ace\r
20\r
A 2 3 4 5 6 7 8 9 10\r
Dealer showing\r
Player sum\r
HIT\r
STICK 19\r
21\r
11\r
12\r
13\r
14\r
15\r
16\r
17\r
18\r
!\r
*\r
A 2 3 4 5 6 7 8 9 10\r
HIT\r
STICK 20\r
19\r
21\r
11\r
12\r
13\r
14\r
15\r
16\r
17\r
18\r
V*\r
21\r
10 12\r
A\r
Dealer showing\r
Player sum\r
10\r
A 12\r
21\r
+1\r
"1\r
v*\r
Usable\r
ace\r
No\r
usable\r
ace\r
20\r
A 2 3 4 5 6 7 8 9 10\r
Dealer showing\r
Player sum\r
HIT\r
STICK 19\r
21\r
11\r
12\r
13\r
14\r
15\r
16\r
17\r
18\r
!\r
A 2 3 4 5 6 7 8 9 10\r
HIT\r
STICK 20\r
19\r
21\r
11\r
12\r
13\r
14\r
15\r
16\r
17\r
18\r
V\r
21\r
10 12\r
A\r
Dealer showing\r
Player sum\r
10\r
A 12\r
21\r
+1\r
"1\r
v\r
Dealer showing\r
Player sum\r
* *\r
Figure 5.2: The optimal policy and state-value function for blackjack, found by Monte Carlo\r
ES. The state-value function shown was computed from the action-value function found by\r
Monte Carlo ES.\r
5.4 Monte Carlo Control without Exploring Starts\r
How can we avoid the unlikely assumption of exploring starts? The only general way to\r
ensure that all actions are selected infinitely often is for the agent to continue to select\r
them. There are two approaches to ensuring this, resulting in what we call on-policy\r
methods and o↵-policy methods. On-policy methods attempt to evaluate or improve the\r
policy that is used to make decisions, whereas o↵-policy methods evaluate or improve\r
a policy di↵erent from that used to generate the data. The Monte Carlo ES method\r
developed above is an example of an on-policy method. In this section we show how an\r
on-policy Monte Carlo control method can be designed that does not use the unrealistic\r
assumption of exploring starts. O↵-policy methods are considered in the next section.\r
In on-policy control methods the policy is generally soft, meaning that ⇡(a|s) > 0\r
for all s 2 S and all a 2 A(s), but gradually shifted closer and closer to a deterministic\r
optimal policy. Many of the methods discussed in Chapter 2 provide mechanisms for\r
this. The on-policy method we present in this section uses "-greedy policies, meaning\r
that most of the time they choose an action that has maximal estimated action value,\r
but with probability " they instead select an action at random. That is, all nongreedy\r
actions are given the minimal probability of selection, "\r
|A(s)| , and the remaining bulk of\r
the probability, 1  " + "\r
|A(s)| , is given to the greedy action. The "-greedy policies are\r
examples of "-soft policies, defined as policies for which ⇡(a|s)  "\r
|A(s)| for all states and\r
actions, for some " > 0. Among "-soft policies, "-greedy policies are in some sense those\r
that are closest to greedy.

5.4. Monte Carlo Control without Exploring Starts 101\r
The overall idea of on-policy Monte Carlo control is still that of GPI. As in Monte\r
Carlo ES, we use first-visit MC methods to estimate the action-value function for the\r
current policy. Without the assumption of exploring starts, however, we cannot simply\r
improve the policy by making it greedy with respect to the current value function, because\r
that would prevent further exploration of nongreedy actions. Fortunately, GPI does not\r
require that the policy be taken all the way to a greedy policy, only that it be moved\r
toward a greedy policy. In our on-policy method we will move it only to an "-greedy\r
policy. For any "-soft policy, ⇡, any "-greedy policy with respect to q⇡ is guaranteed to\r
be better than or equal to ⇡. The complete algorithm is given in the box below.\r
On-policy first-visit MC control (for "-soft policies), estimates ⇡ ⇡ ⇡⇤\r
Algorithm parameter: small " > 0\r
Initialize:\r
⇡ an arbitrary "-soft policy\r
Q(s, a) 2 R (arbitrarily), for all s 2 S, a 2 A(s)\r
Returns(s, a) empty list, for all s 2 S, a 2 A(s)\r
Repeat forever (for each episode):\r
Generate an episode following ⇡: S0, A0, R1,...,ST 1, AT 1, RT\r
G 0\r
Loop for each step of episode, t = T 1, T 2,..., 0:\r
G G + Rt+1\r
Unless the pair St, At appears in S0, A0, S1, A1 ...,St1, At1:\r
Append G to Returns(St, At)\r
Q(St, At) average(Returns(St, At))\r
A⇤ argmaxa Q(St, a) (with ties broken arbitrarily)\r
For all a 2 A(St):\r
⇡(a|St) \r
⇢ 1  " + "/|A(St)| if a = A⇤\r
"/|A(St)| if a 6= A⇤\r
That any "-greedy policy with respect to q⇡ is an improvement over any "-soft policy\r
⇡ is assured by the policy improvement theorem. Let ⇡0 be the "-greedy policy. The\r
conditions of the policy improvement theorem apply because for any s 2 S:\r
q⇡(s, ⇡0(s)) = X\r
a\r
⇡0(a|s)q⇡(s, a)\r
= "\r
|A(s)|\r
X\r
a\r
q⇡(s, a) + (1  ") maxa q⇡(s, a) (5.2)\r
\r
"\r
|A(s)|\r
X\r
a\r
q⇡(s, a) + (1  ")\r
X\r
a\r
⇡(a|s)  "\r
|A(s)|\r
1  "\r
q⇡(s, a)

102 Chapter 5: Monte Carlo Methods\r
(the sum is a weighted average with nonnegative weights summing to 1, and as such it\r
must be less than or equal to the largest number averaged)\r
= "\r
|A(s)|\r
X\r
a\r
q⇡(s, a)  "\r
|A(s)|\r
X\r
a\r
q⇡(s, a) + X\r
a\r
⇡(a|s)q⇡(s, a)\r
= v⇡(s).\r
Thus, by the policy improvement theorem, ⇡0  ⇡ (i.e., v⇡0 (s)  v⇡(s), for all s 2 S). We\r
now prove that equality can hold only when both ⇡0 and ⇡ are optimal among the "-soft\r
policies, that is, when they are better than or equal to all other "-soft policies.\r
Consider a new environment that is just like the original environment, except with the\r
requirement that policies be "-soft “moved inside” the environment. The new environment\r
has the same action and state set as the original and behaves as follows. If in state s\r
and taking action a, then with probability 1  " the new environment behaves exactly\r
like the old environment. With probability " it repicks the action at random, with equal\r
probabilities, and then behaves like the old environment with the new, random action.\r
The best one can do in this new environment with general policies is the same as the\r
best one could do in the original environment with "-soft policies. Let ve⇤ and qe⇤ denote\r
the optimal value functions for the new environment. Then a policy ⇡ is optimal among\r
"-soft policies if and only if v⇡ = ve⇤. We know that ve⇤ is the unique solution to the\r
Bellman optimality equation (3.19) with altered transition probabilities:\r
ve⇤(s) = maxa\r
X\r
s0,r\r
h\r
(1  ")p(s0, r|s, a) +X\r
a0\r
"\r
|A(s)|\r
p(s0, r|s, a0)\r
ihr + ve⇤(s0\r
)\r
i\r
= (1  ") maxa\r
X\r
s0,r\r
p(s0, r|s, a)\r
h\r
r + ve⇤(s0)\r
i\r
+\r
"\r
|A(s)|\r
X\r
a\r
X\r
s0,r\r
p(s0, r|s, a)\r
h\r
r + ve⇤(s0)\r
i\r
.\r
When equality holds and the "-soft policy ⇡ is no longer improved, then we also know,\r
from (5.2), that\r
v⇡(s) = (1  ") maxa q⇡(s, a) + "\r
|A(s)|\r
X\r
a\r
q⇡(s, a)\r
= (1  ") maxa\r
X\r
s0,r\r
p(s0, r|s, a)\r
h\r
r + v⇡(s0)\r
i\r
+\r
"\r
|A(s)|\r
X\r
a\r
X\r
s0,r\r
p(s0, r|s, a)\r
h\r
r + v⇡(s0)\r
i\r
.\r
However, this equation is the same as the previous one, except for the substitution of v⇡\r
for ve⇤. Because ve⇤ is the unique solution, it must be that v⇡ = ve⇤.\r
In essence, we have shown in the last few pages that policy iteration works for "-soft\r
policies. Using the natural notion of greedy policy for "-soft policies, one is assured of\r
improvement on every step, except when the best policy has been found among the "-soft\r
policies. This analysis is independent of how the action-value functions are determined

5.5. O↵-policy Prediction via Importance Sampling 103\r
at each stage, but it does assume that they are computed exactly. This brings us to\r
roughly the same point as in the previous section. Now we only achieve the best policy\r
among the "-soft policies, but on the other hand, we have eliminated the assumption of\r
exploring starts.\r
5.5 O↵-policy Prediction via Importance Sampling\r
All learning control methods face a dilemma: They seek to learn action values conditional\r
on subsequent optimal behavior, but they need to behave non-optimally in order to\r
explore all actions (to find the optimal actions). How can they learn about the optimal\r
policy while behaving according to an exploratory policy? The on-policy approach in the\r
preceding section is actually a compromise—it learns action values not for the optimal\r
policy, but for a near-optimal policy that still explores. A more straightforward approach\r
is to use two policies, one that is learned about and that becomes the optimal policy, and\r
one that is more exploratory and is used to generate behavior. The policy being learned\r
about is called the target policy, and the policy used to generate behavior is called the\r
behavior policy. In this case we say that learning is from data “o↵” the target policy, and\r
the overall process is termed o↵-policy learning.\r
Throughout the rest of this book we consider both on-policy and o↵-policy methods.\r
On-policy methods are generally simpler and are considered first. O↵-policy methods\r
require additional concepts and notation, and because the data is due to a di↵erent policy,\r
o↵-policy methods are often of greater variance and are slower to converge. On the other\r
hand, o↵-policy methods are more powerful and general. They include on-policy methods\r
as the special case in which the target and behavior policies are the same. O↵-policy\r
methods also have a variety of additional uses in applications. For example, they can\r
often be applied to learn from data generated by a conventional non-learning controller,\r
or from a human expert. O↵-policy learning is also seen by some as key to learning\r
multi-step predictive models of the world’s dynamics (see Section 17.2; Sutton, 2009;\r
Sutton et al., 2011).\r
In this section we begin the study of o↵-policy methods by considering the prediction\r
problem, in which both target and behavior policies are fixed. That is, suppose we wish\r
to estimate v⇡ or q⇡, but all we have are episodes following another policy b, where\r
b 6= ⇡. In this case, ⇡ is the target policy, b is the behavior policy, and both policies are\r
considered fixed and given.\r
In order to use episodes from b to estimate values for ⇡, we require that every action\r
taken under ⇡ is also taken, at least occasionally, under b. That is, we require that\r
⇡(a|s) > 0 implies b(a|s) > 0. This is called the assumption of coverage. It follows\r
from coverage that b must be stochastic in states where it is not identical to ⇡. The\r
target policy ⇡, on the other hand, may be deterministic, and, in fact, this is a case\r
of particular interest in control applications. In control, the target policy is typically\r
the deterministic greedy policy with respect to the current estimate of the action-value\r
function. This policy becomes a deterministic optimal policy while the behavior policy\r
remains stochastic and more exploratory, for example, an "-greedy policy. In this section,\r
however, we consider the prediction problem, in which ⇡ is unchanging and given.

104 Chapter 5: Monte Carlo Methods\r
Almost all o↵-policy methods utilize importance sampling, a general technique for\r
estimating expected values under one distribution given samples from another. We apply\r
importance sampling to o↵-policy learning by weighting returns according to the relative\r
probability of their trajectories occurring under the target and behavior policies, called\r
the importance-sampling ratio. Given a starting state St, the probability of the subsequent\r
state–action trajectory, At, St+1, At+1,...,ST , occurring under any policy ⇡ is\r
Pr{At, St+1, At+1,...,ST | St, At:T 1 ⇠ ⇡}\r
= ⇡(At|St)p(St+1 |St, At)⇡(At+1|St+1)··· p(ST |ST 1, AT 1)\r
=\r
T\r
Y1\r
k=t\r
⇡(Ak|Sk)p(Sk+1 |Sk, Ak),\r
where p here is the state-transition probability function defined by (3.4). Thus, the relative\r
probability of the trajectory under the target and behavior policies (the importance\u0002sampling ratio) is\r
⇢t:T 1\r
.\r
=\r
QT 1\r
k=t ⇡(Ak|Sk)p(Sk+1 |Sk, Ak)\r
QT 1\r
k=t b(Ak|Sk)p(Sk+1 |Sk, Ak) =\r
T\r
Y1\r
k=t\r
⇡(Ak|Sk)\r
b(Ak|Sk)\r
. (5.3)\r
Although the trajectory probabilities depend on the MDP’s transition probabilities, which\r
are generally unknown, they appear identically in both the numerator and denominator,\r
and thus cancel. The importance sampling ratio ends up depending only on the two\r
policies and the sequence, not on the MDP.\r
Recall that we wish to estimate the expected returns (values) under the target policy,\r
but all we have are returns Gt due to the behavior policy. These returns have the wrong\r
expectation E[Gt|St =s] = vb(s) and so cannot be averaged to obtain v⇡. This is where\r
importance sampling comes in. The ratio ⇢t:T 1 transforms the returns to have the right\r
expected value:\r
E[⇢t:T 1Gt | St =s] = v⇡(s). (5.4)\r
Now we are ready to give a Monte Carlo algorithm that averages returns from a batch\r
of observed episodes following policy b to estimate v⇡(s). It is convenient here to number\r
time steps in a way that increases across episode boundaries. That is, if the first episode\r
of the batch ends in a terminal state at time 100, then the next episode begins at time\r
t = 101. This enables us to use time-step numbers to refer to particular steps in particular\r
episodes. In particular, we can define the set of all time steps in which state s is visited,\r
denoted T(s). This is for an every-visit method; for a first-visit method, T(s) would only\r
include time steps that were first visits to s within their episodes. Also, let T(t) denote\r
the first time of termination following time t, and Gt denote the return after t up through\r
T(t). Then {Gt}t2T(s) are the returns that pertain to state s, and ⇢t:T(t)1\r
 \r
t2T(s) are\r
the corresponding importance-sampling ratios. To estimate v⇡(s), we simply scale the\r
returns by the ratios and average the results:\r
V (s) .=\r
P\r
t2T(s) ⇢t:T(t)1Gt\r
|T(s)| . (5.5)

5.5. O↵-policy Prediction via Importance Sampling 105\r
When importance sampling is done as a simple average in this way it is called ordinary\r
importance sampling.\r
An important alternative is weighted importance sampling, which uses a weighted\r
average, defined as\r
V (s) .=\r
P\r
t2T(s) ⇢t:T(t)1Gt P\r
t2T(s) ⇢t:T(t)1\r
, (5.6)\r
or zero if the denominator is zero. To understand these two varieties of importance\r
sampling, consider the estimates of their first-visit methods after observing a single return\r
from state s. In the weighted-average estimate, the ratio ⇢t:T(t)1 for the single return\r
cancels in the numerator and denominator, so that the estimate is equal to the observed\r
return independent of the ratio (assuming the ratio is nonzero). Given that this return\r
was the only one observed, this is a reasonable estimate, but its expectation is vb(s) rather\r
than v⇡(s), and in this statistical sense it is biased. In contrast, the first-visit version\r
of the ordinary importance-sampling estimator (5.5) is always v⇡(s) in expectation (it\r
is unbiased), but it can be extreme. Suppose the ratio were ten, indicating that the\r
trajectory observed is ten times as likely under the target policy as under the behavior\r
policy. In this case the ordinary importance-sampling estimate would be ten times the\r
observed return. That is, it would be quite far from the observed return even though the\r
episode’s trajectory is considered very representative of the target policy.\r
Formally, the di↵erence between the first-visit methods of the two kinds of importance\r
sampling is expressed in their biases and variances. Ordinary importance sampling is\r
unbiased whereas weighted importance sampling is biased (though the bias converges\r
asymptotically to zero). On the other hand, the variance of ordinary importance sampling\r
is in general unbounded because the variance of the ratios can be unbounded, whereas in\r
the weighted estimator the largest weight on any single return is one. In fact, assuming\r
bounded returns, the variance of the weighted importance-sampling estimator converges\r
to zero even if the variance of the ratios themselves is infinite (Precup, Sutton, and\r
Dasgupta 2001). In practice, the weighted estimator usually has dramatically lower\r
variance and is strongly preferred. Nevertheless, we will not totally abandon ordinary\r
importance sampling as it is easier to extend to the approximate methods using function\r
approximation that we explore in the second part of this book.\r
The every-visit methods for ordinary and weighed importance sampling are both biased,\r
though, again, the bias falls asymptotically to zero as the number of samples increases.\r
In practice, every-visit methods are often preferred because they remove the need to keep\r
track of which states have been visited and because they are much easier to extend to\r
approximations. A complete every-visit MC algorithm for o↵-policy policy evaluation\r
using weighted importance sampling is given in the next section on page 110.\r
Exercise 5.5 Consider an MDP with a single nonterminal state and a single action\r
that transitions back to the nonterminal state with probability p and transitions to the\r
terminal state with probability 1p. Let the reward be +1 on all transitions, and let\r
 = 1. Suppose you observe one episode that lasts 10 steps, with a return of 10. What\r
are the first-visit and every-visit estimators of the value of the nonterminal state? ⇤

106 Chapter 5: Monte Carlo Methods\r
Example 5.4: O↵-policy Estimation of a Blackjack State Value We applied\r
both ordinary and weighted importance-sampling methods to estimate the value of a single\r
blackjack state (Example 5.1) from o↵-policy data. Recall that one of the advantages of\r
Monte Carlo methods is that they can be used to evaluate a single state without forming\r
estimates for any other states. In this example, we evaluated the state in which the dealer\r
is showing a deuce, the sum of the player’s cards is 13, and the player has a usable ace (that\r
is, the player holds an ace and a deuce, or equivalently three aces). The data was generated\r
by starting in this state then choosing to hit or stick at random with equal probability\r
(the behavior policy). The target policy was to stick only on a sum of 20 or 21, as in\r
Example 5.1. The value of this state under the target policy is approximately 0.27726\r
(this was determined by separately generating one-hundred million episodes using the\r
target policy and averaging their returns). Both o↵-policy methods closely approximated\r
this value after 1000 o↵-policy episodes using the random policy. To make sure they did\r
this reliably, we performed 100 independent runs, each starting from estimates of zero\r
and learning for 10,000 episodes. Figure 5.3 shows the resultant learning curves—the\r
squared error of the estimates of each method as a function of number of episodes,\r
averaged over the 100 runs. The error approaches zero for both algorithms, but the\r
weighted importance-sampling method has much lower error at the beginning, as is typical\r
in practice.\r
Ordinary \r
importance \r
sampling\r
Weighted importance sampling\r
Episodes (log scale)\r
0 10 100 1000 10,000\r
Mean\r
square\r
error\r
(average over\r
100 runs)\r
0\r
5\r
Figure 5.3: Weighted importance sampling produces lower error estimates of the value of a\r
single blackjack state from o↵-policy episodes.\r
Example 5.5: Infinite Variance The estimates of ordinary importance sampling will\r
typically have infinite variance, and thus unsatisfactory convergence properties, whenever\r
the scaled returns have infinite variance—and this can easily happen in o↵-policy learning\r
when trajectories contain loops. A simple example is shown inset in Figure 5.4. There is\r
only one nonterminal state s and two actions, right and left. The right action causes a\r
deterministic transition to termination, whereas the left action transitions, with probability\r
0.9, back to s or, with probability 0.1, on to termination. The rewards are +1 on the\r
latter transition and otherwise zero. Consider the target policy that always selects left.\r
All episodes under this policy consist of some number (possibly zero) of transitions back

5.5. O↵-policy Prediction via Importance Sampling 107\r
to s followed by termination with a reward and return of +1. Thus the value of s under\r
the target policy is 1 ( = 1). Suppose we are estimating this value from o↵-policy data\r
using the behavior policy that selects right and left with equal probability.\r
1\r
100,000 1,000,000 10,000,000 100,000,000\r
2\r
0.1\r
0.9\r
R = +1\r
s\r
⇡(left|s)=1\r
left right\r
R = 0\r
R = 0\r
b(left|s) = 1\r
2\r
v⇡(s)\r
Monte-Carlo \r
estimate of \r
 with \r
ordinary\r
importance \r
sampling\r
(ten runs)\r
Episodes (log scale)\r
1 10 100 1000 10,000\r
0\r
Figure 5.4: Ordinary importance sampling produces surprisingly unstable estimates on the\r
one-state MDP shown inset (Example 5.5). The correct estimate here is 1 ( = 1), and, even\r
though this is the expected value of a sample return (after importance sampling), the variance\r
of the samples is infinite, and the estimates do not converge to this value. These results are for\r
o↵-policy first-visit MC.\r
The lower part of Figure 5.4 shows ten independent runs of the first-visit MC algorithm\r
using ordinary importance sampling. Even after millions of episodes, the estimates fail\r
to converge to the correct value of 1. In contrast, the weighted importance-sampling\r
algorithm would give an estimate of exactly 1 forever after the first episode that ended\r
with the left action. All returns not equal to 1 (that is, ending with the right action)\r
would be inconsistent with the target policy and thus would have a ⇢t:T(t)1 of zero and\r
contribute neither to the numerator nor denominator of (5.6). The weighted importance\u0002sampling algorithm produces a weighted average of only the returns consistent with the\r
target policy, and all of these would be exactly 1.\r
We can verify that the variance of the importance-sampling-scaled returns is infinite\r
in this example by a simple calculation. The variance of any random variable X is the\r
expected value of the deviation from its mean X¯, which can be written\r
Var[X] .= E\r
h\r
X  X¯2\r
i\r
= E\r
⇥\r
X2  2XX¯ + X¯ 2⇤= E\r
⇥\r
X2⇤ X¯ 2.\r
Thus, if the mean is finite, as it is in our case, the variance is infinite if and only if the\r
expectation of the square of the random variable is infinite. Thus, we need only show

108 Chapter 5: Monte Carlo Methods\r
that the expected square of the importance-sampling-scaled return is infinite:\r
E\r
2\r
4\r
 T\r
Y1\r
t=0\r
⇡(At|St)\r
b(At|St)\r
G0\r
!23\r
5 .\r
To compute this expectation, we break it down into cases based on episode length and\r
termination. First note that, for any episode ending with the right action, the importance\r
sampling ratio is zero, because the target policy would never take this action; these\r
episodes thus contribute nothing to the expectation (the quantity in parenthesis will be\r
zero) and can be ignored. We need only consider episodes that involve some number\r
(possibly zero) of left actions that transition back to the nonterminal state, followed by a\r
left action transitioning to termination. All of these episodes have a return of 1, so the\r
G0 factor can be ignored. To get the expected square we need only consider each length\r
of episode, multiplying the probability of the episode’s occurrence by the square of its\r
importance-sampling ratio, and add these up:\r
= 1\r
2 · 0.1\r
✓ 1\r
0.5\r
◆2\r
(the length 1 episode)\r
+\r
1\r
2 · 0.9 ·\r
1\r
2 · 0.1\r
✓ 1\r
0.5\r
1\r
0.5\r
◆2\r
(the length 2 episode)\r
+\r
1\r
2 · 0.9 ·\r
1\r
2 · 0.9 ·\r
1\r
2 · 0.1\r
✓ 1\r
0.5\r
1\r
0.5\r
1\r
0.5\r
◆2\r
(the length 3 episode)\r
+ ···\r
= 0.1\r
X1\r
k=0\r
0.9k · 2k · 2= 0.2\r
X1\r
k=0\r
1.8k = 1.\r
Exercise 5.6 What is the equation analogous to (5.6) for action values Q(s, a) instead of\r
state values V (s), again given returns generated using b? ⇤\r
Exercise 5.7 In learning curves such as those shown in Figure 5.3 error generally decreases\r
with training, as indeed happened for the ordinary importance-sampling method. But for\r
the weighted importance-sampling method error first increased and then decreased. Why\r
do you think this happened? ⇤\r
Exercise 5.8 The results with Example 5.5 and shown in Figure 5.4 used a first-visit MC\r
method. Suppose that instead an every-visit MC method was used on the same problem.\r
Would the variance of the estimator still be infinite? Why or why not? ⇤

5.7. O↵-policy Monte Carlo Control 109\r
5.6 Incremental Implementation\r
Monte Carlo prediction methods can be implemented incrementally, on an episode-by\u0002episode basis, using extensions of the techniques described in Chapter 2 (Section 2.4).\r
Whereas in Chapter 2 we averaged rewards, in Monte Carlo methods we average returns.\r
In all other respects exactly the same methods as used in Chapter 2 can be used for on\u0002policy Monte Carlo methods. For o↵-policy Monte Carlo methods, we need to separately\r
consider those that use ordinary importance sampling and those that use weighted\r
importance sampling.\r
In ordinary importance sampling, the returns are scaled by the importance sampling\r
ratio ⇢t:T(t)1 (5.3), then simply averaged, as in (5.5). For these methods we can again\r
use the incremental methods of Chapter 2, but using the scaled returns in place of\r
the rewards of that chapter. This leaves the case of o↵-policy methods using weighted\r
importance sampling. Here we have to form a weighted average of the returns, and a\r
slightly di↵erent incremental algorithm is required.\r
Suppose we have a sequence of returns G1, G2,...,Gn1, all starting in the same state\r
and each with a corresponding random weight Wi (e.g., Wi = ⇢ti:T(ti)1). We wish to\r
form the estimate\r
Vn\r
.\r
=\r
Pn1\r
k=1 WkGk\r
Pn1\r
k=1 Wk\r
, n  2, (5.7)\r
and keep it up-to-date as we obtain a single additional return Gn. In addition to keeping\r
track of Vn, we must maintain for each state the cumulative sum Cn of the weights given\r
to the first n returns. The update rule for Vn is\r
Vn+1\r
.\r
= Vn +\r
Wn\r
Cn\r
h\r
Gn  Vn\r
i\r
, n  1, (5.8)\r
and\r
Cn+1\r
.\r
= Cn + Wn+1,\r
where C0\r
.\r
= 0 (and V1 is arbitrary and thus need not be specified). The box on the\r
next page contains a complete episode-by-episode incremental algorithm for Monte Carlo\r
policy evaluation. The algorithm is nominally for the o↵-policy case, using weighted\r
importance sampling, but applies as well to the on-policy case just by choosing the\r
target and behavior policies as the same (in which case (⇡ = b), W is always 1). The\r
approximation Q converges to q⇡ (for all encountered state–action pairs) while actions\r
are selected according to a potentially di↵erent policy, b.\r
Exercise 5.9 Modify the algorithm for first-visit MC policy evaluation (Section 5.1) to\r
use the incremental implementation for sample averages described in Section 2.4. ⇤\r
Exercise 5.10 Derive the weighted-average update rule (5.8) from (5.7). Follow the\r
pattern of the derivation of the unweighted rule (2.3). ⇤

110 Chapter 5: Monte Carlo Methods\r
O↵-policy MC prediction (policy evaluation) for estimating Q ⇡ q⇡\r
Input: an arbitrary target policy ⇡\r
Initialize, for all s 2 S, a 2 A(s):\r
Q(s, a) 2 R (arbitrarily)\r
C(s, a) 0\r
Loop forever (for each episode):\r
b any policy with coverage of ⇡\r
Generate an episode following b: S0, A0, R1,...,ST 1, AT 1, RT\r
G 0\r
W 1\r
Loop for each step of episode, t = T 1, T 2,..., 0, while W 6= 0:\r
G G + Rt+1\r
C(St, At) C(St, At) + W\r
Q(St, At) Q(St, At) + W\r
C(St,At) [G  Q(St, At)]\r
W W ⇡(At|St)\r
b(At|St)\r
5.7 O↵-policy Monte Carlo Control\r
We are now ready to present an example of the second class of learning control methods\r
we consider in this book: o↵-policy methods. Recall that the distinguishing feature of\r
on-policy methods is that they estimate the value of a policy while using it for control.\r
In o↵-policy methods these two functions are separated. The policy used to generate\r
behavior, called the behavior policy, may in fact be unrelated to the policy that is\r
evaluated and improved, called the target policy. An advantage of this separation is\r
that the target policy may be deterministic (e.g., greedy), while the behavior policy can\r
continue to sample all possible actions.\r
O↵-policy Monte Carlo control methods use one of the techniques presented in the\r
preceding two sections. They follow the behavior policy while learning about and\r
improving the target policy. These techniques require that the behavior policy has a\r
nonzero probability of selecting all actions that might be selected by the target policy\r
(coverage). To explore all possibilities, we require that the behavior policy be soft (i.e.,\r
that it select all actions in all states with nonzero probability).\r
The box on the next page shows an o↵-policy Monte Carlo control method, based on\r
GPI and weighted importance sampling, for estimating ⇡⇤ and q⇤. The target policy\r
⇡ ⇡ ⇡⇤ is the greedy policy with respect to Q, which is an estimate of q⇡. The behavior\r
policy b can be anything, but in order to assure convergence of ⇡ to the optimal policy, an\r
infinite number of returns must be obtained for each pair of state and action. This can be\r
assured by choosing b to be "-soft. The policy ⇡ converges to optimal at all encountered\r
states even though actions are selected according to a di↵erent soft policy b, which may\r
change between or even within episodes.

5.7. O↵-policy Monte Carlo Control 111\r
O↵-policy MC control, for estimating ⇡ ⇡ ⇡⇤\r
Initialize, for all s 2 S, a 2 A(s):\r
Q(s, a) 2 R (arbitrarily)\r
C(s, a) 0\r
⇡(s) argmaxa Q(s, a) (with ties broken consistently)\r
Loop forever (for each episode):\r
b any soft policy\r
Generate an episode using b: S0, A0, R1,...,ST 1, AT 1, RT\r
G 0\r
W 1\r
Loop for each step of episode, t = T 1, T 2,..., 0:\r
G G + Rt+1\r
C(St, At) C(St, At) + W\r
Q(St, At) Q(St, At) + W\r
C(St,At) [G  Q(St, At)]\r
⇡(St) argmaxa Q(St, a) (with ties broken consistently)\r
If At 6= ⇡(St) then exit inner Loop (proceed to next episode)\r
W W 1\r
b(At|St)\r
A potential problem is that this method learns only from the tails of episodes, when\r
all of the remaining actions in the episode are greedy. If nongreedy actions are common,\r
then learning will be slow, particularly for states appearing in the early portions of\r
long episodes. Potentially, this could greatly slow learning. There has been insucient\r
experience with o↵-policy Monte Carlo methods to assess how serious this problem is. If\r
it is serious, the most important way to address it is probably by incorporating temporal\u0002di↵erence learning, the algorithmic idea developed in the next chapter. Alternatively, if \r
is less than 1, then the idea developed in the next section may also help significantly.\r
Exercise 5.11 In the boxed algorithm for o↵-policy MC control, you may have been\r
expecting the W update to have involved the importance-sampling ratio ⇡(At|St)\r
b(At|St) , but\r
instead it involves 1\r
b(At|St) . Why is this nevertheless correct? ⇤\r
Exercise 5.12: Racetrack (programming) Consider driving a race car around a turn\r
like those shown in Figure 5.5. You want to go as fast as possible, but not so fast as\r
to run o↵ the track. In our simplified racetrack, the car is at one of a discrete set of\r
grid positions, the cells in the diagram. The velocity is also discrete, a number of grid\r
cells moved horizontally and vertically per time step. The actions are increments to the\r
velocity components. Each may be changed by +1, 1, or 0 in each step, for a total of\r
nine (3 ⇥ 3) actions. Both velocity components are restricted to be nonnegative and less\r
than 5, and they cannot both be zero except at the starting line. Each episode begins\r
in one of the randomly selected start states with both velocity components zero and\r
ends when the car crosses the finish line. The rewards are 1 for each step until the car\r
crosses the finish line. If the car hits the track boundary, it is moved back to a random\r
position on the starting line, both velocity components are reduced to zero, and the

112 Chapter 5: Monte Carlo Methods\r
Starting line\r
Finish\r
line\r
Starting line\r
Finish\r
line\r
Figure 5.5: A couple of right turns for the racetrack task.\r
episode continues. Before updating the car’s location at each time step, check to see if\r
the projected path of the car intersects the track boundary. If it intersects the finish line,\r
the episode ends; if it intersects anywhere else, the car is considered to have hit the track\r
boundary and is sent back to the starting line. To make the task more challenging, with\r
probability 0.1 at each time step the velocity increments are both zero, independently of\r
the intended increments. Apply a Monte Carlo control method to this task to compute\r
the optimal policy from each starting state. Exhibit several trajectories following the\r
optimal policy (but turn the noise o↵ for these trajectories). ⇤\r
5.8 *Discounting-aware Importance Sampling\r
The o↵-policy methods that we have considered so far are based on forming importance\u0002sampling weights for returns considered as unitary wholes, without taking into account\r
the returns’ internal structures as sums of discounted rewards. We now briefly consider\r
cutting-edge research ideas for using this structure to significantly reduce the variance of\r
o↵-policy estimators.\r
For example, consider the case where episodes are long and  is significantly less than\r
1. For concreteness, say that episodes last 100 steps and that  = 0. The return from\r
time 0 will then be just G0 = R1, but its importance sampling ratio will be a product of\r
100 factors, ⇡(A0|S0)\r
b(A0|S0)\r
⇡(A1|S1)\r
b(A1|S1) ··· ⇡(A99|S99)b(A99|S99) . In ordinary importance sampling, the return\r
will be scaled by the entire product, but it is really only necessary to scale by the first\r
factor, by ⇡(A0|S0)\r
b(A0|S0) . The other 99 factors ⇡(A1|S1)b(A1|S1) ··· ⇡(A99|S99)b(A99|S99) are irrelevant because\r
after the first reward the return has already been determined. These later factors are\r
all independent of the return and of expected value 1; they do not change the expected\r
update, but they add enormously to its variance. In some cases they could even make the\r
variance infinite. Let us now consider an idea for avoiding this large extraneous variance.

5.9. Per-decision Importance Sampling 113\r
The essence of the idea is to think of discounting as determining a probability of\r
termination or, equivalently, a degree of partial termination. For any  2 [0, 1), we can\r
think of the return G0 as partly terminating in one step, to the degree 1  , producing\r
a return of just the first reward, R1, and as partly terminating after two steps, to the\r
degree (1  ), producing a return of R1 + R2, and so on. The latter degree corresponds\r
to terminating on the second step, 1  , and not having already terminated on the\r
first step, . The degree of termination on the third step is thus (1  )2, with the 2\r
reflecting that termination did not occur on either of the first two steps. The partial\r
returns here are called flat partial returns:\r
G¯t:h\r
.\r
= Rt+1 + Rt+2 + ··· + Rh, 0  t<h  T,\r
where “flat” denotes the absence of discounting, and “partial” denotes that these returns\r
do not extend all the way to termination but instead stop at h, called the horizon (and T\r
is the time of termination of the episode). The conventional full return Gt can be viewed\r
as a sum of flat partial returns as suggested above as follows:\r
Gt\r
.\r
= Rt+1 + Rt+2 + 2Rt+3 + ··· + T t1RT\r
= (1  )Rt+1\r
+ (1  ) (Rt+1 + Rt+2)\r
+ (1  )2 (Rt+1 + Rt+2 + Rt+3)\r
.\r
.\r
.\r
+ (1  )T t2 (Rt+1 + Rt+2 + ··· + RT 1)\r
+ T t1 (Rt+1 + Rt+2 + ··· + RT )\r
= (1  )\r
T\r
X1\r
h=t+1\r
ht1G¯t:h + T t1G¯t:T .\r
Now we need to scale the flat partial returns by an importance sampling ratio that\r
is similarly truncated. As G¯t:h only involves rewards up to a horizon h, we only need\r
the ratio of the probabilities up to h  1. We define an ordinary importance-sampling\r
estimator, analogous to (5.5), as\r
V (s) .=\r
P\r
t2T(s)\r
⇣\r
(1  )\r
PT(t)1\r
h=t+1 ht1⇢t:h1G¯t:h + T(t)t1⇢t:T(t)1G¯t:T(t)\r
⌘\r
|T(s)| , (5.9)\r
and a weighted importance-sampling estimator, analogous to (5.6), as\r
V (s) .=\r
P\r
t2T(s)\r
⇣\r
(1  )\r
PT(t)1\r
h=t+1 ht1⇢t:h1G¯t:h + T(t)t1⇢t:T(t)1G¯t:T(t)\r
⌘\r
P\r
t2T(s)\r
⇣\r
(1  )\r
PT(t)1\r
h=t+1 ht1⇢t:h1 + T(t)t1⇢t:T(t)1\r
⌘ . (5.10)\r
We call these two estimators discounting-aware importance sampling estimators. They\r
take into account the discount rate but have no e↵ect (are the same as the o↵-policy\r
estimators from Section 5.5) if  = 1.

114 Chapter 5: Monte Carlo Methods\r
5.9 *Per-decision Importance Sampling\r
There is one more way in which the structure of the return as a sum of rewards can be\r
taken into account in o↵-policy importance sampling, a way that may be able to reduce\r
variance even in the absence of discounting (that is, even if  = 1). In the o↵-policy\r
estimators (5.5) and (5.6), each term of the sum in the numerator is itself a sum:\r
⇢t:T 1Gt = ⇢t:T 1\r
\r
Rt+1 + Rt+2 + ··· + T t1RT\r
\r
= ⇢t:T 1Rt+1 + ⇢t:T 1Rt+2 + ··· + T t1⇢t:T 1RT . (5.11)\r
The o↵-policy estimators rely on the expected values of these terms, which can be written\r
in a simpler way. Note that each sub-term of (5.11) is a product of a random reward and\r
a random importance-sampling ratio. For example, the first sub-term can be written,\r
using (5.3), as\r
⇢t:T 1Rt+1 = ⇡(At|St)\r
b(At|St)\r
⇡(At+1|St+1)\r
b(At+1|St+1)\r
⇡(At+2|St+2)\r
b(At+2|St+2) ··· ⇡(AT 1|ST 1)b(AT 1|ST 1)\r
Rt+1. (5.12)\r
Of all these factors, one might suspect that only the first and the last (the reward)\r
are related; all the others are for events that occurred after the reward. Moreover, the\r
expected value of all these other factors is one:\r
E\r
\r
⇡(Ak|Sk)\r
b(Ak|Sk)\r
 .\r
= X\r
a\r
b(a|Sk)\r
⇡(a|Sk)\r
b(a|Sk) = X\r
a\r
⇡(a|Sk)=1. (5.13)\r
With a few more steps, one can show that, as suspected, all of these other factors have\r
no e↵ect in expectation, in other words, that\r
E[⇢t:T 1Rt+1] = E[⇢t:tRt+1] . (5.14)\r
If we repeat this process for the kth sub-term of (5.11), we get\r
E[⇢t:T 1Rt+k] = E[⇢t:t+k1Rt+k] .\r
It follows then that the expectation of our original term (5.11) can be written\r
E[⇢t:T 1Gt] = E\r
h\r
G˜t\r
i\r
,\r
where\r
G˜t = ⇢t:tRt+1 + ⇢t:t+1Rt+2 + 2⇢t:t+2Rt+3 + ··· + T t1⇢t:T 1RT .\r
We call this idea per-decision importance sampling.

5.10. Summary 115\r
It follows immediately that there is an alternate importance-sampling estimator, with\r
the same unbiased expectation (in the first-visit case) as the ordinary-importance-sampling\r
estimator (5.5), using G˜t:\r
V (s) .=\r
P\r
t2T(s) G˜t\r
|T(s)| , (5.15)\r
which we might expect to sometimes be of lower variance.\r
Is there a per-decision version of weighted importance sampling? This is less clear. So\r
far, all the estimators that have been proposed for this that we know of are not consistent\r
(that is, they do not converge to the true value with infinite data).\r
⇤\r
Exercise 5.13 Show the steps to derive (5.14) from (5.12). ⇤\r
⇤\r
Exercise 5.14 Modify the algorithm for o↵-policy Monte Carlo control (page 111) to use\r
the idea of the truncated weighted-average estimator (5.10). Note that you will first need\r
to convert this equation to action values. ⇤\r
5.10 Summary\r
The Monte Carlo methods presented in this chapter learn value functions and optimal\r
policies from experience in the form of sample episodes. This gives them at least three\r
kinds of advantages over DP methods. First, they can be used to learn optimal behavior\r
directly from interaction with the environment, with no model of the environment’s\r
dynamics. Second, they can be used with simulation or sample models. For surprisingly\r
many applications it is easy to simulate sample episodes even though it is dicult to\r
construct the kind of explicit model of transition probabilities required by DP methods.\r
Third, it is easy and ecient to focus Monte Carlo methods on a small subset of the states.\r
A region of special interest can be accurately evaluated without going to the expense of\r
accurately evaluating the rest of the state set (we explore this further in Chapter 8).\r
A fourth advantage of Monte Carlo methods, which we discuss later in the book, is\r
that they may be less harmed by violations of the Markov property. This is because they\r
do not update their value estimates on the basis of the value estimates of successor states.\r
In other words, it is because they do not bootstrap.\r
In designing Monte Carlo control methods we have followed the overall schema of\r
generalized policy iteration (GPI) introduced in Chapter 4. GPI involves interacting\r
processes of policy evaluation and policy improvement. Monte Carlo methods provide an\r
alternative policy evaluation process. Rather than use a model to compute the value of\r
each state, they simply average many returns that start in the state. Because a state’s\r
value is the expected return, this average can become a good approximation to the\r
value. In control methods we are particularly interested in approximating action-value\r
functions, because these can be used to improve the policy without requiring a model of\r
the environment’s transition dynamics. Monte Carlo methods intermix policy evaluation\r
and policy improvement steps on an episode-by-episode basis, and can be incrementally\r
implemented on an episode-by-episode basis.

116 Chapter 5: Monte Carlo Methods\r
Maintaining sucient exploration is an issue in Monte Carlo control methods. It is\r
not enough just to select the actions currently estimated to be best, because then no\r
returns will be obtained for alternative actions, and it may never be learned that they\r
are actually better. One approach is to ignore this problem by assuming that episodes\r
begin with state–action pairs randomly selected to cover all possibilities. Such exploring\r
starts can sometimes be arranged in applications with simulated episodes, but are unlikely\r
in learning from real experience. In on-policy methods, the agent commits to always\r
exploring and tries to find the best policy that still explores. In o↵-policy methods, the\r
agent also explores, but learns a deterministic optimal policy that may be unrelated to\r
the policy followed.\r
O↵-policy prediction refers to learning the value function of a target policy from data\r
generated by a di↵erent behavior policy. Such learning methods are based on some form\r
of importance sampling, that is, on weighting returns by the ratio of the probabilities of\r
taking the observed actions under the two policies, thereby transforming their expectations\r
from the behavior policy to the target policy. Ordinary importance sampling uses a\r
simple average of the weighted returns, whereas weighted importance sampling uses a\r
weighted average. Ordinary importance sampling produces unbiased estimates, but has\r
larger, possibly infinite, variance, whereas weighted importance sampling always has\r
finite variance and is preferred in practice. Despite their conceptual simplicity, o↵-policy\r
Monte Carlo methods for both prediction and control remain unsettled and are a subject\r
of ongoing research.\r
The Monte Carlo methods treated in this chapter di↵er from the DP methods treated\r
in the previous chapter in two major ways. First, they operate on sample experience,\r
and thus can be used for direct learning without a model. Second, they do not bootstrap.\r
That is, they do not update their value estimates on the basis of other value estimates.\r
These two di↵erences are not tightly linked, and can be separated. In the next chapter\r
we consider methods that learn from experience, like Monte Carlo methods, but also\r
bootstrap, like DP methods.\r
Bibliographical and Historical Remarks\r
The term “Monte Carlo” dates from the 1940s, when physicists at Los Alamos devised\r
games of chance that they could study to help understand complex physical phenomena\r
relating to the atom bomb. Coverage of Monte Carlo methods in this sense can be found\r
in several textbooks (e.g., Kalos and Whitlock, 1986; Rubinstein, 1981).\r
5.1–2 Singh and Sutton (1996) distinguished between every-visit and first-visit MC\r
methods and proved results relating these methods to reinforcement learning\r
algorithms. The blackjack example is based on an example used by Widrow,\r
Gupta, and Maitra (1973). The soap bubble example is a classical Dirichlet\r
problem whose Monte Carlo solution was first proposed by Kakutani (1945; see\r
Hersh and Griego, 1969; Doyle and Snell, 1984).\r
Barto and Du↵ (1994) discussed policy evaluation in the context of classical\r
Monte Carlo algorithms for solving systems of linear equations. They used the

5.10. Summary 117\r
analysis of Curtiss (1954) to point out the computational advantages of Monte\r
Carlo policy evaluation for large problems.\r
5.3–4 Monte Carlo ES was introduced in the 1998 edition of this book. That may have\r
been the first explicit connection between Monte Carlo estimation and control\r
methods based on policy iteration. An early use of Monte Carlo methods to\r
estimate action values in a reinforcement learning context was by Michie and\r
Chambers (1968). In pole balancing (page 56), they used averages of episode\r
durations to assess the worth (expected balancing “life”) of each possible action\r
in each state, and then used these assessments to control action selections. Their\r
method is similar in spirit to Monte Carlo ES with every-visit MC estimates.\r
Narendra and Wheeler (1986) studied a Monte Carlo method for ergodic finite\r
Markov chains that used the return accumulated between successive visits to the\r
same state as a reward for adjusting a learning automaton’s action probabilities.\r
5.5 Ecient o↵-policy learning has become recognized as an important challenge\r
that arises in several fields. For example, it is closely related to the idea of\r
“interventions” and “counterfactuals” in probabilistic graphical (Bayesian) models\r
(e.g., Pearl, 1995; Balke and Pearl, 1994). O↵-policy methods using importance\r
sampling have a long history and yet still are not well understood. Weighted\r
importance sampling, which is also sometimes called normalized importance\r
sampling (e.g., Koller and Friedman, 2009), is discussed by Rubinstein (1981),\r
Hesterberg (1988), Shelton (2001), and Liu (2001) among others.\r
The target policy in o↵-policy learning is sometimes referred to in the literature\r
as the “estimation” policy, as it was in the first edition of this book.\r
5.7 The racetrack exercise is adapted from Barto, Bradtke, and Singh (1995), and\r
from Gardner (1973).\r
5.8 Our treatment of the idea of discounting-aware importance sampling is based on\r
the analysis of Sutton, Mahmood, Precup, and van Hasselt (2014). It has been\r
worked out most fully to date by Mahmood (2017; Mahmood, van Hasselt, and\r
Sutton, 2014).\r
5.9 Per-decision importance sampling was introduced by Precup, Sutton, and Singh\r
(2000). They also combined o↵-policy learning with temporal-di↵erence learning,\r
eligibility traces, and approximation methods, introducing subtle issues that we\r
consider in later chapters.\r
Exercise 5.15 Make new equations analogous to the importance-sampling Monte Carlo\r
estimates (5.5) and (5.6), but for action value estimates Q(s, a). You will need new\r
notation T(s, a) for the time steps on which the state–action pair s, a is visited on the\r
episode. Do these estimates involve more or less importance-sampling correction?

Chapter 6\r
Temporal-Di↵erence Learning\r
If one had to identify one idea as central and novel to reinforcement learning, it would\r
undoubtedly be temporal-di↵erence (TD) learning. TD learning is a combination of\r
Monte Carlo ideas and dynamic programming (DP) ideas. Like Monte Carlo methods,\r
TD methods can learn directly from raw experience without a model of the environment’s\r
dynamics. Like DP, TD methods update estimates based in part on other learned\r
estimates, without waiting for a final outcome (they bootstrap). The relationship between\r
TD, DP, and Monte Carlo methods is a recurring theme in the theory of reinforcement\r
learning; this chapter is the beginning of our exploration of it. Before we are done, we\r
will see that these ideas and methods blend into each other and can be combined in many\r
ways. In particular, in Chapter 7 we introduce n-step algorithms, which provide a bridge\r
from TD to Monte Carlo methods, and in Chapter 12 we introduce the TD() algorithm,\r
which seamlessly unifies them.\r
As usual, we start by focusing on the policy evaluation or prediction problem, the\r
problem of estimating the value function v⇡ for a given policy ⇡. For the control problem\r
(finding an optimal policy), DP, TD, and Monte Carlo methods all use some variation of\r
generalized policy iteration (GPI). The di↵erences in the methods are primarily di↵erences\r
in their approaches to the prediction problem.\r
6.1 TD Prediction\r
Both TD and Monte Carlo methods use experience to solve the prediction problem. Given\r
some experience following a policy ⇡, both methods update their estimate V of v⇡ for\r
the nonterminal states St occurring in that experience. Roughly speaking, Monte Carlo\r
methods wait until the return following the visit is known, then use that return as a\r
target for V (St). A simple every-visit Monte Carlo method suitable for nonstationary\r
environments is\r
V (St) V (St) + ↵\r
h\r
Gt  V (St)\r
i\r
, (6.1)

120 Chapter 6: Temporal-Di↵erence Learning\r
where Gt is the actual return following time t, and ↵ is a constant step-size parameter (c.f.,\r
Equation 2.4). Let us call this method constant-↵ MC. Whereas Monte Carlo methods\r
must wait until the end of the episode to determine the increment to V (St) (only then is\r
Gt known), TD methods need to wait only until the next time step. At time t + 1 they\r
immediately form a target and make a useful update using the observed reward Rt+1 and\r
the estimate V (St+1). The simplest TD method makes the update\r
V (St) V (St) + ↵\r
h\r
Rt+1 + V (St+1)  V (St)\r
i\r
(6.2)\r
immediately on transition to St+1 and receiving Rt+1. In e↵ect, the target for the Monte\r
Carlo update is Gt, whereas the target for the TD update is Rt+1 + V (St+1). This TD\r
method is called TD(0), or one-step TD, because it is a special case of the TD() and\r
n-step TD methods developed in Chapter 12 and Chapter 7. The box below specifies\r
TD(0) completely in procedural form.\r
Tabular TD(0) for estimating v⇡\r
Input: the policy ⇡ to be evaluated\r
Algorithm parameter: step size ↵ 2 (0, 1]\r
Initialize V (s), for all s 2 S+, arbitrarily except that V (terminal)=0\r
Loop for each episode:\r
Initialize S\r
Loop for each step of episode:\r
A action given by ⇡ for S\r
Take action A, observe R, S0\r
V (S) V (S) + ↵\r
⇥\r
R + V (S0)  V (S)\r
⇤\r
S S0\r
until S is terminal\r
Because TD(0) bases its update in part on an existing estimate, we say that it is a\r
bootstrapping method, like DP. We know from Chapter 3 that\r
v⇡(s) .= E⇡[Gt | St =s] (6.3)\r
= E⇡[Rt+1 + Gt+1 | St =s] (from (3.9))\r
= E⇡[Rt+1 + v⇡(St+1) | St =s] . (6.4)\r
Roughly speaking, Monte Carlo methods use an estimate of (6.3) as a target, whereas\r
DP methods use an estimate of (6.4) as a target. The Monte Carlo target is an estimate\r
because the expected value in (6.3) is not known; a sample return is used in place of the\r
real expected return. The DP target is an estimate not because of the expected values,\r
which are assumed to be completely provided by a model of the environment, but because\r
v⇡(St+1) is not known and the current estimate, V (St+1), is used instead. The TD target\r
is an estimate for both reasons: it samples the expected values in (6.4) and it uses the\r
current estimate V instead of the true v⇡. Thus, TD methods combine the sampling of

6.1. TD Prediction 121\r
Monte Carlo with the bootstrapping of DP. As we shall see, with care and imagination\r
this can take us a long way toward obtaining the advantages of both Monte Carlo and\r
DP methods.\r
TD(0)\r
Shown to the right is the backup diagram for tabular TD(0). The value\r
estimate for the state node at the top of the backup diagram is updated on\r
the basis of the one sample transition from it to the immediately following\r
state. We refer to TD and Monte Carlo updates as sample updates because\r
they involve looking ahead to a sample successor state (or state–action pair),\r
using the value of the successor and the reward along the way to compute a\r
backed-up value, and then updating the value of the original state (or state–\r
action pair) accordingly. Sample updates di↵er from the expected updates\r
of DP methods in that they are based on a single sample successor rather than on a\r
complete distribution of all possible successors.\r
Finally, note that the quantity in brackets in the TD(0) update is a sort of error,\r
measuring the di↵erence between the estimated value of St and the better estimate\r
Rt+1 + V (St+1). This quantity, called the TD error, arises in various forms throughout\r
reinforcement learning:\r
t\r
.\r
= Rt+1 + V (St+1)  V (St). (6.5)\r
Notice that the TD error at each time is the error in the estimate made at that time.\r
Because the TD error depends on the next state and next reward, it is not actually\r
available until one time step later. That is, t is the error in V (St), available at time\r
t + 1. Also note that if the array V does not change during the episode (as it does not in\r
Monte Carlo methods), then the Monte Carlo error can be written as a sum of TD errors:\r
Gt  V (St) = Rt+1 + Gt+1  V (St) + V (St+1)  V (St+1) (from (3.9))\r
= t + \r
\r
Gt+1  V (St+1)\r
\r
= t + t+1 + 2Gt+2  V (St+2)\r
\r
= t + t+1 + 2t+2 + ··· + T t1T 1 + T t\r
\r
GT  V (ST )\r
\r
= t + t+1 + 2t+2 + ··· + T t1T 1 + T t\r
\r
0  0\r
\r
=\r
T\r
X1\r
k=t\r
ktk. (6.6)\r
This identity is not exact if V is updated during the episode (as it is in TD(0)), but if the\r
step size is small then it may still hold approximately. Generalizations of this identity\r
play an important role in the theory and algorithms of temporal-di↵erence learning.\r
Exercise 6.1 If V changes during the episode, then (6.6) only holds approximately; what\r
would the di↵erence be between the two sides? Let Vt denote the array of state values\r
used at time t in the TD error (6.5) and in the TD update (6.2). Redo the derivation\r
above to determine the additional amount that must be added to the sum of TD errors\r
in order to equal the Monte Carlo error. ⇤

122 Chapter 6: Temporal-Di↵erence Learning\r
Example 6.1: Driving Home Each day as you drive home from work, you try to\r
predict how long it will take to get home. When you leave your oce, you note the time,\r
the day of week, the weather, and anything else that might be relevant. Say on this\r
Friday you are leaving at exactly 6 o’clock, and you estimate that it will take 30 minutes\r
to get home. As you reach your car it is 6:05, and you notice it is starting to rain. Trac\r
is often slower in the rain, so you reestimate that it will take 35 minutes from then, or a\r
total of 40 minutes. Fifteen minutes later you have completed the highway portion of\r
your journey in good time. As you exit onto a secondary road you cut your estimate of\r
total travel time to 35 minutes. Unfortunately, at this point you get stuck behind a slow\r
truck, and the road is too narrow to pass. You end up having to follow the truck until\r
you turn onto the side street where you live at 6:40. Three minutes later you are home.\r
The sequence of states, times, and predictions is thus as follows:\r
Elapsed Time Predicted Predicted\r
State (minutes) Time to Go Total Time\r
leaving oce, friday at 6 0 30 30\r
reach car, raining 5 35 40\r
exiting highway 20 15 35\r
2ndary road, behind truck 30 10 40\r
entering home street 40 3 43\r
arrive home 43 0 43\r
The rewards in this example are the elapsed times on each leg of the journey.1 We are\r
not discounting ( = 1), and thus the return for each state is the actual time to go from\r
that state. The value of each state is the expected time to go. The second column of\r
numbers gives the current estimated value for each state encountered.\r
A simple way to view the operation of Monte Carlo methods is to plot the predicted\r
total time (the last column) over the sequence, as in Figure 6.1 (left). The red arrows\r
show the changes in predictions recommended by the constant-↵ MC method (6.1), for\r
↵ = 1. These are exactly the errors between the estimated value (predicted time to go)\r
in each state and the actual return (actual time to go). For example, when you exited\r
the highway you thought it would take only 15 minutes more to get home, but in fact it\r
took 23 minutes. Equation 6.1 applies at this point and determines an increment in the\r
estimate of time to go after exiting the highway. The error, Gt  V (St), at this time is\r
eight minutes. Suppose the step-size parameter, ↵, is 1/2. Then the predicted time to go\r
after exiting the highway would be revised upward by four minutes as a result of this\r
experience. This is probably too large a change in this case; the truck was probably just\r
an unlucky break. In any event, the change can only be made o↵-line, that is, after you\r
have reached home. Only at this point do you know any of the actual returns.\r
Is it necessary to wait until the final outcome is known before learning can begin?\r
Suppose on another day you again estimate when leaving your oce that it will take 30\r
minutes to drive home, but then you become stuck in a massive trac jam. Twenty-five\r
minutes after leaving the oce you are still bumper-to-bumper on the highway. You now\r
1If this were a control problem with the objective of minimizing travel time, then we would of course\r
make the rewards the negative of the elapsed time. But because we are concerned here only with\r
prediction (policy evaluation), we can keep things simple by using positive numbers.

6.1. TD Prediction 123\r
road\r
30\r
35\r
40\r
45\r
Predicted\r
total\r
travel\r
time\r
leaving\r
office\r
exiting\r
highway\r
2ndary home arrive\r
Situation\r
actual outcome\r
reach\r
car street home\r
actual\r
outcome\r
Situation\r
30\r
35\r
40\r
45\r
Predicted\r
total\r
travel\r
time\r
road\r
leaving\r
office\r
exiting\r
highway\r
reach 2ndary home arrive\r
car street home\r
Figure 6.1: Changes recommended in the driving home example by Monte Carlo methods (left)\r
and TD methods (right).\r
estimate that it will take another 25 minutes to get home, for a total of 50 minutes. As\r
you wait in trac, you already know that your initial estimate of 30 minutes was too\r
optimistic. Must you wait until you get home before increasing your estimate for the\r
initial state? According to the Monte Carlo approach you must, because you don’t yet\r
know the true return.\r
According to a TD approach, on the other hand, you would learn immediately, shifting\r
your initial estimate from 30 minutes toward 50. In fact, each estimate would be shifted\r
toward the estimate that immediately follows it. Returning to our first day of driving,\r
Figure 6.1 (right) shows the changes in the predictions recommended by the TD rule\r
(6.2) (these are the changes made by the rule if ↵ = 1). Each error is proportional to the\r
change over time of the prediction, that is, to the temporal di↵erences in predictions.\r
Besides giving you something to do while waiting in trac, there are several computa\u0002tional reasons why it is advantageous to learn based on your current predictions rather\r
than waiting until termination when you know the actual return. We briefly discuss some\r
of these in the next section.\r
Exercise 6.2 This is an exercise to help develop your intuition about why TD methods\r
are often more ecient than Monte Carlo methods. Consider the driving home example\r
and how it is addressed by TD and Monte Carlo methods. Can you imagine a scenario\r
in which a TD update would be better on average than a Monte Carlo update? Give\r
an example scenario—a description of past experience and a current state—in which\r
you would expect the TD update to be better. Here’s a hint: Suppose you have lots\r
of experience driving home from work. Then you move to a new building and a new\r
parking lot (but you still enter the highway at the same place). Now you are starting\r
to learn predictions for the new building. Can you see why TD updates are likely to be\r
much better, at least initially, in this case? Might the same sort of thing happen in the\r
original scenario? ⇤

124 Chapter 6: Temporal-Di↵erence Learning\r
6.2 Advantages of TD Prediction Methods\r
TD methods update their estimates based in part on other estimates. They learn a\r
guess from a guess—they bootstrap. Is this a good thing to do? What advantages do\r
TD methods have over Monte Carlo and DP methods? Developing and answering such\r
questions will take the rest of this book and more. In this section we briefly anticipate\r
some of the answers.\r
Obviously, TD methods have an advantage over DP methods in that they do not\r
require a model of the environment, of its reward and next-state probability distributions.\r
The next most obvious advantage of TD methods over Monte Carlo methods is that\r
they are naturally implemented in an online, fully incremental fashion. With Monte\r
Carlo methods one must wait until the end of an episode, because only then is the return\r
known, whereas with TD methods one need wait only one time step. Surprisingly often\r
this turns out to be a critical consideration. Some applications have very long episodes, so\r
that delaying all learning until the end of the episode is too slow. Other applications are\r
continuing tasks and have no episodes at all. Finally, as we noted in the previous chapter,\r
some Monte Carlo methods must ignore or discount episodes on which experimental\r
actions are taken, which can greatly slow learning. TD methods are much less susceptible\r
to these problems because they learn from each transition regardless of what subsequent\r
actions are taken.\r
But are TD methods sound? Certainly it is convenient to learn one guess from the\r
next, without waiting for an actual outcome, but can we still guarantee convergence\r
to the correct answer? Happily, the answer is yes. For any fixed policy ⇡, TD(0) has\r
been proved to converge to v⇡, in the mean for a constant step-size parameter if it is\r
suciently small, and with probability 1 if the step-size parameter decreases according to\r
the usual stochastic approximation conditions (2.7). Most convergence proofs apply only\r
to the table-based case of the algorithm presented above (6.2), but some also apply to\r
the case of general linear function approximation. These results are discussed in a more\r
general setting in Section 9.4.\r
If both TD and Monte Carlo methods converge asymptotically to the correct predictions,\r
then a natural next question is “Which gets there first?” In other words, which method\r
learns faster? Which makes the more ecient use of limited data? At the current time\r
this is an open question in the sense that no one has been able to prove mathematically\r
that one method converges faster than the other. In fact, it is not even clear what is the\r
most appropriate formal way to phrase this question! In practice, however, TD methods\r
have usually been found to converge faster than constant-↵ MC methods on stochastic\r
tasks, as illustrated in Example 6.2.

6.2. Advantages of TD Prediction Methods 125\r
Example 6.2 Random Walk\r
In this example we empirically compare the prediction abilities of TD(0) and\r
constant-↵ MC when applied to the following Markov reward process:\r
A B CDE\r
0 0 0 0 0 1\r
start\r
A Markov reward process, or MRP, is a Markov decision process without actions.\r
We will often use MRPs when focusing on the prediction problem, in which there is\r
no need to distinguish the dynamics due to the environment from those due to the\r
agent. In this MRP, all episodes start in the center state, C, then proceed either left\r
or right by one state on each step, with equal probability. Episodes terminate either\r
on the extreme left or the extreme right. When an episode terminates on the right,\r
a reward of +1 occurs; all other rewards are zero. For example, a typical episode\r
might consist of the following state-and-reward sequence: C, 0,B, 0, C, 0, D, 0, E, 1.\r
Because this task is undiscounted, the true value of each state is the probability of\r
terminating on the right if starting from that state. Thus, the true value of the\r
center state is v⇡(C)=0.5. The true values of all the states, A through E, are\r
1\r
6 , 26 , 36 , 46 , and 56 .\r
0.8\r
0\r
0.2\r
0.4\r
0.6\r
A B C D E\r
0\r
10\r
1\r
100\r
State\r
Estimated\r
value\r
True \r
values\r
Estimated\r
value\r
0\r
0.05\r
0.1\r
0.15\r
0.2\r
0.25\r
0 25 50 75 100\r
Walks / Episodes\r
TD\r
MC\r
RMS error,\r
averaged\r
over states\r
_\u001B\f\u000E\u000F\r
_\u001B\f\u000F\r
_\u001B\f\u000E\u0010\r
_\u001B\f\u000E\u0011\r
_\u001B\f\u000E\u0012\r
_\u001B\f\u000F\u0013\r
_\u001B\f\u000E\u0013\r
Empirical RMS error, \r
averaged over states\r
The left graph above shows the values learned after various numbers of episodes on\r
a single run of TD(0). The estimates after 100 episodes are about as close as they\r
ever come to the true values—with a constant step-size parameter (↵ = 0.1 in this\r
example), the values fluctuate indefinitely in response to the outcomes of the most\r
recent episodes. The right graph shows learning curves for the two methods for\r
various values of ↵. The performance measure shown is the root mean square (RMS)\r
error between the value function learned and the true value function, averaged over\r
the five states, then averaged over 100 runs. In all cases the approximate value\r
function was initialized to the intermediate value V (s)=0.5, for all s. The TD\r
method was consistently better than the MC method on this task.

126 Chapter 6: Temporal-Di↵erence Learning\r
Exercise 6.3 From the results shown in the left graph of the random walk example it\r
appears that the first episode results in a change in only V (A). What does this tell you\r
about what happened on the first episode? Why was only the estimate for this one state\r
changed? By exactly how much was it changed? ⇤\r
Exercise 6.4 The specific results shown in the right graph of the random walk example\r
are dependent on the value of the step-size parameter, ↵. Do you think the conclusions\r
about which algorithm is better would be a↵ected if a wider range of ↵ values were used?\r
Is there a di↵erent, fixed value of ↵ at which either algorithm would have performed\r
significantly better than shown? Why or why not? ⇤\r
⇤\r
Exercise 6.5 In the right graph of the random walk example, the RMS error of the\r
TD method seems to go down and then up again, particularly at high ↵’s. What could\r
have caused this? Do you think this always occurs, or might it be a function of how the\r
approximate value function was initialized? ⇤\r
Exercise 6.6 In Example 6.2 we stated that the true values for the random walk example\r
are 1\r
6 , 26 , 36 , 46 , and 56 , for states A through E. Describe at least two di↵erent ways that\r
these could have been computed. Which would you guess we actually used? Why? ⇤\r
6.3 Optimality of TD(0)\r
Suppose there is available only a finite amount of experience, say 10 episodes or 100\r
time steps. In this case, a common approach with incremental learning methods is to\r
present the experience repeatedly until the method converges upon an answer. Given an\r
approximate value function, V , the increments specified by (6.1) or (6.2) are computed\r
for every time step t at which a nonterminal state is visited, but the value function is\r
changed only once, by the sum of all the increments. Then all the available experience is\r
processed again with the new value function to produce a new overall increment, and so\r
on, until the value function converges. We call this batch updating because updates are\r
made only after processing each complete batch of training data.\r
Under batch updating, TD(0) converges deterministically to a single answer independent\r
of the step-size parameter, ↵, as long as ↵ is chosen to be suciently small. The constant-\r
↵ MC method also converges deterministically under the same conditions, but to a\r
di↵erent answer. Understanding these two answers will help us understand the di↵erence\r
between the two methods. Under normal updating the methods do not move all the way\r
to their respective batch answers, but in some sense they take steps in these directions.\r
Before trying to understand the two answers in general, for all possible tasks, we first\r
look at a few examples.\r
Example 6.3: Random walk under batch updating Batch-updating versions of\r
TD(0) and constant-↵ MC were applied as follows to the random walk prediction example\r
(Example 6.2). After each new episode, all episodes seen so far were treated as a batch.\r
They were repeatedly presented to the algorithm, either TD(0) or constant-↵ MC, with\r
↵ suciently small that the value function converged. The resulting value function was\r
then compared with v⇡, and the average root mean square error across the five states\r
(and across 100 independent repetitions of the whole experiment) was plotted to obtain

6.3. Optimality of TD(0) 127\r
the learning curves shown in Figure 6.2. Note that the batch TD method was consistently\r
better than the batch Monte Carlo method.\r
. 0\r
.05\r
. 1\r
.15\r
. 2\r
.25\r
0 25 50 75 100\r
TD\r
MC\r
BATCH TRAINING\r
Walks / Episodes\r
RMS error,\r
averaged\r
over states\r
Figure 6.2: Performance of TD(0) and constant-↵\r
MC under batch training on the random walk task.\r
Under batch training, constant-↵\r
MC converges to values, V (s), that\r
are sample averages of the actual re\u0002turns experienced after visiting each\r
state s. These are optimal estimates\r
in the sense that they minimize the\r
mean square error from the actual\r
returns in the training set. In this\r
sense it is surprising that the batch\r
TD method was able to perform\r
better according to the root mean\r
square error measure shown in the\r
figure to the right. How is it that\r
batch TD was able to perform better\r
than this optimal method? The an\u0002swer is that the Monte Carlo method\r
is optimal only in a limited way, and\r
that TD is optimal in a way that is more relevant to predicting returns.\r
Example 6.4: You are the Predictor Place yourself now in the role of the predictor\r
of returns for an unknown Markov reward process. Suppose you observe the following\r
eight episodes:\r
A, 0, B, 0 B, 1\r
B, 1 B, 1\r
B, 1 B, 1\r
B, 1 B, 0\r
This means that the first episode started in state A, transitioned to B with a reward of\r
0, and then terminated from B with a reward of 0. The other seven episodes were even\r
shorter, starting from B and terminating immediately. Given this batch of data, what\r
would you say are the optimal predictions, the best values for the estimates V (A) and\r
V (B)? Everyone would probably agree that the optimal value for V (B) is 3\r
4 , because six\r
out of the eight times in state B the process terminated immediately with a return of 1,\r
and the other two times in B the process terminated immediately with a return of 0.\r
But what is the optimal value for the estimate V (A) given this data? Here there are\r
A B\r
r = 1\r
100%\r
75%\r
25%\r
r = 0\r
r = 0\r
two reasonable answers. One is to observe that 100% of the\r
times the process was in state A it traversed immediately to\r
B (with a reward of 0); and because we have already decided\r
that B has value 3\r
4 , therefore A must have value 34 as well.\r
One way of viewing this answer is that it is based on first\r
modeling the Markov process, in this case as shown to the\r
right, and then computing the correct estimates given the\r
model, which indeed in this case gives V (A) = 3\r
4 . This is\r
also the answer that batch TD(0) gives.

128 Chapter 6: Temporal-Di↵erence Learning\r
The other reasonable answer is simply to observe that we have seen A once and the\r
return that followed it was 0; we therefore estimate V (A) as 0. This is the answer that\r
batch Monte Carlo methods give. Notice that it is also the answer that gives minimum\r
squared error on the training data. In fact, it gives zero error on the data. But still we\r
expect the first answer to be better. If the process is Markov, we expect that the first\r
answer will produce lower error on future data, even though the Monte Carlo answer is\r
better on the existing data.\r
Example 6.4 illustrates a general di↵erence between the estimates found by batch\r
TD(0) and batch Monte Carlo methods. Batch Monte Carlo methods always find the\r
estimates that minimize mean square error on the training set, whereas batch TD(0)\r
always finds the estimates that would be exactly correct for the maximum-likelihood\r
model of the Markov process. In general, the maximum-likelihood estimate of a parameter\r
is the parameter value whose probability of generating the data is greatest. In this case,\r
the maximum-likelihood estimate is the model of the Markov process formed in the\r
obvious way from the observed episodes: the estimated transition probability from i to j\r
is the fraction of observed transitions from i that went to j, and the associated expected\r
reward is the average of the rewards observed on those transitions. Given this model,\r
we can compute the estimate of the value function that would be exactly correct if the\r
model were exactly correct. This is called the certainty-equivalence estimate because it\r
is equivalent to assuming that the estimate of the underlying process was known with\r
certainty rather than being approximated. In general, batch TD(0) converges to the\r
certainty-equivalence estimate.\r
This helps explain why TD methods converge more quickly than Monte Carlo methods.\r
In batch form, TD(0) is faster than Monte Carlo methods because it computes the\r
true certainty-equivalence estimate. This explains the advantage of TD(0) shown in the\r
batch results on the random walk task (Figure 6.2). The relationship to the certainty\u0002equivalence estimate may also explain in part the speed advantage of nonbatch TD(0)\r
(e.g., Example 6.2, page 125, right graph). Although the nonbatch methods do not achieve\r
either the certainty-equivalence or the minimum squared error estimates, they can be\r
understood as moving roughly in these directions. Nonbatch TD(0) may be faster than\r
constant-↵ MC because it is moving toward a better estimate, even though it is not\r
getting all the way there. At the current time nothing more definite can be said about\r
the relative eciency of online TD and Monte Carlo methods.\r
Finally, it is worth noting that although the certainty-equivalence estimate is in some\r
sense an optimal solution, it is almost never feasible to compute it directly. If n = |S| is\r
the number of states, then just forming the maximum-likelihood estimate of the process\r
may require on the order of n2 memory, and computing the corresponding value function\r
requires on the order of n3 computational steps if done conventionally. In these terms it\r
is indeed striking that TD methods can approximate the same solution using memory\r
no more than order n and repeated computations over the training set. On tasks with\r
large state spaces, TD methods may be the only feasible way of approximating the\r
certainty-equivalence solution.\r
⇤\r
Exercise 6.7 Design an o↵-policy version of the TD(0) update that can be used with\r
arbitrary target policy ⇡ and covering behavior policy b, using at each step t the importance\r
sampling ratio ⇢t:t (5.3). ⇤

6.4. Sarsa: On-policy TD Control 129\r
6.4 Sarsa: On-policy TD Control\r
We turn now to the use of TD prediction methods for the control problem. As usual, we\r
follow the pattern of generalized policy iteration (GPI), only this time using TD methods\r
for the evaluation or prediction part. As with Monte Carlo methods, we face the need to\r
trade o↵ exploration and exploitation, and again approaches fall into two main classes:\r
on-policy and o↵-policy. In this section we present an on-policy TD control method.\r
The first step is to learn an action-value function rather than a state-value function.\r
In particular, for an on-policy method we must estimate q⇡(s, a) for the current behavior\r
policy ⇡ and for all states s and actions a. This can be done using essentially the same TD\r
method described above for learning v⇡. Recall that an episode consists of an alternating\r
sequence of states and state–action pairs:\r
At\r
Rt+1 St At+1Rt+2 St+1 At+2Rt+3 St+2 At+3\r
St+3 . . . . . .\r
In the previous section we considered transitions from state to state and learned the\r
values of states. Now we consider transitions from state–action pair to state–action pair,\r
and learn the values of state–action pairs. Formally these cases are identical: they are\r
both Markov chains with a reward process. The theorems assuring the convergence of\r
state values under TD(0) also apply to the corresponding algorithm for action values:\r
Q(St, At) Q(St, At) + ↵\r
h\r
Rt+1 + Q(St+1, At+1)  Q(St, At)\r
i\r
. (6.7)\r
Sarsa\r
This update is done after every transition from a nonterminal state St. If\r
St+1 is terminal, then Q(St+1, At+1) is defined as zero. This rule uses every\r
element of the quintuple of events, (St, At, Rt+1, St+1, At+1), that make up a\r
transition from one state–action pair to the next. This quintuple gives rise to\r
the name Sarsa for the algorithm. The backup diagram for Sarsa is as shown\r
to the right.\r
It is straightforward to design an on-policy control algorithm based on the Sarsa\r
prediction method. As in all on-policy methods, we continually estimate q⇡ for the\r
behavior policy ⇡, and at the same time change ⇡ toward greediness with respect to q⇡.\r
The general form of the Sarsa control algorithm is given in the box on the next page.\r
The convergence properties of the Sarsa algorithm depend on the nature of the policy’s\r
dependence on Q. For example, one could use "-greedy or "-soft policies. Sarsa converges\r
with probability 1 to an optimal policy and action-value function, under the usual\r
conditions on the step sizes (2.7), as long as all state–action pairs are visited an infinite\r
number of times and the policy converges in the limit to the greedy policy (which can be\r
arranged, for example, with "-greedy policies by setting " = 1/t).\r
Exercise 6.8 Show that an action-value version of (6.6) holds for the action-value form\r
of the TD error t = Rt+1 + Q(St+1, At+1)  Q(St, At), again assuming that the values\r
don’t change from step to step. ⇤

130 Chapter 6: Temporal-Di↵erence Learning\r
Sarsa (on-policy TD control) for estimating Q ⇡ q⇤\r
Algorithm parameters: step size ↵ 2 (0, 1], small " > 0\r
Initialize Q(s, a), for all s 2 S+, a 2 A(s), arbitrarily except that Q(terminal, ·)=0\r
Loop for each episode:\r
Initialize S\r
Choose A from S using policy derived from Q (e.g., "-greedy)\r
Loop for each step of episode:\r
Take action A, observe R, S0\r
Choose A0 from S0 using policy derived from Q (e.g., "-greedy)\r
Q(S, A) Q(S, A) + ↵\r
⇥\r
R + Q(S0, A0)  Q(S, A)\r
⇤\r
S S0; A A0;\r
until S is terminal\r
Example 6.5: Windy Gridworld Shown inset below is a standard gridworld, with\r
start and goal states, but with one di↵erence: there is a crosswind running upward\r
through the middle of the grid. The actions are the standard four—up, down, right,\r
and left—but in the middle region the resultant next states are shifted upward by a\r
“wind,” the strength of which varies from column to column. The strength of the wind\r
0 1000 2000 3000 4000 5000 6000 7000 8000\r
0\r
50\r
100\r
150\r
170\r
Time steps\r
S G\r
000 1 1 1 12 2 0\r
Actions\r
Episodes\r
is given below each column, in num\u0002ber of cells shifted upward. For ex\u0002ample, if you are one cell to the\r
right of the goal, then the action\r
left takes you to the cell just above\r
the goal. This is an undiscounted\r
episodic task, with constant rewards\r
of 1 until the goal state is reached.\r
The graph to the right shows the\r
results of applying "-greedy Sarsa to\r
this task, with " = 0.1, ↵ = 0.5,\r
and the initial values Q(s, a)=0\r
for all s, a. The increasing slope of\r
the graph shows that the goal was\r
reached more quickly over time. By\r
8000 time steps, the greedy policy was long since optimal (a trajectory from it is shown\r
inset); continued "-greedy exploration kept the average episode length at about 17 steps,\r
two more than the minimum of 15. Note that Monte Carlo methods cannot easily be\r
used here because termination is not guaranteed for all policies. If a policy was ever\r
found that caused the agent to stay in the same state, then the next episode would\r
never end. Online learning methods such as Sarsa do not have this problem because they\r
quickly learn during the episode that such policies are poor, and switch to something\r
else.

6.5. Q-learning: O↵-policy TD Control 131\r
Exercise 6.9: Windy Gridworld with King’s Moves (programming) Re-solve the windy\r
gridworld assuming eight possible actions, including the diagonal moves, rather than four.\r
How much better can you do with the extra actions? Can you do even better by including\r
a ninth action that causes no movement at all other than that caused by the wind? ⇤\r
Exercise 6.10: Stochastic Wind (programming) Re-solve the windy gridworld task with\r
King’s moves, assuming that the e↵ect of the wind, if there is any, is stochastic, sometimes\r
varying by 1 from the mean values given for each column. That is, a third of the time\r
you move exactly according to these values, as in the previous exercise, but also a third\r
of the time you move one cell above that, and another third of the time you move one\r
cell below that. For example, if you are one cell to the right of the goal and you move\r
left, then one-third of the time you move one cell above the goal, one-third of the time\r
you move two cells above the goal, and one-third of the time you move to the goal. ⇤\r
6.5 Q-learning: O↵-policy TD Control\r
One of the early breakthroughs in reinforcement learning was the development of an\r
o↵-policy TD control algorithm known as Q-learning (Watkins, 1989), defined by\r
Q(St, At) Q(St, At) + ↵\r
h\r
Rt+1 +  maxa Q(St+1, a)  Q(St, At)\r
i\r
. (6.8)\r
In this case, the learned action-value function, Q, directly approximates q⇤, the optimal\r
action-value function, independent of the policy being followed. This dramatically\r
simplifies the analysis of the algorithm and enabled early convergence proofs. The policy\r
still has an e↵ect in that it determines which state–action pairs are visited and updated.\r
However, all that is required for correct convergence is that all pairs continue to be\r
updated. As we observed in Chapter 5, this is a minimal requirement in the sense that\r
any method guaranteed to find optimal behavior in the general case must require it.\r
Under this assumption and a variant of the usual stochastic approximation conditions on\r
the sequence of step-size parameters, Q has been shown to converge with probability 1 to\r
q⇤. The Q-learning algorithm is shown below in procedural form.\r
Q-learning (o↵-policy TD control) for estimating ⇡ ⇡ ⇡⇤\r
Algorithm parameters: step size ↵ 2 (0, 1], small " > 0\r
Initialize Q(s, a), for all s 2 S+, a 2 A(s), arbitrarily except that Q(terminal, ·)=0\r
Loop for each episode:\r
Initialize S\r
Loop for each step of episode:\r
Choose A from S using policy derived from Q (e.g., "-greedy)\r
Take action A, observe R, S0\r
Q(S, A) Q(S, A) + ↵\r
⇥\r
R +  maxa Q(S0, a)  Q(S, A)\r
⇤\r
S S0\r
until S is terminal

132 Chapter 6: Temporal-Di↵erence Learning\r
What is the backup diagram for Q-learning? The rule (6.8) updates a state–action\r
pair, so the top node, the root of the update, must be a small, filled action node. The\r
update is also from action nodes, maximizing over all those actions possible in the next\r
state. Thus the bottom nodes of the backup diagram should be all these action nodes.\r
Finally, remember that we indicate taking the maximum of these “next action” nodes\r
with an arc across them (Figure 3.4-right). Can you guess now what the diagram is? If\r
so, please do make a guess before turning to the answer in Figure 6.4 on page 134.\r
Example 6.6: Cli↵ Walking This gridworld example compares Sarsa and Q-learning,\r
highlighting the di↵erence between on-policy (Sarsa) and o↵-policy (Q-learning) methods.\r
Reward\r
per\r
epsiode\r
\u0004 \u0004 \r
\u0004 \u0005\r
\u0004 \r
\u0004 \u0006\r
 \u0004 \u0006 \u0007 ~ \r
Episodes\r
Sarsa\r
Q-learning\r
S T h e C l i f f G\r
R\r
Sum of \r
rewards\r
during\r
episode\r
R = -1\r
Safer path\r
Optimal path\r
R = -100\r
Episodes\r
Sarsa\r
Q-learning\r
S G\r
r = \u0004 \u0004 \r
T h e C l i f f\r
r =\b\u0004 \u0004 sa\r
op\r
R\r
R\r
Sum of \r
rewards\r
during\r
episode\r
R = -1\r
safe path\r
optimal path\r
R = -100\r
Episodes\r
-25\r
-50\r
-75\r
-100\r
0 100 200 300 400 500\r
Consider the gridworld shown to the\r
right. This is a standard undis\u0002counted, episodic task, with start\r
and goal states, and the usual ac\u0002tions causing movement up, down,\r
right, and left. Reward is 1 on all\r
transitions except those into the re\u0002gion marked “The Cli↵.” Stepping\r
into this region incurs a reward of\r
100 and sends the agent instantly\r
back to the start.\r
The graph to the right shows the\r
performance of the Sarsa and Q\u0002learning methods with "-greedy ac\u0002tion selection, " = 0.1. After an\r
initial transient, Q-learning learns\r
values for the optimal policy, that\r
which travels right along the edge\r
of the cli↵. Unfortunately, this re\u0002sults in its occasionally falling o↵\r
the cli↵ because of the "-greedy ac\u0002tion selection. Sarsa, on the other\r
hand, takes the action selection into\r
account and learns the longer but\r
safer path through the upper part\r
of the grid. Although Q-learning ac\u0002tually learns the values of the opti\u0002mal policy, its online performance\r
is worse than that of Sarsa, which\r
learns the roundabout policy. Of course, if " were gradually reduced, then both methods\r
would asymptotically converge to the optimal policy.\r
Exercise 6.11 Why is Q-learning considered an o↵-policy control method? ⇤\r
Exercise 6.12 Suppose action selection is greedy. Is Q-learning then exactly the same\r
algorithm as Sarsa? Will they make exactly the same action selections and weight\r
updates? ⇤

6.6. Expected Sarsa 133\r
6.6 Expected Sarsa\r
Consider the learning algorithm that is just like Q-learning except that instead of the\r
maximum over next state–action pairs it uses the expected value, taking into account\r
how likely each action is under the current policy. That is, consider the algorithm with\r
the update rule\r
Q(St, At) Q(St, At) + ↵\r
h\r
Rt+1 + E⇡[Q(St+1, At+1) | St+1]  Q(St, At)\r
i\r
= Q(St, At) + ↵\r
h\r
Rt+1 + \r
X\r
a\r
⇡(a|St+1)Q(St+1, a)  Q(St, At)\r
i\r
, (6.9)\r
but that otherwise follows the schema of Q-learning. Given the next state, St+1, this\r
algorithm moves deterministically in the same direction as Sarsa moves in expectation,\r
and accordingly it is called Expected Sarsa. Its backup diagram is shown on the right in\r
Figure 6.4.\r
Expected Sarsa is more complex computationally than Sarsa but, in return, it eliminates\r
the variance due to the random selection of At+1. Given the same amount of experience\r
we might expect it to perform slightly better than Sarsa, and indeed it generally does.\r
Figure 6.3 shows summary results on the cli↵-walking task with Expected Sarsa compared\r
to Sarsa and Q-learning. Expected Sarsa retains the significant advantage of Sarsa over\r
Q-learning on this problem. In addition, Expected Sarsa shows a significant improvement\r
ts on two versions of the windy\r
with a deterministic environment\r
c environment. We do so in order\r
e of environment stochasticity on\r
nce between Expected Sarsa and\r
rst part of Hypothesis 2. We then\r
ent amounts of policy stochasticity\r
of Hypothesis 2. For completeness,\r
ance of Q-learning on this problem.\r
ts in other domains verifying the\r
arsa in a broader setting. All results\r
raged over numerous independent\r
ard error becomes negligible.\r
ypothesis 1 using the cliff walking\r
isodic navigation task in which the\r
from start to goal in a deterministic\r
ge of the grid world is a cliff (see\r
take any of four movement actions:\r
each of which moves the agent one\r
ng direction. Each step results in a\r
n the agent steps into the cliff area,\r
d of -100 and an immediate return\r
isode ends upon reaching the goal\r
G\r
. The agent has to move from the start [S]\r
stepping into the cliff (grey area).\r
rmance over the first n episodes as\r
g rate ↵ using an -greedy policy\r
hows the result for n = 100 and\r
ed the results over 50,000 runs and\r
on the edge of the cliff immediately, resulting in a slightly\r
better on-line performance.\r
For n = 100, 000, the average return is equal for all\r
↵ values in case of Expected Sarsa and Q-learning. This\r
indicates that the algorithms have converged long before the\r
end of the run for all ↵ values, since we do not see any\r
effect of the initial learning phase. For Sarsa the performance\r
comes close to the performance of Expected Sarsa only for\r
↵ = 0.1, while for large ↵, the performance for n = 100, 000\r
even drops below the performance for n = 100. The reason\r
is that for large values of ↵ the Q values of Sarsa diverge.\r
Although the policy is still improved over the initial random\r
policy during the early stages of learning, divergence causes\r
the policy to get worse in the long run.\r
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 −160\r
−140\r
−120\r
−100\r
−80\r
−60\r
−40\r
−20\r
0\r
alpha\r
average return\r
n = 100, Sarsa\r
n = 100, Q−learning\r
n = 100, Expected Sarsa\r
n = 1E5, Sarsa\r
n = 1E5, Q−learning\r
n = 1E5, Expected Sarsa\r
Fig. 2. Average return on the cliff walking task over the first n episodes\r
for n = 100 and n = 100, 000 using an -greedy policy with  = 0.1. The\r
big dots indicate the maximal values.\r
B. Windy Grid World\r
We turn to the windy grid world task to further test Hy\u0002pothesis 2. The windy grid world task is another navigation\r
task, where the agent has to find its way from start to goal.\r
Expected Sarsa\r
Sarsa Q-learning\r
Asymptotic Performance\r
Interim Performance\r
Q-learning\r
Sum of rewards\r
per episode\r
↵\r
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1\r
0\r
-40\r
-80\r
-120\r
Figure 6.3: Interim and asymptotic performance of TD control methods on the cli↵-walking\r
task as a function of ↵. All algorithms used an "-greedy policy with " = 0.1. Asymptotic\r
performance is an average over 100,000 episodes whereas interim performance is an average\r
over the first 100 episodes. These data are averages of over 50,000 and 10 runs for the interim\r
and asymptotic cases respectively. The solid circles mark the best interim performance of each\r
method. Adapted from van Seijen et al. (2009).

134 Chapter 6: Temporal-Di↵erence Learning\r
Q-learning Expected Sarsa\r
Figure 6.4: The backup diagrams for Q-learning and Expected Sarsa.\r
over Sarsa over a wide range of values for the step-size parameter ↵. In cli↵ walking\r
the state transitions are all deterministic and all randomness comes from the policy. In\r
such cases, Expected Sarsa can safely set ↵ = 1 without su↵ering any degradation of\r
asymptotic performance, whereas Sarsa can only perform well in the long run at a small\r
value of ↵, at which short-term performance is poor. In this and other examples there is\r
a consistent empirical advantage of Expected Sarsa over Sarsa.\r
In these cli↵ walking results Expected Sarsa was used on-policy, but in general it\r
might use a policy di↵erent from the target policy ⇡ to generate behavior, in which case\r
it becomes an o↵-policy algorithm. For example, suppose ⇡ is the greedy policy while\r
behavior is more exploratory; then Expected Sarsa is exactly Q-learning. In this sense\r
Expected Sarsa subsumes and generalizes Q-learning while reliably improving over Sarsa.\r
Except for the small additional computational cost, Expected Sarsa may completely\r
dominate both of the other more-well-known TD control algorithms.\r
6.7 Maximization Bias and Double Learning\r
All the control algorithms that we have discussed so far involve maximization in the\r
construction of their target policies. For example, in Q-learning the target policy is\r
the greedy policy given the current action values, which is defined with a max, and in\r
Sarsa the policy is often "-greedy, which also involves a maximization operation. In these\r
algorithms, a maximum over estimated values is used implicitly as an estimate of the\r
maximum value, which can lead to a significant positive bias. To see why, consider a\r
single state s where there are many actions a whose true values, q(s, a), are all zero but\r
whose estimated values, Q(s, a), are uncertain and thus distributed some above and some\r
below zero. The maximum of the true values is zero, but the maximum of the estimates\r
is positive, a positive bias. We call this maximization bias.\r
Example 6.7: Maximization Bias Example The small MDP shown inset in\r
Figure 6.5 provides a simple example of how maximization bias can harm the performance\r
of TD control algorithms. The MDP has two non-terminal states A and B. Episodes\r
always start in A with a choice between two actions, left and right. The right action\r
transitions immediately to the terminal state with a reward and return of zero. The\r
left action transitions to B, also with a reward of zero, from which there are many\r
possible actions all of which cause immediate termination with a reward drawn from a\r
normal distribution with mean 0.1 and variance 1.0. Thus, the expected return for\r
any trajectory starting with left is 0.1, and thus taking left in state A is always a

6.7. Maximization Bias and Double Learning 135\r
B A left right\r
0\r
. . .\r
N(0.1, 1)\r
0\r
Q-learning\r
Double\r
Q-learning\r
Episodes\r
1 100 200 300\r
% left\r
actions\r
from A\r
100%\r
75%\r
50%\r
25%\r
5%\r
0\r
optimal\r
Figure 6.5: Comparison of Q-learning and Double Q-learning on a simple episodic MDP (shown\r
inset). Q-learning initially learns to take the left action much more often than the right action,\r
and always takes it significantly more often than the 5% minimum probability enforced by\r
"-greedy action selection with " = 0.1. In contrast, Double Q-learning is essentially una↵ected by\r
maximization bias. These data are averaged over 10,000 runs. The initial action-value estimates\r
were zero. Any ties in "-greedy action selection were broken randomly.\r
mistake. Nevertheless, our control methods may favor left because of maximization bias\r
making B appear to have a positive value. Figure 6.5 shows that Q-learning with "-greedy\r
action selection initially learns to strongly favor the left action on this example. Even at\r
asymptote, Q-learning takes the left action about 5% more often than is optimal at our\r
parameter settings (" = 0.1, ↵ = 0.1, and  = 1).\r
Are there algorithms that avoid maximization bias? To start, consider a bandit case in\r
which we have noisy estimates of the value of each of many actions, obtained as sample\r
averages of the rewards received on all the plays with each action. As we discussed above,\r
there will be a positive maximization bias if we use the maximum of the estimates as\r
an estimate of the maximum of the true values. One way to view the problem is that\r
it is due to using the same samples (plays) both to determine the maximizing action\r
and to estimate its value. Suppose we divided the plays in two sets and used them to\r
learn two independent estimates, call them Q1(a) and Q2(a), each an estimate of the\r
true value q(a), for all a 2 A. We could then use one estimate, say Q1, to determine\r
the maximizing action A⇤ = argmaxa Q1(a), and the other, Q2, to provide the estimate\r
of its value, Q2(A⇤) = Q2(argmaxa Q1(a)). This estimate will then be unbiased in the\r
sense that E[Q2(A⇤)] = q(A⇤). We can also repeat the process with the role of the two\r
estimates reversed to yield a second unbiased estimate Q1(argmaxa Q2(a)). This is the\r
idea of double learning. Note that although we learn two estimates, only one estimate is\r
updated on each play; double learning doubles the memory requirements, but does not\r
increase the amount of computation per step.\r
The idea of double learning extends naturally to algorithms for full MDPs. For example,\r
the double learning algorithm analogous to Q-learning, called Double Q-learning, divides\r
the time steps in two, perhaps by flipping a coin on each step. If the coin comes up heads,\r
the update is

136 Chapter 6: Temporal-Di↵erence Learning\r
Q1(St, At) Q1(St, At)+↵\r
h\r
Rt+1+Q2\r
\r
St+1, argmax aQ1(St+1, a)\r
\r
Q1(St, At)\r
i\r
. (6.10)\r
If the coin comes up tails, then the same update is done with Q1 and Q2 switched,\r
so that Q2 is updated. The two approximate value functions are treated completely\r
symmetrically. The behavior policy can use both action-value estimates. For example, an\r
"-greedy policy for Double Q-learning could be based on the average (or sum) of the two\r
action-value estimates. A complete algorithm for Double Q-learning is given in the box\r
below. This is the algorithm used to produce the results in Figure 6.5. In that example,\r
double learning seems to eliminate the harm caused by maximization bias. Of course\r
there are also double versions of Sarsa and Expected Sarsa.\r
Double Q-learning, for estimating Q1 ⇡ Q2 ⇡ q⇤\r
Algorithm parameters: step size ↵ 2 (0, 1], small " > 0\r
Initialize Q1(s, a) and Q2(s, a), for all s 2 S+, a 2 A(s), such that Q(terminal, ·)=0\r
Loop for each episode:\r
Initialize S\r
Loop for each step of episode:\r
Choose A from S using the policy "-greedy in Q1 + Q2\r
Take action A, observe R, S0\r
With 0.5 probabilility:\r
Q1(S, A) Q1(S, A) + ↵\r
⇣\r
R + Q2\r
\r
S0, argmaxa Q1(S0, a)\r
\r
 Q1(S, A)\r
⌘\r
else:\r
Q2(S, A) Q2(S, A) + ↵\r
⇣\r
R + Q1\r
\r
S0, argmaxa Q2(S0, a)\r
\r
 Q2(S, A)\r
⌘\r
S S0\r
until S is terminal\r
⇤\r
Exercise 6.13 What are the update equations for Double Expected Sarsa with an\r
"-greedy target policy? ⇤\r
6.8 Games, Afterstates, and Other Special Cases\r
In this book we try to present a uniform approach to a wide class of tasks, but of\r
course there are always exceptional tasks that are better treated in a specialized way. For\r
example, our general approach involves learning an action-value function, but in Chapter 1\r
we presented a TD method for learning to play tic-tac-toe that learned something much\r
more like a state-value function. If we look closely at that example, it becomes apparent\r
that the function learned there is neither an action-value function nor a state-value\r
function in the usual sense. A conventional state-value function evaluates states in which\r
the agent has the option of selecting an action, but the state-value function used in

6.8. Games, Afterstates, and Other Special Cases 137\r
tic-tac-toe evaluates board positions after the agent has made its move. Let us call these\r
afterstates, and value functions over these, afterstate value functions. Afterstates are\r
useful when we have knowledge of an initial part of the environment’s dynamics but not\r
necessarily of the full dynamics. For example, in games we typically know the immediate\r
e↵ects of our moves. We know for each possible chess move what the resulting position\r
will be, but not how our opponent will reply. Afterstate value functions are a natural\r
way to take advantage of this kind of knowledge and thereby produce a more ecient\r
learning method.\r
The reason it is more ecient to design algorithms in terms of afterstates is ap\u0002parent from the tic-tac-toe example. A conventional action-value function would map\r
from positions and moves to an estimate of the value. But many position—move\r
pairs produce the same resulting position, as in the example below: In such cases the\r
X\r
O X\r
X\r
O + X O X + X\r
position–move pairs are di↵er\u0002ent but produce the same “af\u0002terposition,” and thus must have\r
the same value. A conventional\r
action-value function would have\r
to separately assess both pairs,\r
whereas an afterstate value func\u0002tion would immediately assess\r
both equally. Any learning about\r
the position–move pair on the left\r
would immediately transfer to the\r
pair on the right.\r
Afterstates arise in many tasks,\r
not just games. For example, in\r
queuing tasks there are actions\r
such as assigning customers to servers, rejecting customers, or discarding information. In\r
such cases the actions are in fact defined in terms of their immediate e↵ects, which are\r
completely known.\r
It is impossible to describe all the possible kinds of specialized problems and corre\u0002sponding specialized learning algorithms. However, the principles developed in this book\r
should apply widely. For example, afterstate methods are still aptly described in terms\r
of generalized policy iteration, with a policy and (afterstate) value function interacting in\r
essentially the same way. In many cases one will still face the choice between on-policy\r
and o↵-policy methods for managing the need for persistent exploration.\r
Exercise 6.14 Describe how the task of Jack’s Car Rental (Example 4.2) could be\r
reformulated in terms of afterstates. Why, in terms of this specific task, would such a\r
reformulation be likely to speed convergence? ⇤

138 Chapter 6: Temporal-Di↵erence Learning\r
6.9 Summary\r
In this chapter we introduced a new kind of learning method, temporal-di↵erence (TD)\r
learning, and showed how it can be applied to the reinforcement learning problem. As\r
usual, we divided the overall problem into a prediction problem and a control problem.\r
TD methods are alternatives to Monte Carlo methods for solving the prediction problem.\r
In both cases, the extension to the control problem is via the idea of generalized policy\r
iteration (GPI) that we abstracted from dynamic programming. This is the idea that\r
approximate policy and value functions should interact in such a way that they both\r
move toward their optimal values.\r
One of the two processes making up GPI drives the value function to accurately predict\r
returns for the current policy; this is the prediction problem. The other process drives\r
the policy to improve locally (e.g., to be "-greedy) with respect to the current value\r
function. When the first process is based on experience, a complication arises concerning\r
maintaining sucient exploration. We can classify TD control methods according to\r
whether they deal with this complication by using an on-policy or o↵-policy approach.\r
Sarsa is an on-policy method, and Q-learning is an o↵-policy method. Expected Sarsa\r
is also an o↵-policy method as we present it here. There is a third way in which TD\r
methods can be extended to control which we did not include in this chapter, called\r
actor–critic methods. These methods are covered in full in Chapter 13.\r
The methods presented in this chapter are today the most widely used reinforcement\r
learning methods. This is probably due to their great simplicity: they can be applied\r
online, with a minimal amount of computation, to experience generated from interaction\r
with an environment; they can be expressed nearly completely by single equations that\r
can be implemented with small computer programs. In the next few chapters we extend\r
these algorithms, making them slightly more complicated and significantly more powerful.\r
All the new algorithms will retain the essence of those introduced here: they will be able\r
to process experience online, with relatively little computation, and they will be driven\r
by TD errors. The special cases of TD methods introduced in the present chapter should\r
rightly be called one-step, tabular, model-free TD methods. In the next two chapters we\r
extend them to n-step forms (a link to Monte Carlo methods) and forms that include\r
a model of the environment (a link to planning and dynamic programming). Then, in\r
the second part of the book we extend them to various forms of function approximation\r
rather than tables (a link to deep learning and artificial neural networks).\r
Finally, in this chapter we have discussed TD methods entirely within the context of\r
reinforcement learning problems, but TD methods are actually more general than this.\r
They are general methods for learning to make long-term predictions about dynamical\r
systems. For example, TD methods may be relevant to predicting financial data, life\r
spans, election outcomes, weather patterns, animal behavior, demands on power stations,\r
or customer purchases. It was only when TD methods were analyzed as pure prediction\r
methods, independent of their use in reinforcement learning, that their theoretical\r
properties first came to be well understood. Even so, these other potential applications\r
of TD learning methods have not yet been extensively explored.

6.9. Summary 139\r
Bibliographical and Historical Remarks\r
As we outlined in Chapter 1, the idea of TD learning has its early roots in animal learning\r
psychology and artificial intelligence, most notably the work of Samuel (1959) and Klopf\r
(1972). Samuel’s work is described as a case study in Section 16.2. Also related to TD\r
learning are Holland’s (1975, 1976) early ideas about consistency among value predictions.\r
These influenced one of the authors (Barto), who was a graduate student from 1970 to\r
1975 at the University of Michigan, where Holland was teaching. Holland’s ideas led to\r
a number of TD-related systems, including the work of Booker (1982) and the bucket\r
brigade of Holland (1986), which is related to Sarsa as discussed below.\r
6.1–2 Most of the specific material from these sections is from Sutton (1988), includ\u0002ing the TD(0) algorithm, the random walk example, and the term “temporal\u0002di↵erence learning.” The characterization of the relationship to dynamic pro\u0002gramming and Monte Carlo methods was influenced by Watkins (1989), Werbos\r
(1987), and others. The use of backup diagrams was new to the first edition of\r
this book.\r
Tabular TD(0) was proved to converge in the mean by Sutton (1988) and with\r
probability 1 by Dayan (1992), based on the work of Watkins and Dayan (1992).\r
These results were extended and strengthened by Jaakkola, Jordan, and Singh\r
(1994) and Tsitsiklis (1994) by using extensions of the powerful existing theory\r
of stochastic approximation. Other extensions and generalizations are covered in\r
later chapters.\r
6.3 The optimality of the TD algorithm under batch training was established by\r
Sutton (1988). Illuminating this result is Barnard’s (1993) derivation of the TD\r
algorithm as a combination of one step of an incremental method for learning a\r
model of the Markov chain and one step of a method for computing predictions\r
from the model. The term certainty equivalence is from the adaptive control\r
literature (e.g., Goodwin and Sin, 1984).\r
6.4 The Sarsa algorithm was introduced by Rummery and Niranjan (1994). They\r
explored it in conjunction with artificial neural networks and called it “Modified\r
Connectionist Q-learning”. The name “Sarsa” was introduced by Sutton (1996).\r
The convergence of one-step tabular Sarsa (the form treated in this chapter) has\r
been proved by Singh, Jaakkola, Littman, and Szepesv´ari (2000). The “windy\r
gridworld” example was suggested by Tom Kalt.\r
Holland’s (1986) bucket brigade idea evolved into an algorithm closely related to\r
Sarsa. The original idea of the bucket brigade involved chains of rules triggering\r
each other; it focused on passing credit back from the current rule to the rules\r
that triggered it. Over time, the bucket brigade came to be more like TD learning\r
in passing credit back to any temporally preceding rule, not just to the ones\r
that triggered the current rule. The modern form of the bucket brigade, when\r
simplified in various natural ways, is nearly identical to one-step Sarsa, as detailed\r
by Wilson (1994).

140 Chapter 6: Temporal-Di↵erence Learning\r
6.5 Q-learning was introduced by Watkins (1989), whose outline of a convergence\r
proof was made rigorous by Watkins and Dayan (1992). More general convergence\r
results were proved by Jaakkola, Jordan, and Singh (1994) and Tsitsiklis (1994).\r
6.6 The Expected Sarsa algorithm was introduced by George John (1994), who\r
called it “Q-learning” and stressed its advantages over Q-learning as an o↵-policy\r
algorithm. John’s work was not known to us when we presented Expected\r
Sarsa in the first edition of this book as an exercise, or to van Seijen, van\r
Hasselt, Whiteson, and Weiring (2009) when they established Expected Sarsa’s\r
convergence properties and conditions under which it will outperform regular\r
Sarsa and Q-learning. Our Figure 6.3 is adapted from their results. Van Seijen\r
et al. defined “Expected Sarsa” to be an on-policy method exclusively (as we\r
did in the first edition), whereas now we use this name for the general algorithm\r
in which the target and behavior policies may di↵er. The general o↵-policy\r
view of Expected Sarsa was noted by van Hasselt (2011), who called it “General\r
Q-learning.”\r
6.7 Maximization bias and double learning were introduced and extensively investi\u0002gated by van Hasselt (2010, 2011). The example MDP in Figure 6.5 was adapted\r
from that in his Figure 4.1 (van Hasselt, 2011).\r
6.8 The notion of an afterstate is the same as that of a “post-decision state” (Van\r
Roy, Bertsekas, Lee, and Tsitsiklis, 1997; Powell, 2011).

Chapter 7\r
n-step Bootstrapping\r
In this chapter we unify the Monte Carlo (MC) methods and the one-step temporal\u0002di↵erence (TD) methods presented in the previous two chapters. Neither MC methods nor\r
one-step TD methods are always the best. In this chapter we present n-step TD methods\r
that generalize both methods so that one can shift from one to the other smoothly as\r
needed to meet the demands of a particular task. n-step methods span a spectrum with\r
MC methods at one end and one-step TD methods at the other. The best methods are\r
often intermediate between the two extremes.\r
Another way of looking at the benefits of n-step methods is that they free you from\r
the tyranny of the time step. With one-step TD methods the same time step determines\r
how often the action can be changed and the time interval over which bootstrapping\r
is done. In many applications one wants to be able to update the action very fast to\r
take into account anything that has changed, but bootstrapping works best if it is over a\r
length of time in which a significant and recognizable state change has occurred. With\r
one-step TD methods, these time intervals are the same, and so a compromise must be\r
made. n-step methods enable bootstrapping to occur over multiple steps, freeing us from\r
the tyranny of the single time step.\r
The idea of n-step methods is usually used as an introduction to the algorithmic\r
idea of eligibility traces (Chapter 12), which enable bootstrapping over multiple time\r
intervals simultaneously. Here we instead consider the n-step bootstrapping idea on its\r
own, postponing the treatment of eligibility-trace mechanisms until later. This allows us\r
to separate the issues better, dealing with as many of them as possible in the simpler\r
n-step setting.\r
As usual, we first consider the prediction problem and then the control problem. That\r
is, we first consider how n-step methods can help in predicting returns as a function of\r
state for a fixed policy (i.e., in estimating v⇡). Then we extend the ideas to action values\r
and control methods.

142 Chapter 7: n-step Bootstrapping\r
7.1 n-step TD Prediction\r
What is the space of methods lying between Monte Carlo and TD methods? Consider\r
estimating v⇡ from sample episodes generated using ⇡. Monte Carlo methods perform\r
an update for each state based on the entire sequence of observed rewards from that\r
state until the end of the episode. The update of one-step TD methods, on the other\r
hand, is based on just the one next reward, bootstrapping from the value of the state\r
one step later as a proxy for the remaining rewards. One kind of intermediate method,\r
then, would perform an update based on an intermediate number of rewards: more than\r
one, but less than all of them until termination. For example, a two-step update would\r
be based on the first two rewards and the estimated value of the state two steps later.\r
Similarly, we could have three-step updates, four-step updates, and so on. Figure 7.1\r
shows the backup diagrams of the spectrum of n-step updates for v⇡, with the one-step\r
TD update on the left and the up-until-termination Monte Carlo update on the right.\r
1-step TD\r
and TD(0) 2-step TD 3-step TD n-step TD\r
∞-step TD\r
and Monte Carlo\r
···\r
···\r
···\r
···\r
Figure 7.1: The backup diagrams of n-step methods. These methods form a spectrum ranging\r
from one-step TD methods to Monte Carlo methods.\r
The methods that use n-step updates are still TD methods because they still change\r
an earlier estimate based on how it di↵ers from a later estimate. Now the later estimate\r
is not one step later, but n steps later. Methods in which the temporal di↵erence extends\r
over n steps are called n-step TD methods. The TD methods introduced in the previous\r
chapter all used one-step updates, which is why we called them one-step TD methods.\r
More formally, consider the update of the estimated value of state St as a result of the\r
state–reward sequence, St, Rt+1, St+1, Rt+2,...,RT , ST (omitting the actions). We know\r
that in Monte Carlo updates the estimate of v⇡(St) is updated in the direction of the

7.1. n-step TD Prediction 143\r
complete return:\r
Gt\r
.\r
= Rt+1 + Rt+2 + 2Rt+3 + ··· + T t1RT ,\r
where T is the last time step of the episode. Let us call this quantity the target of the\r
update. Whereas in Monte Carlo updates the target is the return, in one-step updates\r
the target is the first reward plus the discounted estimated value of the next state, which\r
we call the one-step return:\r
Gt:t+1\r
.\r
= Rt+1 + Vt(St+1),\r
where Vt : S ! R here is the estimate at time t of v⇡. The subscripts on Gt:t+1 indicate\r
that it is a truncated return for time t using rewards up until time t+1, with the discounted\r
estimate Vt(St+1) taking the place of the other terms Rt+2 + 2Rt+3 + ···+ T t1RT\r
of the full return, as discussed in the previous chapter. Our point now is that this idea\r
makes just as much sense after two steps as it does after one. The target for a two-step\r
update is the two-step return:\r
Gt:t+2\r
.\r
= Rt+1 + Rt+2 + 2Vt+1(St+2),\r
where now 2Vt+1(St+2) corrects for the absence of the terms 2Rt+3 + 3Rt+4 + ··· +\r
T t1RT . Similarly, the target for an arbitrary n-step update is the n-step return:\r
Gt:t+n\r
.\r
= Rt+1 + Rt+2 + ··· + n1Rt+n + nVt+n1(St+n), (7.1)\r
for all n, t such that n  1 and 0  t<T  n. All n-step returns can be considered\r
approximations to the full return, truncated after n steps and then corrected for the\r
remaining missing terms by Vt+n1(St+n). If t + n  T (if the n-step return extends\r
to or beyond termination), then all the missing terms are taken as zero, and the n-step\r
return defined to be equal to the ordinary full return (Gt:t+n\r
.\r
= Gt if t + n  T).\r
Note that n-step returns for n > 1 involve future rewards and states that are not\r
available at the time of transition from t to t + 1. No real algorithm can use the n-step\r
return until after it has seen Rt+n and computed Vt+n1. The first time these are\r
available is t + n. The natural state-value learning algorithm for using n-step returns is\r
thus\r
Vt+n(St) .= Vt+n1(St) + ↵\r
⇥\r
Gt:t+n  Vt+n1(St)\r
⇤\r
, 0  t < T, (7.2)\r
while the values of all other states remain unchanged: Vt+n(s) = Vt+n1(s), for all s6=St.\r
We call this algorithm n-step TD. Note that no changes at all are made during the first\r
n  1 steps of each episode. To make up for that, an equal number of additional updates\r
are made at the end of the episode, after termination and before starting the next episode.\r
Complete pseudocode is given in the box on the next page.\r
Exercise 7.1 In Chapter 6 we noted that the Monte Carlo error can be written as the\r
sum of TD errors (6.6) if the value estimates don’t change from step to step. Show that\r
the n-step error used in (7.2) can also be written as a sum of TD errors (again if the\r
value estimates don’t change) generalizing the earlier result. ⇤\r
Exercise 7.2 (programming) With an n-step method, the value estimates do change from\r
step to step, so an algorithm that used the sum of TD errors (see previous exercise) in

144 Chapter 7: n-step Bootstrapping\r
n-step TD for estimating V ⇡ v⇡\r
Input: a policy ⇡\r
Algorithm parameters: step size ↵ 2 (0, 1], a positive integer n\r
Initialize V (s) arbitrarily, for all s 2 S\r
All store and access operations (for St and Rt) can take their index mod n + 1\r
Loop for each episode:\r
Initialize and store S0 6= terminal\r
T 1\r
Loop for t = 0, 1, 2,... :\r
| If t<T, then:\r
| Take an action according to ⇡(·|St)\r
| Observe and store the next reward as Rt+1 and the next state as St+1\r
| If St+1 is terminal, then T t + 1\r
| ⌧ t  n +1 (⌧ is the time whose state’s estimate is being updated)\r
| If ⌧  0:\r
| G Pmin(⌧+n,T)\r
i=⌧+1 i⌧1Ri\r
| If ⌧ + n<T, then: G G + nV (S⌧+n) (G⌧:⌧+n)\r
| V (S⌧ ) V (S⌧ ) + ↵ [G  V (S⌧ )]\r
Until ⌧ = T  1\r
place of the error in (7.2) would actually be a slightly di↵erent algorithm. Would it be a\r
better algorithm or a worse one? Devise and program a small experiment to answer this\r
question empirically. ⇤\r
The n-step return uses the value function Vt+n1 to correct for the missing rewards\r
beyond Rt+n. An important property of n-step returns is that their expectation is\r
guaranteed to be a better estimate of v⇡ than Vt+n1 is, in a worst-state sense. That is,\r
the worst error of the expected n-step return is guaranteed to be less than or equal to n\r
times the worst error under Vt+n1:\r
maxs\r
\r
\r
\r
E⇡[Gt:t+n|St =s]  v⇡(s)\r
\r
\r
  n maxs\r
\r
\r
\r
Vt+n1(s)  v⇡(s)\r
\r
\r
, (7.3)\r
for all n  1. This is called the error reduction property of n-step returns. Because of the\r
error reduction property, one can show formally that all n-step TD methods converge to\r
the correct predictions under appropriate technical conditions. The n-step TD methods\r
thus form a family of sound methods, with one-step TD methods and Monte Carlo\r
methods as extreme members.\r
Example 7.1: n-step TD Methods on the Random Walk Consider using n-step\r
TD methods on the 5-state random walk task described in Example 6.2 (page 125).\r
Suppose the first episode progressed directly from the center state, C, to the right,\r
through D and E, and then terminated on the right with a return of 1. Recall that the\r
estimated values of all the states started at an intermediate value, V (s)=0.5. As a result\r
of this experience, a one-step method would change only the estimate for the last state,

7.2. n-step Sarsa 145\r
V (E), which would be incremented toward 1, the observed return. A two-step method,\r
on the other hand, would increment the values of the two states preceding termination:\r
V (D) and V (E) both would be incremented toward 1. A three-step method, or any n-step\r
method for n > 2, would increment the values of all three of the visited states toward 1,\r
all by the same amount.\r
Which value of n is better? Figure 7.2 shows the results of a simple empirical test for\r
a larger random walk process, with 19 states instead of 5 (and with a 1 outcome on the\r
left, all values initialized to 0), which we use as a running example in this chapter. Results\r
are shown for n-step TD methods with a range of values for n and ↵. The performance\r
measure for each parameter setting, shown on the vertical axis, is the square-root of\r
the average squared error between the predictions at the end of the episode for the 19\r
states and their true values, then averaged over the first 10 episodes and 100 repetitions\r
of the whole experiment (the same sets of walks were used for all parameter settings).\r
Note that methods with an intermediate value of n worked best. This illustrates how\r
the generalization of TD and Monte Carlo methods to n-step methods can potentially\r
perform better than either of the two extreme methods.\r
↵\r
Average\r
RMS error\r
over 19 states\r
and first 10 \r
episodes n=1\r
n=2\r
n=4\r
n=8\r
n=16\r
n=32\r
n=32 n=64 128 512256 0.55\r
0.5\r
0.45\r
0.35\r
0.3\r
0.25\r
0.4\r
0 0.2 0.4 0.6 0.8 1\r
Figure 7.2: Performance of n-step TD methods as a function of ↵, for various values of n, on\r
a 19-state random walk task (Example 7.1).\r
Exercise 7.3 Why do you think a larger random walk task (19 states instead of 5) was\r
used in the examples of this chapter? Would a smaller walk have shifted the advantage\r
to a di↵erent value of n? How about the change in left-side outcome from 0 to 1 made\r
in the larger walk? Do you think that made any di↵erence in the best value of n? ⇤\r
7.2 n-step Sarsa\r
How can n-step methods be used not just for prediction, but for control? In this section\r
we show how n-step methods can be combined with Sarsa in a straightforward way to

146 Chapter 7: n-step Bootstrapping\r
produce an on-policy TD control method. The n-step version of Sarsa we call n-step\r
Sarsa, and the original version presented in the previous chapter we henceforth call\r
one-step Sarsa, or Sarsa(0).\r
The main idea is to simply switch states for actions (state–action pairs) and then use\r
an "-greedy policy. The backup diagrams for n-step Sarsa (shown in Figure 7.3), like\r
those of n-step TD (Figure 7.1), are strings of alternating states and actions, except that\r
the Sarsa ones all start and end with an action rather a state. We redefine n-step returns\r
(update targets) in terms of estimated action values:\r
Gt:t+n\r
.\r
= Rt+1+Rt+2+···+n1Rt+n+nQt+n1(St+n, At+n), n  1, 0  t<T n,\r
(7.4)\r
with Gt:t+n\r
.\r
= Gt if t + n  T. The natural algorithm is then\r
Qt+n(St, At) .= Qt+n1(St, At) + ↵ [Gt:t+n  Qt+n1(St, At)] , 0  t < T, (7.5)\r
while the values of all other states remain unchanged: Qt+n(s, a) = Qt+n1(s, a), for all\r
s, a such that s 6= St or a 6= At. This is the algorithm we call n-step Sarsa. Pseudocode\r
is shown in the box on the next page, and an example of why it can speed up learning\r
compared to one-step methods is given in Figure 7.4.\r
1-step Sarsa\r
aka Sarsa(0) 2-step Sarsa 3-step Sarsa n-step Sarsa\r
∞-step Sarsa\r
aka Monte Carlo\r
n-step \r
Expected Sarsa\r
Figure 7.3: The backup diagrams for the spectrum of n-step methods for state–action values.\r
They range from the one-step update of Sarsa(0) to the up-until-termination update of the\r
Monte Carlo method. In between are the n-step updates, based on n steps of real rewards and\r
the estimated value of the nth next state–action pair, all appropriately discounted. On the far\r
right is the backup diagram for n-step Expected Sarsa.

7.2. n-step Sarsa 147\r
n-step Sarsa for estimating Q ⇡ q⇤ or q⇡\r
Initialize Q(s, a) arbitrarily, for all s 2 S, a 2 A\r
Initialize ⇡ to be "-greedy with respect to Q, or to a fixed given policy\r
Algorithm parameters: step size ↵ 2 (0, 1], small " > 0, a positive integer n\r
All store and access operations (for St, At, and Rt) can take their index mod n + 1\r
Loop for each episode:\r
Initialize and store S0 6= terminal\r
Select and store an action A0 ⇠ ⇡(·|S0)\r
T 1\r
Loop for t = 0, 1, 2,... :\r
| If t<T, then:\r
| Take action At\r
| Observe and store the next reward as Rt+1 and the next state as St+1\r
| If St+1 is terminal, then:\r
| T t + 1\r
| else:\r
| Select and store an action At+1 ⇠ ⇡(·|St+1)\r
| ⌧ t  n +1 (⌧ is the time whose estimate is being updated)\r
| If ⌧  0:\r
| G Pmin(⌧+n,T)\r
i=⌧+1 i⌧1Ri\r
| If ⌧ + n<T, then G G + nQ(S⌧+n, A⌧+n) (G⌧:⌧+n)\r
| Q(S⌧ , A⌧ ) Q(S⌧ , A⌧ ) + ↵ [G  Q(S⌧ , A⌧ )]\r
| If ⇡ is being learned, then ensure that ⇡(·|S⌧ ) is "-greedy wrt Q\r
Until ⌧ = T  1\r
Path taken\r
Action values increased\r
by one-step Sarsa\r
Action values increased\r
by 10-step Sarsa\r
G G G\r
Figure 7.4: Gridworld example of the speedup of policy learning due to the use of n-step\r
methods. The first panel shows the path taken by an agent in a single episode, ending at a\r
location of high reward, marked by the G. In this example the values were all initially 0, and all\r
rewards were zero except for a positive reward at G. The arrows in the other two panels show\r
which action values were strengthened as a result of this path by one-step and n-step Sarsa\r
methods. The one-step method strengthens only the last action of the sequence of actions that\r
led to the high reward, whereas the n-step method strengthens the last n actions of the sequence,\r
so that much more is learned from the one episode.

148 Chapter 7: n-step Bootstrapping\r
Exercise 7.4 Prove that the n-step return of Sarsa (7.4) can be written exactly in terms\r
of a novel TD error, as\r
Gt:t+n = Qt1(St, At)+\r
min(t\r
X\r
+n,T)1\r
k=t\r
kt [Rk+1 + Qk(Sk+1, Ak+1)  Qk1(Sk, Ak)] .\r
(7.6)\r
⇤\r
What about Expected Sarsa? The backup diagram for the n-step version of Expected\r
Sarsa is shown on the far right in Figure 7.3. It consists of a linear string of sample\r
actions and states, just as in n-step Sarsa, except that its last element is a branch over\r
all action possibilities weighted, as always, by their probability under ⇡. This algorithm\r
can be described by the same equation as n-step Sarsa (above) except with the n-step\r
return redefined as\r
Gt:t+n\r
.\r
= Rt+1 + ··· + n1Rt+n + nV¯t+n1(St+n), t + n < T, (7.7)\r
(with Gt:t+n\r
.\r
=Gt for t + n  T) where V¯t(s) is the expected approximate value of state s,\r
using the estimated action values at time t, under the target policy:\r
V¯t(s) .= X\r
a\r
⇡(a|s)Qt(s, a), for all s 2 S. (7.8)\r
Expected approximate values are used in developing many of the action-value methods\r
in the rest of this book. If s is terminal, then its expected approximate value is defined\r
to be 0.\r
7.3 n-step O↵-policy Learning\r
Recall that o↵-policy learning is learning the value function for one policy, ⇡, while\r
following another policy, b. Often, ⇡ is the greedy policy for the current action-value\u0002function estimate, and b is a more exploratory policy, perhaps "-greedy. In order to\r
use the data from b we must take into account the di↵erence between the two policies,\r
using their relative probability of taking the actions that were taken (see Section 5.5). In\r
n-step methods, returns are constructed over n steps, so we are interested in the relative\r
probability of just those n actions. For example, to make a simple o↵-policy version of\r
n-step TD, the update for time t (actually made at time t + n) can simply be weighted\r
by ⇢t:t+n1:\r
Vt+n(St) .= Vt+n1(St) + ↵⇢t:t+n1 [Gt:t+n  Vt+n1(St)] , 0  t < T, (7.9)\r
where ⇢t:t+n1, called the importance sampling ratio, is the relative probability under\r
the two policies of taking the n actions from At to At+n1 (cf. Eq. 5.3):\r
⇢t:h\r
.\r
=\r
min(\r
Y\r
h,T 1)\r
k=t\r
⇡(Ak|Sk)\r
b(Ak|Sk)\r
. (7.10)

7.3. n-step O↵-policy Learning 149\r
For example, if any one of the actions would never be taken by ⇡ (i.e., ⇡(Ak|Sk) = 0) then\r
the n-step return should be given zero weight and be totally ignored. On the other hand,\r
if by chance an action is taken that ⇡ would take with much greater probability than b\r
does, then this will increase the weight that would otherwise be given to the return. This\r
makes sense because that action is characteristic of ⇡ (and therefore we want to learn\r
about it) but is selected only rarely by b and thus rarely appears in the data. To make\r
up for this we have to over-weight it when it does occur. Note that if the two policies\r
are actually the same (the on-policy case) then the importance sampling ratio is always\r
1. Thus our new update (7.9) generalizes and can completely replace our earlier n-step\r
TD update. Similarly, our previous n-step Sarsa update can be completely replaced by a\r
simple o↵-policy form:\r
Qt+n(St, At) .= Qt+n1(St, At) + ↵⇢t+1:t+n [Gt:t+n  Qt+n1(St, At)] , (7.11)\r
for 0  t<T. Note that the importance sampling ratio here starts and ends one step\r
later than for n-step TD (7.9). This is because here we are updating a state–action\r
pair. We do not have to care how likely we were to select the action; now that we have\r
selected it we want to learn fully from what happens, with importance sampling only for\r
subsequent actions. Pseudocode for the full algorithm is shown in the box below.\r
O↵-policy n-step Sarsa for estimating Q ⇡ q⇤ or q⇡\r
Input: an arbitrary behavior policy b such that b(a|s) > 0, for all s 2 S, a 2 A\r
Initialize Q(s, a) arbitrarily, for all s 2 S, a 2 A\r
Initialize ⇡ to be greedy with respect to Q, or as a fixed given policy\r
Algorithm parameters: step size ↵ 2 (0, 1], a positive integer n\r
All store and access operations (for St, At, and Rt) can take their index mod n + 1\r
Loop for each episode:\r
Initialize and store S0 6= terminal\r
Select and store an action A0 ⇠ b(·|S0)\r
T 1\r
Loop for t = 0, 1, 2,... :\r
| If t<T, then:\r
| Take action At\r
| Observe and store the next reward as Rt+1 and the next state as St+1\r
| If St+1 is terminal, then:\r
| T t + 1\r
| else:\r
| Select and store an action At+1 ⇠ b(·|St+1)\r
| ⌧ t  n +1 (⌧ is the time whose estimate is being updated)\r
| If ⌧  0:\r
| ⇢ Qmin(⌧+n,T 1)\r
i=⌧+1\r
⇡(Ai|Si)\r
b(Ai|Si) (⇢⌧+1:⌧+n)\r
| G Pmin(⌧+n,T )\r
i=⌧+1 i⌧1Ri\r
| If ⌧ + n<T, then: G G + nQ(S⌧+n, A⌧+n) (G⌧:⌧+n)\r
| Q(S⌧ , A⌧ ) Q(S⌧ , A⌧ ) + ↵⇢ [G  Q(S⌧ , A⌧ )]\r
| If ⇡ is being learned, then ensure that ⇡(·|S⌧ ) is greedy wrt Q\r
Until ⌧ = T  1

150 Chapter 7: n-step Bootstrapping\r
The o↵-policy version of n-step Expected Sarsa would use the same update as above\r
for n-step Sarsa except that the importance sampling ratio would have one less factor in\r
it. That is, the above equation would use ⇢t+1:t+n1 instead of ⇢t+1:t+n, and of course\r
it would use the Expected Sarsa version of the n-step return (7.7). This is because in\r
Expected Sarsa all possible actions are taken into account in the last state; the one\r
actually taken has no e↵ect and does not have to be corrected for.\r
7.4 *Per-decision Methods with Control Variates\r
The multi-step o↵-policy methods presented in the previous section are simple and\r
conceptually clear, but are probably not the most ecient. A more sophisticated approach\r
would use per-decision importance sampling ideas such as were introduced in Section 5.9.\r
To understand this approach, first note that the ordinary n-step return (7.1), like all\r
returns, can be written recursively. For the n steps ending at horizon h, the n-step return\r
can be written\r
Gt:h = Rt+1 + Gt+1:h, t < h < T, (7.12)\r
where Gh:h\r
.\r
= Vh1(Sh). (Recall that this return is used at time h, previously denoted\r
t + n.) Now consider the e↵ect of following a behavior policy b that is not the same\r
as the target policy ⇡. All of the resulting experience, including the first reward Rt+1\r
and the next state St+1, must be weighted by the importance sampling ratio for time t,\r
⇢t = ⇡(At|St)\r
b(At|St) . One might be tempted to simply weight the righthand side of the above\r
equation, but one can do better. Suppose the action at time t would never be selected by\r
⇡, so that ⇢t is zero. Then a simple weighting would result in the n-step return being\r
zero, which could result in high variance when it was used as a target. Instead, in this\r
more sophisticated approach, one uses an alternate, o↵-policy definition of the n-step\r
return ending at horizon h, as\r
Gt:h\r
.\r
= ⇢t (Rt+1 + Gt+1:h) + (1  ⇢t)Vh1(St), t < h < T, (7.13)\r
where again Gh:h\r
.\r
= Vh1(Sh). In this approach, if ⇢t is zero, then instead of the target\r
being zero and causing the estimate to shrink, the target is the same as the estimate and\r
causes no change. The importance sampling ratio being zero means we should ignore the\r
sample, so leaving the estimate unchanged seems appropriate. The second, additional\r
term in (7.13) is called a control variate (for obscure reasons). Notice that the control\r
variate does not change the expected update; the importance sampling ratio has expected\r
value one (Section 5.9) and is uncorrelated with the estimate, so the expected value\r
of the control variate is zero. Also note that the o↵-policy definition (7.13) is a strict\r
generalization of the earlier on-policy definition of the n-step return (7.1), as the two are\r
identical in the on-policy case, in which ⇢t is always 1.\r
For a conventional n-step method, the learning rule to use in conjunction with (7.13)\r
is the n-step TD update (7.2), which has no explicit importance sampling ratios other\r
than those embedded in the return.\r
Exercise 7.5 Write the pseudocode for the o↵-policy state-value prediction algorithm\r
described above. ⇤

7.4. Per-decision Methods with Control Variates 151\r
For action values, the o↵-policy definition of the n-step return is a little di↵erent\r
because the first action does not play a role in the importance sampling. That first action\r
is the one being learned; it does not matter if it was unlikely or even impossible under the\r
target policy—it has been taken and now full unit weight must be given to the reward\r
and state that follows it. Importance sampling will apply only to the actions that follow\r
it.\r
First note that for action values the n-step on-policy return ending at horizon h,\r
expectation form (7.7), can be written recursively just as in (7.12), except that for action\r
values the recursion ends with Gh:h\r
.\r
= V¯h1(Sh) as in (7.8). An o↵-policy form with\r
control variates is\r
Gt:h\r
.\r
= Rt+1 + \r
⇣\r
⇢t+1Gt+1:h + V¯h1(St+1)  ⇢t+1Qh1(St+1, At+1)\r
⌘\r
,\r
= Rt+1 + ⇢t+1⇣Gt+1:h  Qh1(St+1, At+1)\r
⌘\r
+ V¯h1(St+1), t<h  T.\r
(7.14)\r
If h<T, then the recursion ends with Gh:h\r
.\r
= Qh1(Sh, Ah), whereas, if h  T,\r
the recursion ends with and GT 1:h\r
.\r
= RT . The resultant prediction algorithm (after\r
combining with (7.5)) is analogous to Expected Sarsa.\r
Exercise 7.6 Prove that the control variate in the above equations does not change the\r
expected value of the return. ⇤\r
⇤\r
Exercise 7.7 Write the pseudocode for the o↵-policy action-value prediction algorithm\r
described immediately above. Pay particular attention to the termination conditions for\r
the recursion upon hitting the horizon or the end of episode. ⇤\r
Exercise 7.8 Show that the general (o↵-policy) version of the n-step return (7.13) can\r
still be written exactly and compactly as the sum of state-based TD errors (6.5) if the\r
approximate state value function does not change. ⇤\r
Exercise 7.9 Repeat the above exercise for the action version of the o↵-policy n-step\r
return (7.14) and the Expected Sarsa TD error (the quantity in brackets in Equation 6.9).\r
⇤\r
Exercise 7.10 (programming) Devise a small o↵-policy prediction problem and use it to\r
show that the o↵-policy learning algorithm using (7.13) and (7.2) is more data ecient\r
than the simpler algorithm using (7.1) and (7.9). ⇤\r
The importance sampling that we have used in this section, the previous section, and\r
in Chapter 5, enables sound o↵-policy learning, but also results in high variance updates,\r
forcing the use of a small step-size parameter and thereby causing learning to be slow. It\r
is probably inevitable that o↵-policy training is slower than on-policy training—after all,\r
the data is less relevant to what is being learned. However, it is probably also true that\r
these methods can be improved on. The control variates are one way of reducing the\r
variance. Another is to rapidly adapt the step sizes to the observed variance, as in the\r
Autostep method (Mahmood, Sutton, Degris and Pilarski, 2012). Yet another promising\r
approach is the invariant updates of Karampatziakis and Langford (2010) as extended\r
to TD by Tian (in preparation). The usage technique of Mahmood (2017; Mahmood

152 Chapter 7: n-step Bootstrapping\r
and Sutton, 2015) may also be part of the solution. In the next section we consider an\r
o↵-policy learning method that does not use importance sampling.\r
7.5 O↵-policy Learning Without Importance Sampling:\r
The n-step Tree Backup Algorithm\r
Is o↵-policy learning possible without importance sampling? Q-learning and Expected\r
Sarsa from Chapter 6 do this for the one-step case, but is there a corresponding multi-step\r
algorithm? In this section we present just such an n-step method, called the tree-backup\r
algorithm.\r
St, At\r
At+1\r
Rt+1\r
St+1\r
St+2\r
Rt+2\r
At+2 Rt+3\r
St+3\r
the 3-step\r
tree-backup\r
update\r
The idea of the algorithm is suggested by the 3-step tree-backup backup\r
diagram shown to the right. Down the central spine and labeled in the\r
diagram are three sample states and rewards, and two sample actions.\r
These are the random variables representing the events occurring after the\r
initial state–action pair St, At. Hanging o↵ to the sides of each state are\r
the actions that were not selected. (For the last state, all the actions are\r
considered to have not (yet) been selected.) Because we have no sample\r
data for the unselected actions, we bootstrap and use the estimates of\r
their values in forming the target for the update. This slightly extends the\r
idea of a backup diagram. So far we have always updated the estimated\r
value of the node at the top of the diagram toward a target combining\r
the rewards along the way (appropriately discounted) and the estimated\r
values of the nodes at the bottom. In the tree-backup update, the target\r
includes all these things plus the estimated values of the dangling action\r
nodes hanging o↵ the sides, at all levels. This is why it is called a tree\u0002backup update; it is an update from the entire tree of estimated action\r
values.\r
More precisely, the update is from the estimated action values of the\r
leaf nodes of the tree. The action nodes in the interior, corresponding to\r
the actual actions taken, do not participate. Each leaf node contributes to the target\r
with a weight proportional to its probability of occurring under the target policy ⇡. Thus\r
each first-level action a contributes with a weight of ⇡(a|St+1), except that the action\r
actually taken, At+1, does not contribute at all. Its probability, ⇡(At+1|St+1), is used\r
to weight all the second-level action values. Thus, each non-selected second-level action\r
a0 contributes with weight ⇡(At+1|St+1)⇡(a0|St+2). Each third-level action contributes\r
with weight ⇡(At+1|St+1)⇡(At+2|St+2)⇡(a00|St+3), and so on. It is as if each arrow to an\r
action node in the diagram is weighted by the action’s probability of being selected under\r
the target policy and, if there is a tree below the action, then that weight applies to all\r
the leaf nodes in the tree.

7.5. O↵-policy Learning Without Importance Sampling: n-step Tree Backup 153\r
We can think of the 3-step tree-backup update as consisting of 6 half-steps, alternating\r
between sample half-steps from an action to a subsequent state, and expected half-steps\r
considering from that state all possible actions with their probabilities of occurring under\r
the policy.\r
Now let us develop the detailed equations for the n-step tree-backup algorithm. The\r
one-step return (target) is the same as that of Expected Sarsa,\r
Gt:t+1\r
.\r
= Rt+1 + \r
X\r
a\r
⇡(a|St+1)Qt(St+1, a), (7.15)\r
for t<T  1, and the two-step tree-backup return is\r
Gt:t+2\r
.\r
= Rt+1 + \r
X\r
a6=At+1\r
⇡(a|St+1)Qt+1(St+1, a)\r
+ ⇡(At+1|St+1)\r
⇣\r
Rt+2 + \r
X\r
a\r
⇡(a|St+2)Qt+1(St+2, a)\r
⌘\r
= Rt+1 + \r
X\r
a6=At+1\r
⇡(a|St+1)Qt+1(St+1, a) + ⇡(At+1|St+1)Gt+1:t+2,\r
for t<T  2. The latter form suggests the general recursive definition of the tree-backup\r
n-step return:\r
Gt:t+n\r
.\r
= Rt+1 + \r
X\r
a6=At+1\r
⇡(a|St+1)Qt+n1(St+1, a) +  ⇡(At+1|St+1)Gt+1:t+n, (7.16)\r
for t<T  1, n  2, with the n = 1 case handled by (7.15) except for GT 1:t+n\r
.\r
= RT .\r
This target is then used with the usual action-value update rule from n-step Sarsa:\r
Qt+n(St, At) .= Qt+n1(St, At) + ↵ [Gt:t+n  Qt+n1(St, At)] ,\r
for 0  t<T, while the values of all other state–action pairs remain unchanged:\r
Qt+n(s, a) = Qt+n1(s, a), for all s, a such that s 6= St or a 6= At. Pseudocode for this\r
algorithm is shown in the box on the next page.\r
Exercise 7.11 Show that if the approximate action values are unchanging, then the\r
tree-backup return (7.16) can be written as a sum of expectation-based TD errors:\r
Gt:t+n = Q(St, At) +\r
min(t+\r
Xn1,T 1)\r
k=t\r
k\r
Y\r
k\r
i=t+1\r
⇡(Ai|Si),\r
where t\r
.\r
= Rt+1 + V¯t(St+1)  Q(St, At) and V¯t is given by (7.8). ⇤

154 Chapter 7: n-step Bootstrapping\r
n-step Tree Backup for estimating Q ⇡ q⇤ or q⇡\r
Initialize Q(s, a) arbitrarily, for all s 2 S, a 2 A\r
Initialize ⇡ to be greedy with respect to Q, or as a fixed given policy\r
Algorithm parameters: step size ↵ 2 (0, 1], a positive integer n\r
All store and access operations can take their index mod n + 1\r
Loop for each episode:\r
Initialize and store S0 6= terminal\r
Choose an action A0 arbitrarily as a function of S0; Store A0\r
T 1\r
Loop for t = 0, 1, 2,... :\r
| If t<T:\r
| Take action At; observe and store the next reward and state as Rt+1, St+1\r
| If St+1 is terminal:\r
| T t + 1\r
| else:\r
| Choose an action At+1 arbitrarily as a function of St+1; Store At+1\r
| ⌧ t + 1  n (⌧ is the time whose estimate is being updated)\r
| If ⌧  0:\r
| If t + 1  T:\r
| G RT\r
| else\r
| G Rt+1 + \r
P\r
a ⇡(a|St+1)Q(St+1, a)\r
| Loop for k = min(t, T  1) down through ⌧ + 1:\r
| G Rk + \r
P\r
a6=Ak ⇡(a|Sk)Q(Sk, a) + ⇡(Ak|Sk)G\r
| Q(S⌧ , A⌧ ) Q(S⌧ , A⌧ ) + ↵ [G  Q(S⌧ , A⌧ )]\r
| If ⇡ is being learned, then ensure that ⇡(·|S⌧ ) is greedy wrt Q\r
Until ⌧ = T  1\r
7.6 *A Unifying Algorithm: n-step Q()\r
So far in this chapter we have considered three di↵erent kinds of action-value algorithms,\r
corresponding to the first three backup diagrams shown in Figure 7.5. n-step Sarsa has\r
all sample transitions, the tree-backup algorithm has all state-to-action transitions fully\r
branched without sampling, and n-step Expected Sarsa has all sample transitions except\r
for the last state-to-action one, which is fully branched with an expected value. To what\r
extent can these algorithms be unified?\r
One idea for unification is suggested by the fourth backup diagram in Figure 7.5. This\r
is the idea that one might decide on a step-by-step basis whether one wanted to take the\r
action as a sample, as in Sarsa, or consider the expectation over all actions instead, as in\r
the tree-backup update. Then, if one chose always to sample, one would obtain Sarsa,\r
whereas if one chose never to sample, one would get the tree-backup algorithm. Expected\r
Sarsa would be the case where one chose to sample for all steps except for the last one.

7.6. A Unifying Algorithm: n-step Q() 155\r
⇢\r
⇢\r
⇢\r
⇢\r
⇢\r
⇢\r
⇢\r
⇢\r
⇢\r
 = 1\r
 = 0\r
 = 1\r
 = 0\r
4-step\r
Sarsa\r
4-step\r
Tree backup\r
4-step\r
Expected Sarsa\r
4-step\r
Q()\r
Figure 7.5: The backup diagrams of the three kinds of n-step action-value updates considered\r
so far in this chapter (4-step case) plus the backup diagram of a fourth kind of update that unifies\r
them all. The label ‘⇢’ indicates half transitions on which importance sampling is required in the\r
o↵-policy case. The fourth kind of update unifies all the others by choosing on a state-by-state\r
basis whether to sample (t = 1) or not (t = 0).\r
And of course there would be many other possibilities, as suggested by the last diagram\r
in the figure. To increase the possibilities even further we can consider a continuous\r
variation between sampling and expectation. Let t 2 [0, 1] denote the degree of sampling\r
on step t, with  = 1 denoting full sampling and  = 0 denoting a pure expectation with\r
no sampling. The random variable t might be set as a function of the state, action, or\r
state–action pair at time t. We call this proposed new algorithm n-step Q().\r
Now let us develop the equations of n-step Q(). First we write the tree-backup\r
n-step return (7.16) in terms of the horizon h = t + n and then in terms of the expected\r
approximate value V¯ (7.8):\r
Gt:h = Rt+1 + \r
X\r
a6=At+1\r
⇡(a|St+1)Qh1(St+1, a) +  ⇡(At+1|St+1)Gt+1:h\r
= Rt+1 + V¯h1(St+1)  ⇡(At+1|St+1)Qh1(St+1, At+1) + ⇡(At+1|St+1)Gt+1:h\r
= Rt+1 + ⇡(At+1|St+1)\r
⇣\r
Gt+1:h  Qh1(St+1, At+1)\r
⌘\r
+ V¯h1(St+1),\r
after which it is exactly like the n-step return for Sarsa with control variates (7.14) except\r
with the action probability ⇡(At+1|St+1) substituted for the importance-sampling ratio\r
⇢t+1. For Q(), we slide linearly between these two cases:\r
Gt:h\r
.\r
= Rt+1 + \r
⇣\r
t+1⇢t+1 + (1  t+1)⇡(At+1|St+1)\r
⌘⇣Gt+1:h  Qh1(St+1, At+1)⌘\r
+ V¯h1(St+1), (7.17)

156 Chapter 7: n-step Bootstrapping\r
for t<h  T. The recursion ends with Gh:h\r
.\r
= Qh1(Sh, Ah) if h<T, or with\r
GT 1:T\r
.\r
= RT if h = T. Then we use the earlier update for n-step Sarsa without\r
importance-sampling ratios (7.5) instead of (7.11), because now the ratios are incorporated\r
in the n-step return. A complete algorithm is given in the box.\r
O↵-policy n-step Q() for estimating Q ⇡ q⇤ or q⇡\r
Input: an arbitrary behavior policy b such that b(a|s) > 0, for all s 2 S, a 2 A\r
Initialize Q(s, a) arbitrarily, for all s 2 S, a 2 A\r
Initialize ⇡ to be greedy with respect to Q, or else it is a fixed given policy\r
Algorithm parameters: step size ↵ 2 (0, 1], a positive integer n\r
All store and access operations can take their index mod n + 1\r
Loop for each episode:\r
Initialize and store S0 6= terminal\r
Choose and store an action A0 ⇠ b(·|S0)\r
T 1\r
Loop for t = 0, 1, 2,... :\r
| If t<T:\r
| Take action At; observe and store the next reward and state as Rt+1, St+1\r
| If St+1 is terminal:\r
| T t + 1\r
| else:\r
| Choose and store an action At+1 ⇠ b(·|St+1)\r
| Select and store t+1\r
| Store ⇡(At+1|St+1)\r
b(At+1|St+1) as ⇢t+1\r
| ⌧ t  n +1 (⌧ is the time whose estimate is being updated)\r
| If ⌧  0:\r
| If t + 1 < T:\r
| G Q(St+1, At+1)\r
| Loop for k = min(t + 1, T) down through ⌧ + 1:\r
| if k = T:\r
| G RT\r
| else:\r
| V¯ P\r
a ⇡(a|Sk)Q(Sk, a)\r
| G Rk + \r
\r
k⇢k + (1  k)⇡(Ak|Sk)\r
G  Q(Sk, Ak)\r
+ V¯\r
| Q(S⌧ , A⌧ ) Q(S⌧ , A⌧ ) + ↵ [G  Q(S⌧ , A⌧ )]\r
| If ⇡ is being learned, then ensure that ⇡(·|S⌧ ) is greedy wrt Q\r
Until ⌧ = T  1

7.7. Summary 157\r
7.7 Summary\r
In this chapter we have developed a range of temporal-di↵erence learning methods that lie\r
in between the one-step TD methods of the previous chapter and the Monte Carlo methods\r
of the chapter before. Methods that involve an intermediate amount of bootstrapping\r
are important because they will typically perform better than either extreme.\r
⇢\r
⇢\r
= 1\r
= 0\r
= 1\r
= 0\r
4-step\r
Q()\r
⇢\r
⇢\r
⇢\r
⇢\r
4-step\r
TD\r
Our focus in this chapter has been on n-step methods, which\r
look ahead to the next n rewards, states, and actions. The two\r
4-step backup diagrams to the right together summarize most of the\r
methods introduced. The state-value update shown is for n-step\r
TD with importance sampling, and the action-value update is for\r
n-step Q(), which generalizes Expected Sarsa and Q-learning. All\r
n-step methods involve a delay of n time steps before updating,\r
as only then are all the required future events known. A further\r
drawback is that they involve more computation per time step\r
than previous methods. Compared to one-step methods, n-step\r
methods also require more memory to record the states, actions,\r
rewards, and sometimes other variables over the last n time steps.\r
Eventually, in Chapter 12, we will see how multi-step TD methods\r
can be implemented with minimal memory and computational\r
complexity using eligibility traces, but there will always be some\r
additional computation beyond one-step methods. Such costs can\r
be well worth paying to escape the tyranny of the single time step.\r
Although n-step methods are more complex than those using\r
eligibility traces, they have the great benefit of being conceptually\r
clear. We have sought to take advantage of this by developing two\r
approaches to o↵-policy learning in the n-step case. One, based on\r
importance sampling is conceptually simple but can be of high variance. If the target and\r
behavior policies are very di↵erent it probably needs some new algorithmic ideas before\r
it can be ecient and practical. The other, based on tree-backup updates, is the natural\r
extension of Q-learning to the multi-step case with stochastic target policies. It involves\r
no importance sampling but, again if the target and behavior policies are substantially\r
di↵erent, the bootstrapping may span only a few steps even if n is large.

158 Chapter 7: n-step Bootstrapping\r
Bibliographical and Historical Remarks\r
The notion of n-step returns is due to Watkins (1989), who also first discussed their error\r
reduction property. n-step algorithms were explored in the first edition of this book,\r
in which they were treated as of conceptual interest, but not feasible in practice. The\r
work of Cichosz (1995) and particularly van Seijen (2016) showed that they are actually\r
completely practical algorithms. Given this, and their conceptual clarity and simplicity,\r
we have chosen to highlight them here in the second edition. In particular, we now\r
postpone all discussion of the backward view and of eligibility traces until Chapter 12.\r
7.1–2 The results in the random walk examples were made for this text based on work\r
of Sutton (1988) and Singh and Sutton (1996). The use of backup diagrams to\r
describe these and other algorithms in this chapter is new.\r
7.3–5 The developments in these sections are based on the work of Precup, Sutton,\r
and Singh (2000), Precup, Sutton, and Dasgupta (2001), and Sutton, Mahmood,\r
Precup, and van Hasselt (2014).\r
The tree-backup algorithm is due to Precup, Sutton, and Singh (2000), but the\r
presentation of it here is new.\r
7.6 The Q() algorithm is new to this text, but closely related algorithms have been\r
explored further by De Asis, Hernandez-Garcia, Holland, and Sutton (2017).

Chapter 8\r
Planning and Learning with\r
Tabular Methods\r
In this chapter we develop a unified view of reinforcement learning methods that require\r
a model of the environment, such as dynamic programming and heuristic search, and\r
methods that can be used without a model, such as Monte Carlo and temporal-di↵erence\r
methods. These are respectively called model-based and model-free reinforcement learning\r
methods. Model-based methods rely on planning as their primary component, while\r
model-free methods primarily rely on learning. Although there are real di↵erences between\r
these two kinds of methods, there are also great similarities. In particular, the heart of\r
both kinds of methods is the computation of value functions. Moreover, all the methods\r
are based on looking ahead to future events, computing a backed-up value, and then\r
using it as an update target for an approximate value function. Earlier in this book we\r
presented Monte Carlo and temporal-di↵erence methods as distinct alternatives, then\r
showed how they can be unified by n-step methods. Our goal in this chapter is a similar\r
integration of model-based and model-free methods. Having established these as distinct\r
in earlier chapters, we now explore the extent to which they can be intermixed.\r
8.1 Models and Planning\r
By a model of the environment we mean anything that an agent can use to predict how the\r
environment will respond to its actions. Given a state and an action, a model produces a\r
prediction of the resultant next state and next reward. If the model is stochastic, then\r
there are several possible next states and next rewards, each with some probability of\r
occurring. Some models produce a description of all possibilities and their probabilities;\r
these we call distribution models. Other models produce just one of the possibilities,\r
sampled according to the probabilities; these we call sample models. For example, consider\r
modeling the sum of a dozen dice. A distribution model would produce all possible sums\r
and their probabilities of occurring, whereas a sample model would produce an individual

160 Chapter 8: Planning and Learning with Tabular Methods\r
sum drawn according to this probability distribution. The kind of model assumed in\r
dynamic programming—estimates of the MDP’s dynamics, p(s0, r|s, a)—is a distribution\r
model. The kind of model used in the blackjack example in Chapter 5 is a sample model.\r
Distribution models are stronger than sample models in that they can always be used\r
to produce samples. However, in many applications it is much easier to obtain sample\r
models than distribution models. The dozen dice are a simple example of this. It would\r
be easy to write a computer program to simulate the dice rolls and return the sum, but\r
harder and more error-prone to figure out all the possible sums and their probabilities.\r
Models can be used to mimic or simulate experience. Given a starting state and action,\r
a sample model produces a possible transition, and a distribution model generates all\r
possible transitions weighted by their probabilities of occurring. Given a starting state\r
and a policy, a sample model could produce an entire episode, and a distribution model\r
could generate all possible episodes and their probabilities. In either case, we say the\r
model is used to simulate the environment and produce simulated experience.\r
The word planning is used in several di↵erent ways in di↵erent fields. We use the\r
term to refer to any computational process that takes a model as input and produces or\r
improves a policy for interacting with the modeled environment:\r
planning model policy\r
In artificial intelligence, there are two distinct approaches to planning according to our\r
definition. State-space planning, which includes the approach we take in this book,\r
is viewed primarily as a search through the state space for an optimal policy or an\r
optimal path to a goal. Actions cause transitions from state to state, and value functions\r
are computed over states. In what we call plan-space planning, planning is instead a\r
search through the space of plans. Operators transform one plan into another, and\r
value functions, if any, are defined over the space of plans. Plan-space planning includes\r
evolutionary methods and “partial-order planning,” a common kind of planning in artificial\r
intelligence in which the ordering of steps is not completely determined at all stages of\r
planning. Plan-space methods are dicult to apply eciently to the stochastic sequential\r
decision problems that are the focus in reinforcement learning, and we do not consider\r
them further (but see, e.g., Russell and Norvig, 2010).\r
The unified view we present in this chapter is that all state-space planning methods\r
share a common structure, a structure that is also present in the learning methods\r
presented in this book. It takes the rest of the chapter to develop this view, but there are\r
two basic ideas: (1) all state-space planning methods involve computing value functions\r
as a key intermediate step toward improving the policy, and (2) they compute value\r
functions by updates or backup operations applied to simulated experience. This common\r
structure can be diagrammed as follows:\r
values\r
backups model simulated\r
experience policy backups updates\r
Dynamic programming methods clearly fit this structure: they make sweeps through the\r
space of states, generating for each state the distribution of possible transitions. Each\r
distribution is then used to compute a backed-up value (update target) and update the

8.2. Dyna: Integrated Planning, Acting, and Learning 161\r
state’s estimated value. In this chapter we argue that various other state-space planning\r
methods also fit this structure, with individual methods di↵ering only in the kinds of\r
updates they do, the order in which they do them, and in how long the backed-up\r
information is retained.\r
Viewing planning methods in this way emphasizes their relationship to the learning\r
methods that we have described in this book. The heart of both learning and planning\r
methods is the estimation of value functions by backing-up update operations. The\r
di↵erence is that whereas planning uses simulated experience generated by a model,\r
learning methods use real experience generated by the environment. Of course this\r
di↵erence leads to a number of other di↵erences, for example, in how performance is\r
assessed and in how flexibly experience can be generated. But the common structure\r
means that many ideas and algorithms can be transferred between planning and learning.\r
In particular, in many cases a learning algorithm can be substituted for the key update\r
step of a planning method. Learning methods require only experience as input, and in\r
many cases they can be applied to simulated experience just as well as to real experience.\r
The box below shows a simple example of a planning method based on one-step tabular\r
Q-learning and on random samples from a sample model. This method, which we call\r
random-sample one-step tabular Q-planning, converges to the optimal policy for the model\r
under the same conditions that one-step tabular Q-learning converges to the optimal\r
policy for the real environment (each state–action pair must be selected an infinite number\r
of times in Step 1, and ↵ must decrease appropriately over time).\r
Random-sample one-step tabular Q-planning\r
Loop forever:\r
1. Select a state, S 2 S, and an action, A 2 A(S), at random\r
2. Send S, A to a sample model, and obtain\r
a sample next reward, R, and a sample next state, S0\r
3. Apply one-step tabular Q-learning to S, A, R, S0:\r
Q(S, A) Q(S, A) + ↵\r
⇥\r
R +  maxa Q(S0, a)  Q(S, A)\r
⇤\r
In addition to the unified view of planning and learning methods, a second theme in\r
this chapter is the benefits of planning in small, incremental steps. This enables planning\r
to be interrupted or redirected at any time with little wasted computation, which appears\r
to be a key requirement for eciently intermixing planning with acting and with learning\r
of the model. Planning in very small steps may be the most ecient approach even on\r
pure planning problems if the problem is too large to be solved exactly.\r
8.2 Dyna: Integrated Planning, Acting, and Learning\r
When planning is done online, while interacting with the environment, a number of\r
interesting issues arise. New information gained from the interaction may change the\r
model and thereby interact with planning. It may be desirable to customize the planning\r
process in some way to the states or decisions currently under consideration, or expected

162 Chapter 8: Planning and Learning with Tabular Methods\r
in the near future. If decision making and model learning are both computation-intensive\r
processes, then the available computational resources may need to be divided between\r
them. To begin exploring these issues, in this section we present Dyna-Q, a simple\r
architecture integrating the major functions needed in an online planning agent. Each\r
function appears in Dyna-Q in a simple, almost trivial, form. In subsequent sections we\r
elaborate some of the alternate ways of achieving each function and the trade-o↵s between\r
them. For now, we seek merely to illustrate the ideas and stimulate your intuition.\r
Within a planning agent, there are at least two roles for real experience: it can be\r
used to improve the model (to make it more accurately match the real environment)\r
and it can be used to directly improve the value function and policy using the kinds of\r
planning\r
value/policy\r
model experience\r
model\r
learning\r
acting\r
direct\r
RL\r
reinforcement learning methods we have discussed\r
in previous chapters. The former we call model\u0002learning, and the latter we call direct reinforcement\r
learning (direct RL). The possible relationships\r
between experience, model, values, and policy are\r
summarized in the diagram to the right. Each ar\u0002row shows a relationship of influence and presumed\r
improvement. Note how experience can improve\r
value functions and policies either directly or in\u0002directly via the model. It is the latter, which is\r
sometimes called indirect reinforcement learning,\r
that is involved in planning.\r
Both direct and indirect methods have advantages and disadvantages. Indirect methods\r
often make fuller use of a limited amount of experience and thus achieve a better policy\r
with fewer environmental interactions. On the other hand, direct methods are much\r
simpler and are not a↵ected by biases in the design of the model. Some have argued\r
that indirect methods are always superior to direct ones, while others have argued that\r
direct methods are responsible for most human and animal learning. Related debates\r
in psychology and artificial intelligence concern the relative importance of cognition as\r
opposed to trial-and-error learning, and of deliberative planning as opposed to reactive\r
decision making (see Chapter 14 for discussion of some of these issues from the perspective\r
of psychology). Our view is that the contrast between the alternatives in all these debates\r
has been exaggerated, that more insight can be gained by recognizing the similarities\r
between these two sides than by opposing them. For example, in this book we have\r
emphasized the deep similarities between dynamic programming and temporal-di↵erence\r
methods, even though one was designed for planning and the other for model-free learning.\r
Dyna-Q includes all of the processes shown in the diagram above—planning, acting,\r
model-learning, and direct RL—all occurring continually. The planning method is the\r
random-sample one-step tabular Q-planning method on page 161. The direct RL method\r
is one-step tabular Q-learning. The model-learning method is also table-based and assumes\r
the environment is deterministic. After each transition St, At ! Rt+1, St+1, the model\r
records in its table entry for St, At the prediction that Rt+1, St+1 will deterministically\r
follow. Thus, if the model is queried with a state–action pair that has been experienced\r
before, it simply returns the last-observed next state and next reward as its prediction.

8.2. Dyna: Integrated Planning, Acting, and Learning 163\r
During planning, the Q-planning algorithm randomly samples only from state–action\r
pairs that have previously been experienced (in Step 1), so the model is never queried\r
with a pair about which it has no information.\r
The overall architecture of Dyna agents, of which the Dyna-Q algorithm is one example,\r
is shown in Figure 8.1. The central column represents the basic interaction between\r
agent and environment, giving rise to a trajectory of real experience. The arrow on the\r
left of the figure represents direct reinforcement learning operating on real experience to\r
improve the value function and the policy. On the right are model-based processes. The\r
model is learned from real experience and gives rise to simulated experience. We use the\r
term search control to refer to the process that selects the starting states and actions\r
for the simulated experiences generated by the model. Finally, planning is achieved by\r
applying reinforcement learning methods to the simulated experiences just as if they had\r
really happened. Typically, as in Dyna-Q, the same reinforcement learning method is\r
used both for learning from real experience and for planning from simulated experience.\r
The reinforcement learning method is thus the “final common path” for both learning\r
and planning. Learning and planning are deeply integrated in the sense that they share\r
almost all the same machinery, di↵ering only in the source of their experience.\r
real\r
direct RL\r
update\r
Model\r
planning update\r
search\r
control\r
Policy/value functions\r
experience\r
model\r
learning\r
Environment\r
simulated\r
experience\r
Figure 8.1: The general Dyna Architecture. Real experience, passing back and forth between\r
the environment and the policy, a↵ects policy and value functions in much the same way as does\r
simulated experience generated by the model of the environment.\r
Conceptually, planning, acting, model-learning, and direct RL occur simultaneously\r
and in parallel in Dyna agents. For concreteness and implementation on a serial computer,\r
however, we fully specify the order in which they occur within a time step. In Dyna-Q,\r
the acting, model-learning, and direct RL processes require little computation, and we\r
assume they consume just a fraction of the time. The remaining time in each step can be\r
devoted to the planning process, which is inherently computation-intensive. Let us assume\r
that there is time in each step, after acting, model-learning, and direct RL, to complete

164 Chapter 8: Planning and Learning with Tabular Methods\r
n iterations (Steps 1–3) of the Q-planning algorithm. In the pseudocode algorithm for\r
Dyna-Q in the box below, Model(s, a) denotes the contents of the model (predicted\r
next state and reward) for state–action pair (s, a). Direct reinforcement learning, model\u0002learning, and planning are implemented by steps (d), (e), and (f), respectively. If (e) and\r
(f) were omitted, the remaining algorithm would be one-step tabular Q-learning.\r
Tabular Dyna-Q\r
Initialize Q(s, a) and Model(s, a) for all s 2 S and a 2 A(s)\r
Loop forever:\r
(a) S current (nonterminal) state\r
(b) A "-greedy(S, Q)\r
(c) Take action A; observe resultant reward, R, and state, S0\r
(d) Q(S, A) Q(S, A) + ↵\r
⇥\r
R +  maxa Q(S0, a)  Q(S, A)\r
⇤\r
(e) Model(S, A) R, S0 (assuming deterministic environment)\r
(f) Loop repeat n times:\r
S random previously observed state\r
A random action previously taken in S\r
R, S0 Model(S, A)\r
Q(S, A) Q(S, A) + ↵\r
⇥\r
R +  maxa Q(S0, a)  Q(S, A)\r
⇤\r
Example 8.1: Dyna Maze Consider the simple maze shown inset in Figure 8.2. In\r
each of the 47 states there are four actions, up, down, right, and left, which take the\r
agent deterministically to the corresponding neighboring states, except when movement\r
is blocked by an obstacle or the edge of the maze, in which case the agent remains where\r
it is. Reward is zero on all transitions, except those into the goal state, on which it is +1.\r
After reaching the goal state (G), the agent returns to the start state (S) to begin a new\r
episode. This is a discounted, episodic task with  = 0.95.\r
The main part of Figure 8.2 shows average learning curves from an experiment in\r
which Dyna-Q agents were applied to the maze task. The initial action values were zero,\r
the step-size parameter was ↵ = 0.1, and the exploration parameter was " = 0.1. When\r
selecting greedily among actions, ties were broken randomly. The agents varied in the\r
number of planning steps, n, they performed per real step. For each n, the curves show\r
the number of steps taken by the agent to reach the goal in each episode, averaged over 30\r
repetitions of the experiment. In each repetition, the initial seed for the random number\r
generator was held constant across algorithms. Because of this, the first episode was\r
exactly the same (about 1700 steps) for all values of n, and its data are not shown in\r
the figure. After the first episode, performance improved for all values of n, but much\r
more rapidly for larger values. Recall that the n = 0 agent is a nonplanning agent, using\r
only direct reinforcement learning (one-step tabular Q-learning). This was by far the\r
slowest agent on this problem, despite the fact that the parameter values (↵ and ") were\r
optimized for it. The nonplanning agent took about 25 episodes to reach ("-)optimal\r
performance, whereas the n = 5 agent took about five episodes, and the n = 50 agent\r
took only three episodes.

8.2. Dyna: Integrated Planning, Acting, and Learning 165\r
2\r
800\r
600\r
400\r
200\r
14\r
10 20 30 40 50\r
0 planning steps\r
(direct RL only)\r
Episodes\r
Steps\r
per\r
episode 5 planning steps\r
50 planning steps\r
S\r
G\r
actions\r
Figure 8.2: A simple maze (inset) and the average learning curves for Dyna-Q agents varying\r
in their number of planning steps (n) per real step. The task is to travel from S to G as quickly\r
as possible.\r
Figure 8.3 shows why the planning agents found the solution so much faster than\r
the nonplanning agent. Shown are the policies found by the n = 0 and n = 50 agents\r
halfway through the second episode. Without planning (n = 0), each episode adds only\r
one additional step to the policy, and so only one step (the last) has been learned so far.\r
With planning, again only one step is learned during the first episode, but here during\r
the second episode an extensive policy has been developed that by the end of the episode\r
will reach almost back to the start state. This policy is built by the planning process\r
while the agent is still wandering near the start state. By the end of the third episode a\r
complete optimal policy will have been found and perfect performance attained.\r
S\r
G\r
S\r
G\r
WITHOUT PLANNING (n=0) WITH PLANNING (n=50)\r
Figure 8.3: Policies found by planning and nonplanning Dyna-Q agents halfway through the\r
second episode. The arrows indicate the greedy action in each state; if no arrow is shown for a\r
state, then all of its action values were equal. The black square indicates the location of the\r
agent.

166 Chapter 8: Planning and Learning with Tabular Methods\r
In Dyna-Q, learning and planning are accomplished by exactly the same algorithm,\r
operating on real experience for learning and on simulated experience for planning.\r
Because planning proceeds incrementally, it is trivial to intermix planning and acting.\r
Both proceed as fast as they can. The agent is always reactive and always deliberative,\r
responding instantly to the latest sensory information and yet always planning in the\r
background. Also ongoing in the background is the model-learning process. As new\r
information is gained, the model is updated to better match reality. As the model changes,\r
the ongoing planning process will gradually compute a di↵erent way of behaving to match\r
the new model.\r
Exercise 8.1 The nonplanning method looks particularly poor in Figure 8.3 because it is\r
a one-step method; a method using multi-step bootstrapping would do better. Do you\r
think one of the multi-step bootstrapping methods from Chapter 7 could do as well as\r
the Dyna method? Explain why or why not. ⇤\r
8.3 When the Model Is Wrong\r
In the maze example presented in the previous section, the changes in the model were\r
relatively modest. The model started out empty, and was then filled only with exactly\r
correct information. In general, we cannot expect to be so fortunate. Models may be\r
incorrect because the environment is stochastic and only a limited number of samples\r
have been observed, or because the model was learned using function approximation that\r
has generalized imperfectly, or simply because the environment has changed and its new\r
behavior has not yet been observed. When the model is incorrect, the planning process is\r
likely to compute a suboptimal policy.\r
In some cases, the suboptimal policy computed by planning quickly leads to the\r
discovery and correction of the modeling error. This tends to happen when the model\r
is optimistic in the sense of predicting greater reward or better state transitions than\r
are actually possible. The planned policy attempts to exploit these opportunities and in\r
doing so discovers that they do not exist.\r
Example 8.2: Blocking Maze A maze example illustrating this relatively minor\r
kind of modeling error and recovery from it is shown in Figure 8.4. Initially, there is a\r
short path from start to goal, to the right of the barrier, as shown in the upper left of the\r
figure. After 1000 time steps, the short path is “blocked,” and a longer path is opened up\r
along the left-hand side of the barrier, as shown in upper right of the figure. The graph\r
shows average cumulative reward for a Dyna-Q agent and an enhanced Dyna-Q+ agent\r
to be described shortly. The first part of the graph shows that both Dyna agents found\r
the short path within 1000 steps. When the environment changed, the graphs become\r
flat, indicating a period during which the agents obtained no reward because they were\r
wandering around behind the barrier. After a while, however, they were able to find the\r
new opening and the new optimal behavior.\r
Greater diculties arise when the environment changes to become better than it was\r
before, and yet the formerly correct policy does not reveal the improvement. In these\r
cases the modeling error may not be detected for a long time, if ever.

8.3. When the Model Is Wrong 167\r
Cumulative\r
reward\r
0 1000 2000 3000\r
Time steps\r
150\r
0\r
Dyna-Q+\r
S\r
G G\r
S\r
Dyna-Q\r
Figure 8.4: Average performance of Dyna agents on a blocking task. The left environment\r
was used for the first 1000 steps, the right environment for the rest. Dyna-Q+ is Dyna-Q with\r
an exploration bonus that encourages exploration.\r
Cumulative\r
reward\r
S\r
G G\r
S\r
0 3000 6000\r
Time steps\r
400\r
0\r
Dyna-Q+\r
Dyna-Q\r
Figure 8.5: Average performance of Dyna agents on\r
a shortcut task. The left environment was used for the\r
first 3000 steps, the right environment for the rest.\r
Example 8.3: Shortcut Maze\r
The problem caused by this kind of\r
environmental change is illustrated\r
by the maze example shown in Fig\u0002ure 8.5. Initially, the optimal path is\r
to go around the left side of the bar\u0002rier (upper left). After 3000 steps,\r
however, a shorter path is opened up\r
along the right side, without disturb\u0002ing the longer path (upper right).\r
The graph shows that the regular\r
Dyna-Q agent never switched to the\r
shortcut. In fact, it never realized\r
that it existed. Its model said that\r
there was no shortcut, so the more it\r
planned, the less likely it was to step\r
to the right and discover it. Even\r
with an "-greedy policy, it is very\r
unlikely that an agent will take so\r
many exploratory actions as to dis\u0002cover the shortcut.\r
The general problem here is another version of the conflict between exploration and\r
exploitation. In a planning context, exploration means trying actions that improve the\r
model, whereas exploitation means behaving in the optimal way given the current model.

168 Chapter 8: Planning and Learning with Tabular Methods\r
We want the agent to explore to find changes in the environment, but not so much that\r
performance is greatly degraded. As in the earlier exploration/exploitation conflict, there\r
probably is no solution that is both perfect and practical, but simple heuristics are often\r
e↵ective.\r
The Dyna-Q+ agent that did solve the shortcut maze uses one such heuristic. This\r
agent keeps track for each state–action pair of how many time steps have elapsed since\r
the pair was last tried in a real interaction with the environment. The more time that\r
has elapsed, the greater (we might presume) the chance that the dynamics of this pair\r
has changed and that the model of it is incorrect. To encourage behavior that tests\r
long-untried actions, a special “bonus reward” is given on simulated experiences involving\r
these actions. In particular, if the modeled reward for a transition is r, and the transition\r
has not been tried in ⌧ time steps, then planning updates are done as if that transition\r
produced a reward of r + \r
p⌧ , for some small . This encourages the agent to keep\r
testing all accessible state transitions and even to find long sequences of actions in order\r
to carry out such tests.1 Of course all this testing has its cost, but in many cases, as in the\r
shortcut maze, this kind of computational curiosity is well worth the extra exploration.\r
Exercise 8.2 Why did the Dyna agent with exploration bonus, Dyna-Q+, perform\r
better in the first phase as well as in the second phase of the blocking and shortcut\r
experiments? ⇤\r
Exercise 8.3 Careful inspection of Figure 8.5 reveals that the di↵erence between Dyna-Q+\r
and Dyna-Q narrowed slightly over the first part of the experiment. What is the reason\r
for this? ⇤\r
Exercise 8.4 (programming) The exploration bonus described above actually changes\r
the estimated values of states and actions. Is this necessary? Suppose the bonus \r
p⌧\r
was used not in updates, but solely in action selection. That is, suppose the action\r
selected was always that for which Q(St, a) + \r
p⌧ (St, a) was maximal. Carry out a\r
gridworld experiment that tests and illustrates the strengths and weaknesses of this\r
alternate approach. ⇤\r
Exercise 8.5 How might the tabular Dyna-Q algorithm shown on page 164 be modified\r
to handle stochastic environments? How might this modification perform poorly on\r
changing environments such as considered in this section? How could the algorithm be\r
modified to handle stochastic environments and changing environments? ⇤\r
8.4 Prioritized Sweeping\r
In the Dyna agents presented in the preceding sections, simulated transitions are started in\r
state–action pairs selected uniformly at random from all previously experienced pairs. But\r
a uniform selection is usually not the best; planning can be much more ecient if simulated\r
transitions and updates are focused on particular state–action pairs. For example, consider\r
1The Dyna-Q+ agent was changed in two other ways as well. First, actions that had never been\r
tried before from a state were allowed to be considered in the planning step (f) of the Tabular Dyna-Q\r
algorithm in the box above. Second, the initial model for such actions was that they would lead back to\r
the same state with a reward of zero.

8.4. Prioritized Sweeping 169\r
what happens during the second episode of the first maze task (Figure 8.3). At the\r
beginning of the second episode, only the state–action pair leading directly into the goal\r
has a positive value; the values of all other pairs are still zero. This means that it is\r
pointless to perform updates along almost all transitions, because they take the agent\r
from one zero-valued state to another, and thus the updates would have no e↵ect. Only\r
an update along a transition into the state just prior to the goal, or from it, will change\r
any values. If simulated transitions are generated uniformly, then many wasteful updates\r
will be made before stumbling onto one of these useful ones. As planning progresses, the\r
region of useful updates grows, but planning is still far less ecient than it would be if\r
focused where it would do the most good. In the much larger problems that are our real\r
objective, the number of states is so large that an unfocused search would be extremely\r
inecient.\r
This example suggests that search might be usefully focused by working backward from\r
goal states. Of course, we do not really want to use any methods specific to the idea of\r
“goal state.” We want methods that work for general reward functions. Goal states are\r
just a special case, convenient for stimulating intuition. In general, we want to work back\r
not just from goal states but from any state whose value has changed. Suppose that the\r
values are initially correct given the model, as they were in the maze example prior to\r
discovering the goal. Suppose now that the agent discovers a change in the environment\r
and changes its estimated value of one state, either up or down. Typically, this will imply\r
that the values of many other states should also be changed, but the only useful one-step\r
updates are those of actions that lead directly into the one state whose value has been\r
changed. If the values of these actions are updated, then the values of the predecessor\r
states may change in turn. If so, then actions leading into them need to be updated, and\r
then their predecessor states may have changed. In this way one can work backward\r
from arbitrary states that have changed in value, either performing useful updates or\r
terminating the propagation. This general idea might be termed backward focusing of\r
planning computations.\r
As the frontier of useful updates propagates backward, it often grows rapidly, producing\r
many state–action pairs that could usefully be updated. But not all of these will be\r
equally useful. The values of some states may have changed a lot, whereas others may\r
have changed little. The predecessor pairs of those that have changed a lot are more\r
likely to also change a lot. In a stochastic environment, variations in estimated transition\r
probabilities also contribute to variations in the sizes of changes and in the urgency with\r
which pairs need to be updated. It is natural to prioritize the updates according to a\r
measure of their urgency, and perform them in order of priority. This is the idea behind\r
prioritized sweeping. A queue is maintained of every state–action pair whose estimated\r
value would change nontrivially if updated , prioritized by the size of the change. When\r
the top pair in the queue is updated, the e↵ect on each of its predecessor pairs is computed.\r
If the e↵ect is greater than some small threshold, then the pair is inserted in the queue\r
with the new priority (if there is a previous entry of the pair in the queue, then insertion\r
results in only the higher priority entry remaining in the queue). In this way the e↵ects\r
of changes are eciently propagated backward until quiescence. The full algorithm for\r
the case of deterministic environments is given in the box on the next page.

170 Chapter 8: Planning and Learning with Tabular Methods\r
Prioritized sweeping for a deterministic environment\r
Initialize Q(s, a), Model(s, a), for all s, a, and P Queue to empty\r
Loop forever:\r
(a) S current (nonterminal) state\r
(b) A policy(S, Q)\r
(c) Take action A; observe resultant reward, R, and state, S0\r
(d) Model(S, A) R, S0\r
(e) P |R +  maxa Q(S0, a)  Q(S, A)|.\r
(f) if P > ✓, then insert S, A into P Queue with priority P\r
(g) Loop repeat n times, while P Queue is not empty:\r
S, A f irst(P Queue)\r
R, S0 Model(S, A)\r
Q(S, A) Q(S, A) + ↵\r
⇥\r
R +  maxa Q(S0, a)  Q(S, A)\r
⇤\r
Loop for all S, ¯ A¯ predicted to lead to S:\r
R¯ predicted reward for S, ¯ A, S ¯\r
P |R¯ +  maxa Q(S, a)  Q(S, ¯ A¯)|.\r
if P > ✓ then insert S, ¯ A¯ into P Queue with priority P\r
Backups\r
until\r
optimal\r
solution\r
10\r
103\r
104\r
105\r
106\r
107\r
102\r
0 47 94 186 376 752 1504 3008 6016\r
Gridworld size (#states)\r
Dyna-Q\r
Prioritized\r
sweeping\r
Updates Updates\r
until\r
optimal\r
solution\r
Example 8.4: Prioritized Sweeping\r
on Mazes Prioritized sweeping has been\r
found to dramatically increase the speed\r
at which optimal solutions are found in\r
maze tasks, often by a factor of 5 to 10.\r
A typical example is shown to the right.\r
These data are for a sequence of maze\r
tasks of exactly the same structure as the\r
one shown in Figure 8.2, except that they\r
vary in the grid resolution. Prioritized\r
sweeping maintained a decisive advantage\r
over unprioritized Dyna-Q. Both systems\r
made at most n = 5 updates per environ\u0002mental interaction. Adapted from Peng\r
and Williams (1993).\r
Extensions of prioritized sweeping to stochastic environments are straightforward. The\r
model is maintained by keeping counts of the number of times each state–action pair has\r
been experienced and of what the next states were. It is natural then to update each pair\r
not with a sample update, as we have been using so far, but with an expected update,\r
taking into account all possible next states and their probabilities of occurring.\r
Prioritized sweeping is just one way of distributing computations to improve planning\r
eciency, and probably not the best way. One of prioritized sweeping’s limitations is that\r
it uses expected updates, which in stochastic environments may waste lots of computation\r
on low-probability transitions. As we show in the following section, sample updates

8.4. Prioritized Sweeping 171\r
Example 8.5 Prioritized Sweeping for Rod Maneuvering\r
Start\r
Goal\r
The objective in this task is to\r
maneuver a rod around some awk\u0002wardly placed obstacles within a\r
limited rectangular work space to a\r
goal position in the fewest number\r
of steps. The rod can be translated\r
along its long axis or perpendicu\u0002lar to that axis, or it can be ro\u0002tated in either direction around its\r
center. The distance of each move\u0002ment is approximately 1/20 of the\r
work space, and the rotation incre\u0002ment is 10 degrees. Translations\r
are deterministic and quantized to\r
one of 20 ⇥ 20 positions. To the\r
right is shown the obstacles and the\r
shortest solution from start to goal,\r
found by prioritized sweeping. This problem is deterministic, but has four actions\r
and 14,400 potential states (some of these are unreachable because of the obstacles).\r
This problem is probably too large to be solved with unprioritized methods. Figure\r
reprinted from Moore and Atkeson (1993).\r
can in many cases get closer to the true value function with less computation despite\r
the variance introduced by sampling. Sample updates can win because they break the\r
overall backing-up computation into smaller pieces—those corresponding to individual\r
transitions—which then enables it to be focused more narrowly on the pieces that will\r
have the largest impact. This idea was taken to what may be its logical limit in the “small\r
backups” introduced by van Seijen and Sutton (2013). These are updates along a single\r
transition, like a sample update, but based on the probability of the transition without\r
sampling, as in an expected update. By selecting the order in which small updates\r
are done it is possible to greatly improve planning eciency beyond that possible with\r
prioritized sweeping.\r
We have suggested in this chapter that all kinds of state-space planning can be viewed\r
as sequences of value updates, varying only in the type of update, expected or sample,\r
large or small, and in the order in which the updates are done. In this section we have\r
emphasized backward focusing, but this is just one strategy. For example, another would\r
be to focus on states according to how easily they can be reached from the states that\r
are visited frequently under the current policy, which might be called forward focusing.\r
Peng and Williams (1993) and Barto, Bradtke and Singh (1995) have explored versions\r
of forward focusing, and the methods introduced in the next few sections take it to an\r
extreme form.

172 Chapter 8: Planning and Learning with Tabular Methods\r
8.5 Expected vs. Sample Updates\r
The examples in the previous sections give some idea of the range of possibilities for\r
combining methods of learning and planning. In the rest of this chapter, we analyze some\r
of the component ideas involved, starting with the relative advantages of expected and\r
sample updates.\r
Much of this book has been about di↵erent kinds of value-function updates, and we\r
have considered a great many varieties. Focusing for the moment on one-step updates,\r
they vary primarily along three binary dimensions. The first two dimensions are whether\r
they update state values or action values and whether they estimate the value for the\r
optimal policy or for an arbitrary given policy. These two dimensions give rise to four\r
classes of updates for approximating the four value functions, q⇤, v⇤, q⇡, and v⇡. The\r
Value\r
estimated\r
Expected updates\r
(DP)\r
Sample updates \r
(one-step TD)\r
⇡\r
s\r
s0\r
⇡\r
r p\r
a\r
q⇡(s, a)\r
q⇤(s, a)\r
v⇡(s)\r
v⇤(s)\r
s\r
s0\r
r\r
max\r
a\r
p\r
policy evaluation\r
value iteration\r
r\r
s0\r
s, a\r
a0\r
⇡\r
p\r
q-policy evaluation\r
r\r
s0\r
s, a\r
a0\r
max\r
p\r
q-value iteration\r
s\r
A\r
S0\r
R\r
R\r
S0\r
s, a\r
A0\r
R\r
S0\r
s, a\r
max\r
TD(0)\r
Sarsa\r
Q-learning\r
a0\r
Figure 8.6: Backup diagrams for all the one-step\r
updates considered in this book.\r
other binary dimension is whether the\r
updates are expected updates, consider\u0002ing all possible events that might hap\u0002pen, or sample updates, considering a\r
single sample of what might happen.\r
These three binary dimensions give rise\r
to eight cases, seven of which corre\u0002spond to specific algorithms, as shown\r
in the figure to the right. (The eighth\r
case does not seem to correspond to\r
any useful update.) Any of these one\u0002step updates can be used in planning\r
methods. The Dyna-Q agents discussed\r
earlier use q⇤ sample updates, but they\r
could just as well use q⇤ expected up\u0002dates, or either expected or sample q⇡\r
updates. The Dyna-AC system uses v⇡\r
sample updates together with a learning\r
policy structure (as in Chapter 13). For\r
stochastic problems, prioritized sweep\u0002ing is always done using one of the ex\u0002pected updates.\r
When we introduced one-step sam\u0002ple updates in Chapter 6, we presented\r
them as substitutes for expected up\u0002dates. In the absence of a distribution\r
model, expected updates are not pos\u0002sible, but sample updates can be done\r
using sample transitions from the envi\u0002ronment or a sample model. Implicit in\r
that point of view is that expected up\u0002dates, if possible, are preferable to sam\u0002ple updates. But are they? Expected

8.5. Expected vs. Sample Updates 173\r
updates certainly yield a better estimate because they are uncorrupted by sampling error,\r
but they also require more computation, and computation is often the limiting resource\r
in planning. To properly assess the relative merits of expected and sample updates for\r
planning we must control for their di↵erent computational requirements.\r
For concreteness, consider the expected and sample updates for approximating q⇤,\r
and the special case of discrete states and actions, a table-lookup representation of\r
the approximate value function, Q, and a model in the form of estimated dynamics,\r
pˆ(s0, r|s, a). The expected update for a state–action pair, s, a, is:\r
Q(s, a) X\r
s0,r\r
pˆ(s0, r|s, a)\r
h\r
r +  max\r
a0 Q(s0\r
, a0)\r
i\r
. (8.1)\r
The corresponding sample update for s, a, given a sample next state and reward, S0 and\r
R (from the model), is the Q-learning-like update:\r
Q(s, a) Q(s, a) + ↵\r
h\r
R +  max\r
a0 Q(S0\r
, a0)  Q(s, a)\r
i\r
, (8.2)\r
where ↵ is the usual positive step-size parameter.\r
The di↵erence between these expected and sample updates is significant to the extent\r
that the environment is stochastic, specifically, to the extent that, given a state and\r
action, many possible next states may occur with various probabilities. If only one next\r
state is possible, then the expected and sample updates given above are identical (taking\r
↵ = 1). If there are many possible next states, then there may be significant di↵erences.\r
In favor of the expected update is that it is an exact computation, resulting in a new\r
Q(s, a) whose correctness is limited only by the correctness of the Q(s0, a0) at successor\r
states. The sample update is in addition a↵ected by sampling error. On the other hand,\r
the sample update is cheaper computationally because it considers only one next state,\r
not all possible next states. In practice, the computation required by update operations\r
is usually dominated by the number of state–action pairs at which Q is evaluated. For a\r
particular starting pair, s, a, let b be the branching factor (i.e., the number of possible\r
next states, s0, for which pˆ(s0 |s, a) > 0). Then an expected update of this pair requires\r
roughly b times as much computation as a sample update.\r
If there is enough time to complete an expected update, then the resulting estimate is\r
generally better than that of b sample updates because of the absence of sampling error.\r
But if there is insucient time to complete an expected update, then sample updates are\r
always preferable because they at least make some improvement in the value estimate\r
with fewer than b updates. In a large problem with many state–action pairs, we are often\r
in the latter situation. With so many state–action pairs, expected updates of all of them\r
would take a very long time. Before that we may be much better o↵ with a few sample\r
updates at many state–action pairs than with expected updates at a few pairs. Given a\r
unit of computational e↵ort, is it better devoted to a few expected updates or to b times\r
as many sample updates?\r
Figure 8.7 shows the results of an analysis that suggests an answer to this question. It\r
shows the estimation error as a function of computation time for expected and sample\r
updates for a variety of branching factors, b. The case considered is that in which all

174 Chapter 8: Planning and Learning with Tabular Methods\r
b=2 (branching factor)\r
b=10\r
b=100\r
b=1000 b=10,000\r
sample\r
updates\r
expected\r
updates\r
1\r
0\r
0 1b 2b\r
RMS error\r
in value\r
estimate\r
Number of computations max\r
a0 Q(s0\r
, a0)\r
Figure 8.7: Comparison of eciency of expected and sample updates.\r
b successor states are equally likely and in which the error in the initial estimate is\r
1. The values at the next states are assumed correct, so the expected update reduces\r
the error to zero upon its completion. In this case, sample updates reduce the error\r
according to qb1\r
bt where t is the number of sample updates that have been performed\r
(assuming sample averages, i.e., ↵ = 1/t). The key observation is that for moderately\r
large b the error falls dramatically with a tiny fraction of b updates. For these cases,\r
many state–action pairs could have their values improved dramatically, to within a few\r
percent of the e↵ect of an expected update, in the same time that a single state–action\r
pair could undergo an expected update.\r
The advantage of sample updates shown in Figure 8.7 is probably an underestimate of\r
the real e↵ect. In a real problem, the values of the successor states would be estimates\r
that are themselves updated. By causing estimates to be more accurate sooner, sample\r
updates will have a second advantage in that the values backed up from the successor\r
states will be more accurate. These results suggest that sample updates are likely to be\r
superior to expected updates on problems with large stochastic branching factors and\r
too many states to be solved exactly.\r
Exercise 8.6 The analysis above assumed that all of the b possible next states were\r
equally likely to occur. Suppose instead that the distribution was highly skewed, that\r
some of the b states were much more likely to occur than most. Would this strengthen or\r
weaken the case for sample updates over expected updates? Support your answer. ⇤\r
8.6 Trajectory Sampling\r
In this section we compare two ways of distributing updates. The classical approach, from\r
dynamic programming, is to perform sweeps through the entire state (or state–action)\r
space, updating each state (or state–action pair) once per sweep. This is problematic

8.6. Trajectory Sampling 175\r
on large tasks because there may not be time to complete even one sweep. In many\r
tasks the vast majority of the states are irrelevant because they are visited only under\r
very poor policies or with very low probability. Exhaustive sweeps implicitly devote\r
equal time to all parts of the state space rather than focusing where it is needed. As we\r
discussed in Chapter 4, exhaustive sweeps and the equal treatment of all states that they\r
imply are not necessary properties of dynamic programming. In principle, updates can\r
be distributed any way one likes (to assure convergence, all states or state–action pairs\r
must be visited in the limit an infinite number of times; although an exception to this is\r
discussed in Section 8.7 below), but in practice exhaustive sweeps are often used.\r
The second approach is to sample from the state or state–action space according\r
to some distribution. One could sample uniformly, as in the Dyna-Q agent, but this\r
would su↵er from some of the same problems as exhaustive sweeps. More appealing\r
is to distribute updates according to the on-policy distribution, that is, according to\r
the distribution observed when following the current policy. One advantage of this\r
distribution is that it is easily generated; one simply interacts with the model, following\r
the current policy. In an episodic task, one starts in a start state (or according to the\r
starting-state distribution) and simulates until the terminal state. In a continuing task,\r
one starts anywhere and just keeps simulating. In either case, sample state transitions\r
and rewards are given by the model, and sample actions are given by the current policy.\r
In other words, one simulates explicit individual trajectories and performs updates at the\r
state or state–action pairs encountered along the way. We call this way of generating\r
experience and updates trajectory sampling.\r
It is hard to imagine any ecient way of distributing updates according to the on-policy\r
distribution other than by trajectory sampling. If one had an explicit representation\r
of the on-policy distribution, then one could sweep through all states, weighting the\r
update of each according to the on-policy distribution, but this leaves us again with all\r
the computational costs of exhaustive sweeps. Possibly one could sample and update\r
individual state–action pairs from the distribution, but even if this could be done eciently,\r
what benefit would this provide over simulating trajectories? Even knowing the on-policy\r
distribution in an explicit form is unlikely. The distribution changes whenever the policy\r
changes, and computing the distribution requires computation comparable to a complete\r
policy evaluation. Consideration of such other possibilities makes trajectory sampling\r
seem both ecient and elegant.\r
Is the on-policy distribution of updates a good one? Intuitively it seems like a good\r
choice, at least better than the uniform distribution. For example, if you are learning to\r
play chess, you study positions that might arise in real games, not random positions of\r
chess pieces. The latter may be valid states, but to be able to accurately value them is a\r
di↵erent skill from evaluating positions in real games. We will also see in Part II that the\r
on-policy distribution has significant advantages when function approximation is used.\r
Whether or not function approximation is used, one might expect on-policy focusing to\r
significantly improve the speed of planning.\r
Focusing on the on-policy distribution could be beneficial because it causes vast,\r
uninteresting parts of the space to be ignored, or it could be detrimental because it causes\r
the same old parts of the space to be updated over and over. We conducted a small

176 Chapter 8: Planning and Learning with Tabular Methods\r
experiment to assess the e↵ect empirically. To isolate the e↵ect of the update distribution,\r
we used entirely one-step expected tabular updates, as defined by (8.1). In the uniform\r
case, we cycled through all state–action pairs, updating each in place, and in the on-policy\r
case we simulated episodes, all starting in the same state, updating each state–action pair\r
that occurred under the current "-greedy policy ("= 0.1). The tasks were undiscounted\r
episodic tasks, generated randomly as follows. From each of the |S| states, two actions\r
were possible, each of which resulted in one of b next states, all equally likely, with a\r
di↵erent random selection of b states for each state–action pair. The branching factor, b,\r
was the same for all state–action pairs. In addition, on all transitions there was a 0.1\r
probability of transition to the terminal state, ending the episode. The expected reward\r
on each transition was selected from a Gaussian distribution with mean 0 and variance 1.\r
b=10\r
b=3\r
b=1\r
b=1\r
ion-pol cy\r
ion-pol cy\r
uniform\r
uniform\r
0\r
1\r
2\r
3\r
Value of\r
start state\r
under\r
greedy\r
policy\r
0 5,000 10,000 15,000 20,000\r
Computation time, in full backups\r
0\r
1\r
2\r
3\r
Value of\r
start state\r
under\r
greedy\r
policy\r
0 50,000 100,000 150,000 200,000\r
Computation time, in full backups\r
uniform\r
uniform\r
on-policy\r
on-policy\r
expected updates\r
expected updates\r
1,000 STATES\r
10,000 STATES\r
Figure 8.8: Relative eciency of updates dis\u0002tributed uniformly across the state space versus\r
focused on simulated on-policy trajectories, each\r
starting in the same state. Results are for randomly\r
generated tasks of two sizes and various branching\r
factors, b.\r
At any point in the planning process\r
one can stop and exhaustively compute\r
v⇡˜(s0), the true value of the start state\r
under the greedy policy, ⇡˜, given the cur\u0002rent action-value function Q, as an indi\u0002cation of how well the agent would do on\r
a new episode on which it acted greed\u0002ily (all the while assuming the model is\r
correct).\r
The upper part of the figure to\r
the right shows results averaged over\r
200 sample tasks with 1000 states and\r
branching factors of 1, 3, and 10. The\r
quality of the policies found is plotted as\r
a function of the number of expected up\u0002dates completed. In all cases, sampling\r
according to the on-policy distribution\r
resulted in faster planning initially and\r
retarded planning in the long run. The\r
e↵ect was stronger, and the initial pe\u0002riod of faster planning was longer, at\r
smaller branching factors. In other ex\u0002periments, we found that these e↵ects\r
also became stronger as the number of\r
states increased. For example, the lower\r
part of the figure shows results for a\r
branching factor of 1 for tasks with\r
10,000 states. In this case the advan\u0002tage of on-policy focusing is large and\r
long-lasting.\r
All of these results make sense. In the\r
short term, sampling according to the\r
on-policy distribution helps by focusing\r
on states that are near descendants of

8.7. Real-time Dynamic Programming 177\r
the start state. If there are many states and a small branching factor, this e↵ect will be\r
large and long-lasting. In the long run, focusing on the on-policy distribution may hurt\r
because the commonly occurring states all already have their correct values. Sampling\r
them is useless, whereas sampling other states may actually perform some useful work.\r
This presumably is why the exhaustive, unfocused approach does better in the long run,\r
at least for small problems. These results are not conclusive because they are only for\r
problems generated in a particular, random way, but they do suggest that sampling\r
according to the on-policy distribution can be a great advantage for large problems, in\r
particular for problems in which a small subset of the state–action space is visited under\r
the on-policy distribution.\r
Exercise 8.7 Some of the graphs in Figure 8.8 seem to be scalloped in their early portions,\r
particularly the upper graph for b = 1 and the uniform distribution. Why do you think\r
this is? What aspects of the data shown support your hypothesis? ⇤\r
Exercise 8.8 (programming) Replicate the experiment whose results are shown in the\r
lower part of Figure 8.8, then try the same experiment but with b = 3. Discuss the\r
meaning of your results. ⇤\r
8.7 Real-time Dynamic Programming\r
Real-time dynamic programming, or RTDP, is an on-policy trajectory-sampling version of\r
the value-iteration algorithm of dynamic programming (DP). Because it is closely related\r
to conventional sweep-based policy iteration, RTDP illustrates in a particularly clear way\r
some of the advantages that on-policy trajectory sampling can provide. RTDP updates\r
the values of states visited in actual or simulated trajectories by means of expected\r
tabular value-iteration updates as defined by (4.10). It is basically the algorithm that\r
produced the on-policy results shown in Figure 8.8.\r
The close connection between RTDP and conventional DP makes it possible to derive\r
some theoretical results by adapting existing theory. RTDP is an example of an asyn\u0002chronous DP algorithm as described in Section 4.5. Asynchronous DP algorithms are\r
not organized in terms of systematic sweeps of the state set; they update state values in\r
any order whatsoever, using whatever values of other states happen to be available. In\r
RTDP, the update order is dictated by the order states are visited in real or simulated\r
trajectories.\r
Start States\r
Irrelevant States: \r
unreachable from any start state\r
under any optimal policy\r
Relevant States\r
reachable from some start state \r
under some optimal policy\r
If trajectories can start only from a designated\r
set of start states, and if you are interested in\r
the prediction problem for a given policy, then on\u0002policy trajectory sampling allows the algorithm to\r
completely skip states that cannot be reached by\r
the given policy from any of the start states: such\r
states are irrelevant to the prediction problem.\r
For a control problem, where the goal is to find\r
an optimal policy instead of evaluating a given\r
policy, there might well be states that cannot be

178 Chapter 8: Planning and Learning with Tabular Methods\r
reached by any optimal policy from any of the start states, and there is no need to specify\r
optimal actions for these irrelevant states. What is needed is an optimal partial policy,\r
meaning a policy that is optimal for the relevant states but can specify arbitrary actions,\r
or even be undefined, for the irrelevant states.\r
But finding such an optimal partial policy with an on-policy trajectory-sampling\r
control method, such as Sarsa (Section 6.4), in general requires visiting all state–action\r
pairs—even those that will turn out to be irrelevant—an infinite number of times. This\r
can be done, for example, by using exploring starts (Section 5.3). This is true for RTDP\r
as well: for episodic tasks with exploring starts, RTDP is an asynchronous value-iteration\r
algorithm that converges to optimal policies for discounted finite MDPs (and for the\r
undiscounted case under certain conditions). Unlike the situation for a prediction problem,\r
it is generally not possible to stop updating any state or state–action pair if convergence\r
to an optimal policy is important.\r
The most interesting result for RTDP is that for certain types of problems satisfying\r
reasonable conditions, RTDP is guaranteed to find a policy that is optimal on the relevant\r
states without visiting every state infinitely often, or even without visiting some states at\r
all. Indeed, in some problems, only a small fraction of the states need to be visited. This\r
can be a great advantage for problems with very large state sets, where even a single\r
sweep may not be feasible.\r
The tasks for which this result holds are undiscounted episodic tasks for MDPs with\r
absorbing goal states that generate zero rewards, as described in Section 3.4. At every step\r
of a real or simulated trajectory, RTDP selects a greedy action (breaking ties randomly)\r
and applies the expected value-iteration update operation to the current state. It can\r
also update the values of an arbitrary collection of other states at each step; for example,\r
it can update the values of states visited in a limited-horizon look-ahead search from the\r
current state.\r
For these problems, with each episode beginning in a state randomly chosen from the\r
set of start states and ending at a goal state, RTDP converges with probability one to a\r
policy that is optimal for all the relevant states provided: (1) the initial value of every\r
goal state is zero, (2) there exists at least one policy that guarantees that a goal state\r
will be reached with probability one from any start state, (3) all rewards for transitions\r
from non-goal states are strictly negative, and (4) all the initial values are equal to, or\r
greater than, their optimal values (which can be satisfied by simply setting the initial\r
values of all states to zero). This result was proved by Barto, Bradtke, and Singh (1995)\r
by combining results for asynchronous DP with results about a heuristic search algorithm\r
known as learning real-time A* due to Korf (1990).\r
Tasks having these properties are examples of stochastic optimal path problems, which\r
are usually stated in terms of cost minimization instead of as reward maximization as\r
we do here. Maximizing the negative returns in our version is equivalent to minimizing\r
the costs of paths from a start state to a goal state. Examples of this kind of task are\r
minimum-time control tasks, where each time step required to reach a goal produces a\r
reward of 1, or problems like the Golf example in Section 3.5, whose objective is to hit\r
the hole with the fewest strokes.

8.7. Real-time Dynamic Programming 179\r
Example 8.6: RTDP on the Racetrack The racetrack problem of Exercise 5.12\r
(page 111) is a stochastic optimal path problem. Comparing RTDP and the conventional\r
DP value iteration algorithm on an example racetrack problem illustrates some of the\r
advantages of on-policy trajectory sampling.\r
Recall from the exercise that an agent has to learn how to drive a car around a turn\r
like those shown in Figure 5.5 and cross the finish line as quickly as possible while staying\r
on the track. Start states are all the zero-speed states on the starting line; the goal states\r
are all the states that can be reached in one time step by crossing the finish line from\r
inside the track. Unlike Exercise 5.12, here there is no limit on the car’s speed, so the\r
state set is potentially infinite. However, the set of states that can be reached from the\r
set of start states via any policy is finite and can be considered to be the state set of the\r
problem. Each episode begins in a randomly selected start state and ends when the car\r
crosses the finish line. The rewards are 1 for each step until the car crosses the finish\r
line. If the car hits the track boundary, it is moved back to a random start state, and the\r
episode continues.\r
A racetrack similar to the small racetrack on the left of Figure 5.5 has 9,115 states\r
reachable from start states by any policy, only 599 of which are relevant, meaning that\r
they are reachable from some start state via some optimal policy. (The number of relevant\r
states was estimated by counting the states visited while executing optimal actions for\r
107 episodes.)\r
The table below compares solving this task by conventional DP and by RTDP. These\r
results are averages over 25 runs, each begun with a di↵erent random number seed.\r
Conventional DP in this case is value iteration using exhaustive sweeps of the state set,\r
with values updated one state at a time in place, meaning that the update for each state\r
uses the most recent values of the other states (This is the Gauss-Seidel version of value\r
iteration, which was found to be approximately twice as fast as the Jacobi version on\r
this problem. See Section 4.8.) No special attention was paid to the ordering of the\r
updates; other orderings could have produced faster convergence. Initial values were all\r
zero for each run of both methods. DP was judged to have converged when the maximum\r
change in a state value over a sweep was less than 104, and RTDP was judged to have\r
converged when the average time to cross the finish line over 20 episodes appeared to\r
stabilize at an asymptotic number of steps. This version of RTDP updated only the value\r
of the current state on each step.\r
DP RTDP\r
Average computation to convergence 28 sweeps 4000 episodes\r
Average number of updates to convergence 252,784 127,600\r
Average number of updates per episode — 31.9\r
% of states updated  100 times — 98.45\r
% of states updated  10 times — 80.51\r
% of states updated 0 times — 3.18\r
Both methods produced policies averaging between 14 and 15 steps to cross the finish\r
line, but RTDP required only roughly half of the updates that DP did. This is the result\r
of RTDP’s on-policy trajectory sampling. Whereas the value of every state was updated

180 Chapter 8: Planning and Learning with Tabular Methods\r
in each sweep of DP, RTDP focused updates on fewer states. In an average run, RTDP\r
updated the values of 98.45% of the states no more than 100 times and 80.51% of the\r
states no more than 10 times; the values of about 290 states were not updated at all in\r
an average run.\r
Another advantage of RTDP is that as the value function approaches the optimal\r
value function v⇤, the policy used by the agent to generate trajectories approaches an\r
optimal policy because it is always greedy with respect to the current value function.\r
This is in contrast to the situation in conventional value iteration. In practice, value\r
iteration terminates when the value function changes by only a small amount in a sweep,\r
which is how we terminated it to obtain the results in the table above. At this point,\r
the value function closely approximates v⇤, and a greedy policy is close to an optimal\r
policy. However, it is possible that policies that are greedy with respect to the latest\r
value function were optimal, or nearly so, long before value iteration terminates. (Recall\r
from Chapter 4 that optimal policies can be greedy with respect to many di↵erent\r
value functions, not just v⇤.) Checking for the emergence of an optimal policy before\r
value iteration converges is not a part of the conventional DP algorithm and requires\r
considerable additional computation.\r
In the racetrack example, by running many test episodes after each DP sweep, with\r
actions selected greedily according to the result of that sweep, it was possible to estimate\r
the earliest point in the DP computation at which the approximated optimal evaluation\r
function was good enough so that the corresponding greedy policy was nearly optimal.\r
For this racetrack, a close-to-optimal policy emerged after 15 sweeps of value iteration, or\r
after 136,725 value-iteration updates. This is considerably less than the 252,784 updates\r
DP needed to converge to v⇤, but still more than the 127,600 updates RTDP required.\r
Although these simulations are certainly not definitive comparisons of the RTDP with\r
conventional sweep-based value iteration, they illustrate some of advantages of on-policy\r
trajectory sampling. Whereas conventional value iteration continued to update the value\r
of all the states, RTDP strongly focused on subsets of the states that were relevant to\r
the problem’s objective. This focus became increasingly narrow as learning continued.\r
Because the convergence theorem for RTDP applies to the simulations, we know that\r
RTDP eventually would have focused only on relevant states, i.e., on states making up\r
optimal paths. RTDP achieved nearly optimal control with about 50% of the computation\r
required by sweep-based value iteration.\r
8.8 Planning at Decision Time\r
Planning can be used in at least two ways. The one we have considered so far in this\r
chapter, typified by dynamic programming and Dyna, is to use planning to gradually\r
improve a policy or value function on the basis of simulated experience obtained from a\r
model (either a sample or a distribution model). Selecting actions is then a matter of\r
comparing the current state’s action values obtained from a table in the tabular case we\r
have thus far considered, or by evaluating a mathematical expression in the approximate\r
methods we consider in Part II below. Well before an action is selected for any current\r
state St, planning has played a part in improving the table entries, or the function

8.9. Heuristic Search 181\r
approximation parameters, needed to select actions for many states, including St. Used\r
this way, planning is not focused on the current state. We call planning used in this way\r
background planning.\r
The other way to use planning is to begin and complete it after encountering each\r
new state St, as a computation whose output is the selection of a single action At; on\r
the next step planning begins anew with St+1 to produce At+1, and so on. The simplest,\r
and almost degenerate, example of this use of planning is when only state values are\r
available, and an action is selected by comparing the values of model-predicted next states\r
for each action (or by comparing the values of afterstates as in the tic-tac-toe example\r
in Chapter 1). More generally, planning used in this way can look much deeper than\r
one-step-ahead and evaluate action choices leading to many di↵erent predicted state and\r
reward trajectories. Unlike the first use of planning, here planning focuses on a particular\r
state. We call this decision-time planning.\r
These two ways of thinking about planning—using simulated experience to gradually\r
improve a policy or value function, or using simulated experience to select an action for\r
the current state—can blend together in natural and interesting ways, but they have\r
tended to be studied separately, and that is a good way to first understand them. Let us\r
now take a closer look at decision-time planning.\r
Even when planning is only done at decision time, we can still view it, as we did\r
in Section 8.1, as proceeding from simulated experience to updates and values, and\r
ultimately to a policy. It is just that now the values and policy are specific to the current\r
state and the action choices available there, so much so that the values and policy created\r
by the planning process are typically discarded after being used to select the current\r
action. In many applications this is not a great loss because there are very many states\r
and we are unlikely to return to the same state for a long time. In general, one may\r
want to do a mix of both: focus planning on the current state and store the results\r
of planning so as to be that much farther along should one return to the same state\r
later. Decision-time planning is most useful in applications in which fast responses are\r
not required. In chess playing programs, for example, one may be permitted seconds or\r
minutes of computation for each move, and strong programs may plan dozens of moves\r
ahead within this time. On the other hand, if low latency action selection is the priority,\r
then one is generally better o↵ doing planning in the background to compute a policy\r
that can then be rapidly applied to each newly encountered state.\r
8.9 Heuristic Search\r
The classical state-space planning methods in artificial intelligence are decision-time\r
planning methods collectively known as heuristic search. In heuristic search, for each\r
state encountered, a large tree of possible continuations is considered. The approximate\r
value function is applied to the leaf nodes and then backed up toward the current state\r
at the root. The backing up within the search tree is just the same as in the expected\r
updates with maxes (those for v⇤ and q⇤) discussed throughout this book. The backing\r
up stops at the state–action nodes for the current state. Once the backed-up values of\r
these nodes are computed, the best of them is chosen as the current action, and then all\r
backed-up values are discarded.

182 Chapter 8: Planning and Learning with Tabular Methods\r
In conventional heuristic search no e↵ort is made to save the backed-up values by\r
changing the approximate value function. In fact, the value function is generally designed\r
by people and never changed as a result of search. However, it is natural to consider\r
allowing the value function to be improved over time, using either the backed-up values\r
computed during heuristic search or any of the other methods presented throughout\r
this book. In a sense we have taken this approach all along. Our greedy, "-greedy, and\r
UCB (Section 2.7) action-selection methods are not unlike heuristic search, albeit on a\r
smaller scale. For example, to compute the greedy action given a model and a state-value\r
function, we must look ahead from each possible action to each possible next state, take\r
into account the rewards and estimated values, and then pick the best action. Just as\r
in conventional heuristic search, this process computes backed-up values of the possible\r
actions, but does not attempt to save them. Thus, heuristic search can be viewed as an\r
extension of the idea of a greedy policy beyond a single step.\r
The point of searching deeper than one step is to obtain better action selections. If one\r
has a perfect model and an imperfect action-value function, then in fact deeper search\r
will usually yield better policies.2 Certainly, if the search is all the way to the end of\r
the episode, then the e↵ect of the imperfect value function is eliminated, and the action\r
determined in this way must be optimal. If the search is of sucient depth k such that k\r
is very small, then the actions will be correspondingly near optimal. On the other hand,\r
the deeper the search, the more computation is required, usually resulting in a slower\r
response time. A good example is provided by Tesauro’s grandmaster-level backgammon\r
player, TD-Gammon (Section 16.1). This system used TD learning to learn an afterstate\r
value function through many games of self-play, using a form of heuristic search to make\r
its moves. As a model, TD-Gammon used a priori knowledge of the probabilities of dice\r
rolls and the assumption that the opponent always selected the actions that TD-Gammon\r
rated as best for it. Tesauro found that the deeper the heuristic search, the better the\r
moves made by TD-Gammon, but the longer it took to make each move. Backgammon\r
has a large branching factor, yet moves must be made within a few seconds. It was\r
only feasible to search ahead selectively a few steps, but even so the search resulted in\r
significantly better action selections.\r
We should not overlook the most obvious way in which heuristic search focuses updates:\r
on the current state. Much of the e↵ectiveness of heuristic search is due to its search tree\r
being tightly focused on the states and actions that might immediately follow the current\r
state. You may spend more of your life playing chess than checkers, but when you play\r
checkers, it pays to think about checkers and about your particular checkers position,\r
your likely next moves, and successor positions. No matter how you select actions, it\r
is these states and actions that are of highest priority for updates and where you most\r
urgently want your approximate value function to be accurate. Not only should your\r
computation be preferentially devoted to imminent events, but so should your limited\r
memory resources. In chess, for example, there are far too many possible positions to\r
store distinct value estimates for each of them, but chess programs based on heuristic\r
search can easily store distinct estimates for the millions of positions they encounter\r
2There are interesting exceptions to this (see Pearl, 1984).

8.10. Rollout Algorithms 183\r
looking ahead from a single position. This great focusing of memory and computational\r
resources on the current decision is presumably the reason why heuristic search can be so\r
e↵ective.\r
The distribution of updates can be altered in similar ways to focus on the current\r
state and its likely successors. As a limiting case we might use exactly the methods of\r
heuristic search to construct a search tree, and then perform the individual, one-step\r
updates from bottom up, as suggested by Figure 8.9. If the updates are ordered in this\r
way and a tabular representation is used, then exactly the same overall update would\r
be achieved as in depth-first heuristic search. Any state-space search can be viewed in\r
this way as the piecing together of a large number of individual one-step updates. Thus,\r
the performance improvement observed with deeper searches is not due to the use of\r
multistep updates as such. Instead, it is due to the focus and concentration of updates\r
on states and actions immediately downstream from the current state. By devoting a\r
large amount of computation specifically relevant to the candidate actions, decision-time\r
planning can produce better decisions than can be produced by relying on unfocused\r
updates.\r
1 2\r
3\r
4 5\r
6\r
7\r
8 9\r
10\r
Figure 8.9: Heuristic search can be implemented as a sequence of one-step updates (shown\r
here outlined in blue) backing up values from the leaf nodes toward the root. The ordering\r
shown here is for a selective depth-first search.\r
8.10 Rollout Algorithms\r
Rollout algorithms are decision-time planning algorithms based on Monte Carlo control\r
applied to simulated trajectories that all begin at the current environment state. They\r
estimate action values for a given policy by averaging the returns of many simulated\r
trajectories that start with each possible action and then follow the given policy. When\r
the action-value estimates are considered to be accurate enough, the action (or one of the

184 Chapter 8: Planning and Learning with Tabular Methods\r
actions) having the highest estimated value is executed, after which the process is carried\r
out anew from the resulting next state. As explained by Tesauro and Galperin (1997),\r
who experimented with rollout algorithms for playing backgammon, the term “rollout”\r
comes from estimating the value of a backgammon position by playing out, i.e., “rolling\r
out,” the position many times to the game’s end with randomly generated sequences of\r
dice rolls, where the moves of both players are made by some fixed policy.\r
Unlike the Monte Carlo control algorithms described in Chapter 5, the goal of a\r
rollout algorithm is not to estimate a complete optimal action-value function, q⇤, or a\r
complete action-value function, q⇡, for a given policy ⇡. Instead, they produce Monte\r
Carlo estimates of action values only for each current state and for a given policy usually\r
called the rollout policy. As decision-time planning algorithms, rollout algorithms make\r
immediate use of these action-value estimates, then discard them. This makes rollout\r
algorithms relatively simple to implement because there is no need to sample outcomes\r
for every state-action pair, and there is no need to approximate a function over either\r
the state space or the state-action space.\r
What then do rollout algorithms accomplish? The policy improvement theorem\r
described in Section 4.2 tells us that given any two policies ⇡ and ⇡0 that are identical\r
except that ⇡0(s) = a 6= ⇡(s) for some state s, if q⇡(s, a)  v⇡(s), then policy ⇡0 is as good\r
as, or better, than ⇡. Moreover, if the inequality is strict, then ⇡0 is in fact better than ⇡.\r
This applies to rollout algorithms where s is the current state and ⇡ is the rollout policy.\r
Averaging the returns of the simulated trajectories produces estimates of q⇡(s, a0) for\r
each action a0 2 A(s). Then the policy that selects an action in s that maximizes these\r
estimates and thereafter follows ⇡ is a good candidate for a policy that improves over\r
⇡. The result is like one step of the policy-iteration algorithm of dynamic programming\r
discussed in Section 4.3 (though it is more like one step of asynchronous value iteration\r
described in Section 4.5 because it changes the action for just the current state).\r
In other words, the aim of a rollout algorithm is to improve upon the rollout policy;\r
not to find an optimal policy. Experience has shown that rollout algorithms can be\r
surprisingly e↵ective. For example, Tesauro and Galperin (1997) were surprised by the\r
dramatic improvements in backgammon playing ability produced by the rollout method.\r
In some applications, a rollout algorithm can produce good performance even if the\r
rollout policy is completely random. But the performance of the improved policy depends\r
on properties of the rollout policy and the ranking of actions produced by the Monte\r
Carlo value estimates. Intuition suggests that the better the rollout policy and the more\r
accurate the value estimates, the better the policy produced by a rollout algorithm is\r
likely be (but see Gelly and Silver, 2007).\r
This involves important tradeo↵s because better rollout policies typically mean that\r
more time is needed to simulate enough trajectories to obtain good value estimates.\r
As decision-time planning methods, rollout algorithms usually have to meet strict time\r
constraints. The computation time needed by a rollout algorithm depends on the number\r
of actions that have to be evaluated for each decision, the number of time steps in the\r
simulated trajectories needed to obtain useful sample returns, the time it takes the rollout\r
policy to make decisions, and the number of simulated trajectories needed to obtain good\r
Monte Carlo action-value estimates.

8.11. Monte Carlo Tree Search 185\r
Balancing these factors is important in any application of rollout methods, though there\r
are several ways to ease the challenge. Because the Monte Carlo trials are independent of\r
one another, it is possible to run many trials in parallel on separate processors. Another\r
approach is to truncate the simulated trajectories short of complete episodes, correcting\r
the truncated returns by means of a stored evaluation function (which brings into play\r
all that we have said about truncated returns and updates in the preceding chapters).\r
It is also possible, as Tesauro and Galperin (1997) suggest, to monitor the Monte Carlo\r
simulations and prune away candidate actions that are unlikely to turn out to be the\r
best, or whose values are close enough to that of the current best that choosing them\r
instead would make no real di↵erence (though Tesauro and Galperin point out that this\r
would complicate a parallel implementation).\r
We do not ordinarily think of rollout algorithms as learning algorithms because they\r
do not maintain long-term memories of values or policies. However, these algorithms take\r
advantage of some of the features of reinforcement learning that we have emphasized\r
in this book. As instances of Monte Carlo control, they estimate action values by\r
averaging the returns of a collection of sample trajectories, in this case trajectories of\r
simulated interactions with a sample model of the environment. In this way they are\r
like reinforcement learning algorithms in avoiding the exhaustive sweeps of dynamic\r
programming by trajectory sampling, and in avoiding the need for distribution models\r
by relying on sample, instead of expected, updates. Finally, rollout algorithms take\r
advantage of the policy improvement property by acting greedily with respect to the\r
estimated action values.\r
8.11 Monte Carlo Tree Search\r
Monte Carlo Tree Search (MCTS) is a recent and strikingly successful example of decision\u0002time planning. At its base, MCTS is a rollout algorithm as described above, but enhanced\r
by the addition of a means for accumulating value estimates obtained from the Monte\r
Carlo simulations in order to successively direct simulations toward more highly-rewarding\r
trajectories. MCTS is largely responsible for the improvement in computer Go from\r
a weak amateur level in 2005 to a grandmaster level (6 dan or more) in 2015. Many\r
variations of the basic algorithm have been developed, including a variant that we discuss\r
in Section 16.6 that was critical for the stunning 2016 victories of the program AlphaGo\r
over an 18-time world champion Go player. MCTS has proved to be e↵ective in a wide\r
variety of competitive settings, including general game playing (e.g., see Finnsson and\r
Bj¨ornsson, 2008; Genesereth and Thielscher, 2014), but it is not limited to games; it can\r
be e↵ective for single-agent sequential decision problems if there is an environment model\r
simple enough for fast multistep simulation.\r
MCTS is executed after encountering each new state to select the agent’s action for\r
that state; it is executed again to select the action for the next state, and so on. As in a\r
rollout algorithm, each execution is an iterative process that simulates many trajectories\r
starting from the current state and running to a terminal state (or until discounting\r
makes any further reward negligible as a contribution to the return). The core idea\r
of MCTS is to successively focus multiple simulations starting at the current state by

186 Chapter 8: Planning and Learning with Tabular Methods\r
extending the initial portions of trajectories that have received high evaluations from\r
earlier simulations. MCTS does not have to retain approximate value functions or policies\r
from one action selection to the next, though in many implementations it retains selected\r
action values likely to be useful for its next execution.\r
For the most part, the actions in the simulated trajectories are generated using a simple\r
policy, usually called a rollout policy as it is for simpler rollout algorithms. When both\r
the rollout policy and the model do not require a lot of computation, many simulated\r
trajectories can be generated in a short period of time. As in any tabular Monte Carlo\r
method, the value of a state–action pair is estimated as the average of the (simulated)\r
returns from that pair. Monte Carlo value estimates are maintained only for the subset\r
of state–action pairs that are most likely to be reached in a few steps, which form a tree\r
rooted at the current state, as illustrated in Figure 8.10. MCTS incrementally extends\r
the tree by adding nodes representing states that look promising based on the results\r
of the simulated trajectories. Any simulated trajectory will pass through the tree and\r
then exit it at some leaf node. Outside the tree and at the leaf nodes the rollout policy is\r
used for action selections, but at the states inside the tree something better is possible.\r
For these states we have value estimates for at least some of the actions, so we can pick\r
among them using an informed policy, called the tree policy, that balances exploration\r
Selection Expansion Simulation Backup\r
Repeat while time remains \r
Tree\r
 Policy\r
Rollout\r
Policy\r
Figure 8.10: Monte Carlo Tree Search. When the environment changes to a new state, MCTS\r
executes as many iterations as possible before an action needs to be selected, incrementally\r
building a tree whose root node represents the current state. Each iteration consists of the four\r
operations Selection, Expansion (though possibly skipped on some iterations), Simulation,\r
and Backup, as explained in the text and illustrated by the bold arrows in the trees. Adapted\r
from Chaslot, Bakkes, Szita, and Spronck (2008).

8.11. Monte Carlo Tree Search 187\r
and exploitation. For example, the tree policy could select actions using an "-greedy or\r
UCB selection rule (Chapter 2).\r
In more detail, each iteration of a basic version of MCTS consists of the following four\r
steps as illustrated in Figure 8.10:\r
1. Selection. Starting at the root node, a tree policy based on the action values\r
attached to the edges of the tree traverses the tree to select a leaf node.\r
2. Expansion. On some iterations (depending on details of the application), the tree\r
is expanded from the selected leaf node by adding one or more child nodes reached\r
from the selected node via unexplored actions.\r
3. Simulation. From the selected node, or from one of its newly-added child nodes\r
(if any), simulation of a complete episode is run with actions selected by the rollout\r
policy. The result is a Monte Carlo trial with actions selected first by the tree\r
policy and beyond the tree by the rollout policy.\r
4. Backup. The return generated by the simulated episode is backed up to update,\r
or to initialize, the action values attached to the edges of the tree traversed by\r
the tree policy in this iteration of MCTS. No values are saved for the states and\r
actions visited by the rollout policy beyond the tree. Figure 8.10 illustrates this by\r
showing a backup from the terminal state of the simulated trajectory directly to the\r
state–action node in the tree where the rollout policy began (though in general, the\r
entire return over the simulated trajectory is backed up to this state–action node).\r
MCTS continues executing these four steps, starting each time at the tree’s root node,\r
until no more time is left, or some other computational resource is exhausted. Then,\r
finally, an action from the root node (which still represents the current state of the\r
environment) is selected according to some mechanism that depends on the accumulated\r
statistics in the tree; for example, it may be an action having the largest action value\r
of all the actions available from the root state, or perhaps the action with the largest\r
visit count to avoid selecting outliers. This is the action MCTS actually selects. After\r
the environment transitions to a new state, MCTS is run again, sometimes starting\r
with a tree of a single root node representing the new state, but often starting with a\r
tree containing any descendants of this node left over from the tree constructed by the\r
previous execution of MCTS; all the remaining nodes are discarded, along with the action\r
values associated with them.\r
MCTS was first proposed to select moves in programs playing two-person competitive\r
games, such as Go. For game playing, each simulated episode is one complete play of the\r
game in which both players select actions by the tree and rollout policies. Section 16.6\r
describes an extension of MCTS used in the AlphaGo program that combines the Monte\r
Carlo evaluations of MCTS with action values learned by a deep artificial neural network\r
via self-play reinforcement learning.\r
Relating MCTS to the reinforcement learning principles we describe in this book\r
provides some insight into how it achieves such impressive results. At its base, MCTS is\r
a decision-time planning algorithm based on Monte Carlo control applied to simulations

188 Chapter 8: Planning and Learning with Tabular Methods\r
that start from the root state; that is, it is a kind of rollout algorithm as described in\r
the previous section. It therefore benefits from online, incremental, sample-based value\r
estimation and policy improvement. Beyond this, it saves action-value estimates attached\r
to the tree edges and updates them using reinforcement learning’s sample updates. This\r
has the e↵ect of focusing the Monte Carlo trials on trajectories whose initial segments\r
are common to high-return trajectories previously simulated. Further, by incrementally\r
expanding the tree, MCTS e↵ectively grows a lookup table to store a partial action-value\r
function, with memory allocated to the estimated values of state–action pairs visited in\r
the initial segments of high-yielding sample trajectories. MCTS thus avoids the problem\r
of globally approximating an action-value function while it retains the benefit of using\r
past experience to guide exploration.\r
The striking success of decision-time planning by MCTS has deeply influenced artificial\r
intelligence, and many researchers are studying modifications and extensions of the basic\r
procedure for use in both games and single-agent applications.\r
8.12 Summary of the Chapter\r
Planning requires a model of the environment. A distribution model consists of the\r
probabilities of next states and rewards for possible actions; a sample model produces\r
single transitions and rewards generated according to these probabilities. Dynamic\r
programming requires a distribution model because it uses expected updates, which involve\r
computing expectations over all the possible next states and rewards. A sample model,\r
on the other hand, is what is needed to simulate interacting with the environment during\r
which sample updates, like those used by many reinforcement learning algorithms, can be\r
used. Sample models are generally much easier to obtain than distribution models.\r
We have presented a perspective emphasizing the surprisingly close relationships be\u0002tween planning optimal behavior and learning optimal behavior. Both involve estimating\r
the same value functions, and in both cases it is natural to update the estimates incre\u0002mentally, in a long series of small backing-up operations. This makes it straightforward\r
to integrate learning and planning processes simply by allowing both to update the same\r
estimated value function. In addition, any of the learning methods can be converted into\r
planning methods simply by applying them to simulated (model-generated) experience\r
rather than to real experience. In this case learning and planning become even more\r
similar; they are possibly identical algorithms operating on two di↵erent sources of\r
experience.\r
It is straightforward to integrate incremental planning methods with acting and model\u0002learning. Planning, acting, and model-learning interact in a circular fashion (as in\r
the diagram on page 162), each producing what the other needs to improve; no other\r
interaction among them is either required or prohibited. The most natural approach\r
is for all processes to proceed asynchronously and in parallel. If the processes must\r
share computational resources, then the division can be handled almost arbitrarily—by\r
whatever organization is most convenient and ecient for the task at hand.\r
In this chapter we have touched upon a number of dimensions of variation among\r
state-space planning methods. One dimension is the variation in the size of updates. The

8.13. Summary of Part I: Dimensions 189\r
smaller the updates, the more incremental the planning methods can be. Among the\r
smallest updates are one-step sample updates, as in Dyna. Another important dimension\r
is the distribution of updates, that is, of the focus of search. Prioritized sweeping focuses\r
backward on the predecessors of states whose values have recently changed. On-policy\r
trajectory sampling focuses on states or state–action pairs that the agent is likely to\r
encounter when controlling its environment. This can allow computation to skip over\r
parts of the state space that are irrelevant to the prediction or control problem. Real\u0002time dynamic programming, an on-policy trajectory sampling version of value iteration,\r
illustrates some of the advantages this strategy has over conventional sweep-based policy\r
iteration.\r
Planning can also focus forward from pertinent states, such as states actually encoun\u0002tered during an agent-environment interaction. The most important form of this is when\r
planning is done at decision time, that is, as part of the action-selection process. Classical\r
heuristic search as studied in artificial intelligence is an example of this. Other examples\r
are rollout algorithms and Monte Carlo Tree Search that benefit from online, incremental,\r
sample-based value estimation and policy improvement.\r
8.13 Summary of Part I: Dimensions\r
This chapter concludes Part I of this book. In it we have tried to present reinforcement\r
learning not as a collection of individual methods, but as a coherent set of ideas cutting\r
across methods. Each idea can be viewed as a dimension along which methods vary. The\r
set of such dimensions spans a large space of possible methods. By exploring this space\r
at the level of dimensions we hope to obtain the broadest and most lasting understanding.\r
In this section we use the concept of dimensions in method space to recapitulate the view\r
of reinforcement learning developed so far in this book.\r
All of the methods we have explored so far in this book have three key ideas in common:\r
first, they all seek to estimate value functions; second, they all operate by backing up\r
values along actual or possible state trajectories; and third, they all follow the general\r
strategy of generalized policy iteration (GPI), meaning that they maintain an approximate\r
value function and an approximate policy, and they continually try to improve each on the\r
basis of the other. These three ideas are central to the subjects covered in this book. We\r
suggest that value functions, backing up value updates, and GPI are powerful organizing\r
principles potentially relevant to any model of intelligence, whether artificial or natural.\r
Two of the most important dimensions along which the methods vary are shown in\r
Figure 8.11. These dimensions have to do with the kind of update used to improve the\r
value function. The horizontal dimension is whether they are sample updates (based on a\r
sample trajectory) or expected updates (based on a distribution of possible trajectories).\r
Expected updates require a distribution model, whereas sample updates need only a\r
sample model, or can be done from actual experience with no model at all (another\r
dimension of variation). The vertical dimension of Figure 8.11 corresponds to the depth\r
of updates, that is, to the degree of bootstrapping. At three of the four corners of the\r
space are the three primary methods for estimating values: dynamic programming, TD,\r
and Monte Carlo. Along the left edge of the space are the sample-update methods,

190 Chapter 8: Planning and Learning with Tabular Methods\r
width\r
of update\r
depth\r
(length)\r
of update\r
Temporal\u0002difference\r
learning\r
Dynamic\r
programming\r
Monte\r
Carlo\r
...\r
Exhaustive\r
search\r
Figure 8.11: A slice through the space of reinforcement learning methods, highlighting the\r
two of the most important dimensions explored in Part I of this book: the depth and width of\r
the updates.\r
ranging from one-step TD updates to full-return Monte Carlo updates. Between these\r
is a spectrum including methods based on n-step updates (and in Chapter 12 we will\r
extend this to mixtures of n-step updates such as the -updates implemented by eligibility\r
traces).\r
Dynamic programming methods are shown in the extreme upper-right corner of the\r
space because they involve one-step expected updates. The lower-right corner is the\r
extreme case of expected updates so deep that they run all the way to terminal states\r
(or, in a continuing task, until discounting has reduced the contribution of any further\r
rewards to a negligible level). This is the case of exhaustive search. Intermediate methods\r
along this dimension include heuristic search and related methods that search and update\r
up to a limited depth, perhaps selectively. There are also methods that are intermediate\r
along the horizontal dimension. These include methods that mix expected and sample\r
updates, as well as the possibility of methods that mix samples and distributions within\r
a single update. The interior of the square is filled in to represent the space of all such\r
intermediate methods.\r
A third dimension that we have emphasized in this book is the binary distinction\r
between on-policy and o↵-policy methods. In the former case, the agent learns the value\r
function for the policy it is currently following, whereas in the latter case it learns the

8.13. Summary of Part I: Dimensions 191\r
value function for the policy for a di↵erent policy, often the one that the agent currently\r
thinks is best. The policy generating behavior is typically di↵erent from what is currently\r
thought best because of the need to explore. This third dimension might be visualized as\r
perpendicular to the plane of the page in Figure 8.11.\r
In addition to the three dimensions just discussed, we have identified a number of\r
others throughout the book:\r
Definition of return Is the task episodic or continuing, discounted or undiscounted?\r
Action values vs. state values vs. afterstate values What kind of values should\r
be estimated? If only state values are estimated, then either a model or a separate\r
policy (as in actor–critic methods) is required for action selection.\r
Action selection/exploration How are actions selected to ensure a suitable trade-o↵\r
between exploration and exploitation? We have considered only the simplest ways to\r
do this: "-greedy, optimistic initialization of values, soft-max, and upper confidence\r
bound.\r
Synchronous vs. asynchronous Are the updates for all states performed simultane\u0002ously or one by one in some order?\r
Real vs. simulated Should one update based on real experience or simulated experi\u0002ence? If both, how much of each?\r
Location of updates What states or state–action pairs should be updated? Model\u0002free methods can choose only among the states and state–action pairs actually\r
encountered, but model-based methods can choose arbitrarily. There are many\r
possibilities here.\r
Timing of updates Should updates be done as part of selecting actions, or only after\u0002ward?\r
Memory for updates How long should updated values be retained? Should they be\r
retained permanently, or only while computing an action selection, as in heuristic\r
search?\r
Of course, these dimensions are neither exhaustive nor mutually exclusive. Individual\r
algorithms di↵er in many other ways as well, and many algorithms lie in several places\r
along several dimensions. For example, Dyna methods use both real and simulated\r
experience to a↵ect the same value function. It is also perfectly sensible to maintain\r
multiple value functions computed in di↵erent ways or over di↵erent state and action\r
representations. These dimensions do, however, constitute a coherent set of ideas for\r
describing and exploring a wide space of possible methods.\r
The most important dimension not mentioned here, and not covered in Part I of\r
this book, is that of function approximation. Function approximation can be viewed as\r
an orthogonal spectrum of possibilities ranging from tabular methods at one extreme\r
through state aggregation, a variety of linear methods, and then a diverse set of nonlinear\r
methods. This dimension is explored in Part II.

192 Chapter 8: Planning and Learning with Tabular Methods\r
Bibliographical and Historical Remarks\r
8.1 The overall view of planning and learning presented here has developed gradually\r
over a number of years, in part by the authors (Sutton, 1990, 1991a, 1991b;\r
Barto, Bradtke, and Singh, 1991, 1995; Sutton and Pinette, 1985; Sutton and\r
Barto, 1981b); it has been strongly influenced by Agre and Chapman (1990; Agre\r
1988), Bertsekas and Tsitsiklis (1989), Singh (1993), and others. The authors\r
were also strongly influenced by psychological studies of latent learning (Tolman,\r
1932) and by psychological views of the nature of thought (e.g., Galanter and\r
Gerstenhaber, 1956; Craik, 1943; Campbell, 1960; Dennett, 1978). In Part\r
III of the book, Section 14.6 relates model-based and model-free methods to\r
psychological theories of learning and behavior, and Section 15.11 discusses ideas\r
about how the brain might implement these types of methods.\r
8.2 The terms direct and indirect, which we use to describe di↵erent kinds of\r
reinforcement learning, are from the adaptive control literature (e.g., Goodwin\r
and Sin, 1984), where they are used to make the same kind of distinction. The\r
term system identification is used in adaptive control for what we call model\u0002learning (e.g., Goodwin and Sin, 1984; Ljung and S¨oderstrom, 1983; Young,\r
1984). The Dyna architecture is due to Sutton (1990), and the results in this\r
and the next section are based on results reported there. Barto and Singh\r
(1990) consider some of the issues in comparing direct and indirect reinforcement\r
learning methods. Early work extending Dyna to linear function approximation\r
was done by Sutton, Szepesv´ari, Geramifard, and Bowling (2008) and by Parr,\r
Li, Taylor, Painter-Wakefield, and Littman (2008).\r
8.3 There have been several works with model-based reinforcement learning that take\r
the idea of exploration bonuses and optimistic initialization to its logical extreme,\r
in which all incompletely explored choices are assumed maximally rewarding\r
and optimal paths are computed to test them. The E3 algorithm of Kearns and\r
Singh (2002) and the R-max algorithm of Brafman and Tennenholtz (2003) are\r
guaranteed to find a near-optimal solution in time polynomial in the number\r
of states and actions. This is usually too slow for practical algorithms but is\r
probably the best that can be done in the worst case.\r
8.4 Prioritized sweeping was developed simultaneously and independently by Moore\r
and Atkeson (1993) and Peng and Williams (1993). The results in the box on\r
page 170 are due to Peng and Williams (1993). The results in the box on page 171\r
are due to Moore and Atkeson. Key subsequent work in this area includes that\r
by McMahan and Gordon (2005) and by van Seijen and Sutton (2013).\r
8.5 This section was strongly influenced by the experiments of Singh (1993).\r
8.6–7 Trajectory sampling has implicitly been a part of reinforcement learning from\r
the outset, but it was most explicitly emphasized by Barto, Bradtke, and Singh\r
(1995) in their introduction of RTDP. They recognized that Korf’s (1990) learning

8.13. Summary of Part I: Dimensions 193\r
real-time A* (LRTA*) algorithm is an asynchronous DP algorithm that applies\r
to stochastic problems as well as the deterministic problems on which Korf\r
focused. Beyond LRTA*, RTDP includes the option of updating the values of\r
many states in the time intervals between the execution of actions. Barto et\r
al. (1995) proved the convergence result described here by combining Korf’s (1990)\r
convergence proof for LRTA* with the result of Bertsekas (1982) (also Bertsekas\r
and Tsitsiklis, 1989) ensuring convergence of asynchronous DP for stochastic\r
shortest path problems in the undiscounted case. Combining model-learning\r
with RTDP is called Adaptive RTDP, also presented by Barto et al. (1995) and\r
discussed by Barto (2011).\r
8.9 For further reading on heuristic search, the reader is encouraged to consult texts\r
and surveys such as those by Russell and Norvig (2009) and Korf (1988). Peng\r
and Williams (1993) explored a forward focusing of updates much as is suggested\r
in this section.\r
8.10 Abramson’s (1990) expected-outcome model is a rollout algorithm applied to two\u0002person games in which the play of both simulated players is random. He argued\r
that even with random play, it is a “powerful heuristic” that is “precise, accurate,\r
easily estimable, eciently calculable, and domain-independent.” Tesauro and\r
Galperin (1997) demonstrated the e↵ectiveness of rollout algorithms for improving\r
the play of backgammon programs, adopting the term “rollout” from its use\r
in evaluating backgammon positions by playing out positions with di↵erent\r
randomly generating sequences of dice rolls. Bertsekas, Tsitsiklis, and Wu (1997)\r
examine rollout algorithms applied to combinatorial optimization problems, and\r
Bertsekas (2013) surveys their use in discrete deterministic optimization problems,\r
remarking that they are “often surprisingly e↵ective.”\r
8.11 The central ideas of MCTS were introduced by Coulom (2006) and by Kocsis\r
and Szepesv´ari (2006). They built upon previous research with Monte Carlo\r
planning algorithms as reviewed by these authors. Browne, Powley, Whitehouse,\r
Lucas, Cowling, Rohlfshagen, Tavener, Perez, Samothrakis, and Colton (2012)\r
is an excellent survey of MCTS methods and their applications. David Silver\r
contributed to the ideas and presentation in this section.

Part II:\r
Approximate Solution Methods\r
In the second part of the book we extend the tabular methods presented in the first part\r
to apply to problems with arbitrarily large state spaces. In many of the tasks to which we\r
would like to apply reinforcement learning the state space is combinatorial and enormous;\r
the number of possible camera images, for example, is much larger than the number of\r
atoms in the universe. In such cases we cannot expect to find an optimal policy or the\r
optimal value function even in the limit of infinite time and data; our goal instead is to\r
find a good approximate solution using limited computational resources. In this part of\r
the book we explore such approximate solution methods.\r
The problem with large state spaces is not just the memory needed for large tables,\r
but the time and data needed to fill them accurately. In many of our target tasks, almost\r
every state encountered will never have been seen before. To make sensible decisions in\r
such states it is necessary to generalize from previous encounters with di↵erent states\r
that are in some sense similar to the current one. In other words, the key issue is that of\r
generalization. How can experience with a limited subset of the state space be usefully\r
generalized to produce a good approximation over a much larger subset?\r
Fortunately, generalization from examples has already been extensively studied, and\r
we do not need to invent totally new methods for use in reinforcement learning. To some\r
extent we need only combine reinforcement learning methods with existing generalization\r
methods. The kind of generalization we require is often called function approximation\r
because it takes examples from a desired function (e.g., a value function) and attempts\r
to generalize from them to construct an approximation of the entire function. Function\r
approximation is an instance of supervised learning, the primary topic studied in machine\r
learning, artificial neural networks, pattern recognition, and statistical curve fitting. In\r
theory, any of the methods studied in these fields can be used in the role of function\r
approximator within reinforcement learning algorithms, although in practice some fit\r
more easily into this role than others.\r
Reinforcement learning with function approximation involves a number of new issues\r
that do not normally arise in conventional supervised learning, such as nonstationarity,\r
bootstrapping, and delayed targets. We introduce these and other issues successively over\r
the five chapters of this part. Initially we restrict attention to on-policy training, treating\r
in Chapter 9 the prediction case, in which the policy is given and only its value function\r
is approximated, and then in Chapter 10 the control case, in which an approximation to\r
the optimal policy is found. The challenging problem of o↵-policy learning with function\r
approximation is treated in Chapter 11. In each of these three chapters we will have

196 Part II: Approximate Solution Methods\r
to return to first principles and re-examine the objectives of the learning to take into\r
account function approximation. Chapter 12 introduces and analyzes the algorithmic\r
mechanism of eligibility traces, which dramatically improves the computational properties\r
of multi-step reinforcement learning methods in many cases. The final chapter of this\r
part explores a di↵erent approach to control, policy-gradient methods, which approximate\r
the optimal policy directly and need never form an approximate value function (although\r
they may be much more ecient if they do approximate a value function as well the\r
policy).

Chapter 9\r
On-policy Prediction with\r
Approximation\r
In this chapter, we begin our study of function approximation in reinforcement learning\r
by considering its use in estimating the state-value function from on-policy data, that is,\r
in approximating v⇡ from experience generated using a known policy ⇡. The novelty in\r
this chapter is that the approximate value function is represented not as a table but as a\r
parameterized functional form with weight vector w 2 Rd. We will write vˆ(s,w) ⇡ v⇡(s)\r
for the approximate value of state s given weight vector w. For example, vˆ might be\r
a linear function in features of the state, with w the vector of feature weights. More\r
generally, vˆ might be the function computed by a multi-layer artificial neural network,\r
with w the vector of connection weights in all the layers. By adjusting the weights, any\r
of a wide range of di↵erent functions can be implemented by the network. Or vˆ might be\r
the function computed by a decision tree, where w is all the numbers defining the split\r
points and leaf values of the tree. Typically, the number of weights (the dimensionality of\r
w) is much less than the number of states (d ⌧ |S|), and changing one weight changes the\r
estimated value of many states. Consequently, when a single state is updated, the change\r
generalizes from that state to a↵ect the values of many other states. Such generalization\r
makes the learning potentially more powerful but also potentially more dicult to manage\r
and understand.\r
Perhaps surprisingly, extending reinforcement learning to function approximation also\r
makes it applicable to partially observable problems, in which the full state is not available\r
to the agent. If the parameterized function form for vˆ does not allow the estimated\r
value to depend on certain aspects of the state, then it is just as if those aspects are\r
unobservable. In fact, all the theoretical results for methods using function approximation\r
presented in this part of the book apply equally well to cases of partial observability.\r
What function approximation can’t do, however, is augment the state representation\r
with memories of past observations. Some such possible further extensions are discussed\r
briefly in Section 17.3.

198 Chapter 9: On-policy Prediction with Approximation\r
9.1 Value-function Approximation\r
All of the prediction methods covered in this book have been described as updates to an\r
estimated value function that shift its value at particular states toward a “backed-up value,”\r
or update target, for that state. Let us refer to an individual update by the notation s 7! u,\r
where s is the state updated and u is the update target that s’s estimated value is shifted\r
toward. For example, the Monte Carlo update for value prediction is St 7! Gt, the TD(0)\r
update is St 7! Rt+1 +vˆ(St+1,wt), and the n-step TD update is St 7! Gt:t+n. In the DP\r
(dynamic programming) policy-evaluation update, s 7! E⇡[Rt+1 + vˆ(St+1,wt) | St =s],\r
an arbitrary state s is updated, whereas in the other cases the state encountered in actual\r
experience, St, is updated.\r
It is natural to interpret each update as specifying an example of the desired input–\r
output behavior of the value function. In a sense, the update s 7! u means that the\r
estimated value for state s should be more like the update target u. Up to now, the\r
actual update has been trivial: the table entry for s’s estimated value has simply been\r
shifted a fraction of the way toward u, and the estimated values of all other states\r
were left unchanged. Now we permit arbitrarily complex and sophisticated methods to\r
implement the update, and updating at s generalizes so that the estimated values of\r
many other states are changed as well. Machine learning methods that learn to mimic\r
input–output examples in this way are called supervised learning methods, and when the\r
outputs are numbers, like u, the process is often called function approximation. Function\r
approximation methods expect to receive examples of the desired input–output behavior\r
of the function they are trying to approximate. We use these methods for value prediction\r
simply by passing to them the s 7! u of each update as a training example. We then\r
interpret the approximate function they produce as an estimated value function.\r
Viewing each update as a conventional training example in this way enables us to use\r
any of a wide range of existing function approximation methods for value prediction. In\r
principle, we can use any method for supervised learning from examples, including artificial\r
neural networks, decision trees, and various kinds of multivariate regression. However,\r
not all function approximation methods are equally well suited for use in reinforcement\r
learning. The most sophisticated artificial neural network and statistical methods all\r
assume a static training set over which multiple passes are made. In reinforcement\r
learning, however, it is important that learning be able to occur online, while the agent\r
interacts with its environment or with a model of its environment. To do this requires\r
methods that are able to learn eciently from incrementally acquired data. In addition,\r
reinforcement learning generally requires function approximation methods able to handle\r
nonstationary target functions (target functions that change over time). For example,\r
in control methods based on GPI (generalized policy iteration) we often seek to learn\r
q⇡ while ⇡ changes. Even if the policy remains the same, the target values of training\r
examples are nonstationary if they are generated by bootstrapping methods (DP and TD\r
learning). Methods that cannot easily handle such nonstationarity are less suitable for\r
reinforcement learning.

9.2. The Prediction Objective (VE) 199\r
9.2 The Prediction Objective (VE)\r
Up to now we have not specified an explicit objective for prediction. In the tabular case\r
a continuous measure of prediction quality was not necessary because the learned value\r
function could come to equal the true value function exactly. Moreover, the learned\r
values at each state were decoupled—an update at one state a↵ected no other. But with\r
genuine approximation, an update at one state a↵ects many others, and it is not possible\r
to get the values of all states exactly correct. By assumption we have far more states\r
than weights, so making one state’s estimate more accurate invariably means making\r
others’ less accurate. We are obligated then to say which states we care most about. We\r
must specify a state distribution µ(s)  0, P\r
s µ(s) = 1, representing how much we care\r
about the error in each state s. By the error in a state s we mean the square of the\r
di↵erence between the approximate value vˆ(s,w) and the true value v⇡(s). Weighting\r
this over the state space by µ, we obtain a natural objective function, the mean square\r
value error, denoted VE:\r
VE(w) .= X\r
s2S\r
µ(s)\r
h\r
v⇡(s)  vˆ(s,w)\r
i2\r
. (9.1)\r
The square root of this measure, the root VE, gives a rough measure of how much the\r
approximate values di↵er from the true values and is often used in plots. Often µ(s) is\r
chosen to be the fraction of time spent in s. Under on-policy training this is called the\r
on-policy distribution; we focus entirely on this case in this chapter. In continuing tasks,\r
the on-policy distribution is the stationary distribution under ⇡.\r
The on-policy distribution in episodic tasks\r
In an episodic task, the on-policy distribution is a little di↵erent in that it depends\r
on how the initial states of episodes are chosen. Let h(s) denote the probability\r
that an episode begins in each state s, and let ⌘(s) denote the number of time\r
steps spent, on average, in state s in a single episode. Time is spent in a state s\r
if episodes start in s, or if transitions are made into s from a preceding state s¯ in\r
which time is spent:\r
⌘(s) = h(s) +X\r
s¯\r
⌘(¯s)\r
X\r
a\r
⇡(a|s¯)p(s|s, a ¯ ), for all s 2 S. (9.2)\r
This system of equations can be solved for the expected number of visits ⌘(s). The\r
on-policy distribution is then the fraction of time spent in each state normalized to\r
sum to one:\r
µ(s) = ⌘(s)\r
P\r
s0 ⌘(s0\r
)\r
, for all s 2 S. (9.3)\r
This is the natural choice without discounting. If there is discounting ( < 1) it\r
should be treated as a form of termination, which can be done simply by including\r
a factor of  in the second term of (9.2).

200 Chapter 9: On-policy Prediction with Approximation\r
The two cases, continuing and episodic, behave similarly, but with approximation they\r
must be treated separately in formal analyses, as we will see repeatedly in this part of\r
the book. This completes the specification of the learning objective.\r
It is not completely clear that the VE is the right performance objective for rein\u0002forcement learning. Remember that our ultimate purpose—the reason we are learning\r
a value function—is to find a better policy. The best value function for this purpose is\r
not necessarily the best for minimizing VE. Nevertheless, it is not yet clear what a more\r
useful alternative goal for value prediction might be. For now, we will focus on VE.\r
An ideal goal in terms of VE would be to find a global optimum, a weight vector w⇤\r
for which VE(w⇤)  VE(w) for all possible w. Reaching this goal is sometimes possible\r
for simple function approximators such as linear ones, but is rarely possible for complex\r
function approximators such as artificial neural networks and decision trees. Short of\r
this, complex function approximators may seek to converge instead to a local optimum,\r
a weight vector w⇤ for which VE(w⇤)  VE(w) for all w in some neighborhood of w⇤.\r
Although this guarantee is only slightly reassuring, it is typically the best that can be\r
said for nonlinear function approximators, and often it is enough. Still, for many cases of\r
interest in reinforcement learning there is no guarantee of convergence to an optimum, or\r
even to within a bounded distance of an optimum. Some methods may in fact diverge,\r
with their VE approaching infinity in the limit.\r
In the last two sections we outlined a framework for combining a wide range of\r
reinforcement learning methods for value prediction with a wide range of function\r
approximation methods, using the updates of the former to generate training examples\r
for the latter. We also described a VE performance measure which these methods may\r
aspire to minimize. The range of possible function approximation methods is far too\r
large to cover all, and anyway too little is known about most of them to make a reliable\r
evaluation or recommendation. Of necessity, we consider only a few possibilities. In\r
the rest of this chapter we focus on function approximation methods based on gradient\r
principles, and on linear gradient-descent methods in particular. We focus on these\r
methods in part because we consider them to be particularly promising and because they\r
reveal key theoretical issues, but also because they are simple and our space is limited.\r
9.3 Stochastic-gradient and Semi-gradient Methods\r
We now develop in detail one class of learning methods for function approximation in\r
value prediction, those based on stochastic gradient descent (SGD). SGD methods are\r
among the most widely used of all function approximation methods and are particularly\r
well suited to online reinforcement learning.\r
In gradient-descent methods, the weight vector is a column vector with a fixed number\r
of real valued components, w .= (w1, w2,...,wd)>,\r
1 and the approximate value function\r
vˆ(s,w) is a di↵erentiable function of w for all s 2 S. We will be updating w at each of\r
a series of discrete time steps, t = 0, 1, 2, 3,..., so we will need a notation wt for the\r
1The > denotes transpose, needed here to turn the horizontal row vector in the text into a vertical\r
column vector; in this book vectors are generally taken to be column vectors unless explicitly written out\r
horizontally or transposed.

9.3. Stochastic-gradient and Semi-gradient Methods 201\r
weight vector at each step. For now, let us assume that, on each step, we observe a new\r
example St 7! v⇡(St) consisting of a (possibly randomly selected) state St and its true\r
value under the policy. These states might be successive states from an interaction with\r
the environment, but for now we do not assume so. Even though we are given the exact,\r
correct values, v⇡(St) for each St, there is still a dicult problem because our function\r
approximator has limited resources and thus limited resolution. In particular, there is\r
generally no w that gets all the states, or even all the examples, exactly correct. In\r
addition, we must generalize to all the other states that have not appeared in examples.\r
We assume that states appear in examples with the same distribution, µ, over which\r
we are trying to minimize the VE as given by (9.1). A good strategy in this case is\r
to try to minimize error on the observed examples. Stochastic gradient-descent (SGD)\r
methods do this by adjusting the weight vector after each example by a small amount in\r
the direction that would most reduce the error on that example:\r
wt+1\r
.\r
= wt  1\r
2\r
↵r\r
h\r
v⇡(St)  vˆ(St,wt)\r
i2\r
(9.4)\r
= wt + ↵\r
h\r
v⇡(St)  vˆ(St,wt)\r
i\r
rvˆ(St,wt), (9.5)\r
where ↵ is a positive step-size parameter, and rf(w), for any scalar expression f(w)\r
that is a function of a vector (here w), denotes the column vector of partial derivatives\r
of the expression with respect to the components of the vector:\r
rf(w) .=\r
✓@f(w)\r
@w1\r
,\r
@f(w)\r
@w2\r
,..., @f(w)\r
@wd\r
◆>\r
. (9.6)\r
This derivative vector is the gradient of f with respect to w. SGD methods are “gradient\r
descent” methods because the overall step in wt is proportional to the negative gradient\r
of the example’s squared error (9.4). This is the direction in which the error falls most\r
rapidly. Gradient descent methods are called “stochastic” when the update is done, as\r
here, on only a single example, which might have been selected stochastically. Over many\r
examples, making small steps, the overall e↵ect is to minimize an average performance\r
measure such as the VE.\r
It may not be immediately apparent why SGD takes only a small step in the direction\r
of the gradient. Could we not move all the way in this direction and completely eliminate\r
the error on the example? In many cases this could be done, but usually it is not desirable.\r
Remember that we do not seek or expect to find a value function that has zero error for\r
all states, but only an approximation that balances the errors in di↵erent states. If we\r
completely corrected each example in one step, then we would not find such a balance.\r
In fact, the convergence results for SGD methods assume that ↵ decreases over time. If\r
it decreases in such a way as to satisfy the standard stochastic approximation conditions\r
(2.7), then the SGD method (9.5) is guaranteed to converge to a local optimum.\r
We turn now to the case in which the target output, here denoted Ut 2 R, of the tth\r
training example, St 7! Ut, is not the true value, v⇡(St), but some, possibly random,\r
approximation to it. For example, Ut might be a noise-corrupted version of v⇡(St), or it\r
might be one of the bootstrapping targets using vˆ mentioned in the previous section. In

202 Chapter 9: On-policy Prediction with Approximation\r
these cases we cannot perform the exact update (9.5) because v⇡(St) is unknown, but\r
we can approximate it by substituting Ut in place of v⇡(St). This yields the following\r
general SGD method for state-value prediction:\r
wt+1\r
.\r
= wt + ↵\r
h\r
Ut  vˆ(St,wt)\r
i\r
rvˆ(St,wt). (9.7)\r
If Ut is an unbiased estimate, that is, if E[Ut|St =s] = v⇡(s), for each t, then wt is\r
guaranteed to converge to a local optimum under the usual stochastic approximation\r
conditions (2.7) for decreasing ↵.\r
For example, suppose the states in the examples are the states generated by interaction\r
(or simulated interaction) with the environment using policy ⇡. Because the true value of\r
a state is the expected value of the return following it, the Monte Carlo target Ut\r
.\r
= Gt is\r
by definition an unbiased estimate of v⇡(St). With this choice, the general SGD method\r
(9.7) converges to a locally optimal approximation to v⇡(St). Thus, the gradient-descent\r
version of Monte Carlo state-value prediction is guaranteed to find a locally optimal\r
solution. Pseudocode for a complete algorithm is shown in the box below.\r
Gradient Monte Carlo Algorithm for Estimating vˆ ⇡ v⇡\r
Input: the policy ⇡ to be evaluated\r
Input: a di↵erentiable function ˆv : S ⇥ Rd ! R\r
Algorithm parameter: step size ↵ > 0\r
Initialize value-function weights w 2 Rd arbitrarily (e.g., w = 0)\r
Loop forever (for each episode):\r
Generate an episode S0, A0, R1, S1, A1,...,RT , ST using ⇡\r
Loop for each step of episode, t = 0, 1,...,T  1:\r
w w + ↵\r
⇥\r
Gt  vˆ(St,w)\r
⇤\r
rvˆ(St,w)\r
One does not obtain the same guarantees if a bootstrapping estimate of v⇡(St) is used\r
as the target Ut in (9.7). Bootstrapping targets such as n-step returns Gt:t+n or the DP\r
target P\r
a,s0,r ⇡(a|St)p(s0\r
, r|St, a)[r + vˆ(s0,wt)] all depend on the current value of the\r
weight vector wt, which implies that they will be biased and that they will not produce a\r
true gradient-descent method. One way to look at this is that the key step from (9.4)\r
to (9.5) relies on the target being independent of wt. This step would not be valid if\r
a bootstrapping estimate were used in place of v⇡(St). Bootstrapping methods are not\r
in fact instances of true gradient descent (Barnard, 1993). They take into account the\r
e↵ect of changing the weight vector wt on the estimate, but ignore its e↵ect on the target.\r
They include only a part of the gradient and, accordingly, we call them semi-gradient\r
methods.\r
Although semi-gradient (bootstrapping) methods do not converge as robustly as\r
gradient methods, they do converge reliably in important cases such as the linear case\r
discussed in the next section. Moreover, they o↵er important advantages that make them\r
often clearly preferred. One reason for this is that they typically enable significantly faster\r
learning, as we have seen in Chapters 6 and 7. Another is that they enable learning to

9.3. Stochastic-gradient and Semi-gradient Methods 203\r
be continual and online, without waiting for the end of an episode. This enables them to\r
be used on continuing problems and provides computational advantages. A prototypical\r
semi-gradient method is semi-gradient TD(0), which uses Ut\r
.\r
= Rt+1 + vˆ(St+1,w) as its\r
target. Complete pseudocode for this method is given in the box below.\r
Semi-gradient TD(0) for estimating vˆ ⇡ v⇡\r
Input: the policy ⇡ to be evaluated\r
Input: a di↵erentiable function ˆv : S+ ⇥ Rd ! R such that ˆv(terminal,·)=0\r
Algorithm parameter: step size ↵ > 0\r
Initialize value-function weights w 2 Rd arbitrarily (e.g., w = 0)\r
Loop for each episode:\r
Initialize S\r
Loop for each step of episode:\r
Choose A ⇠ ⇡(·|S)\r
Take action A, observe R, S0\r
w w + ↵\r
⇥\r
R + vˆ(S0,w)  vˆ(S,w)\r
⇤\r
rvˆ(S,w)\r
S S0\r
until S is terminal\r
State aggregation is a simple form of generalizing function approximation in which\r
states are grouped together, with one estimated value (one component of the weight\r
vector w) for each group. The value of a state is estimated as its group’s component,\r
and when the state is updated, that component alone is updated. State aggregation\r
is a special case of SGD (9.7) in which the gradient, rvˆ(St,wt), is 1 for St’s group’s\r
component and 0 for the other components.\r
Example 9.1: State Aggregation on the 1000-state Random Walk Consider a\r
1000-state version of the random walk task (Examples 6.2 and 7.1 on pages 125 and\r
144). The states are numbered from 1 to 1000, left to right, and all episodes begin near\r
the center, in state 500. State transitions are from the current state to one of the 100\r
neighboring states to its left, or to one of the 100 neighboring states to its right, all with\r
equal probability. Of course, if the current state is near an edge, then there may be fewer\r
than 100 neighbors on that side of it. In this case, all the probability that would have\r
gone into those missing neighbors goes into the probability of terminating on that side\r
(thus, state 1 has a 0.5 chance of terminating on the left, and state 950 has a 0.25 chance\r
of terminating on the right). As usual, termination on the left produces a reward of\r
1, and termination on the right produces a reward of +1. All other transitions have a\r
reward of zero. We use this task as a running example throughout this section.\r
Figure 9.1 shows the true value function v⇡ for this task. It is nearly a straight line,\r
curving very slightly toward the horizontal for the last 100 states at each end. Also shown\r
is the final approximate value function learned by the gradient Monte-Carlo algorithm\r
with state aggregation after 100,000 episodes with a step size of ↵ = 2 ⇥ 105. For the\r
state aggregation, the 1000 states were partitioned into 10 groups of 100 states each (i.e.,\r
states 1–100 were one group, states 101–200 were another, and so on). The staircase e↵ect

204 Chapter 9: On-policy Prediction with Approximation\r
0\r
State\r
Value\r
scale\r
 True \r
value v⇡\r
 Approximate \r
MC value vˆ\r
 State distribution \r
0.0017\r
0.0137\r
Distribution\r
scale\r
1 1000\r
0\r
-1\r
1\r
µ\r
Figure 9.1: Function approximation by state aggregation on the 1000-state random walk task,\r
using the gradient Monte Carlo algorithm (page 202).\r
shown in the figure is typical of state aggregation; within each group, the approximate\r
value is constant, and it changes abruptly from one group to the next. These approximate\r
values are close to the global minimum of the VE (9.1).\r
Some of the details of the approximate values are best appreciated by reference to\r
the state distribution µ for this task, shown in the lower portion of the figure with a\r
right-side scale. State 500, in the center, is the first state of every episode, but is rarely\r
visited again. On average, about 1.37% of the time steps are spent in the start state.\r
The states reachable in one step from the start state are the second most visited, with\r
about 0.17% of the time steps being spent in each of them. From there µ falls o↵ almost\r
linearly, reaching about 0.0147% at the extreme states 1 and 1000. The most visible\r
e↵ect of the distribution is on the leftmost groups, whose values are clearly shifted higher\r
than the unweighted average of the true values of states within the group, and on the\r
rightmost groups, whose values are clearly shifted lower. This is due to the states in\r
these areas having the greatest asymmetry in their weightings by µ. For example, in the\r
leftmost group, state 100 is weighted more than 3 times more strongly than state 1. Thus\r
the estimate for the group is biased toward the true value of state 100, which is higher\r
than the true value of state 1.\r
9.4 Linear Methods\r
One of the most important special cases of function approximation is that in which the\r
approximate function, vˆ(·,w), is a linear function of the weight vector, w. Corresponding\r
to every state s, there is a real-valued vector x(s) .= (x1(s), x2(s),...,xd(s))>, with the\r
same number of components as w. Linear methods approximate the state-value function

9.4. Linear Methods 205\r
by the inner product between w and x(s):\r
vˆ(s,w) .= w>x(s) .= X\r
d\r
i=1\r
wixi(s). (9.8)\r
In this case the approximate value function is said to be linear in the weights, or simply\r
linear.\r
The vector x(s) is called a feature vector representing state s. Each component xi(s)\r
of x(s) is the value of a function xi : S ! R. We think of a feature as the entirety of one\r
of these functions, and we call its value for a state s a feature of s. For linear methods,\r
features are basis functions because they form a linear basis for the set of approximate\r
functions. Constructing d-dimensional feature vectors to represent states is the same as\r
selecting a set of d basis functions. Features may be defined in many di↵erent ways; we\r
cover a few possibilities in the next sections.\r
It is natural to use SGD updates with linear function approximation. The gradient of\r
the approximate value function with respect to w in this case is\r
rvˆ(s,w) = x(s).\r
Thus, in the linear case the general SGD update (9.7) reduces to a particularly simple\r
form:\r
wt+1\r
.\r
= wt + ↵\r
h\r
Ut  vˆ(St,wt)\r
i\r
x(St).\r
Because it is so simple, the linear SGD case is one of the most favorable for mathematical\r
analysis. Almost all useful convergence results for learning systems of all kinds are for\r
linear (or simpler) function approximation methods.\r
In particular, in the linear case there is only one optimum (or, in degenerate cases,\r
one set of equally good optima), and thus any method that is guaranteed to converge to\r
or near a local optimum is automatically guaranteed to converge to or near the global\r
optimum. For example, the gradient Monte Carlo algorithm presented in the previous\r
section converges to the global optimum of the VE under linear function approximation\r
if ↵ is reduced over time according to the usual conditions.\r
The semi-gradient TD(0) algorithm presented in the previous section also converges\r
under linear function approximation, but this does not follow from general results on\r
SGD; a separate theorem is necessary. The weight vector converged to is also not the\r
global optimum, but rather a point near the local optimum. It is useful to consider this\r
important case in more detail, specifically for the continuing case. The update at each\r
time t is\r
wt+1\r
.\r
= wt + ↵\r
⇣\r
Rt+1 + w>\r
t xt+1  w>t xt\r
⌘\r
xt (9.9)\r
= wt + ↵\r
⇣\r
Rt+1xt  xt\r
\r
xt  xt+1>wt\r
⌘\r
,\r
where here we have used the notational shorthand xt = x(St). Once the system has\r
reached steady state, for any given wt, the expected next weight vector can be written\r
E[wt+1|wt] = wt + ↵(b  Awt), (9.10)

206 Chapter 9: On-policy Prediction with Approximation\r
where\r
b .= E[Rt+1xt] 2 Rd and A .= E\r
h\r
xt\r
\r
xt  xt+1>i2 Rd⇥d (9.11)\r
From (9.10) it is clear that, if the system converges, it must converge to the weight vector\r
wTD at which\r
b  AwTD = 0\r
) b = AwTD\r
) wTD\r
.\r
= A1b. (9.12)\r
This quantity is called the TD fixed point. In fact linear semi-gradient TD(0) converges\r
to this point. Some of the theory proving its convergence, and the existence of the inverse\r
above, is given in the box.\r
Proof of Convergence of Linear TD(0)\r
What properties assure convergence of the linear TD(0) algorithm (9.9)? Some\r
insight can be gained by rewriting (9.10) as\r
E[wt+1|wt]=(I  ↵A)wt + ↵b. (9.13)\r
Note that the matrix A multiplies the weight vector wt and not b; only A is\r
important to convergence. To develop intuition, consider the special case in which\r
A is a diagonal matrix. If any of the diagonal elements are negative, then the\r
corresponding diagonal element of I  ↵A will be greater than one, and the\r
corresponding component of wt will be amplified, which will lead to divergence if\r
continued. On the other hand, if the diagonal elements of A are all positive, then\r
↵ can be chosen smaller than one over the largest of them, such that I  ↵A is\r
diagonal with all diagonal elements between 0 and 1. In this case the first term\r
of the update tends to shrink wt, and stability is assured. In general, wt will be\r
reduced toward zero whenever A is positive definite, meaning y>Ay > 0 for any\r
real vector y 6= 0. Positive definiteness also ensures that the inverse A1 exists.\r
For linear TD(0), in the continuing case with  < 1, the A matrix (9.11) can be\r
written\r
A = X\r
s\r
µ(s)\r
X\r
a\r
⇡(a|s)\r
X\r
r,s0\r
p(r, s0|s, a)x(s)\r
\r
x(s)  x(s0)\r
>\r
= X\r
s\r
µ(s)\r
X\r
s0\r
p(s0|s)x(s)\r
\r
x(s)  x(s0)\r
>\r
= X\r
s\r
µ(s)x(s)\r
✓\r
x(s)  \r
X\r
s0\r
p(s0|s)x(s0)\r
◆>\r
= X>D(I  P)X,\r
where µ(s) is the stationary distribution under ⇡, p(s0 |s) is the probability of\r
transition from s to s0 under policy ⇡, P is the |S|⇥|S| matrix of these probabilities,

9.4. Linear Methods 207\r
D is the |S| ⇥ |S| diagonal matrix with the µ(s) on its diagonal, and X is the |S| ⇥ d\r
matrix with x(s) as its rows. From here it is clear that the inner matrix D(I  P)\r
is key to determining the positive definiteness of A.\r
For a key matrix of this form, positive definiteness is assured if all of its columns\r
sum to a nonnegative number. This was shown by Sutton (1988, p. 27) based\r
on two previously established theorems. One theorem says that any matrix M\r
is positive definite if and only if the symmetric matrix S = M + M> is positive\r
definite (Sutton 1988, appendix). The second theorem says that any symmetric\r
real matrix S is positive definite if all of its diagonal entries are positive and greater\r
than the sum of the absolute values of the corresponding o↵-diagonal entries (Varga\r
1962, p. 23). For our key matrix, D(I  P), the diagonal entries are positive\r
and the o↵-diagonal entries are negative, so all we have to show is that each row\r
sum plus the corresponding column sum is positive. The row sums are all positive\r
because P is a stochastic matrix and  < 1. Thus it only remains to show that\r
the column sums are nonnegative. Note that the row vector of the column sums\r
of any matrix M can be written as 1>M, where 1 is the column vector with all\r
components equal to 1. Let µ denote the |S|-vector of the µ(s), where µ = P>µ by\r
virtue of µ being the stationary distribution. The column sums of our key matrix,\r
then, are:\r
1>D(I  P) = µ>(I  P)\r
= µ>  µ>P\r
= µ>  µ> (because µ is the stationary distribution)\r
= (1  )µ>,\r
all components of which are positive. Thus, the key matrix and its A matrix\r
are positive definite, and on-policy TD(0) is stable. (Additional conditions and a\r
schedule for reducing ↵ over time are needed to prove convergence with probability\r
one.)\r
At the TD fixed point, it has also been proven (in the continuing case) that the VE is\r
within a bounded expansion of the lowest possible error:\r
VE(wTD) \r
1\r
1  \r
min\r
w VE(w). (9.14)\r
That is, the asymptotic error of the TD method is no more than 1\r
1\r
times the smallest\r
possible error, that attained in the limit by the Monte Carlo method. Because  is often\r
near one, this expansion factor can be quite large, so there is substantial potential loss in\r
asymptotic performance with the TD method. On the other hand, recall that the TD\r
methods are often of vastly reduced variance compared to Monte Carlo methods, and\r
thus faster, as we saw in Chapters 6 and 7. Which method will be best depends on the\r
nature of the approximation and problem, and on how long learning continues.

208 Chapter 9: On-policy Prediction with Approximation\r
A bound analogous to (9.14) applies to other on-policy bootstrapping methods as well.\r
For example, linear semi-gradient DP (Eq. 9.7 with Ut\r
.\r
= P\r
a ⇡(a|St)\r
P\r
s0,r p(s0\r
, r|St, a)[r+\r
vˆ(s0,wt)]) with updates according to the on-policy distribution will also converge to\r
the TD fixed point. One-step semi-gradient action-value methods, such as semi-gradient\r
Sarsa(0) covered in the next chapter converge to an analogous fixed point and an analogous\r
bound. For episodic tasks, there is a slightly di↵erent but related bound (see Bertsekas\r
and Tsitsiklis, 1996). There are also a few technical conditions on the rewards, features,\r
and decrease in the step-size parameter, which we have omitted here. The full details\r
can be found in the original paper (Tsitsiklis and Van Roy, 1997).\r
Critical to these convergence results is that states are updated according to the on\u0002policy distribution. For other update distributions, bootstrapping methods using function\r
approximation may actually diverge to infinity. Examples of this and a discussion of\r
possible solution methods are given in Chapter 11.\r
Example 9.2: Bootstrapping on the 1000-state Random Walk State aggregation\r
is a special case of linear function approximation, so let’s return to the 1000-state random\r
walk to illustrate some of the observations made in this chapter. The left panel of\r
Figure 9.2 shows the final value function learned by the semi-gradient TD(0) algorithm\r
(page 203) using the same state aggregation as in Example 9.1. We see that the near\u0002asymptotic TD approximation is indeed farther from the true values than the Monte\r
Carlo approximation shown in Figure 9.1.\r
Nevertheless, TD methods retain large potential advantages in learning rate, and\r
generalize Monte Carlo methods, as we investigated fully with n-step TD methods in\r
Chapter 7. The right panel of Figure 9.2 shows results with an n-step semi-gradient\r
TD method using state aggregation on the 1000-state random walk that are strikingly\r
similar to those we obtained earlier with tabular methods and the 19-state random\r
walk (Figure 7.2). To obtain such quantitatively similar results we switched the state\r
aggregation to 20 groups of 50 states each. The 20 groups were then quantitatively close\r
0.55\r
0.5\r
0.45\r
0.35\r
0.3\r
0.25\r
0.4\r
0 0.2 0.4 0.6 0.8 1\r
↵\r
Average\r
RMS error\r
over 1000 states\r
and first 10 \r
episodes\r
n=1\r
n=2 n=4 n=8\r
n=16\r
n=32\r
n=64\r
128 512256\r
State\r
 True \r
value v⇡\r
 Approximate \r
TD value\r
1\r
0\r
-1\r
1\r
1000\r
vˆ\r
Figure 9.2: Bootstrapping with state aggregation on the 1000-state random walk task. Left:\r
Asymptotic values of semi-gradient TD are worse than the asymptotic Monte Carlo values in\r
Figure 9.1. Right: Performance of n-step methods with state-aggregation are strikingly similar\r
to those with tabular representations (cf. Figure 7.2). These data are averages over 100 runs.

9.4. Linear Methods 209\r
to the 19 states of the tabular problem. In particular, recall that state transitions were\r
up to 100 states to the left or right. A typical transition would then be of 50 states to\r
the right or left, which is quantitatively analogous to the single-state state transitions of\r
the 19-state tabular system. To complete the match, we use here the same performance\r
measure—an unweighted average of the RMS error over all states and over the first\r
10 episodes—rather than a VE objective as is otherwise more appropriate when using\r
function approximation.\r
The semi-gradient n-step TD algorithm used in the example above is the natural\r
extension of the tabular n-step TD algorithm presented in Chapter 7 to semi-gradient\r
function approximation. Pseudocode is given in the box below.\r
n-step semi-gradient TD for estimating vˆ ⇡ v⇡\r
Input: the policy ⇡ to be evaluated\r
Input: a di↵erentiable function ˆv : S+ ⇥ Rd ! R such that ˆv(terminal,·)=0\r
Algorithm parameters: step size ↵ > 0, a positive integer n\r
Initialize value-function weights w arbitrarily (e.g., w = 0)\r
All store and access operations (St and Rt) can take their index mod n + 1\r
Loop for each episode:\r
Initialize and store S0 6= terminal\r
T 1\r
Loop for t = 0, 1, 2,... :\r
| If t<T, then:\r
| Take an action according to ⇡(·|St)\r
| Observe and store the next reward as Rt+1 and the next state as St+1\r
| If St+1 is terminal, then T t + 1\r
| ⌧ t  n +1 (⌧ is the time whose state’s estimate is being updated)\r
| If ⌧  0:\r
| G Pmin(⌧+n,T)\r
i=⌧+1 i⌧1Ri\r
| If ⌧ + n<T, then: G G + nvˆ(S⌧+n,w) (G⌧:⌧+n)\r
| w w + ↵ [G  vˆ(S⌧ ,w)] rvˆ(S⌧ ,w)\r
Until ⌧ = T  1\r
The key equation of this algorithm, analogous to (7.2), is\r
wt+n\r
.\r
= wt+n1 + ↵ [Gt:t+n  vˆ(St,wt+n1)] rvˆ(St,wt+n1), 0  t < T, (9.15)\r
where the n-step return is generalized from (7.1) to\r
Gt:t+n\r
.\r
= Rt+1 +Rt+2 +···+n1Rt+n +nvˆ(St+n,wt+n1), 0  t  T n. (9.16)\r
Exercise 9.1 Show that tabular methods such as presented in Part I of this book are a\r
special case of linear function approximation. What would the feature vectors be? ⇤

210 Chapter 9: On-policy Prediction with Approximation\r
9.5 Feature Construction for Linear Methods\r
Linear methods are interesting because of their convergence guarantees, but also because\r
in practice they can be very ecient in terms of both data and computation. Whether or\r
not this is so depends critically on how the states are represented in terms of features,\r
which we investigate in this large section. Choosing features appropriate to the task is\r
an important way of adding prior domain knowledge to reinforcement learning systems.\r
Intuitively, the features should correspond to the aspects of the state space along which\r
generalization may be appropriate. If we are valuing geometric objects, for example,\r
we might want to have features for each possible shape, color, size, or function. If we\r
are valuing states of a mobile robot, then we might want to have features for locations,\r
degrees of remaining battery power, recent sonar readings, and so on.\r
A limitation of the linear form is that it cannot take into account any interactions\r
between features, such as the presence of feature i being good only in the absence of\r
feature j. For example, in the pole-balancing task (Example 3.4), high angular velocity\r
can be either good or bad depending on the angle. If the angle is high, then high angular\r
velocity means an imminent danger of falling—a bad state—whereas if the angle is low,\r
then high angular velocity means the pole is righting itself—a good state. A linear value\r
function could not represent this if its features coded separately for the angle and the\r
angular velocity. It needs instead, or in addition, features for combinations of these two\r
underlying state dimensions. In the following subsections we consider a variety of general\r
ways of doing this.\r
9.5.1 Polynomials\r
The states of many problems are initially expressed as numbers, such as positions and\r
velocities in the pole-balancing task (Example 3.4), the number of cars in each lot in the\r
Jack’s car rental problem (Example 4.2), or the gambler’s capital in the gambler problem\r
(Example 4.3). In these types of problems, function approximation for reinforcement\r
learning has much in common with the familiar tasks of interpolation and regression.\r
Various families of features commonly used for interpolation and regression can also be\r
used in reinforcement learning. Polynomials make up one of the simplest families of\r
features used for interpolation and regression. While the basic polynomial features we\r
discuss here do not work as well as other types of features in reinforcement learning, they\r
serve as a good introduction because they are simple and familiar.\r
As an example, suppose a reinforcement learning problem has states with two numerical\r
dimensions. For a single representative state s, let its two numbers be s1 2 R and s2 2 R.\r
You might choose to represent s simply by its two state dimensions, so that x(s) =\r
(s1, s2)>, but then you would not be able to take into account any interactions between\r
these dimensions. In addition, if both s1 and s2 were zero, then the approximate value\r
would have to also be zero. Both limitations can be overcome by instead representing s by\r
the four-dimensional feature vector x(s) = (1, s1, s2, s1s2)>. The initial 1 feature allows\r
the representation of ane functions in the original state numbers, and the final product\r
feature, s1s2, enables interactions to be taken into account. Or you might choose to use\r
higher-dimensional feature vectors like x(s) = (1, s1, s2, s1s2, s2\r
1, s22, s1s22, s21s2, s21s22)> to

9.5. Feature Construction for Linear Methods 211\r
take more complex interactions into account. Such feature vectors enable approximations\r
as arbitrary quadratic functions of the state numbers—even though the approximation is\r
still linear in the weights that have to be learned. Generalizing this example from two\r
to k numbers, we can represent highly-complex interactions among a problem’s state\r
dimensions:\r
Suppose each state s corresponds to k numbers, s1, s2, ..., sk, with each si 2 R.\r
For this k-dimensional state space, each order-n polynomial-basis feature xi can be\r
written as\r
xi(s) = ⇧k\r
j=1s\r
ci,j\r
j , (9.17)\r
where each ci,j is an integer in the set {0, 1,...,n} for an integer n  0. These\r
features make up the order-n polynomial basis for dimension k, which contains\r
(n + 1)k di↵erent features.\r
Higher-order polynomial bases allow for more accurate approximations of more compli\u0002cated functions. But because the number of features in an order-n polynomial basis grows\r
exponentially with the dimension k of the natural state space (if n>0), it is generally\r
necessary to select a subset of them for function approximation. This can be done using\r
prior beliefs about the nature of the function to be approximated, and some automated\r
selection methods developed for polynomial regression can be adapted to deal with the\r
incremental and nonstationary nature of reinforcement learning.\r
Exercise 9.2 Why does (9.17) define (n + 1)k distinct features for dimension k? ⇤\r
Exercise 9.3 What n and ci,j produce the feature vectors x(s) = (1, s1, s2, s1s2, s2\r
1, s22,\r
s1s2\r
2, s21s2, s21s22)>? ⇤\r
9.5.2 Fourier Basis\r
Another linear function approximation method is based on the time-honored Fourier\r
series, which expresses periodic functions as weighted sums of sine and cosine basis\r
functions (features) of di↵erent frequencies. (A function f is periodic if f(x) = f(x + ⌧ )\r
for all x and some period ⌧ .) The Fourier series and the more general Fourier transform\r
are widely used in applied sciences in part because if a function to be approximated is\r
known, then the basis function weights are given by simple formulae and, further, with\r
enough basis functions essentially any function can be approximated as accurately as\r
desired. In reinforcement learning, where the functions to be approximated are unknown,\r
Fourier basis functions are of interest because they are easy to use and can perform well\r
in a range of reinforcement learning problems.\r
First consider the one-dimensional case. The usual Fourier series representation of a\r
function of one dimension having period ⌧ represents the function as a linear combination\r
of sine and cosine functions that are each periodic with periods that evenly divide ⌧ (in\r
other words, whose frequencies are integer multiples of a fundamental frequency 1/⌧ ).\r
But if you are interested in approximating an aperiodic function defined over a bounded

212 Chapter 9: On-policy Prediction with Approximation\r
interval, then you can use these Fourier basis features with ⌧ set to the length of the\r
interval. The function of interest is then just one period of the periodic linear combination\r
of the sine and cosine features.\r
Furthermore, if you set ⌧ to twice the length of the interval of interest and restrict\r
attention to the approximation over the half interval [0, ⌧/2], then you can use just the\r
cosine features. This is possible because you can represent any even function, that is,\r
any function that is symmetric about the origin, with just the cosine basis. So any\r
function over the half-period [0, ⌧/2] can be approximated as closely as desired with\r
enough cosine features. (Saying “any function” is not exactly correct because the function\r
has to be mathematically well-behaved, but we skip this technicality here.) Alternatively,\r
it is possible to use just sine features, linear combinations of which are always odd\r
functions, that is functions that are anti-symmetric about the origin. But it is generally\r
better to keep just the cosine features because “half-even” functions tend to be easier to\r
approximate than “half-odd” functions because the latter are often discontinuous at the\r
origin. Of course, this does not rule out using both sine and cosine features to approximate\r
over the interval [0, ⌧/2], which might be advantageous in some circumstances.\r
Following this logic and letting ⌧ = 2 so that the features are defined over the half-⌧\r
interval [0, 1], the one-dimensional order-n Fourier cosine basis consists of the n + 1\r
features\r
xi(s) = cos(i⇡s), s 2 [0, 1],\r
for i = 0,...,n. Figure 9.3 shows one-dimensional Fourier cosine features xi, for i =\r
1, 2, 3, 4; x0 is a constant function.\r
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 −1\r
−0.8\r
−0.6\r
−0.4\r
−0.2\r
0\r
0.2\r
0.4\r
0.6\r
0.8\r
1\r
Univariate Fourier Basis Function k=1\r
1\r
-1 0 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 −1\r
−0.8\r
−0.6\r
−0.4\r
−0.2\r
0\r
0.2\r
0.4\r
0.6\r
0.8\r
1\r
Univariate Fourier Basis Function k=2\r
1\r
-1 0 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 −1\r
−0.8\r
−0.6\r
−0.4\r
−0.2\r
0\r
0.2\r
0.4\r
0.6\r
0.8\r
1\r
Univariate Fourier Basis Function k=3\r
1\r
-1 0 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 −1\r
−0.8\r
−0.6\r
−0.4\r
−0.2\r
0\r
0.2\r
0.4\r
0.6\r
0.8\r
1\r
Univariate Fourier Basis Function k=4\r
1\r
-1 0 1\r
Figure 9.3: One-dimensional Fourier cosine-basis features xi, i = 1, 2, 3, 4, for approximating\r
functions over the interval [0, 1]. After Konidaris et al. (2011).\r
This same reasoning applies to the Fourier cosine series approximation in the multi\u0002dimensional case as described in the box below.\r
Suppose each state s corresponds to a vector of k numbers, s = (s1, s2, ..., sk)>,\r
with each si 2 [0, 1]. The ith feature in the order-n Fourier cosine basis can then\r
be written\r
xi(s) = cos ⇡s>ci\r
\r
, (9.18)\r
where ci = (ci\r
1,...,ci\r
k)>, with ci\r
j 2 {0,...,n} for j = 1,...,k and i = 1,...,(n+1)k.\r
This defines a feature for each of the (n + 1)k possible integer vectors ci. The inner

9.5. Feature Construction for Linear Methods 213\r
product s>ci has the e↵ect of assigning an integer in {0,...,n} to each dimension\r
of s. As in the one-dimensional case, this integer determines the feature’s frequency\r
along that dimension. The features can of course be shifted and scaled to suit the\r
bounded state space of a particular application.\r
As an example, consider the k = 2 case in which s = (s1, s2)>, where each ci = (ci\r
1, ci2)>.\r
Figure 9.4 shows a selection of six Fourier cosine features, each labeled by the vector ci\r
that defines it (s1 is the horizontal axis and ci is shown as a row vector with the index i\r
omitted). Any zero in c means the feature is constant along that state dimension. So if\r
c = (0, 0)>, the feature is constant over both dimensions; if c = (c1, 0)> the feature is\r
constant over the second dimension and varies over the first with frequency depending\r
on c1; and similarly, for c = (0, c2)>. When c = (c1, c2)> with neither cj = 0, the\r
feature varies along both dimensions and represents an interaction between the two state\r
variables. The values of c1 and c2 determine the frequency along each dimension, and\r
their ratio gives the direction of the interaction.\r
c = (0, 1)\r
1\r
1\r
0\r
0 1\r
1\r
0\r
0\r
c = (0, 1)> c = (1, 0)\r
1\r
1\r
0\r
0 1\r
1\r
0\r
0\r
c = (1, 0)> c = (1, 1)\r
1\r
1\r
0\r
0 1\r
1\r
0\r
0\r
c = (1, 1)>\r
c = (1, 5)\r
1\r
1\r
0\r
0 1\r
1\r
0\r
0\r
c = (0, 5)> c = (2, 5)\r
1\r
1\r
0\r
0 1\r
1\r
0\r
0\r
c = (2, 5)>\r
1\r
1\r
0\r
0\r
c = (5, 2)>\r
Figure 9.4: A selection of six two-dimensional Fourier cosine features, each labeled by the\r
vector ci that defines it (s1 is the horizontal axis, and ci is shown with the index i omitted).\r
After Konidaris et al. (2011).\r
When using Fourier cosine features with a learning algorithm such as (9.7), semi\u0002gradient TD(0), or semi-gradient Sarsa, it may be helpful to use a di↵erent step-size\r
parameter for each feature. If ↵ is the basic step-size parameter, then Konidaris, Osentoski,\r
and Thomas (2011) suggest setting the step-size parameter for feature xi to ↵i =\r
↵/\r
p(ci\r
1)2 + ··· + (ci\r
k)2 (except when each ci\r
j = 0, in which case ↵i = ↵).

214 Chapter 9: On-policy Prediction with Approximation\r
Fourier cosine features with Sarsa can produce good performance compared to several\r
other collections of basis functions, including polynomial and radial basis functions. Not\r
surprisingly, however, Fourier features have trouble with discontinuities because it is\r
dicult to avoid “ringing” around points of discontinuity unless very high frequency basis\r
functions are included.\r
The number of features in the order-n Fourier basis grows exponentially with the\r
dimension of the state space, but if that dimension is small enough (e.g., k  5), then\r
one can select n so that all of the order-n Fourier features can be used. This makes the\r
selection of features more-or-less automatic. For high dimension state spaces, however, it\r
is necessary to select a subset of these features. This can be done using prior beliefs about\r
the nature of the function to be approximated, and some automated selection methods\r
can be adapted to deal with the incremental and nonstationary nature of reinforcement\r
learning. An advantage of Fourier basis features in this regard is that it is easy to select\r
features by setting the ci vectors to account for suspected interactions among the state\r
variables and by limiting the values in the cj vectors so that the approximation can\r
filter out high frequency components considered to be noise. On the other hand, because\r
Fourier features are non-zero over the entire state space (with the few zeros excepted),\r
they represent global properties of states, which can make it dicult to find good ways\r
to represent local properties.\r
Figure 9.5 shows learning curves comparing the Fourier and polynomial bases on the\r
1000-state random walk example. In general, we do not recommend using polynomials\r
for online learning.2\r
.4\r
.3\r
.2\r
.1\r
0\r
0 5000\r
Episodes\r
Polynomial basis\r
Fourier basis\r
p\r
VE\r
averaged\r
over 30 runs\r
Figure 9.5: Fourier basis vs polynomials on the 1000-state random walk. Shown are learning\r
curves for the gradient Monte Carlo method with Fourier and polynomial bases of order 5, 10,\r
and 20. The step-size parameters were roughly optimized for each case: ↵ = 0.0001 for the\r
polynomial basis and ↵ = 0.00005 for the Fourier basis. The performance measure (y-axis) is\r
the root mean square value error (9.1).\r
2There are families of polynomials more complicated than those we have discussed, for example,\r
di↵erent families of orthogonal polynomials, and these might work better, but at present there is little\r
experience with them in reinforcement learning.

9.5. Feature Construction for Linear Methods 215\r
9.5.3 Coarse Coding\r
s0\r
s\r
Figure 9.6: Coarse coding. Generaliza\u0002tion from state s to state s0 depends on\r
the number of their features whose recep\u0002tive fields (in this case, circles) overlap.\r
These states have one feature in common,\r
so there will be slight generalization be\u0002tween them.\r
Consider a task in which the natural repre\u0002sentation of the state set is a continuous two\u0002dimensional space. One kind of representation for\r
this case is made up of features corresponding to\r
circles in state space, as shown to the right. If\r
the state is inside a circle, then the corresponding\r
feature has the value 1 and is said to be present;\r
otherwise the feature is 0 and is said to be absent.\r
This kind of 1–0-valued feature is called a binary\r
feature. Given a state, which binary features are\r
present indicate within which circles the state lies,\r
and thus coarsely code for its location. Represent\u0002ing a state with features that overlap in this way\r
(although they need not be circles or binary) is\r
known as coarse coding.\r
Assuming linear gradient-descent function ap\u0002proximation, consider the e↵ect of the size and\r
density of the circles. Corresponding to each cir\u0002cle is a single weight (a component of w) that is\r
a↵ected by learning. If we train at one state, a\r
point in the space, then the weights of all circles\r
intersecting that state will be a↵ected. Thus, by (9.8), the approximate value function\r
will be a↵ected at all states within the union of the circles, with a greater e↵ect the more\r
circles a point has “in common” with the state, as shown in Figure 9.6. If the circles are\r
small, then the generalization will be over a short distance, as in Figure 9.7 (left), whereas\r
if they are large, it will be over a large distance, as in Figure 9.7 (middle). Moreover,\r
a) Narrow generalization b) Broad generalization c) Asymmetric generalization\r
Figure 9.7: Generalization in linear function approximation methods is determined by the\r
sizes and shapes of the features’ receptive fields. All three of these cases have roughly the same\r
number and density of features.

216 Chapter 9: On-policy Prediction with Approximation\r
the shape of the features will determine the nature of the generalization. For example, if\r
they are not strictly circular, but are elongated in one direction, then generalization will\r
be similarly a↵ected, as in Figure 9.7 (right).\r
Features with large receptive fields give broad generalization, but might also seem to\r
limit the learned function to a coarse approximation, unable to make discriminations\r
much finer than the width of the receptive fields. Happily, this is not the case. Initial\r
generalization from one point to another is indeed controlled by the size and shape of\r
the receptive fields, but acuity, the finest discrimination ultimately possible, is controlled\r
more by the total number of features.\r
Example 9.3: Coarseness of Coarse Coding This example illustrates the e↵ect on\r
learning of the size of the receptive fields in coarse coding. Linear function approximation\r
based on coarse coding and (9.7) was used to learn a one-dimensional square-wave function\r
(shown at the top of Figure 9.8). The values of this function were used as the targets, Ut.\r
With just one dimension, the receptive fields were intervals rather than circles. Learning\r
was repeated with three di↵erent sizes of the intervals: narrow, medium, and broad, as\r
shown at the bottom of the figure. All three cases had the same density of features,\r
about 50 over the extent of the function being learned. Training examples were generated\r
uniformly at random over this extent. The step-size parameter was ↵ = 0.2\r
n , where n is\r
the number of features that were present at one time. Figure 9.8 shows the functions\r
learned in all three cases over the course of learning. Note that the width of the features\r
had a strong e↵ect early in learning. With broad features, the generalization tended to be\r
broad; with narrow features, only the close neighbors of each trained point were changed,\r
causing the function learned to be more bumpy. However, the final function learned was\r
a↵ected only slightly by the width of the features. Receptive field shape tends to have a\r
strong e↵ect on generalization but little e↵ect on asymptotic solution quality.\r
10\r
40\r
160\r
640\r
2560\r
10240\r
Narrow features\r
desired\r
function\r
Medium features Broad\r
features\r
#Examples approx\u0002imation\r
feature\r
width\r
Figure 9.8: Example of feature width’s strong e↵ect on initial generalization (first row) and\r
weak e↵ect on asymptotic accuracy (last row).

9.5. Feature Construction for Linear Methods 217\r
9.5.4 Tile Coding\r
Tile coding is a form of coarse coding for multi-dimensional continuous spaces that is\r
flexible and computationally ecient. It may be the most practical feature representation\r
for modern sequential digital computers.\r
In tile coding the receptive fields of the features are grouped into partitions of the state\r
space. Each such partition is called a tiling, and each element of the partition is called a\r
tile. For example, the simplest tiling of a two-dimensional state space is a uniform grid\r
such as that shown on the left side of Figure 9.9. The tiles or receptive field here are\r
squares rather than the circles in Figure 9.6. If just this single tiling were used, then the\r
state indicated by the white spot would be represented by the single feature whose tile\r
it falls within; generalization would be complete to all states within the same tile and\r
nonexistent to states outside it. With just one tiling, we would not have coarse coding\r
but just a case of state aggregation.\r
Point in \r
state space\r
to be\r
represented\r
Tiling 1\r
Tiling 2\r
Tiling 3\r
Tiling 4 Continuous \r
2D state \r
space\r
Four active\r
tiles/features \r
overlap the point\r
and are used to \r
represent it\r
Figure 9.9: Multiple, overlapping grid-tilings on a limited two-dimensional space. These tilings\r
are o↵set from one another by a uniform amount in each dimension.\r
To get the strengths of coarse coding requires overlapping receptive fields, and by\r
definition the tiles of a partition do not overlap. To get true coarse coding with tile coding,\r
multiple tilings are used, each o↵set by a fraction of a tile width. A simple case with\r
four tilings is shown on the right side of Figure 9.9. Every state, such as that indicated\r
by the white spot, falls in exactly one tile in each of the four tilings. These four tiles\r
correspond to four features that become active when the state occurs. Specifically, the\r
feature vector x(s) has one component for each tile in each tiling. In this example there\r
are 4 ⇥ 4 ⇥ 4 = 64 components, all of which will be 0 except for the four corresponding to\r
the tiles that s falls within. Figure 9.10 shows the advantage of multiple o↵set tilings\r
(coarse coding) over a single tiling on the 1000-state random walk example.\r
An immediate practical advantage of tile coding is that, because it works with partitions,\r
the overall number of features that are active at one time is the same for any state.\r
Exactly one feature is present in each tiling, so the total number of features present is\r
always the same as the number of tilings. This allows the step-size parameter, ↵, to

218 Chapter 9: On-policy Prediction with Approximation\r
.4\r
.3\r
.2\r
.1\r
0\r
averaged\r
over 30 runs\r
0 5000\r
Episodes\r
State aggregation\r
(one tiling)\r
Tile coding (50 tilings)\r
p\r
VE\r
Figure 9.10: Why we use coarse coding. Shown are learning curves on the 1000-state random\r
walk example for the gradient Monte Carlo algorithm with a single tiling and with multiple\r
tilings. The space of 1000 states was treated as a single continuous dimension, covered with tiles\r
each 200 states wide. The multiple tilings were o↵set from each other by 4 states. The step-size\r
parameter was set so that the initial learning rate in the two cases was the same, ↵ = 0.0001 for\r
the single tiling and ↵ = 0.0001/50 for the 50 tilings.\r
be set in an easy, intuitive way. For example, choosing ↵ = 1\r
n , where n is the number\r
of tilings, results in exact one-trial learning. If the example s 7! v is trained on, then\r
whatever the prior estimate, vˆ(s,wt), the new estimate will be vˆ(s,wt+1) = v. Usually\r
one wishes to change more slowly than this, to allow for generalization and stochastic\r
variation in target outputs. For example, one might choose ↵ = 1\r
10n , in which case the\r
estimate for the trained state would move one-tenth of the way to the target in one\r
update, and neighboring states will be moved less, proportional to the number of tiles\r
they have in common.\r
Tile coding also gains computational advantages from its use of binary feature vectors.\r
Because each component is either 0 or 1, the weighted sum making up the approximate\r
value function (9.8) is almost trivial to compute. Rather than performing d multiplications\r
and additions, one simply computes the indices of the n ⌧ d active features and then\r
adds up the n corresponding components of the weight vector.\r
Generalization occurs to states other than the one trained if those states fall within\r
any of the same tiles, proportional to the number of tiles in common. Even the choice of\r
how to o↵set the tilings from each other a↵ects generalization. If they are o↵set uniformly\r
in each dimension, as they were in Figure 9.9, then di↵erent states can generalize in\r
qualitatively di↵erent ways, as shown in the upper half of Figure 9.11. Each of the eight\r
subfigures show the pattern of generalization from a trained state to nearby points. In this\r
example there are eight tilings, thus 64 subregions within a tile that generalize distinctly,\r
but all according to one of these eight patterns. Note how uniform o↵sets result in a\r
strong e↵ect along the diagonal in many patterns. These artifacts can be avoided if the\r
tilings are o↵set asymmetrically, as shown in the lower half of the figure. These lower\r
generalization patterns are better because they are all well centered on the trained state\r
with no obvious asymmetries.

9.5. Feature Construction for Linear Methods 219\r
Possible \r
generalizations \r
for uniformly \r
offset tilings\r
Possible \r
generalizations\r
for asymmetrically \r
offset tilings\r
Figure 9.11: Why tile asymmetrical o↵sets are preferred in tile coding. Shown is the strength\r
of generalization from a trained state, indicated by the small black plus, to nearby states, for the\r
case of eight tilings. If the tilings are uniformly o↵set (above), then there are diagonal artifacts\r
and substantial variations in the generalization, whereas with asymmetrically o↵set tilings the\r
generalization is more spherical and homogeneous.\r
Tilings in all cases are o↵set from each other by a fraction of a tile width in each\r
dimension. If w denotes the tile width and n the number of tilings, then w\r
n is a fundamental\r
unit. Within small squares w\r
n on a side, all states activate the same tiles, have the same\r
feature representation, and the same approximated value. If a state is moved by w\r
n\r
in any cartesian direction, the feature representation changes by one component/tile.\r
Uniformly o↵set tilings are o↵set from each other by exactly this unit distance. For a\r
two-dimensional space, we say that each tiling is o↵set by the displacement vector (1, 1),\r
meaning that it is o↵set from the previous tiling by w\r
n times this vector. In these terms,\r
the asymmetrically o↵set tilings shown in the lower part of Figure 9.11 are o↵set by a\r
displacement vector of (1, 3).\r
Extensive studies have been made of the e↵ect of di↵erent displacement vectors on the\r
generalization of tile coding (Parks and Militzer, 1991; An, 1991; An, Miller and Parks,

220 Chapter 9: On-policy Prediction with Approximation\r
1991; Miller, An, Glanz and Carter, 1990), assessing their homegeneity and tendency\r
toward diagonal artifacts like those seen for the (1, 1) displacement vectors. Based on this\r
work, Miller and Glanz (1996) recommend using displacement vectors consisting of the\r
first odd integers. In particular, for a continuous space of dimension k, a good choice is\r
to use the first odd integers (1, 3, 5, 7,..., 2k  1), with n (the number of tilings) set to an\r
integer power of 2 greater than or equal to 4k. This is what we have done to produce the\r
tilings in the lower half of Figure 9.11, in which k = 2, n = 23  4k, and the displacement\r
vector is (1, 3). In a three-dimensional case, the first four tilings would be o↵set in total\r
from a base position by (0, 0, 0), (1, 3, 5), (2, 6, 10), and (3, 9, 15). Open-source software\r
that can eciently make tilings like this for any k is readily available.\r
In choosing a tiling strategy, one has to pick the number of the tilings and the shape of\r
the tiles. The number of tilings, along with the size of the tiles, determines the resolution\r
or fineness of the asymptotic approximation, as in general coarse coding and illustrated\r
in Figure 9.8. The shape of the tiles will determine the nature of generalization as in\r
Figure 9.7. Square tiles will generalize roughly equally in each dimension as indicated in\r
Figure 9.11 (lower). Tiles that are elongated along one dimension, such as the stripe tilings\r
in Figure 9.12 (middle), will promote generalization along that dimension. The tilings in\r
Figure 9.12 (middle) are also denser and thinner on the left, promoting discrimination\r
along the horizontal dimension at lower values along that dimension. The diagonal stripe\r
tiling in Figure 9.12 (right) will promote generalization along one diagonal. In higher\r
dimensions, axis-aligned stripes correspond to ignoring some of the dimensions in some\r
of the tilings, that is, to hyperplanar slices. Irregular tilings such as shown in Figure 9.12\r
(left) are also possible, though rare in practice and beyond the standard software.\r
a) Irregular b) Log stripes c) Diagonal stripes\r
Figure 9.12: Tilings need not be grids. They can be arbitrarily shaped and non-uniform, while\r
still in many cases being computationally ecient to compute.\r
In practice, it is often desirable to use di↵erent shaped tiles in di↵erent tilings. For\r
example, one might use some vertical stripe tilings and some horizontal stripe tilings.\r
This would encourage generalization along either dimension. However, with stripe tilings\r
alone it is not possible to learn that a particular conjunction of horizontal and vertical\r
coordinates has a distinctive value (whatever is learned for it will bleed into states with the\r
same horizontal and vertical coordinates). For this one needs the conjunctive rectangular\r
tiles such as originally shown in Figure 9.9. With multiple tilings—some horizontal, some\r
vertical, and some conjunctive—one can get everything: a preference for generalizing\r
along each dimension, yet the ability to learn specific values for conjunctions (see Sutton,

9.5. Feature Construction for Linear Methods 221\r
1996 for examples). The choice of tilings determines generalization, and until this choice\r
can be e↵ectively automated, it is important that tile coding enables the choice to be\r
made flexibly and in a way that makes sense to people.\r
Another useful trick for reducing memory requirements is hashing—a consistent pseudo\u0002random collapsing of a large tiling into a much smaller set of tiles. Hashing produces\r
tiles consisting of noncontiguous, disjoint regions randomly spread throughout the state\r
one\r
tile\r
space, but that still form an exhaustive partition. For example,\r
one tile might consist of the four subtiles shown to the right.\r
Through hashing, memory requirements are often reduced by\r
large factors with little loss of performance. This is possible\r
because high resolution is needed in only a small fraction of the\r
state space. Hashing frees us from the curse of dimensionality\r
in the sense that memory requirements need not be exponential\r
in the number of dimensions, but need merely match the real\r
demands of the task. Open-source implementations of tile coding\r
commonly include ecient hashing.\r
Exercise 9.4 Suppose we believe that one of two state dimensions is more likely to have\r
an e↵ect on the value function than is the other, that generalization should be primarily\r
across this dimension rather than along it. What kind of tilings could be used to take\r
advantage of this prior knowledge? ⇤\r
9.5.5 Radial Basis Functions\r
Radial basis functions (RBFs) are the natural generalization of coarse coding to continuous\u0002valued features. Rather than each feature being either 0 or 1, it can be anything in the\r
interval [0, 1], reflecting various degrees to which the feature is present. A typical RBF\r
feature, xi, has a Gaussian (bell-shaped) response xi(s) dependent only on the distance\r
between the state, s, and the feature’s prototypical or center state, ci, and relative to the\r
feature’s width, i:\r
xi(s) .= exp ✓||s  ci||2\r
22\r
i\r
◆\r
.\r
The norm or distance metric of course can be chosen in whatever way seems most\r
appropriate to the states and task at hand. The figure below shows a one-dimensional\r
example with a Euclidean distance metric.\r
ci\r
!i\r
ci+1 ci-1\r
Figure 9.13: One-dimensional radial basis functions.

222 Chapter 9: On-policy Prediction with Approximation\r
The primary advantage of RBFs over binary features is that they produce approximate\r
functions that vary smoothly and are di↵erentiable. Although this is appealing, in most\r
cases it has no practical significance. Nevertheless, extensive studies have been made of\r
graded response functions such as RBFs in the context of tile coding (An, 1991; Miller et\r
al., 1991; An et al., 1991; Lane, Handelman and Gelfand, 1992). All of these methods\r
require substantial additional computational complexity (over tile coding) and often\r
reduce performance when there are more than two state dimensions. In high dimensions\r
the edges of tiles are much more important, and it has proven dicult to obtain well\r
controlled graded tile activations near the edges.\r
An RBF network is a linear function approximator using RBFs for its features. Learning\r
is defined by equations (9.7) and (9.8), exactly as in other linear function approximators.\r
In addition, some learning methods for RBF networks change the centers and widths of\r
the features as well, bringing them into the realm of nonlinear function approximators.\r
Nonlinear methods may be able to fit target functions much more precisely. The downside\r
to RBF networks, and to nonlinear RBF networks especially, is greater computational\r
complexity and, often, more manual tuning before learning is robust and ecient.\r
9.6 Selecting Step-Size Parameters Manually\r
Most SGD methods require the designer to select an appropriate step-size parameter ↵.\r
Ideally this selection would be automated, and in some cases it has been, but for most\r
cases it is still common practice to set it manually. To do this, and to better understand\r
the algorithms, it is useful to develop some intuitive sense of the role of the step-size\r
parameter. Can we say in general how it should be set?\r
Theoretical considerations are unfortunately of little help. The theory of stochastic\r
approximation gives us conditions (2.7) on a slowly decreasing step-size sequence that are\r
sucient to guarantee convergence, but these tend to result in learning that is too slow.\r
The classical choice ↵t = 1/t, which produces sample averages in tabular MC methods, is\r
not appropriate for TD methods, for nonstationary problems, or for any method using\r
function approximation. For linear methods, there are recursive least-squares methods\r
that set an optimal matrix step size, and these methods can be extended to temporal\u0002di↵erence learning as in the LSTD method described in Section 9.8, but these require\r
O(d2) step-size parameters, or d times more parameters than we are learning. For this\r
reason we rule them out for use on large problems where function approximation is most\r
needed.\r
To get some intuitive feel for how to set the step-size parameter manually, it is best\r
to go back momentarily to the tabular case. There we can understand that a step size\r
of ↵ = 1 will result in a complete elimination of the sample error after one target (see\r
(2.4) with a step size of one). As discussed on page 201, we usually want to learn slower\r
than this. In the tabular case, a step size of ↵ = 1\r
10 would take about 10 experiences to\r
converge approximately to their mean target, and if we wanted to learn in 100 experiences\r
we would use ↵ = 1\r
100 . In general, if ↵ = 1⌧ , then the tabular estimate for a state will\r
approach the mean of its targets, with the most recent targets having the greatest e↵ect,\r
after about ⌧ experiences with the state.

9.7. Nonlinear Function Approximation: Artificial Neural Networks 223\r
With general function approximation there is not such a clear notion of number of\r
experiences with a state, as each state may be similar to and dissimilar from all the others\r
to various degrees. However, there is a similar rule that gives similar behavior in the case\r
of linear function approximation. Suppose you wanted to learn in about ⌧ experiences\r
with substantially the same feature vector. A good rule of thumb for setting the step-size\r
parameter of linear SGD methods is then\r
↵ .= \r
⌧E\r
⇥\r
x>x\r
⇤1\r
, (9.19)\r
where x is a random feature vector chosen from the same distribution as input vectors\r
will be in the SGD. This method works best if the feature vectors do not vary greatly in\r
length; ideally x>x is a constant.\r
Exercise 9.5 Suppose you are using tile coding to transform a seven-dimensional continuous\r
state space into binary feature vectors to estimate a state value function vˆ(s,w) ⇡ v⇡(s).\r
You believe that the dimensions do not interact strongly, so you decide to use eight tilings\r
of each dimension separately (stripe tilings), for 7 ⇥ 8 = 56 tilings. In addition, in case\r
there are some pairwise interactions between the dimensions, you also take all 7\r
2\r
\r
= 21\r
pairs of dimensions and tile each pair conjunctively with rectangular tiles. You make\r
two tilings for each pair of dimensions, making a grand total of 21 ⇥ 2 + 56 = 98 tilings.\r
Given these feature vectors, you suspect that you still have to average out some noise,\r
so you decide that you want learning to be gradual, taking about 10 presentations with\r
the same feature vector before learning nears its asymptote. What step-size parameter ↵\r
should you use? Why? ⇤\r
Exercise 9.6 If ⌧ = 1 and x(St)>x(St) = E\r
⇥\r
x>x\r
⇤\r
, prove that (9.19) together with (9.7)\r
and linear function approximation results in the error being reduced to zero in one update.\r
9.7 Nonlinear Function Approximation:\r
Artificial Neural Networks\r
Artificial neural networks (ANNs) are widely used for nonlinear function approximation.\r
An ANN is a network of interconnected units that have some of the properties of neurons,\r
the main components of nervous systems. ANNs have a long history, with the latest\r
advances in training deeply-layered ANNs (deep learning) being responsible for some\r
of the most impressive abilities of machine learning systems, including reinforcement\r
learning systems. In Chapter 16 we describe several impressive examples of reinforcement\r
learning systems that use ANN function approximation.\r
Figure 9.14 shows a generic feedforward ANN, meaning that there are no loops in the\r
network, that is, there are no paths within the network by which a unit’s output can\r
influence its input. The network in the figure has an output layer consisting of two output\r
units, an input layer with four input units, and two “hidden layers”: layers that are neither\r
input nor output layers. A real-valued weight is associated with each link. A weight\r
roughly corresponds to the ecacy of a synaptic connection in a real neural network (see\r
Section 15.1). If an ANN has at least one loop in its connections, it is a recurrent rather

224 Chapter 9: On-policy Prediction with Approximation\r
Figure 9.14: A generic feedforward ANN with four input units, two output units, and two\r
hidden layers.\r
than a feedforward ANN. Although both feedforward and recurrent ANNs have been\r
used in reinforcement learning, here we look only at the simpler feedforward case.\r
The units (the circles in Figure 9.14) are typically semi-linear units, meaning that they\r
compute a weighted sum of their input signals and then apply to the result a nonlinear\r
function, called the activation function, to produce the unit’s output, or activation.\r
Di↵erent activation functions are used, but they are typically S-shaped, or sigmoid,\r
functions such as the logistic function f(x)=1/(1 + ex), though sometimes the rectifier\r
nonlinearity f(x) = max(0, x) is used. A step function like f(x) = 1 if x  ✓, and 0\r
otherwise, results in a binary unit with threshold ✓. The units in a network’s input layer\r
are somewhat di↵erent in having their activations set to externally-supplied values that\r
are the inputs to the function the network is approximating.\r
The activation of each output unit of a feedforward ANN is a nonlinear function of the\r
activation patterns over the network’s input units. The functions are parameterized by\r
the network’s connection weights. An ANN with no hidden layers can represent only a\r
very small fraction of the possible input-output functions. However an ANN with a single\r
hidden layer containing a large enough finite number of sigmoid units can approximate\r
any continuous function on a compact region of the network’s input space to any degree\r
of accuracy (Cybenko, 1989). This is also true for other nonlinear activation functions\r
that satisfy mild conditions, but nonlinearity is essential: if all the units in a multi-layer\r
feedforward ANN have linear activation functions, the entire network is equivalent to a\r
network with no hidden layers (because linear functions of linear functions are themselves\r
linear).\r
Despite this “universal approximation” property of one-hidden-layer ANNs, both\r
experience and theory show that approximating the complex functions needed for many\r
artificial intelligence tasks is made easier—indeed may require—abstractions that are\r
hierarchical compositions of many layers of lower-level abstractions, that is, abstractions

9.7. Nonlinear Function Approximation: Artificial Neural Networks 225\r
produced by deep architectures such as ANNs with many hidden layers. (See Bengio,\r
2009, for a thorough review.) The successive layers of a deep ANN compute increasingly\r
abstract representations of the network’s “raw” input, with each unit providing a feature\r
contributing to a hierarchical representation of the overall input-output function of the\r
network.\r
Training the hidden layers of an ANN is therefore a way to automatically create\r
features appropriate for a given problem so that hierarchical representations can be\r
produced without relying exclusively on hand-crafted features. This has been an enduring\r
challenge for artificial intelligence and explains why learning algorithms for ANNs with\r
hidden layers have received so much attention over the years. ANNs typically learn by a\r
stochastic gradient method (Section 9.3). Each weight is adjusted in a direction aimed at\r
improving the network’s overall performance as measured by an objective function to\r
be either minimized or maximized. In the most common supervised learning case, the\r
objective function is the expected error, or loss, over a set of labeled training examples. In\r
reinforcement learning, ANNs can use TD errors to learn value functions, or they can aim\r
to maximize expected reward as in a gradient bandit (Section 2.8) or a policy-gradient\r
algorithm (Chapter 13). In all of these cases it is necessary to estimate how a change\r
in each connection weight would influence the network’s overall performance, in other\r
words, to estimate the partial derivative of an objective function with respect to each\r
weight, given the current values of all the network’s weights. The gradient is the vector\r
of these partial derivatives.\r
The most successful way to do this for ANNs with hidden layers (provided the units\r
have di↵erentiable activation functions) is the backpropagation algorithm, which consists\r
of alternating forward and backward passes through the network. Each forward pass\r
computes the activation of each unit given the current activations of the network’s input\r
units. After each forward pass, a backward pass eciently computes a partial derivative\r
for each weight. (As in other stochastic gradient learning algorithms, the vector of these\r
partial derivatives is an estimate of the true gradient.) In Section 15.10 we discuss\r
methods for training ANNs with hidden layers that use reinforcement learning principles\r
instead of backpropagation. These methods are less ecient than the backpropagation\r
algorithm, but they may be closer to how real neural networks learn.\r
The backpropagation algorithm can produce good results for shallow networks having\r
1 or 2 hidden layers, but it may not work well for deeper ANNs. In fact, training a\r
network with k + 1 hidden layers can actually result in poorer performance than training\r
a network with k hidden layers, even though the deeper network can represent all the\r
functions that the shallower network can (Bengio, 2009). Explaining results like these\r
is not easy, but several factors are important. First, the large number of weights in\r
a typical deep ANN makes it dicult to avoid the problem of overfitting, that is, the\r
problem of failing to generalize correctly to cases on which the network has not been\r
trained. Second, backpropagation does not work well for deep ANNs because the partial\r
derivatives computed by its backward passes either decay rapidly toward the input side\r
of the network, making learning by deep layers extremely slow, or the partial derivatives\r
grow rapidly toward the input side of the network, making learning unstable. Methods\r
for dealing with these problems are largely responsible for many impressive recent results

226 Chapter 9: On-policy Prediction with Approximation\r
achieved by systems that use deep ANNs.\r
Overfitting is a problem for any function approximation method that adjusts functions\r
with many degrees of freedom on the basis of limited training data. It is less of a\r
problem for online reinforcement learning that does not rely on limited training sets, but\r
generalizing e↵ectively is still an important issue. Overfitting is a problem for ANNs in\r
general, but especially so for deep ANNs because they tend to have very large numbers\r
of weights. Many methods have been developed for reducing overfitting. These include\r
stopping training when performance begins to decrease on validation data di↵erent\r
from the training data (cross validation), modifying the objective function to discourage\r
complexity of the approximation (regularization), and introducing dependencies among\r
the weights to reduce the number of degrees of freedom (e.g., weight sharing).\r
A particularly e↵ective method for reducing overfitting by deep ANNs is the dropout\r
method introduced by Srivastava, Hinton, Krizhevsky, Sutskever, and Salakhutdinov\r
(2014). During training, units are randomly removed from the network (dropped out)\r
along with their connections. This can be thought of as training a large number of\r
“thinned” networks. Combining the results of these thinned networks at test time is a way\r
to improve generalization performance. The dropout method eciently approximates this\r
combination by multiplying each outgoing weight of a unit by the probability that that\r
unit was retained during training. Srivastava et al. found that this method significantly\r
improves generalization performance. It encourages individual hidden units to learn\r
features that work well with random collections of other features. This increases the\r
versatility of the features formed by the hidden units so that the network does not overly\r
specialize to rarely-occurring cases.\r
Hinton, Osindero, and Teh (2006) took a major step toward solving the problem of\r
training the deep layers of a deep ANN in their work with deep belief networks, layered\r
networks closely related to the deep ANNs discussed here. In their method, the deepest\r
layers are trained one at a time using an unsupervised learning algorithm. Without\r
relying on the overall objective function, unsupervised learning can extract features that\r
capture statistical regularities of the input stream. The deepest layer is trained first, then\r
with input provided by this trained layer, the next deepest layer is trained, and so on,\r
until the weights in all, or many, of the network’s layers are set to values that now act as\r
initial values for supervised learning. The network is then fine-tuned by backpropagation\r
with respect to the overall objective function. Studies show that this approach generally\r
works much better than backpropagation with weights initialized with random values.\r
The better performance of networks trained with weights initialized this way could be\r
due to many factors, but one idea is that this method places the network in a region of\r
weight space from which a gradient-based algorithm can make good progress.\r
Batch normalization (Io↵e and Szegedy, 2015) is another technique that makes it easier\r
to train deep ANNs. It has long been known that ANN learning is easier if the network\r
input is normalized, for example, by adjusting each input variable to have zero mean and\r
unit variance. Batch normalization for training deep ANNs normalizes the output of deep\r
layers before they feed into the following layer. Io↵e and Szegedy (2015) used statistics\r
from subsets, or “mini-batches,” of training examples to normalize these between-layer\r
signals to improve the learning rate of deep ANNs.

9.7. Nonlinear Function Approximation: Artificial Neural Networks 227\r
Another technique useful for training deep ANNs is deep residual learning (He, Zhang,\r
Ren, and Sun, 2016). Sometimes it is easier to learn how a function di↵ers from the\r
identity function than to learn the function itself. Then adding this di↵erence, or residual\r
function, to the input produces the desired function. In deep ANNs, a block of layers\r
can be made to learn a residual function simply by adding shortcut, or skip, connections\r
around the block. These connections add the input to the block to its output, and\r
no additional weights are needed. He et al. (2016) evaluated this method using deep\r
convolutional networks with skip connections around every pair of adjacent layers, finding\r
substantial improvement over networks without the skip connections on benchmark image\r
classification tasks. Both batch normalization and deep residual learning were used in\r
the reinforcement learning application to the game of Go that we describe in Chapter 16.\r
A type of deep ANN that has proven to be very successful in applications, including\r
impressive reinforcement learning applications (Chapter 16), is the deep convolutional\r
network. This type of network is specialized for processing high-dimensional data arranged\r
in spatial arrays, such as images. It was inspired by how early visual processing works in\r
the brain (LeCun, Bottou, Bengio and Ha↵ner, 1998). Because of its special architecture,\r
a deep convolutional network can be trained by backpropagation without resorting to\r
methods like those described above to train the deep layers.\r
Figure 9.15 illustrates the architecture of a deep convolutional network. This instance,\r
from LeCun et al. (1998), was designed to recognize hand-written characters. It consists\r
of alternating convolutional and subsampling layers, followed by several fully connected\r
final layers. Each convolutional layer produces a number of feature maps. A feature\r
map is a pattern of activity over an array of units, where each unit performs the same\r
operation on data in its receptive field, which is the part of the data it “sees” from the\r
preceding layer (or from the external input in the case of the first convolutional layer).\r
The units of a feature map are identical to one another except that their receptive fields,\r
which are all the same size and shape, are shifted to di↵erent locations on the arrays\r
of incoming data. Units in the same feature map share the same weights. This means\r
that a feature map detects the same feature no matter where it is located in the input\r
Figure 9.15: Deep Convolutional Network. Republished with permission of Proceedings of the\r
IEEE, from Gradient-based learning applied to document recognition, LeCun, Bottou, Bengio,\r
and Ha↵ner, volume 86, 1998; permission conveyed through Copyright Clearance Center, Inc.

228 Chapter 9: On-policy Prediction with Approximation\r
array. In the network in Figure 9.15, for example, the first convolutional layer produces\r
6 feature maps, each consisting of 28 ⇥ 28 units. Each unit in each feature map has a\r
5 ⇥ 5 receptive field, and these receptive fields overlap (in this case by four columns and\r
four rows). Consequently, each of the 6 feature maps is specified by just 25 adjustable\r
weights.\r
The subsampling layers of a deep convolutional network reduce the spatial resolution of\r
the feature maps. Each feature map in a subsampling layer consists of units that average\r
over a receptive field of units in the feature maps of the preceding convolutional layer.\r
For example, each unit in each of the 6 feature maps in the first subsampling layer of the\r
network of Figure 9.15 averages over a 2 ⇥ 2 non-overlapping receptive field over one of\r
the feature maps produced by the first convolutional layer, resulting in six 14 ⇥ 14 feature\r
maps. Subsampling layers reduce the network’s sensitivity to the spatial locations of the\r
features detected, that is, they help make the network’s responses spatially invariant.\r
This is useful because a feature detected at one place in an image is likely to be useful at\r
other places as well.\r
Advances in the design and training of ANNs—of which we have only mentioned a\r
few—all contribute to reinforcement learning. Although current reinforcement learning\r
theory is mostly limited to methods using tabular or linear function approximation\r
methods, the impressive performances of notable reinforcement learning applications owe\r
much of their success to nonlinear function approximation by multi-layer ANNs. We\r
discuss several of these applications in Chapter 16.\r
9.8 Least-Squares TD\r
All the methods we have discussed so far in this chapter have required computation per\r
time step proportional to the number of parameters. With more computation, however,\r
one can do better. In this section we present a method for linear function approximation\r
that is arguably the best that can be done for this case.\r
As we established in Section 9.4 TD(0) with linear function approximation converges\r
asymptotically (for appropriately decreasing step sizes) to the TD fixed point:\r
wTD = A1b,\r
where\r
A .= E\r
⇥\r
xt(xt  xt+1)\r
>⇤ and b .\r
= E[Rt+1xt] .\r
Why, one might ask, must we compute this solution iteratively? This is wasteful of data!\r
Could one not do better by computing estimates of A and b, and then directly computing\r
the TD fixed point? The Least-Squares TD algorithm, commonly known as LSTD, does\r
exactly this. It forms the natural estimates\r
Ab t\r
.\r
= Xt1\r
k=0\r
xk(xk  xk+1)\r
> + "I and bbt\r
.\r
= Xt1\r
k=0\r
Rk+1xk, (9.20)

9.8. Least-Squares TD 229\r
where I is the identity matrix, and "I, for some small " > 0, ensures that Ab t is always\r
invertible. It might seem that these estimates should both be divided by t, and indeed\r
they should; as defined here, these are really estimates of t times A and t times b.\r
However, the extra t factors cancel out when LSTD uses these estimates to estimate the\r
TD fixed point as\r
wt\r
.\r
= Ab 1\r
t bbt. (9.21)\r
This algorithm is the most data ecient form of linear TD(0), but it is also more\r
expensive computationally. Recall that semi-gradient TD(0) requires memory and per\u0002step computation that is only O(d).\r
How complex is LSTD? As it is written above the complexity seems to increase with\r
t, but the two approximations in (9.20) could be implemented incrementally using the\r
techniques we have covered earlier (e.g., in Chapter 2) so that they can be done in\r
constant time per step. Even so, the update for Ab t would involve an outer product (a\r
column vector times a row vector) and thus would be a matrix update; its computational\r
complexity would be O(d2), and of course the memory required to hold the Ab t matrix\r
would be O(d2).\r
A potentially greater problem is that our final computation (9.21) uses the inverse\r
of Ab t, and the computational complexity of a general inverse computation is O(d3).\r
Fortunately, an inverse of a matrix of our special form—a sum of outer products—can\r
also be updated incrementally with only O(d2) computations, as\r
Ab 1\r
t =\r
⇣\r
Ab t1 + xt1(xt1  xt)\r
>\r
⌘1\r
(from (9.20))\r
= Ab 1\r
t1  Ab 1\r
t1xt1(xt1  xt)>Ab 1t1\r
1+(xt1  xt)>Ab 1\r
t1xt1\r
, (9.22)\r
for t > 0, with Ab 0\r
.\r
= "I. Although the identity (9.22), known as the Sherman-Morrison\r
formula, is superficially complicated, it involves only vector-matrix and vector-vector\r
multiplications and thus is only O(d2). Thus we can store the inverse matrix Ab 1\r
t ,\r
maintain it with (9.22), and then use it in (9.21), all with only O(d2) memory and\r
per-step computation. The complete algorithm is given in the box on the next page.\r
Of course, O(d2) is still significantly more expensive than the O(d) of semi-gradient\r
TD. Whether the greater data eciency of LSTD is worth this computational expense\r
depends on how large d is, how important it is to learn quickly, and the expense of other\r
parts of the system. The fact that LSTD requires no step-size parameter is sometimes\r
also touted, but the advantage of this is probably overstated. LSTD does not require a\r
step size, but it does requires "; if " is chosen too small the sequence of inverses can vary\r
wildly, and if " is chosen too large then learning is slowed. In addition, LSTD’s lack of a\r
step-size parameter means that it never forgets. This is sometimes desirable, but it is\r
problematic if the target policy ⇡ changes as it does in reinforcement learning and GPI.\r
In control applications, LSTD typically has to be combined with some other mechanism\r
to induce forgetting, mooting any initial advantage of not requiring a step-size parameter.

230 Chapter 9: On-policy Prediction with Approximation\r
LSTD for estimating vˆ = w>x(·) ⇡ v⇡ (O(d2) version)\r
Input: feature representation x : S+ ! Rd such that x(terminal) = 0\r
Algorithm parameter: small " > 0\r
A\r
d1 "1I A d ⇥ d matrix\r
bb 0 A d-dimensional vector\r
Loop for each episode:\r
Initialize S; x x(S)\r
Loop for each step of episode:\r
Choose and take action A ⇠ ⇡(·|S), observe R, S0; x0 x(S0)\r
v A\r
d1\r
>\r
(x  x0)\r
A\r
d1 Ad1  \r
A\r
d1x\r
\r
v>/\r
\r
1 + v>x\r
\r
bb bb + Rx\r
w A\r
d1bb\r
S S0; x x0\r
until S0 is terminal\r
9.9 Memory-based Function Approximation\r
So far we have discussed the parametric approach to approximating value functions. In\r
this approach, a learning algorithm adjusts the parameters of a functional form intended\r
to approximate the value function over a problem’s entire state space. Each update,\r
s 7! g, is a training example used by the learning algorithm to change the parameters\r
with the aim of reducing the approximation error. After the update, the training example\r
can be discarded (although it might be saved to be used again). When an approximate\r
value of a state (which we will call the query state) is needed, the function is simply\r
evaluated at that state using the latest parameters produced by the learning algorithm.\r
Memory-based function approximation methods are very di↵erent. They simply save\r
training examples in memory as they arrive (or at least save a subset of the examples)\r
without updating any parameters. Then, whenever a query state’s value estimate is\r
needed, a set of examples is retrieved from memory and used to compute a value estimate\r
for the query state. This approach is sometimes called lazy learning because processing\r
training examples is postponed until the system is queried to provide an output.\r
Memory-based function approximation methods are prime examples of nonparametric\r
methods. Unlike parametric methods, the approximating function’s form is not limited\r
to a fixed parameterized class of functions, such as linear functions or polynomials, but is\r
instead determined by the training examples themselves, together with some means for\r
combining them to output estimated values for query states. As more training examples\r
accumulate in memory, one expects nonparametric methods to produce increasingly\r
accurate approximations of any target function.

9.9. Memory-based Function Approximation 231\r
There are many di↵erent memory-based methods depending on how the stored training\r
examples are selected and how they are used to respond to a query. Here, we focus on\r
local-learning methods that approximate a value function only locally in the neighborhood\r
of the current query state. These methods retrieve a set of training examples from memory\r
whose states are judged to be the most relevant to the query state, where relevance\r
usually depends on the distance between states: the closer a training example’s state is\r
to the query state, the more relevant it is considered to be, where distance can be defined\r
in many di↵erent ways. After the query state is given a value, the local approximation is\r
discarded.\r
The simplest example of the memory-based approach is the nearest neighbor method,\r
which simply finds the example in memory whose state is closest to the query state and\r
returns that example’s value as the approximate value of the query state. In other words,\r
if the query state is s, and s0 7! g is the example in memory in which s0 is the closest\r
state to s, then g is returned as the approximate value of s. Slightly more complicated\r
are weighted average methods that retrieve a set of nearest neighbor examples and return\r
a weighted average of their target values, where the weights generally decrease with\r
increasing distance between their states and the query state. Locally weighted regression is\r
similar, but it fits a surface to the values of a set of nearest states by means of a parametric\r
approximation method that minimizes a weighted error measure like (9.1), where the\r
weights depend on distances from the query state. The value returned is the evaluation of\r
the locally-fitted surface at the query state, after which the local approximation surface\r
is discarded.\r
Being nonparametric, memory-based methods have the advantage over parametric\r
methods of not limiting approximations to pre-specified functional forms. This allows\r
accuracy to improve as more data accumulates. Memory-based local approximation\r
methods have other properties that make them well suited for reinforcement learning.\r
Because trajectory sampling is of such importance in reinforcement learning, as discussed\r
in Section 8.6, memory-based local methods can focus function approximation on local\r
neighborhoods of states (or state–action pairs) visited in real or simulated trajectories.\r
There may be no need for global approximation because many areas of the state space will\r
never (or almost never) be reached. In addition, memory-based methods allow an agent’s\r
experience to have a relatively immediate a↵ect on value estimates in the neighborhood\r
of the current state, in contrast with a parametric method’s need to incrementally adjust\r
parameters of a global approximation.\r
Avoiding global approximation is also a way to address the curse of dimensionality.\r
For example, for a state space with k dimensions, a tabular method storing a global\r
approximation requires memory exponential in k. On the other hand, in storing examples\r
for a memory-based method, each example requires memory proportional to k, and the\r
memory required to store, say, n examples is linear in n. Nothing is exponential in k or\r
n. Of course, the critical remaining issue is whether a memory-based method can answer\r
queries quickly enough to be useful to an agent. A related concern is how speed degrades\r
as the size of the memory grows. Finding nearest neighbors in a large database can take\r
too long to be practical in many applications.

232 Chapter 9: On-policy Prediction with Approximation\r
Proponents of memory-based methods have developed ways to accelerate the nearest\r
neighbor search. Using parallel computers or special purpose hardware is one approach;\r
another is the use of special multi-dimensional data structures to store the training data.\r
One data structure studied for this application is the k-d tree (short for k-dimensional\r
tree), which recursively splits a k-dimensional space into regions arranged as nodes of a\r
binary tree. Depending on the amount of data and how it is distributed over the state\r
space, nearest-neighbor search using k-d trees can quickly eliminate large regions of the\r
space in the search for neighbors, making the searches feasible in some problems where\r
naive searches would take too long.\r
Locally weighted regression additionally requires fast ways to do the local regression\r
computations which have to be repeated to answer each query. Researchers have developed\r
many ways to address these problems, including methods for forgetting entries in order to\r
keep the size of the database within bounds. The Bibliographic and Historical Comments\r
section at the end of this chapter points to some of the relevant literature, including a\r
selection of papers describing applications of memory-based learning to reinforcement\r
learning.\r
9.10 Kernel-based Function Approximation\r
Memory-based methods such as the weighted average and locally weighted regression\r
methods described above depend on assigning weights to examples s0 7! g in the database\r
depending on the distance between s0 and a query states s. The function that assigns\r
these weights is called a kernel function, or simply a kernel. In the weighted average and\r
locally weighted regressions methods, for example, a kernel function k : R ! R assigns\r
weights to distances between states. More generally, weights do not have to depend on\r
distances; they can depend on some other measure of similarity between states. In this\r
case, k : S ⇥ S ! R, so that k(s, s0) is the weight given to data about s0 in its influence\r
on answering queries about s.\r
Viewed slightly di↵erently, k(s, s0) is a measure of the strength of generalization from\r
s0 to s. Kernel functions numerically express how relevant knowledge about any state\r
is to any other state. As an example, the strengths of generalization for tile coding\r
shown in Figure 9.11 correspond to di↵erent kernel functions resulting from uniform and\r
asymmetrical tile o↵sets. Although tile coding does not explicitly use a kernel function\r
in its operation, it generalizes according to one. In fact, as we discuss more below, the\r
strength of generalization resulting from linear parametric function approximation can\r
always be described by a kernel function.\r
Kernel regression is the memory-based method that computes a kernel weighted average\r
of the targets of all examples stored in memory, assigning the result to the query state.\r
If D is the set of stored examples, and g(s0) denotes the target for state s0 in a stored\r
example, then kernel regression approximates the target function, in this case a value\r
function depending on D, as\r
vˆ(s,D) = X\r
s02D\r
k(s, s0)g(s0). (9.23)

9.10. Kernel-based Function Approximation 233\r
The weighted average method described above is a special case in which k(s, s0) is non-zero\r
only when s and s0 are close to one another so that the sum need not be computed over\r
all of D.\r
A common kernel is the Gaussian radial basis function (RBF) used in RBF function\r
approximation as described in Section 9.5.5. In the method described there, RBFs are\r
features whose centers and widths are either fixed from the start, with centers presumably\r
concentrated in areas where many examples are expected to fall, or are adjusted in some\r
way during learning. Barring methods that adjust centers and widths, this is a linear\r
parametric method whose parameters are the weights of each RBF, which are typically\r
learned by stochastic gradient, or semi-gradient, descent. The form of the approximation\r
is a linear combination of the pre-determined RBFs. Kernel regression with an RBF\r
kernel di↵ers from this in two ways. First, it is memory-based: the RBFs are centered on\r
the states of the stored examples. Second, it is nonparametric: there are no parameters\r
to learn; the response to a query is given by (9.23).\r
Of course, many issues have to be addressed for practical implementation of kernel\r
regression, issues that are beyond the scope of our brief discussion. However, it turns out\r
that any linear parametric regression method like those we described in Section 9.4, with\r
states represented by feature vectors x(s)=(x1(s), x2(s),...,xd(s))>, can be recast as\r
kernel regression where k(s, s0) is the inner product of the feature vector representations\r
of s and s0; that is\r
k(s, s0) = x(s)\r
>x(s0\r
). (9.24)\r
Kernel regression with this kernel function produces the same approximation that a linear\r
parametric method would if it used these feature vectors and learned with the same\r
training data.\r
We skip the mathematical justification for this, which can be found in any modern\r
machine learning text, such as Bishop (2006), and simply point out an important\r
implication. Instead of constructing features for linear parametric function approximators,\r
one can instead construct kernel functions directly without referring at all to feature\r
vectors. Not all kernel functions can be expressed as inner products of feature vectors\r
as in (9.24), but a kernel function that can be expressed like this can o↵er significant\r
advantages over the equivalent parametric method. For many sets of feature vectors,\r
(9.24) has a compact functional form that can be evaluated without any computation\r
taking place in the d-dimensional feature space. In these cases, kernel regression is much\r
less complex than directly using a linear parametric method with states represented by\r
these feature vectors. This is the so-called “kernel trick” that allows e↵ectively working\r
in the high-dimension of an expansive feature space while actually working only with the\r
set of stored training examples. The kernel trick is the basis of many machine learning\r
methods, and researchers have shown how it can sometimes benefit reinforcement learning.

234 Chapter 9: On-policy Prediction with Approximation\r
9.11 Looking Deeper at On-policy Learning:\r
Interest and Emphasis\r
The algorithms we have considered so far in this chapter have treated all the states\r
encountered equally, as if they were all equally important. In some cases, however, we\r
are more interested in some states than others. In discounted episodic problems, for\r
example, we may be more interested in accurately valuing early states in the episode\r
than in later states where discounting may have made the rewards much less important\r
to the value of the start state. Or, if an action-value function is being learned, it may be\r
less important to accurately value poor actions whose value is much less than the greedy\r
action. Function approximation resources are always limited, and if they were used in a\r
more targeted way, then performance could be improved.\r
One reason we have treated all states encountered equally is that then we are updating\r
according to the on-policy distribution, for which stronger theoretical results are available\r
for semi-gradient methods. Recall that the on-policy distribution was defined as the\r
distribution of states encountered in an MDP while following the target policy. Now we\r
will generalize this concept significantly. Rather than having one on-policy distribution\r
for the MDP, we will have many. All of them will have in common that they are a\r
distribution of states encountered in trajectories while following the target policy, but\r
they will vary in how the trajectories are, in a sense, initiated.\r
We now introduce some new concepts. First we introduce a non-negative scalar measure,\r
a random variable It called interest, indicating the degree to which we are interested in\r
accurately valuing the state (or state–action pair) at time t. If we don’t care at all about\r
the state, then the interest should be zero; if we fully care, it might be one, though it is\r
formally allowed to take any non-negative value. The interest can be set in any causal\r
way; for example, it may depend on the trajectory up to time t or the learned parameters\r
at time t. The distribution µ in the VE (9.1) is then defined as the distribution of\r
states encountered while following the target policy, weighted by the interest. Second, we\r
introduce another non-negative scalar random variable, the emphasis Mt. This scalar\r
multiplies the learning update and thus emphasizes or de-emphasizes the learning done\r
at time t. The general n-step learning rule, replacing (9.15), is\r
wt+n\r
.\r
= wt+n1 +↵Mt [Gt:t+n  vˆ(St,wt+n1)] rvˆ(St,wt+n1), 0  t < T, (9.25)\r
with the n-step return given by (9.16) and the emphasis determined recursively from the\r
interest by:\r
Mt = It + nMtn, 0  t < T, (9.26)\r
with Mt\r
.\r
= 0, for all t < 0. These equations are taken to include the Monte Carlo case,\r
for which Gt:t+n = Gt, all the updates are made at end of the episode, n = T  t, and\r
Mt = It.

9.11. Looking Deeper at On-policy Learning: Interest and Emphasis 235\r
Example 9.4 illustrates how interest and emphasis can result in more accurate value\r
estimates.\r
Example 9.4: Interest and Emphasis\r
To see the potential benefits of using interest and emphasis, consider the four-state\r
Markov reward process shown below:\r
+1 +1 +1 +1\r
v⇡ = 4 v⇡ = 3 v⇡ = 2 v⇡ = 1\r
i = 1 i = 0 i = 0 i = 0\r
w1 w1 w2 w2\r
Episodes start in the leftmost state, then transition one state to the right, with a\r
reward of +1, on each step until the terminal state is reached. The true value of\r
the first state is thus 4, of the second state 3, and so on as shown below each state.\r
These are the true values; the estimated values can only approximate these because\r
they are constrained by the parameterization. There are two components to the\r
parameter vector w = (w1, w2)>, and the parameterization is as written inside\r
each state. The estimated values of the first two states are given by w1 alone and\r
thus must be the same even though their true values are di↵erent. Similarly, the\r
estimated values of the third and fourth states are given by w2 alone and must be\r
the same even though their true values are di↵erent. Suppose that we are interested\r
in accurately valuing only the leftmost state; we assign it an interest of 1 while all\r
the other states are assigned an interest of 0, as indicated above the states.\r
First consider applying gradient Monte Carlo algorithms to this problem. The\r
algorithms presented earlier in this chapter that do not take into account interest\r
and emphasis (in (9.7) and the box on page 202) will converge (for decreasing step\r
sizes) to the parameter vector w1 = (3.5, 1.5), which gives the first state—the only\r
one we are interested in—a value of 3.5 (i.e., intermediate between the true values\r
of the first and second states). The methods presented in this section that do use\r
interest and emphasis, on the other hand, will learn the value of the first state\r
exactly correctly; w1 will converge to 4 while w2 will never be updated because the\r
emphasis is zero in all states save the leftmost.\r
Now consider applying two-step semi-gradient TD methods. The methods from\r
earlier in this chapter without interest and emphasis (in (9.15) and (9.16) and\r
the box on page 209) will again converge to w1 = (3.5, 1.5), while the methods\r
with interest and emphasis converge to w1 = (4, 2). The latter produces the\r
exactly correct values for the first state and for the third state (which the first state\r
bootstraps from) while never making any updates corresponding to the second or\r
fourth states.

236 Chapter 9: On-policy Prediction with Approximation\r
9.12 Summary\r
Reinforcement learning systems must be capable of generalization if they are to be\r
applicable to artificial intelligence or to large engineering applications. To achieve this,\r
any of a broad range of existing methods for supervised-learning function approximation\r
can be used simply by treating each update as a training example.\r
Perhaps the most suitable supervised learning methods are those using parameterized\r
function approximation, in which the policy is parameterized by a weight vector w.\r
Although the weight vector has many components, the state space is much larger still,\r
and we must settle for an approximate solution. We defined the mean square value error,\r
VE(w), as a measure of the error in the values v⇡w (s) for a weight vector w under the\r
on-policy distribution, µ. The VE gives us a clear way to rank di↵erent value-function\r
approximations in the on-policy case.\r
To find a good weight vector, the most popular methods are variations of stochastic\r
gradient descent (SGD). In this chapter we have focused on the on-policy case with a fixed\r
policy, also known as policy evaluation or prediction; a natural learning algorithm for this\r
case is n-step semi-gradient TD, which includes gradient Monte Carlo and semi-gradient\r
TD(0) algorithms as the special cases when n=1 and n= 1 respectively. Semi-gradient\r
TD methods are not true gradient methods. In such bootstrapping methods (including\r
DP), the weight vector appears in the update target, yet this is not taken into account in\r
computing the gradient—thus they are semi-gradient methods. As such, they cannot\r
rely on classical SGD results.\r
Nevertheless, good results can be obtained for semi-gradient methods in the special case\r
of linear function approximation, in which the value estimates are sums of features times\r
corresponding weights. The linear case is the most well understood theoretically and\r
works well in practice when provided with appropriate features. Choosing the features\r
is one of the most important ways of adding prior domain knowledge to reinforcement\r
learning systems. They can be chosen as polynomials, but this case generalizes poorly in\r
the online learning setting typically considered in reinforcement learning. Better is to\r
choose features according the Fourier basis, or according to some form of coarse coding\r
with sparse overlapping receptive fields. Tile coding is a form of coarse coding that\r
is particularly computationally ecient and flexible. Radial basis functions are useful\r
for one- or two-dimensional tasks in which a smoothly varying response is important.\r
LSTD is the most data-ecient linear TD prediction method, but requires computation\r
proportional to the square of the number of weights, whereas all the other methods are of\r
complexity linear in the number of weights. Nonlinear methods include artificial neural\r
networks trained by backpropagation and variations of SGD; these methods have become\r
very popular in recent years under the name deep reinforcement learning.\r
Linear semi-gradient n-step TD is guaranteed to converge under standard conditions,\r
for all n, to a VE that is within a bound of the optimal error (achieved asymptotically\r
by Monte Carlo methods). This bound is always tighter for higher n and approaches\r
zero as n ! 1. However, in practice very high n results in very slow learning, and some\r
degree of bootstrapping (n < 1) is usually preferable, just as we saw in comparisons of\r
tabular n-step methods in Chapter 7 and in comparisons of tabular TD and Monte Carlo\r
methods in Chapter 6.

9.12. Summary 237\r
Exercise 9.7 One of the simplest artificial neural networks consists of a single semi-linear\r
unit with a logistic nonlinearity. The need to handle approximate value functions of this\r
form is common in games that end with either a win or a loss, in which case the value of\r
a state can be interpreted as the probability of winning. Derive the learning algorithm\r
for this case, from (9.7), such that no gradient notation appears.\r
⇤\r
Exercise 9.8 Arguably, the squared error used to derive (9.7) is inappropriate for the\r
case treated in the preceding exercise, and the right error measure is the cross-entropy\r
loss (which you can find on Wikipedia). Repeat the derivation in Section 9.3, using the\r
cross-entropy loss instead of the squared error in (9.4), all the way to an explicit form\r
with no gradient or logarithm notation in it. Is your final form more complex, or simpler,\r
than that you obtained in the preceding exercise?\r
Bibliographical and Historical Remarks\r
Generalization and function approximation have always been an integral part of rein\u0002forcement learning. Bertsekas and Tsitsiklis (1996), Bertsekas (2012), and Sugiyama et\r
al. (2013) present the state of the art in function approximation in reinforcement learning.\r
Some of the early work with function approximation in reinforcement learning is discussed\r
at the end of this section.\r
9.3 Gradient-descent methods for minimizing mean square error in supervised learning\r
are well known. Widrow and Ho↵ (1960) introduced the least-mean-square (LMS)\r
algorithm, which is the prototypical incremental gradient-descent algorithm.\r
Details of this and related algorithms are provided in many texts (e.g., Widrow\r
and Stearns, 1985; Bishop, 1995; Duda and Hart, 1973).\r
Semi-gradient TD(0) was first explored by Sutton (1984, 1988), as part of the\r
linear TD() algorithm that we will treat in Chapter 12. The term “semi-gradient”\r
to describe these bootstrapping methods is new to the second edition of this\r
book.\r
The earliest use of state aggregation in reinforcement learning may have been\r
Michie and Chambers’s BOXES system (1968). The theory of state aggregation\r
in reinforcement learning has been developed by Singh, Jaakkola, and Jordan\r
(1995) and Tsitsiklis and Van Roy (1996). State aggregation has been used in\r
dynamic programming from its earliest days (e.g., Bellman, 1957a).\r
9.4 Sutton (1988) proved convergence of linear TD(0) in the mean to the minimal\r
VE solution for the case in which the feature vectors, {x(s) : s 2 S}, are linearly\r
independent. Convergence with probability 1 was proved by several researchers\r
at about the same time (Peng, 1993; Dayan and Sejnowski, 1994; Tsitsiklis,\r
1994; Gurvits, Lin, and Hanson, 1994). In addition, Jaakkola, Jordan, and Singh\r
(1994) proved convergence under online updating. All of these results assumed\r
linearly independent feature vectors, which implies at least as many components\r
to wt as there are states. Convergence for the more important case of general\r
(dependent) feature vectors was first shown by Dayan (1992). A significant

238 Chapter 9: On-policy Prediction with Approximation\r
generalization and strengthening of Dayan’s result was proved by Tsitsiklis and\r
Van Roy (1997). They proved the main result presented in this section, the\r
bound on the asymptotic error of linear bootstrapping methods.\r
9.5 Our presentation of the range of possibilities for linear function approximation is\r
based on that by Barto (1990).\r
9.5.2 Konidaris, Osentoski, and Thomas (2011) introduced the Fourier basis in a\r
simple form suitable for reinforcement learning problems with multi-dimensional\r
continuous state spaces and functions that do not have to be periodic.\r
9.5.3 The term coarse coding is due to Hinton (1984), and our Figure 9.6 is based on\r
one of his figures. Waltz and Fu (1965) provide an early example of this type of\r
function approximation in a reinforcement learning system.\r
9.5.4 Tile coding, including hashing, was introduced by Albus (1971, 1981). He de\u0002scribed it in terms of his “cerebellar model articulator controller,” or CMAC, as\r
tile coding is sometimes known in the literature. The term “tile coding” was new\r
to the first edition of this book, though the idea of describing CMAC in these\r
terms is taken from Watkins (1989). Tile coding has been used in many rein\u0002forcement learning systems (e.g., Shewchuk and Dean, 1990; Lin and Kim, 1991;\r
Miller, Scalera, and Kim, 1994; Sofge and White, 1992; Tham, 1994; Sutton, 1996;\r
Watkins, 1989) as well as in other types of learning control systems (e.g., Kraft and\r
Campagna, 1990; Kraft, Miller, and Dietz, 1992). This section draws heavily on\r
the work of Miller and Glanz (1996). General software for tile coding is available in\r
several languages (e.g., see http://incompleteideas.net/tiles/tiles3.html).\r
9.5.5 Function approximation using radial basis functions has received wide attention\r
ever since being related to ANNs by Broomhead and Lowe (1988). Powell (1987)\r
reviewed earlier uses of RBFs, and Poggio and Girosi (1989, 1990) extensively\r
developed and applied this approach.\r
9.6 Automatic methods for adapting the step-size parameter include RMSprop (Tiele\u0002man and Hinton, 2012), Adam (Kingma and Ba, 2015), stochastic meta-descent\r
methods such as Delta-Bar-Delta (Jacobs, 1988), its incremental generaliza\u0002tion (Sutton, 1992b, c; Mahmood et al., 2012), and nonlinear generalizations\r
(Schraudolph, 1999, 2002). Methods explicitly designed for reinforcement learn\u0002ing include AlphaBound (Dabney and Barto, 2012), SID and NOSID (Dabney,\r
2014), TIDBD (Kearney et al., in preparation) and the application of stochastic\r
meta-descent to policy gradient learning (Schraudolph, Yu, and Aberdeen, 2006).\r
9.7 The introduction of the threshold logic unit as an abstract model neuron by\r
McCulloch and Pitts (1943) was the beginning of ANNs. The history of ANNs as\r
learning methods for classification or regression has passed through several stages:\r
roughly, the Perceptron (Rosenblatt, 1962) and ADALINE (ADAptive LINear\r
Element) (Widrow and Ho↵, 1960) stage of learning by single-layer ANNs, the

9.12. Summary 239\r
error-backpropagation stage (LeCun, 1985; Rumelhart, Hinton, and Williams,\r
1986) of learning by multi-layer ANNs, and the current deep-learning stage with\r
its emphasis on representation learning (e.g., Bengio, Courville, and Vincent,\r
2012; Goodfellow, Bengio, and Courville, 2016). Examples of the many books on\r
ANNs are Haykin (1994), Bishop (1995), and Ripley (2007).\r
ANNs as function approximation for reinforcement learning goes back to the early\r
work of Farley and Clark (1954), who used reinforcement-like learning to modify\r
the weights of linear threshold functions representing policies. Widrow, Gupta,\r
and Maitra (1973) presented a neuron-like linear threshold unit implementing a\r
learning process they called learning with a critic or selective bootstrap adaptation,\r
a reinforcement-learning variant of the ADALINE algorithm. Werbos (1987,\r
1994) developed an approach to prediction and control that uses ANNs trained by\r
error backpropation to learn policies and value functions using TD-like algorithms.\r
Barto, Sutton, and Brouwer (1981) and Barto and Sutton (1981b) extended the\r
idea of an associative memory network (e.g., Kohonen, 1977; Anderson, Silverstein,\r
Ritz, and Jones, 1977) to reinforcement learning. Barto, Anderson, and Sutton\r
(1982) used a two-layer ANN to learn a nonlinear control policy, and emphasized\r
the first layer’s role of learning a suitable representation. Hampson (1983, 1989)\r
was an early proponent of multilayer ANNs for learning value functions. Barto,\r
Sutton, and Anderson (1983) presented an actor–critic algorithm in the form of an\r
ANN learning to balance a simulated pole (see Sections 15.7 and 15.8). Barto and\r
Anandan (1985) introduced a stochastic version of Widrow et al.’s (1973) selective\r
bootstrap algorithm called the associative reward-penalty (ARP ) algorithm.\r
Barto (1985, 1986) and Barto and Jordan (1987) described multi-layer ANNs\r
consisting of ARP units trained with a globally-broadcast reinforcement signal\r
to learn classification rules that are not linearly separable. Barto (1985) discussed\r
this approach to ANNs and how this type of learning rule is related to others in\r
the literature at that time. (See Section 15.10 for additional discussion of this\r
approach to training multi-layer ANNs.) Anderson (1986, 1987, 1989) evaluated\r
numerous methods for training multilayer ANNs and showed that an actor–critic\r
algorithm in which both the actor and critic were implemented by two-layer\r
ANNs trained by error backpropagation outperformed single-layer ANNs in the\r
pole-balancing and tower of Hanoi tasks. Williams (1988) described several ways\r
that backpropagation and reinforcement learning can be combined for training\r
ANNs. Gullapalli (1990) and Williams (1992) devised reinforcement learning\r
algorithms for neuron-like units having continuous, rather than binary, outputs.\r
Barto, Sutton, and Watkins (1990) argued that ANNs can play significant roles\r
for approximating functions required for solving sequential decision problems.\r
Williams (1992) related REINFORCE learning rules (Section 13.3) to the error\r
backpropagation method for training multi-layer ANNs. Tesauro’s TD-Gammon\r
(Tesauro 1992, 1994; Section 16.1) influentially demonstrated the learning abilities\r
of TD() algorithm with function approximation by multi-layer ANNs in learning\r
to play backgammon. The AlphaGo, AlphaGo Zero, and AlphaZero programs\r
of Silver et al. (2016, 2017a, b; Section 16.6) used reinforcement learning with

240 Chapter 9: On-policy Prediction with Approximation\r
deep convolutional ANNs in achieving impressive results with the game of Go.\r
Schmidhuber (2015) reviews applications of ANNs in reinforcement learning,\r
including applications of recurrent ANNs.\r
9.8 LSTD is due to Bradtke and Barto (see Bradtke, 1993, 1994; Bradtke and Barto,\r
1996; Bradtke, Ydstie, and Barto, 1994), and was further developed by Boyan\r
(1999, 2002), Nedi´c and Bertsekas (2003), and Yu (2010). The incremental update\r
of the inverse matrix has been known at least since 1949 (Sherman and Morrison,\r
1949). An extension of least-squares methods to control was introduced by\r
Lagoudakis and Parr (2003; Bu¸soniu, Lazaric, Ghavamzadeh, Munos, Babu˘ska,\r
and De Schutter, 2012).\r
9.9 Our discussion of memory-based function approximation is largely based on\r
the review of locally weighted learning by Atkeson, Moore, and Schaal (1997).\r
Atkeson (1992) discussed the use of locally weighted regression in memory-based\r
robot learning and supplied an extensive bibliography covering the history of\r
the idea. Stanfill and Waltz (1986) influentially argued for the importance of\r
memory based methods in artificial intelligence, especially in light of parallel\r
architectures then becoming available, such as the Connection Machine. Baird\r
and Klopf (1993) introduced a novel memory-based approach and used it as the\r
function approximation method for Q-learning applied to the pole-balancing task.\r
Schaal and Atkeson (1994) applied locally weighted regression to a robot juggling\r
control problem, where it was used to learn a system model. Peng (1995) used\r
the pole-balancing task to experiment with several nearest-neighbor methods\r
for approximating value functions, policies, and environment models. Tadepalli\r
and Ok (1996) obtained promising results with locally-weighted linear regression\r
to learn a value function for a simulated automatic guided vehicle task. Bottou\r
and Vapnik (1992) demonstrated surprising eciency of several local learning\r
algorithms compared to non-local algorithms in some pattern recognition tasks,\r
discussing the impact of local learning on generalization.\r
Bentley (1975) introduced k-d trees and reported observing average running\r
time of O(log n) for nearest neighbor search over n records. Friedman, Bentley,\r
and Finkel (1977) clarified the algorithm for nearest neighbor search with k-d\r
trees. Omohundro (1987) discussed eciency gains possible with hierarchical\r
data structures such as k-d-trees. Moore, Schneider, and Deng (1997) introduced\r
the use of k-d trees for ecient locally weighted regression.\r
9.10 The origin of kernel regression is the method of potential functions of Aizerman,\r
Braverman, and Rozonoer (1964). They likened the data to point electric charges\r
of various signs and magnitudes distributed over space. The resulting electric\r
potential over space produced by summing the potentials of the point charges\r
corresponded to the interpolated surface. In this analogy, the kernel function is\r
the potential of a point charge, which falls o↵ as the reciprocal of the distance\r
from the charge. Connell and Utgo↵ (1987) applied an actor–critic method\r
to the pole-balancing task in which the critic approximated the value function

9.12. Summary 241\r
using kernel regression with an inverse-distance weighting. Predating widespread\r
interest in kernel regression in machine learning, these authors did not use the\r
term kernel, but referred to “Shepard’s method” (Shepard, 1968). Other kernel\u0002based approaches to reinforcement learning include those of Ormoneit and Sen\r
(2002), Dietterich and Wang (2002), Xu, Xie, Hu, and Lu (2005), Taylor and Parr\r
(2009), Barreto, Precup, and Pineau (2011), and Bhat, Farias, and Moallemi\r
(2012).\r
9.11 For Emphatic-TD methods, see the bibliographical notes to Section 11.8.\r
The earliest example we know of in which function approximation methods were\r
used for learning value functions was Samuel’s checkers player (1959, 1967). Samuel\r
followed Shannon’s (1950) suggestion that a value function did not have to be exact to\r
be a useful guide to selecting moves in a game and that it might be approximated by\r
a linear combination of features. In addition to linear function approximation, Samuel\r
experimented with lookup tables and hierarchical lookup tables called signature tables\r
(Grith, 1966, 1974; Page, 1977; Biermann, Fairfield, and Beres, 1982).\r
At about the same time as Samuel’s work, Bellman and Dreyfus (1959) proposed using\r
function approximation methods with DP. (It is tempting to think that Bellman and\r
Samuel had some influence on one another, but we know of no reference to the other in\r
the work of either.) There is now a fairly extensive literature on function approximation\r
methods and DP, such as multigrid methods and methods using splines and orthogonal\r
polynomials (e.g., Bellman and Dreyfus, 1959; Bellman, Kalaba, and Kotkin, 1963; Daniel,\r
1976; Whitt, 1978; Reetz, 1977; Schweitzer and Seidmann, 1985; Chow and Tsitsiklis,\r
1991; Kushner and Dupuis, 1992; Rust, 1996).\r
Holland’s (1986) classifier system used a selective feature-match technique to generalize\r
evaluation information across state–action pairs. Each classifier matched a subset of states\r
having specified values for a subset of features, with the remaining features having arbitrary\r
values (“wild cards”). These subsets were then used in a conventional state-aggregation\r
approach to function approximation. Holland’s idea was to use a genetic algorithm\r
to evolve a set of classifiers that collectively would implement a useful action-value\r
function. Holland’s ideas influenced the early research of the authors on reinforcement\r
learning, but we focused on di↵erent approaches to function approximation. As function\r
approximators, classifiers are limited in several ways. First, they are state-aggregation\r
methods, with concomitant limitations in scaling and in representing smooth functions\r
eciently. In addition, the matching rules of classifiers can implement only aggregation\r
boundaries that are parallel to the feature axes. Perhaps the most important limitation of\r
conventional classifier systems is that the classifiers are learned via the genetic algorithm,\r
an evolutionary method. As we discussed in Chapter 1, there is available during learning\r
much more detailed information about how to learn than can be used by evolutionary\r
methods. This perspective led us to instead adapt supervised learning methods for\r
use in reinforcement learning, specifically gradient-descent and ANN methods. These\r
di↵erences between Holland’s approach and ours are not surprising because Holland’s\r
ideas were developed during a period when ANNs were generally regarded as being too\r
weak in computational power to be useful, whereas our work was at the beginning of

242 Chapter 9: On-policy Prediction with Approximation\r
the period that saw widespread questioning of that conventional wisdom. There remain\r
many opportunities for combining aspects of these di↵erent approaches.\r
Christensen and Korf (1986) experimented with regression methods for modifying\r
coecients of linear value function approximations in the game of chess. Chapman\r
and Kaelbling (1991) and Tan (1991) adapted decision-tree methods for learning value\r
functions. Explanation-based learning methods have also been adapted for learning\r
value functions, yielding compact representations (Yee, Saxena, Utgo↵, and Barto, 1990;\r
Dietterich and Flann, 1995).

Chapter 10\r
On-policy Control with\r
Approximation\r
In this chapter we return to the control problem, now with parametric approximation of\r
the action-value function qˆ(s, a, w) ⇡ q⇤(s, a), where w 2 Rd is a finite-dimensional weight\r
vector. We continue to restrict attention to the on-policy case, leaving o↵-policy methods\r
to Chapter 11. The present chapter features the semi-gradient Sarsa algorithm, the\r
natural extension of semi-gradient TD(0) (last chapter) to action values and to on-policy\r
control. In the episodic case, the extension is straightforward, but in the continuing case\r
we have to take a few steps backward and re-examine how we have used discounting to\r
define an optimal policy. Surprisingly, once we have genuine function approximation we\r
have to give up discounting and switch to a new “average-reward” formulation of the\r
control problem, with new “di↵erential” value functions.\r
Starting first in the episodic case, we extend the function approximation ideas presented\r
in the last chapter from state values to action values. Then we extend them to control\r
following the general pattern of on-policy GPI, using "-greedy for action selection. We\r
show results for n-step linear Sarsa on the Mountain Car problem. Then we turn to the\r
continuing case and repeat the development of these ideas for the average-reward case\r
with di↵erential values.\r
10.1 Episodic Semi-gradient Control\r
The extension of the semi-gradient prediction methods of Chapter 9 to action values is\r
straightforward. In this case it is the approximate action-value function, ˆq ⇡ q⇡, that is\r
represented as a parameterized functional form with weight vector w. Whereas before we\r
considered random training examples of the form St 7! Ut, now we consider examples\r
of the form St, At 7! Ut. The update target Ut can be any approximation of q⇡(St, At),\r
including the usual backed-up values such as the full Monte Carlo return (Gt) or any\r
of the n-step Sarsa returns (7.4). The general gradient-descent update for action-value

244 Chapter 10: On-policy Control with Approximation\r
prediction is\r
wt+1\r
.\r
= wt + ↵\r
h\r
Ut  qˆ(St, At, wt)\r
i\r
rqˆ(St, At, wt). (10.1)\r
For example, the update for the one-step Sarsa method is\r
wt+1\r
.\r
= wt + ↵\r
h\r
Rt+1 + qˆ(St+1, At+1, wt)  qˆ(St, At, wt)\r
i\r
rqˆ(St, At, wt). (10.2)\r
We call this method episodic semi-gradient one-step Sarsa. For a constant policy, this\r
method converges in the same way that TD(0) does, with the same kind of error bound\r
(9.14).\r
To form control methods, we need to couple such action-value prediction methods with\r
techniques for policy improvement and action selection. Suitable techniques applicable to\r
continuous actions, or to actions from large discrete sets, are a topic of ongoing research\r
with as yet no clear resolution. On the other hand, if the action set is discrete and not too\r
large, then we can use the techniques already developed in previous chapters. That is, for\r
each possible action a available in the next state St+1, we can compute qˆ(St+1, a, wt) and\r
then find the greedy action A⇤\r
t+1 = argmaxa qˆ(St+1, a, wt). Policy improvement is then\r
done (in the on-policy case treated in this chapter) by changing the estimation policy to a\r
soft approximation of the greedy policy such as the "-greedy policy. Actions are selected\r
according to this same policy. Pseudocode for the complete algorithm is given in the box.\r
Episodic Semi-gradient Sarsa for Estimating qˆ ⇡ q⇤\r
Input: a di↵erentiable action-value function parameterization ˆq : S ⇥ A ⇥ Rd ! R\r
Algorithm parameters: step size ↵ > 0, small " > 0\r
Initialize value-function weights w 2 Rd arbitrarily (e.g., w = 0)\r
Loop for each episode:\r
S, A initial state and action of episode (e.g., "-greedy)\r
Loop for each step of episode:\r
Take action A, observe R, S0\r
If S0 is terminal:\r
w w + ↵\r
⇥\r
R  qˆ(S, A, w)\r
⇤\r
rqˆ(S, A, w)\r
Go to next episode\r
Choose A0 as a function of ˆq(S0, ·, w) (e.g., "-greedy)\r
w w + ↵\r
⇥\r
R + qˆ(S0, A0, w)  qˆ(S, A, w)\r
⇤\r
rqˆ(S, A, w)\r
S S0\r
A A0\r
Example 10.1: Mountain Car Task Consider the task of driving an underpowered\r
car up a steep mountain road, as suggested by the diagram in the upper left of Figure 10.1.\r
The diculty is that gravity is stronger than the car’s engine, and even at full throttle\r
the car cannot accelerate up the steep slope. The only solution is to first move away from\r
the goal and up the opposite slope on the left. Then, by applying full throttle the car

10.1. Episodic Semi-gradient Control 245\r
!1.2\r
Position\r
0.6\r
Step 428\r
Goal\r
Position\r
4\r
0\r
!.07 .07\r
VelocityVelocity\r
Velocity\r
Velocity\r
Velocity\r
Velocity\r
Position\r
Position\r
Position\r
0\r
2 7\r
0\r
120\r
0\r
104\r
0\r
4 6\r
Episode 12\r
Episode 104 Episode 1000 Episode 9000\r
MOUNTAIN CAR Goal\r
Figure 10.1: The Mountain Car task (upper left panel) and the cost-to-go function\r
( maxa qˆ(s, a, w)) learned during one run.\r
can build up enough inertia to carry it up the steep slope even though it is slowing down\r
the whole way. This is a simple example of a continuous control task where things have\r
to get worse in a sense (farther from the goal) before they can get better. Many control\r
methodologies have great diculties with tasks of this kind unless explicitly aided by a\r
human designer.\r
The reward in this problem is 1 on all time steps until the car moves past its goal\r
position at the top of the mountain, which ends the episode. There are three possible\r
actions: full throttle forward (+1), full throttle reverse (1), and zero throttle (0). The\r
car moves according to a simplified physics. Its position, xt, and velocity, x˙ t, are updated\r
by\r
xt+1\r
.\r
= bound⇥xt + ˙xt+1⇤\r
x˙ t+1\r
.\r
= bound⇥x˙ t + 0.001At  0.0025 cos(3xt)\r
⇤\r
,\r
where the bound operation enforces 1.2  xt+1  0.5 and 0.07  x˙ t+1  0.07. In\r
addition, when xt+1 reached the left bound, x˙ t+1 was reset to zero. When it reached\r
the right bound, the goal was reached and the episode was terminated. Each episode\r
started from a random position xt 2 [0.6, 0.4) and zero velocity. To convert the two\r
continuous state variables to binary features, we used grid-tilings as in Figure 9.9. We\r
used 8 tilings, with each tile covering 1/8th of the bounded distance in each dimension,

246 Chapter 10: On-policy Control with Approximation\r
and asymmetrical o↵sets as described in Section 9.5.4.1 The feature vectors x(s, a) created\r
by tile coding were then combined linearly with the parameter vector to approximate the\r
action-value function:\r
qˆ(s, a, w) .= w>x(s, a) = X\r
d\r
i=1\r
wi · xi(s, a), (10.3)\r
for each pair of state, s, and action, a.\r
Figure 10.1 shows what typically happens while learning to solve this task with this\r
form of function approximation.2 Shown is the negative of the value function (the cost\u0002to-go function) learned on a single run. The initial action values were all zero, which was\r
optimistic (all true values are negative in this task), causing extensive exploration to occur\r
even though the exploration parameter, ", was 0. This can be seen in the middle-top panel\r
of the figure, labeled “Step 428”. At this time not even one episode had been completed,\r
but the car has oscillated back and forth in the valley, following circular trajectories in\r
state space. All the states visited frequently are valued worse than unexplored states,\r
because the actual rewards have been worse than what was (unrealistically) expected.\r
This continually drives the agent away from wherever it has been, to explore new states,\r
until a solution is found.\r
Figure 10.2 shows several learning curves for semi-gradient Sarsa on this problem, with\r
various step sizes.\r
100\r
200\r
400\r
1000\r
0\r
Mountain Car\r
Steps per episode\r
log scale\r
averaged over 100 runs\r
Episode\r
500\r
↵= 0.5/8\r
↵= 0.1/8\r
↵= 0.2/8\r
Figure 10.2: Mountain Car learning curves for the semi-gradient Sarsa method with tile-coding\r
function approximation and "-greedy action selection.\r
1In particular, we used the tile-coding software, available at http://incompleteideas.net/tiles/\r
tiles3.html, with iht=IHT(4096) and tiles(iht,8,[8*x/(0.5+1.2),8*xdot/(0.07+0.07)],[A]) to get\r
the indices of the ones in the feature vector for state (x, xdot) and action A.\r
2This data is actually from the “semi-gradient Sarsa()” algorithm that we will not meet until\r
Chapter 12, but semi-gradient Sarsa would behave similarly.

10.2. Semi-gradient n-step Sarsa 247\r
10.2 Semi-gradient n-step Sarsa\r
We can obtain an n-step version of episodic semi-gradient Sarsa by using an n-step return\r
as the update target in the semi-gradient Sarsa update equation (10.1). The n-step return\r
immediately generalizes from its tabular form (7.4) to a function approximation form:\r
Gt:t+n\r
.\r
= Rt+1+Rt+2+···+n1Rt+n+nqˆ(St+n, At+n, wt+n1), t+n < T, (10.4)\r
with Gt:t+n\r
.\r
= Gt if t + n  T, as usual. The n-step update equation is\r
wt+n\r
.\r
= wt+n1 + ↵ [Gt:t+n  qˆ(St, At, wt+n1)] rqˆ(St, At, wt+n1), 0  t < T.\r
(10.5)\r
Complete pseudocode is given in the box below.\r
Episodic semi-gradient n-step Sarsa for estimating qˆ ⇡ q⇤ or q⇡\r
Input: a di↵erentiable action-value function parameterization ˆq : S ⇥ A ⇥ Rd ! R\r
Input: a policy ⇡ (if estimating q⇡)\r
Algorithm parameters: step size ↵ > 0, small " > 0, a positive integer n\r
Initialize value-function weights w 2 Rd arbitrarily (e.g., w = 0)\r
All store and access operations (St, At, and Rt) can take their index mod n + 1\r
Loop for each episode:\r
Initialize and store S0 6= terminal\r
Select and store an action A0 ⇠ ⇡(·|S0) or "-greedy wrt ˆq(S0, ·, w)\r
T 1\r
Loop for t = 0, 1, 2,... :\r
| If t<T, then:\r
| Take action At\r
| Observe and store the next reward as Rt+1 and the next state as St+1\r
| If St+1 is terminal, then:\r
| T t + 1\r
| else:\r
| Select and store At+1 ⇠ ⇡(·|St+1) or "-greedy wrt ˆq(St+1, ·, w)\r
| ⌧ t  n +1 (⌧ is the time whose estimate is being updated)\r
| If ⌧  0:\r
| G Pmin(⌧+n,T)\r
i=⌧+1 i⌧1Ri\r
| If ⌧ + n<T, then G G + nqˆ(S⌧+n, A⌧+n, w) (G⌧:⌧+n)\r
| w w + ↵ [G  qˆ(S⌧ , A⌧ , w)] rqˆ(S⌧ , A⌧ , w)\r
Until ⌧ = T  1\r
As we have seen before, performance is best if an intermediate level of bootstrapping\r
is used, corresponding to an n larger than 1. Figure 10.3 shows how this algorithm tends\r
to learn faster and obtain a better asymptotic performance at n= 8 than at n= 1 on the\r
Mountain Car task. Figure 10.4 shows the results of a more detailed study of the e↵ect\r
of the parameters ↵ and n on the rate of learning on this task.

248 Chapter 10: On-policy Control with Approximation\r
100\r
200\r
400\r
1000\r
0\r
Mountain Car\r
Steps per episode\r
log scale\r
averaged over 100 runs\r
Episode\r
500\r
n=1\r
n=8\r
Figure 10.3: Performance of one-step vs 8-step semi-gradient Sarsa on the Mountain Car task.\r
Good step sizes were used: ↵ = 0.5/8 for n = 1 and ↵ = 0.3/8 for n = 8.\r
220\r
240\r
260\r
300\r
0 0.5 1 1.5\r
Mountain Car\r
Steps per episode\r
averaged over\r
first 50 episodes\r
and 100 runs\r
↵ × number of tilings (8)\r
280\r
n=1\r
n=2\r
n=4\r
n=8\r
n=16\r
n=8\r
n=4\r
n=2\r
n=16\r
n=1\r
Figure 10.4: E↵ect of the ↵ and n on early performance of n-step semi-gradient Sarsa and\r
tile-coding function approximation on the Mountain Car task. As usual, an intermediate level of\r
bootstrapping (n = 4) performed best. These results are for selected ↵ values, on a log scale,\r
and then connected by straight lines. The standard errors ranged from 0.5 (less than the line\r
width) for n = 1 to about 4 for n = 16, so the main e↵ects are all statistically significant.\r
Exercise 10.1 We have not explicitly considered or given pseudocode for any Monte Carlo\r
methods in this chapter. What would they be like? Why is it reasonable not to give\r
pseudocode for them? How would they perform on the Mountain Car task? ⇤\r
Exercise 10.2 Give pseudocode for semi-gradient one-step Expected Sarsa for control. ⇤\r
Exercise 10.3 Why do the results shown in Figure 10.4 have higher standard errors at\r
large n than at small n? ⇤

10.3. Average Reward: A New Problem Setting for Continuing Tasks 249\r
10.3 Average Reward: A New Problem Setting for\r
Continuing Tasks\r
We now introduce a third classical setting—alongside the episodic and discounted settings—\r
for formulating the goal in Markov decision problems (MDPs). Like the discounted\r
setting, the average reward setting applies to continuing problems, problems for which the\r
interaction between agent and environment goes on and on forever without termination\r
or start states. Unlike that setting, however, there is no discounting—the agent cares just\r
as much about delayed rewards as it does about immediate reward. The average-reward\r
setting is one of the major settings commonly considered in the classical theory of dynamic\r
programming and less-commonly in reinforcement learning. As we discuss in the next\r
section, the discounted setting is problematic with function approximation, and thus the\r
average-reward setting is needed to replace it.\r
In the average-reward setting, the quality of a policy ⇡ is defined as the average rate of\r
reward, or simply average reward, while following that policy, which we denote as r(⇡):\r
r(⇡) .= lim\r
h!1\r
1\r
h\r
X\r
h\r
t=1\r
E[Rt | S0, A0:t1 ⇠⇡] (10.6)\r
= limt!1 E[Rt | S0, A0:t1 ⇠⇡] , (10.7)\r
= X\r
s\r
µ⇡(s)\r
X\r
a\r
⇡(a|s)\r
X\r
s0,r\r
p(s0, r|s, a)r,\r
where the expectations are conditioned on the initial state, S0, and on the subsequent\r
actions, A0, A1,...,At1, being taken according to ⇡. The second and third equations\r
hold if the steady-state distribution, µ⇡(s) .= limt!1 Pr{St =s |A0:t1 ⇠⇡}, exists and\r
is independent of S0, in other words, if the MDP is ergodic. In an ergodic MDP, the\r
starting state and any early decision made by the agent can have only a temporary e↵ect;\r
in the long run the expectation of being in a state depends only on the policy and the\r
MDP transition probabilities. Ergodicity is sucient but not necessary to guarantee the\r
existence of the limit in (10.6).\r
There are subtle distinctions that can be drawn between di↵erent kinds of optimality\r
in the undiscounted continuing case. Nevertheless, for most practical purposes it may\r
be adequate simply to order policies according to their average reward per time step,\r
in other words, according to their r(⇡). This quantity is essentially the average reward\r
under ⇡, as suggested by (10.7), or the reward rate. In particular, we consider all policies\r
that attain the maximal value of r(⇡) to be optimal.\r
Note that the steady state distribution µ⇡ is the special distribution under which, if\r
you select actions according to ⇡, you remain in the same distribution. That is, for which\r
X\r
s\r
µ⇡(s)\r
X\r
a\r
⇡(a|s)p(s0|s, a) = µ⇡(s0). (10.8)

250 Chapter 10: On-policy Control with Approximation\r
In the average-reward setting, returns are defined in terms of di↵erences between\r
rewards and the average reward:\r
Gt\r
.\r
= Rt+1  r(⇡) + Rt+2  r(⇡) + Rt+3  r(⇡) + ··· . (10.9)\r
This is known as the di↵erential return, and the corresponding value functions are\r
known as di↵erential value functions. Di↵erential value functions are defined in terms\r
of the new return just as conventional value functions were defined in terms of the\r
discounted return; thus we will use the same notation, v⇡(s) .= E⇡[Gt|St = s] and\r
q⇡(s, a) .= E⇡[Gt|St = s, At = a] (similarly for v⇤ and q⇤), for di↵erential value functions.\r
Di↵erential value functions also have Bellman equations, just slightly di↵erent from those\r
we have seen earlier. We simply remove all s and replace all rewards by the di↵erence\r
between the reward and the true average reward:\r
v⇡(s) = X\r
a\r
⇡(a|s)\r
X\r
r,s0\r
p(s0, r|s, a)\r
h\r
r  r(⇡) + v⇡(s0)\r
i\r
,\r
q⇡(s, a) = X\r
r,s0\r
p(s0, r|s, a)\r
h\r
r  r(⇡) +X\r
a0\r
⇡(a0|s0)q⇡(s0, a0)\r
i\r
,\r
v⇤(s) = maxa\r
X\r
r,s0\r
p(s0, r|s, a)\r
h\r
r  max ⇡ r(⇡) + v⇤(s0)\r
i\r
, and\r
q⇤(s, a) = X\r
r,s0\r
p(s0, r|s, a)\r
h\r
r  max ⇡ r(⇡) + max a0 q⇤(s0, a0)\r
i\r
(cf. (3.14), Exercise 3.17, (3.19), and (3.20)).\r
There is also a di↵erential form of the two TD errors:\r
t\r
.\r
= Rt+1R¯t + ˆv(St+1,wt)  vˆ(St,wt), (10.10)\r
and\r
t\r
.\r
= Rt+1R¯t + ˆq(St+1, At+1, wt)  qˆ(St, At, wt), (10.11)\r
where R¯t is an estimate at time t of the average reward r(⇡). With these alternate\r
definitions, most of our algorithms and many theoretical results carry through to the\r
average-reward setting without change.\r
For example, an average reward version of semi-gradient Sarsa could be defined just as\r
in (10.2) except with the di↵erential version of the TD error. That is, by\r
wt+1\r
.\r
= wt + ↵trqˆ(St, At, wt), (10.12)\r
with t given by (10.11). Pseudocode for a complete algorithm is given in the box on the\r
next page. One limitation of this algorithm is that it does not converge to the di↵erential\r
values but to the di↵erential values plus an arbitrary o↵set. Notice that the Bellman\r
equations and TD errors given above are una↵ected if all the values are shifted by the\r
same amount. Thus, the o↵set may not matter in practice. How this algorithm could be\r
changed to eliminate the o↵set is an interesting question for future research.\r
Exercise 10.4 Give pseudocode for a di↵erential version of semi-gradient Q-learning. ⇤\r
Exercise 10.5 What equations are needed (beyond 10.10) to specify the di↵erential\r
version of TD(0)? ⇤

10.3. Average Reward: A New Problem Setting for Continuing Tasks 251\r
Di↵erential semi-gradient Sarsa for estimating qˆ ⇡ q⇤\r
Input: a di↵erentiable action-value function parameterization ˆq : S ⇥ A ⇥ Rd ! R\r
Algorithm parameters: step sizes ↵,  > 0, small " > 0\r
Initialize value-function weights w 2 Rd arbitrarily (e.g., w = 0)\r
Initialize average reward estimate R¯ 2 R arbitrarily (e.g., R¯ = 0)\r
Initialize state S, and action A\r
Loop for each step:\r
Take action A, observe R, S0\r
Choose A0 as a function of ˆq(S0, ·, w) (e.g., "-greedy)\r
 R  R¯ + ˆq(S0, A0, w)  qˆ(S, A, w)\r
R¯ R¯ + \r
w w + ↵rqˆ(S, A, w)\r
S S0\r
A A0\r
Exercise 10.6 Suppose there is an MDP that under any policy produces the deterministic\r
sequence of rewards +1, 0, +1, 0, +1, 0,... going on forever. Technically, this violates\r
ergodicity; there is no stationary limiting distribution µ⇡ and the limit (10.7) does not\r
exist. Nevertheless, the average reward (10.6) is well defined. What is it? Now consider\r
two states in this MDP. From A, the reward sequence is exactly as described above,\r
starting with a +1, whereas, from B, the reward sequence starts with a 0 and then\r
continues with +1, 0, +1, 0,.... We would like to compute the di↵erential values of A and\r
B. Unfortunately, the di↵erential return (10.9) is not well defined when starting from\r
these states as the implicit limit does not exist. To repair this, one could alternatively\r
define the di↵erential value of a state as\r
v⇡(s) .= lim\r
!1\r
lim\r
h!1\r
X\r
h\r
t=0\r
t\r
⇣\r
E⇡[Rt+1|S0 =s]  r(⇡)\r
⌘\r
. (10.13)\r
Under this definition, what are the di↵erential values of states A and B? ⇤\r
Exercise 10.7 Consider a Markov reward process consisting of a ring of three states A, B,\r
and C, with state transitions going deterministically around the ring. A reward of +1 is\r
received upon arrival in A and otherwise the reward is 0. What are the di↵erential values\r
of the three states, using (10.13)? ⇤\r
Exercise 10.8 The pseudocode in the box on page 251 updates R¯t using t as an error\r
rather than simply Rt+1  R¯t. Both errors work, but using t is better. To see why,\r
consider the ring MRP of three states from Exercise 10.7. The estimate of the average\r
reward should tend towards its true value of 1\r
3 . Suppose it was already there and was\r
held stuck there. What would the sequence of Rt+1  R¯t errors be? What would the\r
sequence of t errors be (using Equation 10.10)? Which error sequence would produce\r
a more stable estimate of the average reward if the estimate were allowed to change in\r
response to the errors? Why? ⇤

252 Chapter 10: On-policy Control with Approximation\r
Example 10.2: An Access-Control Queuing Task This is a decision task involving\r
access control to a set of 10 servers. Customers of four di↵erent priorities arrive at a\r
single queue. If given access to a server, the customers pay a reward of 1, 2, 4, or 8 to\r
the server, depending on their priority, with higher priority customers paying more. In\r
each time step, the customer at the head of the queue is either accepted (assigned to one\r
of the servers) or rejected (removed from the queue, with a reward of zero). In either\r
case, on the next time step the next customer in the queue is considered. The queue\r
never empties, and the priorities of the customers in the queue are uniformly randomly\r
distributed. Of course a customer cannot be served if there is no free server; the customer\r
is always rejected in this case. Each busy server becomes free with probability p = 0.06\r
on each time step. Although we have just described them for definiteness, let us assume\r
the statistics of arrivals and departures are unknown. The task is to decide on each step\r
whether to accept or reject the next customer, on the basis of his priority and the number\r
of free servers, so as to maximize long-term reward without discounting.\r
In this example we consider a tabular solution to this problem. Although there is no\r
generalization between states, we can still consider it in the general function approximation\r
setting as this setting generalizes the tabular setting. Thus we have a di↵erential action\u0002value estimate for each pair of state (number of free servers and priority of the customer\r
at the head of the queue) and action (accept or reject). Figure 10.5 shows the solution\r
found by di↵erential semi-gradient Sarsa with parameters ↵ = 0.01,  = 0.01, and " = 0.1.\r
The initial action values and R¯ were zero.\r
-10\r
-5\r
0\r
10\r
0\r
Differential\r
value of \r
best action\r
Number of free servers\r
0 1 2 3 4 5 6 7 8 9 10\r
!15\r
!10\r
!5\r
0\r
5\r
7\r
priority 8\r
priority 4\r
priority 2\r
priority 1\r
Number of free servers\r
4\r
2\r
8\r
ACCEPT\r
REJECT\r
1 2 3 4 5 6 7 8 9 10\r
Number of free servers\r
Priority\r
1 POLICY\r
Value of\r
best action\r
VALUE\r
FUNCTION 5\r
1 2 3 4 5 6 7 8 9 10\r
priority 8\r
priority 4\r
priority 2\r
priority 1\r
POLICY\r
VALUE\r
FUNCTION\r
Figure 10.5: The policy and value function found by di↵erential semi-gradient one-step Sarsa\r
on the access-control queuing task after 2 million steps. The drop on the right of the graph\r
is probably due to insucient data; many of these states were never experienced. The value\r
learned for R¯ was about 2.31. (Note that priority 1 here is the lowest priority.)

10.4. Deprecating the Discounted Setting 253\r
10.4 Deprecating the Discounted Setting\r
The continuing, discounted problem formulation has been very useful in the tabular case,\r
in which the returns from each state can be separately identified and averaged. But in the\r
approximate case it is questionable whether one should ever use this problem formulation.\r
To see why, consider an infinite sequence of returns with no beginning or end, and no\r
clearly identified states. The states might be represented only by feature vectors, which\r
may do little to distinguish the states from each other. As a special case, all of the feature\r
vectors may be the same. Thus one really has only the reward sequence (and the actions),\r
and performance has to be assessed purely from these. How could it be done? One way\r
is by averaging the rewards over a long interval—this is the idea of the average-reward\r
setting. How could discounting be used? Well, for each time step we could measure\r
the discounted return. Some returns would be small and some big, so again we would\r
have to average them over a suciently large time interval. In the continuing setting\r
there are no starts and ends, and no special time steps, so there is nothing else that\r
could be done. However, if you do this, it turns out that the average of the discounted\r
returns is proportional to the average reward. In fact, for policy ⇡, the average of the\r
discounted returns is always r(⇡)/(1  ), that is, it is essentially the average reward,\r
r(⇡). In particular, the ordering of all policies in the average discounted return setting\r
would be exactly the same as in the average-reward setting. The discount rate  thus has\r
no e↵ect on the problem formulation. It could in fact be zero and the ranking would be\r
unchanged.\r
This surprising fact is proven in the box on the next page, but the basic idea can\r
be seen via a symmetry argument. Each time step is exactly the same as every other.\r
With discounting, every reward will appear exactly once in each position in some return.\r
The tth reward will appear undiscounted in the t  1st return, discounted once in the\r
t  2nd return, and discounted 999 times in the t  1000th return. The weight on the\r
tth reward is thus 1 +  + 2 + 3 + ··· = 1/(1  ). Because all states are the same,\r
they are all weighted by this, and thus the average of the returns will be this times the\r
average reward, or r(⇡)/(1  ).\r
This example and the more general argument in the box show that if we optimized\r
discounted value over the on-policy distribution, then the e↵ect would be identical to\r
optimizing undiscounted average reward; the actual value of  would have no e↵ect. This\r
strongly suggests that discounting has no role to play in the definition of the control\r
problem with function approximation. One can nevertheless go ahead and use discounting\r
in solution methods. The discounting parameter  changes from a problem parameter\r
to a solution method parameter! Unfortunately, discounting algorithms with function\r
approximation do not optimize discounted value over the on-policy distribution, and thus\r
are not guaranteed to optimize average reward.\r
The root cause of the diculties with the discounted control setting is that with\r
function approximation we have lost the policy improvement theorem (Section 4.2). It is\r
no longer true that if we change the policy to improve the discounted value of one state\r
then we are guaranteed to have improved the overall policy in any useful sense. That\r
guarantee was key to the theory of our reinforcement learning control methods. With

254 Chapter 10: On-policy Control with Approximation\r
The Futility of Discounting in Continuing Problems\r
Perhaps discounting can be saved by choosing an objective that sums discounted\r
values over the distribution with which states occur under the policy:\r
J(⇡) = X\r
s\r
µ⇡(s)v\r
⇡(s) (where v⇡ is the discounted value function)\r
= X\r
s\r
µ⇡(s)\r
X\r
a\r
⇡(a|s)\r
X\r
s0\r
X\r
r\r
p(s0, r|s, a) [r + v\r
⇡(s0\r
)] (Bellman Eq.)\r
= r(⇡) +X\r
s\r
µ⇡(s)\r
X\r
a\r
⇡(a|s)\r
X\r
s0\r
X\r
r\r
p(s0, r|s, a)v\r
⇡(s0\r
) (from (10.7))\r
= r(⇡) + \r
X\r
s0\r
v\r
⇡(s0\r
)\r
X\r
s\r
µ⇡(s)\r
X\r
a\r
⇡(a|s)p(s0|s, a) (from (3.4))\r
= r(⇡) + \r
X\r
s0\r
v\r
⇡(s0\r
)µ⇡(s0) (from (10.8))\r
= r(⇡) + J(⇡)\r
= r(⇡) + r(⇡) + 2J(⇡)\r
= r(⇡) + r(⇡) + 2r(⇡) + 3r(⇡) + ···\r
= 1\r
1  \r
r(⇡).\r
The proposed discounted objective orders policies identically to the undiscounted\r
(average reward) objective. The discount rate  does not influence the ordering!\r
function approximation we have lost it!\r
In fact, the lack of a policy improvement theorem is also a theoretical lacuna for the\r
total-episodic and average-reward settings. Once we introduce function approximation\r
we can no longer guarantee improvement for any setting. In Chapter 13 we introduce an\r
alternative class of reinforcement learning algorithms based on parameterized policies,\r
and there we have a theoretical guarantee called the “policy-gradient theorem” which\r
plays a similar role as the policy improvement theorem. But for methods that learn\r
action values we seem to be currently without a local improvement guarantee (possibly\r
the approach taken by Perkins and Precup (2003) may provide a part of the answer). We\r
do know that "-greedification may sometimes result in an inferior policy, as policies may\r
chatter among good policies rather than converge (Gordon, 1996a). This is an area with\r
multiple open theoretical questions.

10.5. Di↵erential Semi-gradient n-step Sarsa 255\r
10.5 Di↵erential Semi-gradient n-step Sarsa\r
In order to generalize to n-step bootstrapping, we need an n-step version of the TD error.\r
We begin by generalizing the n-step return (7.4) to its di↵erential form, with function\r
approximation:\r
Gt:t+n\r
.\r
= Rt+1R¯t+n1 + ··· + Rt+nR¯t+n1 + ˆq(St+n, At+n, wt+n1), (10.14)\r
where R¯ is an estimate of r(⇡), n  1, and t + n<T. If t + n  T, then we define\r
Gt:t+n\r
.\r
= Gt as usual. The n-step TD error is then\r
t\r
.\r
= Gt:t+n  qˆ(St, At, w), (10.15)\r
after which we can apply our usual semi-gradient Sarsa update (10.12). Pseudocode for\r
the complete algorithm is given in the box.\r
Di↵erential semi-gradient n-step Sarsa for estimating qˆ ⇡ q⇡ or q⇤\r
Input: a di↵erentiable function ˆq : S ⇥ A ⇥ Rd ! R, a policy ⇡\r
Initialize value-function weights w 2 Rd arbitrarily (e.g., w = 0)\r
Initialize average-reward estimate R¯ 2 R arbitrarily (e.g., R¯ = 0)\r
Algorithm parameters: step sizes ↵,  > 0, small " > 0, a positive integer n\r
All store and access operations (St, At, and Rt) can take their index mod n + 1\r
Initialize and store S0 and A0\r
Loop for each step, t = 0, 1, 2,... :\r
Take action At\r
Observe and store the next reward as Rt+1 and the next state as St+1\r
Select and store an action At+1 ⇠ ⇡(·|St+1), or "-greedy wrt ˆq(St+1, ·, w)\r
⌧ t  n +1 (⌧ is the time whose estimate is being updated)\r
If ⌧  0:\r
 P⌧+n\r
i=⌧+1(Ri  R¯)+ˆq(S⌧+n, A⌧+n, w)  qˆ(S⌧ , A⌧ , w)\r
R¯ R¯ + \r
w w + ↵rqˆ(S⌧ , A⌧ , w)\r
Exercise 10.9 In the di↵erential semi-gradient n-step Sarsa algorithm, the step-size\r
parameter on the average reward, , needs to be quite small so that R¯ becomes a good\r
long-term estimate of the average reward. Unfortunately, R¯ will then be biased by its\r
initial value for many steps, which may make learning inecient. Alternatively, one could\r
use a sample average of the observed rewards for R¯. That would initially adapt rapidly\r
but in the long run would also adapt slowly. As the policy slowly changed, R¯ would also\r
change; the potential for such long-term nonstationarity makes sample-average methods\r
ill-suited. In fact, the step-size parameter on the average reward is a perfect place to use\r
the unbiased constant-step-size trick from Exercise 2.7. Describe the specific changes\r
needed to the boxed algorithm for di↵erential semi-gradient n-step Sarsa to use this\r
trick. ⇤

256 Chapter 10: On-policy Control with Approximation\r
10.6 Summary\r
In this chapter we have extended the ideas of parameterized function approximation and\r
semi-gradient descent, introduced in the previous chapter, to control. The extension is\r
immediate for the episodic case, but for the continuing case we have to introduce a whole\r
new problem formulation based on maximizing the average reward setting per time step.\r
Surprisingly, the discounted formulation cannot be carried over to control in the presence\r
of approximations. In the approximate case most policies cannot be represented by a\r
value function. The arbitrary policies that remain need to be ranked, and the scalar\r
average reward r(⇡) provides an e↵ective way to do this.\r
The average reward formulation involves new di↵erential versions of value functions,\r
Bellman equations, and TD errors, but all of these parallel the old ones, and the\r
conceptual changes are small. There is also a new parallel set of di↵erential algorithms\r
for the average-reward case.\r
Bibliographical and Historical Remarks\r
10.1 Semi-gradient Sarsa with function approximation was first explored by Rummery\r
and Niranjan (1994). Linear semi-gradient Sarsa with "-greedy action selection\r
does not converge in the usual sense, but does enter a bounded region near\r
the best solution (Gordon, 1996a, 2001). Precup and Perkins (2003) showed\r
convergence in a di↵erentiable action selection setting. See also Perkins and\r
Pendrith (2002) and Melo, Meyn, and Ribeiro (2008). The mountain–car example\r
is based on a similar task studied by Moore (1990), but the exact form used here\r
is from Sutton (1996).\r
10.2 Episodic n-step semi-gradient Sarsa is based on the forward Sarsa() algorithm\r
of van Seijen (2016). The empirical results shown here are new to the second\r
edition of this text.\r
10.3 The average-reward formulation has been described for dynamic programming\r
(e.g., Puterman, 1994) and from the point of view of reinforcement learning\r
(Mahadevan, 1996; Tadepalli and Ok, 1994; Bertsekas and Tsitsiklis, 1996;\r
Tsitsiklis and Van Roy, 1999). The algorithm described here is the on-policy\r
analog of the “R-learning” algorithm introduced by Schwartz (1993). The name\r
R-learning was probably meant to be the alphabetic successor to Q-learning,\r
but we prefer to think of it as a reference to the learning of di↵erential or\r
relative values. The access-control queuing example was suggested by the work\r
of Carlstr¨om and Nordstr¨om (1997).\r
10.4 The recognition of the limitations of discounting as a formulation of the rein\u0002forcement learning problem with function approximation became apparent to\r
the authors shortly after the publication of the first edition of this text. Singh,\r
Jaakkola, and Jordan (1994) may have been the first to observe it in print.

Chapter 11\r
*O↵-policy Methods with\r
Approximation\r
This book has treated on-policy and o↵-policy learning methods since Chapter 5 primarily\r
as two alternative ways of handling the conflict between exploitation and exploration\r
inherent in learning forms of generalized policy iteration. The two chapters preceding this\r
have treated the on-policy case with function approximation, and in this chapter we treat\r
the o↵ -policy case with function approximation. The extension to function approximation\r
turns out to be significantly di↵erent and harder for o↵-policy learning than it is for\r
on-policy learning. The tabular o↵-policy methods developed in Chapters 6 and 7 readily\r
extend to semi-gradient algorithms, but these algorithms do not converge as robustly as\r
they do under on-policy training. In this chapter we explore the convergence problems,\r
take a closer look at the theory of linear function approximation, introduce a notion of\r
learnability, and then discuss new algorithms with stronger convergence guarantees for the\r
o↵-policy case. In the end we will have improved methods, but the theoretical results will\r
not be as strong, nor the empirical results as satisfying, as they are for on-policy learning.\r
Along the way, we will gain a deeper understanding of approximation in reinforcement\r
learning for on-policy learning as well as o↵-policy learning.\r
Recall that in o↵-policy learning we seek to learn a value function for a target policy\r
⇡, given data due to a di↵erent behavior policy b. In the prediction case, both policies\r
are static and given, and we seek to learn either state values vˆ ⇡ v⇡ or action values\r
qˆ ⇡ q⇡. In the control case, action values are learned, and both policies typically change\r
during learning—⇡ being the greedy policy with respect to qˆ, and b being something\r
more exploratory such as the "-greedy policy with respect to ˆq.\r
The challenge of o↵-policy learning can be divided into two parts, one that arises in\r
the tabular case and one that arises only with function approximation. The first part\r
of the challenge has to do with the target of the update (not to be confused with the\r
target policy), and the second part has to do with the distribution of the updates. The\r
techniques related to importance sampling developed in Chapters 5 and 7 deal with\r
the first part; these may increase variance but are needed in all successful algorithms,

258 Chapter 11: O↵-policy Methods with Approximation\r
tabular and approximate. The extension of these techniques to function approximation\r
are quickly dealt with in the first section of this chapter.\r
Something more is needed for the second part of the challenge of o↵-policy learning\r
with function approximation because the distribution of updates in the o↵-policy case is\r
not according to the on-policy distribution. The on-policy distribution is important to\r
the stability of semi-gradient methods. Two general approaches have been explored to\r
deal with this. One is to use importance sampling methods again, this time to warp the\r
update distribution back to the on-policy distribution, so that semi-gradient methods\r
are guaranteed to converge (in the linear case). The other is to develop true gradient\r
methods that do not rely on any special distribution for stability. We present methods\r
based on both approaches. This is a cutting-edge research area, and it is not clear which\r
of these approaches is most e↵ective in practice.\r
11.1 Semi-gradient Methods\r
We begin by describing how the methods developed in earlier chapters for the o↵-\r
policy case extend readily to function approximation as semi-gradient methods. These\r
methods address the first part of the challenge of o↵-policy learning (changing the update\r
targets) but not the second part (changing the update distribution). Accordingly, these\r
methods may diverge in some cases, and in that sense are not sound, but still they\r
are often successfully used. Remember that these methods are guaranteed stable and\r
asymptotically unbiased for the tabular case, which corresponds to a special case of\r
function approximation. So it may still be possible to combine them with feature selection\r
methods in such a way that the combined system could be assured stable. In any event,\r
these methods are simple and thus a good place to start.\r
In Chapter 7 we described a variety of tabular o↵-policy algorithms. To convert them\r
to semi-gradient form, we simply replace the update to an array (V or Q) to an update\r
to a weight vector (w), using the approximate value function (vˆ or qˆ) and its gradient.\r
Many of these algorithms use the per-step importance sampling ratio:\r
⇢t\r
.\r
= ⇢t:t = ⇡(At|St)\r
b(At|St)\r
. (11.1)\r
For example, the one-step, state-value algorithm is semi-gradient o↵-policy TD(0), which\r
is just like the corresponding on-policy algorithm (page 203) except for the addition of\r
⇢t:\r
wt+1\r
.\r
= wt + ↵⇢ttrvˆ(St,wt), (11.2)\r
where t is defined appropriately depending on whether the problem is episodic and\r
discounted, or continuing and undiscounted using average reward:\r
t\r
.\r
= Rt+1 + vˆ(St+1,wt)  vˆ(St,wt), or (11.3)\r
t\r
.\r
= Rt+1  R¯t + ˆv(St+1,wt)  vˆ(St,wt). (11.4)

11.1. Semi-gradient Methods 259\r
For action values, the one-step algorithm is semi-gradient Expected Sarsa:\r
wt+1\r
.\r
= wt + ↵trqˆ(St, At, wt), with (11.5)\r
t\r
.\r
= Rt+1 + \r
X\r
a\r
⇡(a|St+1)ˆq(St+1, a, wt)  qˆ(St, At, wt), or (episodic)\r
t\r
.\r
= Rt+1  R¯t +X\r
a\r
⇡(a|St+1)ˆq(St+1, a, wt)  qˆ(St, At, wt). (continuing)\r
Note that this algorithm does not use importance sampling. In the tabular case it is clear\r
that this is appropriate because the only sample action is At, and in learning its value we\r
do not have to consider any other actions. With function approximation it is less clear\r
because we might want to weight di↵erent state–action pairs di↵erently once they all\r
contribute to the same overall approximation. Proper resolution of this issue awaits a\r
more thorough understanding of the theory of function approximation in reinforcement\r
learning.\r
In the multi-step generalizations of these algorithms, both the state-value and action\u0002value algorithms involve importance sampling. The n-step version of semi-gradient Sarsa\r
is\r
wt+n\r
.\r
= wt+n1+↵⇢t+1 ··· ⇢t+n [Gt:t+n  qˆ(St, At, wt+n1)] rqˆ(St, At, wt+n1) (11.6)\r
with\r
Gt:t+n\r
.\r
= Rt+1 + ··· + n1Rt+n + nqˆ(St+n, At+n, wt+n1), or (episodic)\r
Gt:t+n\r
.\r
= Rt+1  R¯t + ··· + Rt+n  R¯t+n1 + ˆq(St+n, At+n, wt+n1), (continuing)\r
where here we are being slightly informal in our treatment of the ends of episodes. In the\r
first equation, the ⇢ks for k  T (where T is the last time step of the episode) should be\r
taken to be 1, and Gt:t+n should be taken to be Gt if t + n  T.\r
Recall that we also presented in Chapter 7 an o↵-policy algorithm that does not involve\r
importance sampling at all: the n-step tree-backup algorithm. Here is its semi-gradient\r
version:\r
wt+n\r
.\r
= wt+n1 + ↵ [Gt:t+n  qˆ(St, At, wt+n1)] rqˆ(St, At, wt+n1), (11.7)\r
Gt:t+n\r
.\r
= ˆq(St, At, wt+n1) +\r
t+\r
Xn1\r
k=t\r
k\r
Y\r
k\r
i=t+1\r
⇡(Ai|Si), (11.8)\r
with t as defined at the top of this page for Expected Sarsa. We also defined in Chapter 7\r
an algorithm that unifies all action-value algorithms: n-step Q(). We leave the semi\u0002gradient form of that algorithm, and also of the n-step state-value algorithm, as exercises\r
for the reader.\r
Exercise 11.1 Convert the equation of n-step o↵-policy TD (7.9) to semi-gradient form.\r
Give accompanying definitions of the return for both the episodic and continuing cases. ⇤\r
⇤\r
Exercise 11.2 Convert the equations of n-step Q() (7.11 and 7.17) to semi-gradient\r
form. Give definitions that cover both the episodic and continuing cases. ⇤

260 Chapter 11: O↵-policy Methods with Approximation\r
11.2 Examples of O↵-policy Divergence\r
In this section we begin to discuss the second part of the challenge of o↵-policy learning\r
with function approximation—that the distribution of updates does not match the on\u0002policy distribution. We describe some instructive counterexamples to o↵-policy learning—\r
cases where semi-gradient and other simple algorithms are unstable and diverge.\r
To establish intuitions, it is best to consider first a very simple example. Suppose,\r
perhaps as part of a larger MDP, there are two states whose estimated values are of\r
the functional form w and 2w, where the parameter vector w consists of only a single\r
component w. This occurs under linear function approximation if the feature vectors\r
for the two states are each simple numbers (single-component vectors), in this case 1\r
and 2. In the first state, only one action is available, and it results deterministically in a\r
transition to the second state with a reward of 0:\r
2w\r
0 2w\r
where the expressions inside the two circles indicate the two state’s values.\r
Suppose initially w = 10. The transition will then be from a state of estimated value\r
10 to a state of estimated value 20. It will look like a good transition, and w will be\r
increased to raise the first state’s estimated value. If  is nearly 1, then the TD error will\r
be nearly 10, and, if ↵ = 0.1, then w will be increased to nearly 11 in trying to reduce the\r
TD error. However, the second state’s estimated value will also be increased, to nearly\r
22. If the transition occurs again, then it will be from a state of estimated value ⇡11 to\r
a state of estimated value ⇡22, for a TD error of ⇡11—larger, not smaller, than before.\r
It will look even more like the first state is undervalued, and its value will be increased\r
again, this time to ⇡12.1. This looks bad, and in fact with further updates w will diverge\r
to infinity.\r
To see this definitively we have to look more carefully at the sequence of updates. The\r
TD error on a transition between the two states is\r
t = Rt+1 + vˆ(St+1,wt)  vˆ(St,wt)=0+ 2wt  wt = (2  1)wt,\r
and the o↵-policy semi-gradient TD(0) update (from (11.2)) is\r
wt+1 = wt + ↵⇢ttrvˆ(St,wt) = wt + ↵ · 1 · (2  1)wt · 1 = \r
1 + ↵(2  1)wt.\r
Note that the importance sampling ratio, ⇢t, is 1 on this transition because there is\r
only one action available from the first state, so its probabilities of being taken under\r
the target and behavior policies must both be 1. In the final update above, the new\r
parameter is the old parameter times a scalar constant, 1 + ↵(2  1). If this constant is\r
greater than 1, then the system is unstable and w will go to positive or negative infinity\r
depending on its initial value. Here this constant is greater than 1 whenever  > 0.5.\r
Note that stability does not depend on the specific step size, as long as ↵ > 0. Smaller or\r
larger step sizes would a↵ect the rate at which w goes to infinity, but not whether it goes\r
there or not.\r
Key to this example is that the one transition occurs repeatedly without w being\r
updated on other transitions. This is possible under o↵-policy training because the

11.2. Examples of O↵-policy Divergence 261\r
behavior policy might select actions on those other transitions which the target policy\r
never would. For these transitions, ⇢t would be zero and no update would be made.\r
Under on-policy training, however, ⇢t is always one. Each time there is a transition from\r
the w state to the 2w state, increasing w, there would also have to be a transition out\r
of the 2w state. That transition would reduce w, unless it were to a state whose value\r
was higher (because  < 1) than 2w, and then that state would have to be followed by a\r
state of even higher value, or else again w would be reduced. Each state can support the\r
one before only by creating a higher expectation. Eventually the piper must be paid. In\r
the on-policy case the promise of future reward must be kept and the system is kept in\r
check. But in the o↵-policy case, a promise can be made and then, after taking an action\r
that the target policy never would, forgotten and forgiven.\r
This simple example communicates much of the reason why o↵-policy training can lead\r
to divergence, but it is not completely convincing because it is not complete—it is just a\r
fragment of a complete MDP. Can there really be a complete system with instability? A\r
simple complete example of divergence is Baird’s counterexample. Consider the episodic\r
seven-state, two-action MDP shown in Figure 11.1. The dashed action takes the system\r
to one of the six upper states with equal probability, whereas the solid action takes the\r
system to the seventh state. The behavior policy b selects the dashed and solid actions\r
with probabilities 6\r
7 and 17 , so that the next-state distribution under it is uniform (the\r
same for all nonterminal states), which is also the starting distribution for each episode.\r
The target policy ⇡ always takes the solid action, and so the on-policy distribution (for ⇡)\r
is concentrated in the seventh state. The reward is zero on all transitions. The discount\r
rate is  = 0.99.\r
Consider estimating the state-value under the linear parameterization indicated by\r
the expression shown in each state circle. For example, the estimated value of the\r
leftmost state is 2w1 +w8, where the subscript corresponds to the component of the\r
2w1+w8 2w2+w8 2w3+w8 2w4+w8 2w5+w8 2w6+w8\r
w7+2w8\r
b(dashed|·)=6/7\r
b(solid|·)=1/7\r
⇡(solid|·)=1\r
 = 0.99\r
Figure 11.1: Baird’s counterexample. The approximate state-value function for this Markov\r
process is of the form shown by the linear expressions inside each state. The solid action usually\r
results in the seventh state, and the dashed action usually results in one of the other six states,\r
each with equal probability. The reward is always zero.

262 Chapter 11: O↵-policy Methods with Approximation\r
overall weight vector w 2 R8; this corresponds to a feature vector for the first state\r
being x(1) = (2, 0, 0, 0, 0, 0, 0, 1)>. The reward is zero on all transitions, so the true value\r
function is v⇡(s) = 0, for all s, which can be exactly approximated if w = 0. In fact,\r
there are many solutions, as there are more components to the weight vector (8) than\r
there are nonterminal states (7). Moreover, the set of feature vectors, {x(s) : s 2 S}, is\r
a linearly independent set. In all these ways this task seems a favorable case for linear\r
function approximation.\r
If we apply semi-gradient TD(0) to this problem (11.2), then the weights diverge\r
to infinity, as shown in Figure 11.2 (left). The instability occurs for any positive step\r
size, no matter how small. In fact, it even occurs if an expected update is done as in\r
dynamic programming (DP), as shown in Figure 11.2 (right). That is, if the weight\r
vector, wk, is updated for all states at the same time in a semi-gradient way, using the\r
DP (expectation-based) target:\r
wk+1\r
.\r
= wk +\r
↵\r
|S|\r
X\r
s\r
⇣\r
E⇡[Rt+1 + vˆ(St+1,wk) | St =s]  vˆ(s,wk)\r
⌘\r
rvˆ(s,wk). (11.9)\r
In this case, there is no randomness and no asynchrony, just as in a classical DP update.\r
The method is conventional except in its use of semi-gradient function approximation.\r
Yet still the system is unstable.\r
If we alter just the distribution of DP updates in Baird’s counterexample, from the\r
uniform distribution to the on-policy distribution (which generally requires asynchronous\r
updating), then convergence is guaranteed to a solution with error bounded by (9.14).\r
This example is striking because the TD and DP methods used are arguably the simplest\r
w8\r
w8\r
300\r
200\r
100\r
10\r
1\r
0 1000 0 1000\r
w1– w6\r
Steps\r
w7\r
Sweeps\r
Semi-gradient Off-policy TD Semi-gradient DP\r
w1– w6\r
w7\r
Figure 11.2: Demonstration of instability on Baird’s counterexample. Shown are the evolution\r
of the components of the parameter vector w of the two semi-gradient algorithms. The step size\r
was ↵ = 0.01, and the initial weights were w = (1, 1, 1, 1, 1, 1, 10, 1)>.

11.2. Examples of O↵-policy Divergence 263\r
and best-understood bootstrapping methods, and the linear, semi-descent method used is\r
arguably the simplest and best-understood kind of function approximation. The example\r
shows that even the simplest combination of bootstrapping and function approximation\r
can be unstable if the updates are not done according to the on-policy distribution.\r
There are also counterexamples similar to Baird’s showing divergence for Q-learning.\r
This is cause for concern because otherwise Q-learning has the best convergence guarantees\r
of all control methods. Considerable e↵ort has gone into trying to find a remedy to\r
this problem or to obtain some weaker, but still workable, guarantee. For example, it\r
may be possible to guarantee convergence of Q-learning as long as the behavior policy is\r
suciently close to the target policy, for example, when it is the "-greedy policy. To the\r
best of our knowledge, Q-learning has never been found to diverge in this case, but there\r
has been no theoretical analysis. In the rest of this section we present several other ideas\r
that have been explored.\r
Suppose that instead of taking just a step toward the expected one-step return on each\r
iteration, as in Baird’s counterexample, we actually change the value function all the way\r
to the best, least-squares approximation. Would this solve the instability problem? Of\r
course it would if the feature vectors, {x(s) : s 2 S}, formed a linearly independent set,\r
as they do in Baird’s counterexample, because then exact approximation is possible on\r
each iteration and the method reduces to standard tabular DP. But of course the point\r
here is to consider the case when an exact solution is not possible. In this case stability\r
is not guaranteed even when forming the best approximation at each iteration, as shown\r
in the example.\r
Example 11.1: Tsitsiklis and Van Roy’s Counterexample This example shows\r
that linear function approximation would not work with DP even if the least-squares\r
1  "\r
"\r
w 2w\r
solution was found at each step. The counterexample is formed\r
by extending the w-to-2w example (from earlier in this section)\r
with a terminal state, as shown to the right. As before, the\r
estimated value of the first state is w, and the estimated value\r
of the second state is 2w. The reward is zero on all transitions,\r
so the true values are zero at both states, which is exactly\r
representable with w = 0. If we set wk+1 at each step so\r
as to minimize the VE between the estimated value and the\r
expected one-step return, then we have\r
wk+1 = argmin w2R\r
X\r
s2S\r
⇣\r
vˆ(s,w)  E⇡\r
⇥\r
Rt+1 + vˆ(St+1,wk)\r
\r
 St = s\r
⇤⌘2\r
= argmin w2R\r
\r
w  2wk\r
2\r
+ 2w  (1  ")2wk\r
2\r
= 6  4"\r
5 wk. (11.10)\r
The sequence {wk} diverges when  > 5\r
64" and w0 6= 0.

264 Chapter 11: O↵-policy Methods with Approximation\r
Another way to try to prevent instability is to use special methods for function\r
approximation. In particular, stability is guaranteed for function approximation methods\r
that do not extrapolate from the observed targets. These methods, called averagers,\r
include nearest neighbor methods and locally weighted regression, but not popular\r
methods such as tile coding and artificial neural networks (ANNs).\r
Exercise 11.3 (programming) Apply one-step semi-gradient Q-learning to Baird’s coun\u0002terexample and show empirically that its weights diverge. ⇤\r
11.3 The Deadly Triad\r
Our discussion so far can be summarized by saying that the danger of instability and\r
divergence arises whenever we combine all of the following three elements, making up\r
what we call the deadly triad:\r
Function approximation A powerful, scalable way of generalizing from a state space\r
much larger than the memory and computational resources (e.g., linear function\r
approximation or ANNs).\r
Bootstrapping Update targets that include existing estimates (as in dynamic pro\u0002gramming or TD methods) rather than relying exclusively on actual rewards and\r
complete returns (as in MC methods).\r
O↵-policy training Training on a distribution of transitions other than that produced\r
by the target policy. Sweeping through the state space and updating all states\r
uniformly, as in dynamic programming, does not respect the target policy and is\r
an example of o↵-policy training.\r
In particular, note that the danger is not due to control or to generalized policy iteration.\r
Those cases are more complex to analyze, but the instability arises in the simpler prediction\r
case whenever it includes all three elements of the deadly triad. The danger is also not\r
due to learning or to uncertainties about the environment, because it occurs just as\r
strongly in planning methods, such as dynamic programming, in which the environment\r
is completely known.\r
If any two elements of the deadly triad are present, but not all three, then instability\r
can be avoided. It is natural, then, to go through the three and see if there is any one\r
that can be given up.\r
Of the three, function approximation most clearly cannot be given up. We need\r
methods that scale to large problems and to great expressive power. We need at least\r
linear function approximation with many features and parameters. State aggregation or\r
nonparametric methods whose complexity grows with data are too weak or too expensive.\r
Least-squares methods such as LSTD are of quadratic complexity and are therefore too\r
expensive for large problems.\r
Doing without bootstrapping is possible, at the cost of computational and data eciency.\r
Perhaps most important are the losses in computational eciency. Monte Carlo (non\u0002bootstrapping) methods require memory to save everything that happens between making

11.3. The Deadly Triad 265\r
each prediction and obtaining the final return, and all their computation is done once the\r
final return is obtained. The cost of these computational issues is not apparent on serial\r
von Neumann computers, but would be on specialized hardware. With bootstrapping and\r
eligibility traces (Chapter 12), data can be dealt with when and where it is generated,\r
then need never be used again. The savings in communication and memory made possible\r
by bootstrapping are great.\r
The losses in data eciency by giving up bootstrapping are also significant. We\r
have seen this repeatedly, such as in Chapters 7 (Figure 7.2) and 9 (Figure 9.2), where\r
some degree of bootstrapping performed much better than Monte Carlo methods on\r
the random-walk prediction task, and in Chapter 10 where the same was seen on the\r
Mountain-Car control task (Figure 10.4). Many other problems show much faster learning\r
with bootstrapping (e.g., see Figure 12.14). Bootstrapping often results in faster learning\r
because it allows learning to take advantage of the state property, the ability to recognize\r
a state upon returning to it. On the other hand, bootstrapping can impair learning on\r
problems where the state representation is poor and causes poor generalization (e.g.,\r
this seems to be the case on Tetris, see S¸im¸sek, Alg´orta, and Kothiyal, 2016). A poor\r
state representation can also result in bias; this is the reason for the poorer bound on\r
the asymptotic approximation quality of bootstrapping methods (Equation 9.14). On\r
balance, the ability to bootstrap has to be considered extremely valuable. One may\r
sometimes choose not to use it by selecting long n-step updates (or a large bootstrapping\r
parameter,  ⇡ 1; see Chapter 12) but often bootstrapping greatly increases eciency. It\r
is an ability that we would very much like to keep in our toolkit.\r
Finally, there is o↵-policy learning; can we give that up? On-policy methods are often\r
adequate. For model-free reinforcement learning, one can simply use Sarsa rather than\r
Q-learning. O↵-policy methods free behavior from the target policy. This could be\r
considered an appealing convenience but not a necessity. However, o↵-policy learning\r
is essential to other anticipated use cases, cases that we have not yet mentioned in this\r
book but may be important to the larger goal of creating a powerful intelligent agent.\r
In these use cases, the agent learns not just a single value function and single policy,\r
but large numbers of them in parallel. There is extensive psychological evidence that\r
people and animals learn to predict many di↵erent sensory events, not just rewards. We\r
can be surprised by unusual events, and correct our predictions about them, even if\r
they are of neutral valence (neither good nor bad). This kind of prediction presumably\r
underlies predictive models of the world such as are used in planning. We predict what\r
we will see after eye movements, how long it will take to walk home, the probability of\r
making a jump shot in basketball, and the satisfaction we will get from taking on a new\r
project. In all these cases, the events we would like to predict depend on our acting in\r
a certain way. To learn them all, in parallel, requires learning from the one stream of\r
experience. There are many target policies, and thus the one behavior policy cannot\r
equal all of them. Yet parallel learning is conceptually possible because the behavior\r
policy may overlap in part with many of the target policies. To take full advantage of\r
this requires o↵-policy learning.

266 Chapter 11: O↵-policy Methods with Approximation\r
11.4 Linear Value-function Geometry\r
To better understand the stability challenge of o↵-policy learning, it is helpful to think\r
about value function approximation more abstractly and independently of how learning\r
is done. We can imagine the space of all possible state-value functions—all functions\r
from states to real numbers v : S ! R. Most of these value functions do not correspond\r
to any policy. More important for our purposes is that most are not representable by the\r
function approximator, which by design has far fewer parameters than there are states.\r
Given an enumeration of the state space S = {s1, s2,...,s|S|}, any value function v\r
corresponds to a vector listing the value of each state in order [v(s1), v(s2),...,v(s|S|)]>.\r
This vector representation of a value function has as many components as there are\r
states. In most cases where we want to use function approximation, this would be far\r
too many components to represent the vector explicitly. Nevertheless, the idea of this\r
vector is conceptually useful. In the following, we treat a value function and its vector\r
representation interchangeably.\r
To develop intuitions, consider the case with three states S = {s1, s2, s3} and two\r
parameters w = (w1, w2)>. We can then view all value functions/vectors as points in\r
a three-dimensional space. The parameters provide an alternative coordinate system\r
over a two-dimensional subspace. Any weight vector w = (w1, w2)> is a point in the\r
two-dimensional subspace and thus also a complete value function vw that assigns values\r
to all three states. With general function approximation the relationship between the\r
full space and the subspace of representable functions could be complex, but in the case\r
of linear value-function approximation the subspace is a simple plane, as suggested by\r
Figure 11.3.\r
Now consider a single fixed policy ⇡. We assume that its true value function, v⇡, is too\r
complex to be represented exactly as an approximation. Thus v⇡ is not in the subspace;\r
in the figure it is depicted as being above the planar subspace of representable functions.\r
If v⇡ cannot be represented exactly, what representable value function is closest to\r
it? This turns out to be a subtle question with multiple answers. To begin, we need\r
a measure of the distance between two value functions. Given two value functions v1\r
and v2, we can talk about the vector di↵erence between them, v = v1  v2. If v is small,\r
then the two value functions are close to each other. But how are we to measure the size\r
of this di↵erence vector? The conventional Euclidean norm is not appropriate because,\r
as discussed in Section 9.2, some states are more important than others because they\r
occur more frequently or because we are more interested in them (Section 9.11). As\r
in Section 9.2, let us use the distribution µ : S ! [0, 1] to specify the degree to which\r
we care about di↵erent states being accurately valued (often taken to be the on-policy\r
distribution). We can then define the distance between value functions using the norm\r
kvk\r
2\r
µ\r
.\r
= X\r
s2S\r
µ(s)v(s)\r
2. (11.11)\r
Note that the VE from Section 9.2 can be written simply using this norm as VE(w) =\r
kvw  v⇡k\r
2\r
µ. For any value function v, the operation of finding its closest value function\r
in the subspace of representable value functions is a projection operation. We define a

11.4. Linear Value-function Geometry 267\r
from each state:\r
⇡ = arg\r
⇡\r
where\r
v⇡(s) = E⇡\r
⇥\r
Rt+1 + Rt+2 \r
where  2 [0, 1) is known as the dis\r
indicates that the expectation is cond\r
The function v⇡ is called the state-valu\r
A key subproblem underlying almo\r
evaluation, the computation or estima\r
popular DP algorithm known as policy \r
a sequence of policies, each of which is \r
found. In TDL, algorithms such as TD\r
the current policy, for example as part \r
If the state space is finite, then th\r
computer as a large array with one ent\r
form the estimate. Such tabular meth\r
ones, through discretization, state aggr\r
of the state space increases, these me\r
ine↵ective. This is the e↵ect which gav\r
A more general and flexible approa\r
form of fixed size and fixed structure wi\r
are then changed to reshape the approx\r
function. We denote the parameterized \r
v(s) ⇡ \r
where ✓ 2 Rn, with n ⌧ |S|, is the \r
function can have arbitrary form as lo\r
the weights. For example, it could be \r
layer neural network where ✓ is the con\r
refer to ✓ exclusively as the weights, o\r
for things like the discount-rate param\r
An important special case is that i\r
the weights and in features of the state\r
v\r
where the (s) 2 Rn, s 2 S, are fe\r
denotes the inner product of two vecto\r
The subspace of all value functions representable as \r
Bellman error vector (BE)\r
its projected form:\r
v = ⇧B⇡v, \r
Unlike the original Bellman equation, for most function approximato\r
the projected Bellman equation can be solved exactly. If it can’t be so\r
minimize the mean-squared projected Bellman error :\r
PBE(✓) = X\r
sS\r
d(s)\r
⇥\r
(⇧(B⇡v  v))(s)\r
⇤2\r
. \r
The minimum is achieved at the projection fixpoint, at which\r
X\r
sS\r
d(s)\r
⇥\r
(B⇡v)(s)  v(s)\r
⇤\r
rv(s) = 0. \r
p\r
VE pBE pPBE ⇧v⇡ = v⇤\r
VE v⇤PBE v⇤BE\r
4\r
The \r
(1985). Baird (1995, 1999) extended \r
Engel, Mannor, and Meir (2003) extended it to least squares (O(\r
)) \r
Gaussian Process TDL. In the literature, BE minimization is often referred to as Bellman\r
residual minimization.\r
2.3 Projected Bellman error\r
The third goal for approximation is to approximately solve the projected Bellman equation:\r
v = ⇧B⇡v. (11)\r
Unlike the original Bellman equation, for most function approximators (e.g., linear ones) the\r
projected Bellman equation can be solved exactly. The original TDL methods (Sutton 1988,\r
Dayan 1992) converge to this solution, as does least-squares TDL (Bradke & Barto 1996,\r
Boyan 1999). The goal of achieving (11) exactly is common; less common is to consider\r
approximating it as an objective. The early work on gradient-TD (e.g., Sutton et al. 2009)\r
appears to be first to have explicitly proposed minimizing the d-weighted norm of the error\r
in (11), which we here call the projected Bellman error :\r
PBE(✓) = ||v  ⇧B⇡v||. (12)\r
This objective is best understood by looking at the left side of Figure 1. Starting at v,\r
the Bellman operator takes us outside the subspace, and the projection operator takes us\r
back into it. The distance between where we end up and where we started is the PBE. The\r
distance is minimal (zero) when the trip up and back leaves us in the same place.\r
8\r
Value error (VE)\r
w1\r
w2\r
vw\r
vw\r
B⇡vw\r
⇧B⇡vw\r
wTD\r
PBE = 0\r
The 3D space of \r
all value functions \r
over 3 states\r
min BE\r
min TDE\r
(min VE)\r
Figure 11.3: The geometry of linear value-function approximation. Shown is the three\u0002dimensional space of all value functions over three states, while shown as a plane is the subspace of\r
all value functions representable by a linear function approximator with parameter w = (w1, w2)\r
>.\r
The true value function v⇡ is in the larger space and can be projected down (into the subspace,\r
using a projection operator ⇧) to its best approximation in the value error (VE) sense. The\r
best approximators in the Bellman error (BE), projected Bellman error (PBE), and temporal\r
di↵erence error (TDE) senses are all potentially di↵erent and are shown in the lower right. (VE,\r
BE, and PBE are all treated as the corresponding vectors in this figure.) The Bellman operator\r
takes a value function in the plane to one outside, which can then be projected back. If you\r
iteratively applied the Bellman operator outside the space (shown in gray above) you would\r
reach the true value function, as in conventional dynamic programming. If instead you kept\r
projecting back into the subspace at each step, as in the lower step shown in gray, then the fixed\r
point would be the point of vector-zero PBE.\r
projection operator ⇧ that takes an arbitrary value function to the representable function\r
that is closest in our norm:\r
⇧v .= vw where w = argmin w2Rdkv  vwk\r
2\r
µ . (11.12)\r
The representable value function that is closest to the true value function v⇡ is thus its\r
projection, ⇧v⇡, as suggested in Figure 11.3. This is the solution asymptotically found\r
by Monte Carlo methods, albeit often very slowly. The projection operation is discussed\r
more fully in the box on the next page.\r
TD methods find di↵erent solutions. To understand their rationale, recall that the\r
Bellman equation for value function v⇡ is\r
v⇡(s) = X\r
a\r
⇡(a|s)\r
X\r
s0,r\r
p(s0, r|s, a) [r + v⇡(s0)] , for all s 2 S. (11.16)

268 Chapter 11: O↵-policy Methods with Approximation\r
The projection matrix\r
For a linear function approximator, the projection operation is linear, which implies\r
that it can be represented as an |S| ⇥ |S| matrix:\r
⇧ .= X X>DX1X>D, (11.13)\r
where, as in Section 9.4, D denotes the |S| ⇥ |S| diagonal matrix with the µ(s)\r
on the diagonal, and X denotes the |S| ⇥ d matrix whose rows are the feature\r
vectors x(s)>, one for each state s. If the inverse in (11.13) does not exist, then the\r
pseudoinverse is substituted. Using these matrices, the squared norm of a vector\r
can be written\r
kvk\r
2\r
µ = v>Dv, (11.14)\r
and the approximate linear value function can be written\r
vw = Xw. (11.15)\r
The true value function v⇡ is the only value function that solves (11.16) exactly. If an\r
approximate value function vw were substituted for v⇡, the di↵erence between the right\r
and left sides of the modified equation could be used as a measure of how far o↵ vw is\r
from v⇡. We call this the Bellman error at state s:\r
¯w(s) .\r
=\r
0\r
@X\r
a\r
⇡(a|s)\r
X\r
s0,r\r
p(s0, r|s, a) [r + vw(s0)]\r
1\r
A  vw(s) (11.17)\r
= E⇡\r
⇥\r
Rt+1 + vw(St+1)  vw(St)\r
\r
 St = s, At ⇠ ⇡\r
⇤\r
, (11.18)\r
which shows clearly the relationship of the Bellman error to the TD error (11.3). The\r
Bellman error is the expectation of the TD error.\r
The vector of all the Bellman errors, at all states, ¯w 2 R|S|, is called the Bellman\r
error vector (shown as BE in Figure 11.3). The overall size of this vector, in the norm, is\r
an overall measure of the error in the value function, called the mean square Bellman\r
error :\r
BE(w) = \r
¯w\r
\r
\r
2\r
µ . (11.19)\r
It is not possible in general to reduce the BE to zero (at which point vw = v⇡), but for\r
linear function approximation there is a unique value of w for which the BE is minimized.\r
This point in the representable-function subspace (labeled min BE in Figure 11.3) is\r
di↵erent in general from that which minimizes the VE (shown as ⇧v⇡). Methods that\r
seek to minimize the BE are discussed in the next two sections.\r
The Bellman error vector is shown in Figure 11.3 as the result of applying the Bellman\r
operator B⇡ : R|S| ! R|S| to the approximate value function. The Bellman operator is

11.5. Gradient Descent in the Bellman Error 269\r
defined by\r
(B⇡v)(s) .= X\r
a\r
⇡(a|s)\r
X\r
s0,r\r
p(s0, r|s, a) [r + v(s0)] , (11.20)\r
for all s 2 S, v : S ! R. The Bellman error vector for vw can be written ¯w = B⇡vw vw.\r
If the Bellman operator is applied to a value function in the representable subspace,\r
then, in general, it will produce a new value function that is outside the subspace, as\r
suggested in the figure. In dynamic programming (without function approximation), this\r
operator is applied repeatedly to the points outside the representable space, as suggested\r
by the gray arrows in the top of Figure 11.3. Eventually that process converges to the\r
true value function v⇡, the only fixed point for the Bellman operator, the only value\r
function for which\r
v⇡ = B⇡v⇡, (11.21)\r
which is just another way of writing the Bellman equation for ⇡ (11.16).\r
With function approximation, however, the intermediate value functions lying outside\r
the subspace cannot be represented. The gray arrows in the upper part of Figure 11.3\r
cannot be followed because after the first update (dark line) the value function must\r
be projected back into something representable. The next iteration then begins within\r
the subspace; the value function is again taken outside of the subspace by the Bellman\r
operator and then mapped back by the projection operator, as suggested by the lower\r
gray arrow and line. Following these arrows is a DP-like process with approximation.\r
In this case we are interested in the projection of the Bellman error vector back into\r
the representable space. This is the projected Bellman error vector ⇧¯w, shown in\r
Figure 11.3 as PBE. The size of this vector, in the norm, is another measure of error in\r
the approximate value function. For any approximate value function vw, we define the\r
mean square Projected Bellman error, denoted PBE, as\r
PBE(w) = \r
⇧¯w\r
\r
\r
2\r
µ . (11.22)\r
With linear function approximation there always exists an approximate value function\r
(within the subspace) with zero PBE; this is the TD fixed point, wTD, introduced in\r
Section 9.4. As we have seen, this point is not always stable under semi-gradient TD\r
methods and o↵-policy training. As shown in the figure, this value function is generally\r
di↵erent from those minimizing VE or BE. Methods that are guaranteed to converge to\r
it are discussed in Sections 11.7 and 11.8.\r
11.5 Gradient Descent in the Bellman Error\r
Armed with a better understanding of value function approximation and its various\r
objectives, we return now to the challenge of stability in o↵-policy learning. We would\r
like to apply the approach of stochastic gradient descent (SGD, Section 9.3), in which\r
updates are made that in expectation are equal to the negative gradient of an objective

270 Chapter 11: O↵-policy Methods with Approximation\r
function. These methods always go downhill (in expectation) in the objective and because\r
of this are typically stable with excellent convergence properties. Among the algorithms\r
investigated so far in this book, only the Monte Carlo methods are true SGD methods.\r
These methods converge robustly under both on-policy and o↵-policy training as well\r
as for general nonlinear (di↵erentiable) function approximators, though they are often\r
slower than semi-gradient methods with bootstrapping, which are not SGD methods.\r
Semi-gradient methods may diverge under o↵-policy training, as we have seen earlier in\r
this chapter, and under contrived cases of nonlinear function approximation (Tsitsiklis\r
and Van Roy, 1997). With a true SGD method such divergence would not be possible.\r
The appeal of SGD is so strong that great e↵ort has gone into finding a practical\r
way of harnessing it for reinforcement learning. The starting place of all such e↵orts is\r
the choice of an error or objective function to optimize. In this and the next section\r
we explore the origins and limits of the most popular proposed objective function, that\r
based on the Bellman error introduced in the previous section. Although this has been a\r
popular and influential approach, the conclusion that we reach here is that it is a misstep\r
and yields no good learning algorithms. On the other hand, this approach fails in an\r
interesting way that o↵ers insight into what might constitute a good approach.\r
To begin, let us consider not the Bellman error, but something more immediate\r
and naive. Temporal di↵erence learning is driven by the TD error. Why not take the\r
minimization of the expected square of the TD error as the objective? In the general\r
function-approximation case, the one-step TD error with discounting is\r
t = Rt+1 + vˆ(St+1,wt)  vˆ(St,wt).\r
A possible objective function then is what one might call the mean square TD error :\r
TDE(w) = X\r
s2S\r
µ(s)E\r
⇥\r
2\r
t\r
\r
 St =s, At ⇠⇡\r
⇤\r
= X\r
s2S\r
µ(s)E\r
⇥\r
⇢t2\r
t\r
\r
 St =s, At ⇠b\r
⇤\r
= Eb\r
⇥\r
⇢t2\r
t\r
⇤\r
. (if µ is the distribution encountered under b)\r
The last equation is of the form needed for SGD; it gives the objective as an expectation\r
that can be sampled from experience (remember the experience is due to the behavior\r
policy b). Thus, following the standard SGD approach, one can derive the per-step update\r
based on a sample of this expected value:\r
wt+1 = wt  1\r
2\r
↵r(⇢t2\r
t )\r
= wt  ↵⇢ttrt\r
= wt + ↵⇢tt\r
\r
rvˆ(St,wt)  rvˆ(St+1,wt)\r
\r
, (11.23)\r
which you will recognize as the same as the semi-gradient TD algorithm (11.2) except for\r
the additional final term. This term completes the gradient and makes this a true SGD\r
algorithm with excellent convergence guarantees. Let us call this algorithm the naive

11.5. Gradient Descent in the Bellman Error 271\r
residual-gradient algorithm (after Baird, 1995). Although the naive residual-gradient\r
algorithm converges robustly, it does not necessarily converge to a desirable place.\r
Example 11.2: A-split example,\r
showing the naivet´e of the naive residual-gradient algorithm\r
A\r
B\r
C\r
0\r
0\r
0\r
1\r
Consider the three-state episodic MRP shown to the right.\r
Episodes begin in state A and then ‘split’ stochastically, half\r
the time going to B (and then invariably going on to terminate\r
with a reward of 1) and half the time going to state C (and\r
then invariably terminating with a reward of zero). Reward for\r
the first transition, out of A, is always zero whichever way the\r
episode goes. As this is an episodic problem, we can take  to\r
be 1. We also assume on-policy training, so that ⇢t is always\r
1, and tabular function approximation, so that the learning algorithms are free to\r
give arbitrary, independent values to all three states. Thus, this should be an easy\r
problem.\r
What should the values be? From A, half the time the return is 1, and half the\r
time the return is 0; A should have value 1\r
2 . From B the return is always 1, so its\r
value should be 1, and similarly from C the return is always 0, so its value should\r
be 0. These are the true values and, as this is a tabular problem, all the methods\r
presented previously converge to them exactly.\r
However, the naive residual-gradient algorithm finds di↵erent values for B and\r
C. It converges with B having a value of 3\r
4 and C having a value of 14 (A converges\r
correctly to 1\r
2 ). These are in fact the values that minimize the TDE.\r
Let us compute the TDE for these values. The first transition of each episode is\r
either up from A’s 1\r
2 to B’s 34 , a change of 14 , or down from A’s 12 to C’s 14 , a change\r
of 1\r
4 . Because the reward is zero on these transitions, and  = 1, these changes are\r
the TD errors, and thus the squared TD error is always 1\r
16 on the first transition.\r
The second transition is similar; it is either up from B’s 3\r
4 to a reward of 1 (and a\r
terminal state of value 0), or down from C’s 1\r
4 to a reward of 0 (again with a terminal\r
state of value 0). Thus, the TD error is always ±1\r
4 , for a squared error of 116 on the\r
second step. Thus, for this set of values, the TDE on both steps is 1\r
16 .\r
Now let’s compute the TDE for the true values (B at 1, C at 0, and A at 1\r
2 ). In this\r
case the first transition is either from 1\r
2 up to 1, at B, or from 12 down to 0, at C; in\r
either case the absolute error is 1\r
2 and the squared error is 14 . The second transition\r
has zero error because the starting value, either 1 or 0 depending on whether the\r
transition is from B or C, always exactly matches the immediate reward and return.\r
Thus the squared TD error is 1\r
4 on the first transition and 0 on the second, for a\r
mean reward over the two transitions of 1\r
8 . As 18 is bigger that 116 , this solution is\r
worse according to the TDE. On this simple problem, the true values do not have\r
the smallest TDE.

272 Chapter 11: O↵-policy Methods with Approximation\r
A tabular representation is used in the A-split example, so the true state values can\r
be exactly represented, yet the naive residual-gradient algorithm finds di↵erent values,\r
and these values have lower TDE than do the true values. Minimizing the TDE is naive;\r
by penalizing all TD errors it achieves something more like temporal smoothing than\r
accurate prediction.\r
A better idea would seem to be minimizing the mean square Bellman error (BE). If\r
the exact values are learned, the Bellman error is zero everywhere. Thus, a Bellman\u0002error-minimizing algorithm should have no trouble with the A-split example. We cannot\r
expect to achieve zero Bellman error in general, as it would involve finding the true value\r
function, which we presume is outside the space of representable value functions. But\r
getting close to this ideal is a natural-seeming goal. As we have seen, the Bellman error\r
is also closely related to the TD error. The Bellman error for a state is the expected TD\r
error in that state. So let’s repeat the derivation above with the expected TD error (all\r
expectations here are implicitly conditional on St):\r
wt+1 = wt  1\r
2\r
↵r(E⇡[t]\r
2\r
)\r
= wt  1\r
2\r
↵r(Eb[⇢tt]\r
2\r
)\r
= wt  ↵Eb[⇢tt] rEb[⇢tt]\r
= wt  ↵Eb\r
⇥\r
⇢t(Rt+1 + vˆ(St+1,w)  vˆ(St,w))⇤Eb[⇢trt]\r
= wt + ↵\r
h\r
Eb\r
⇥\r
⇢t(Rt+1 + vˆ(St+1,w))⇤ vˆ(St,w)\r
ihrvˆ(St,w)  Eb\r
⇥\r
⇢trvˆ(St+1,w)\r
⇤i\r
.\r
This update and various ways of sampling it are referred to as the residual-gradient\r
algorithm. If you simply used the sample values in all the expectations, then the equation\r
above reduces almost exactly to (11.23), the naive residual-gradient algorithm.1 But\r
this is naive, because the equation above involves the next state, St+1, appearing in two\r
expectations that are multiplied together. To get an unbiased sample of the product,\r
two independent samples of the next state are required, but during normal interaction\r
with an external environment only one is obtained. One expectation or the other can be\r
sampled, but not both.\r
There are two ways to make the residual-gradient algorithm work. One is in the case\r
of deterministic environments. If the transition to the next state is deterministic, then\r
the two samples will necessarily be the same, and the naive algorithm is valid. The\r
other way is to obtain two independent samples of the next state, St+1, from St, one for\r
the first expectation and another for the second expectation. In real interaction with\r
an environment, this would not seem possible, but when interacting with a simulated\r
environment, it is. One simply rolls back to the previous state and obtains an alternate\r
next state before proceeding forward from the first next state. In either of these cases the\r
residual-gradient algorithm is guaranteed to converge to a minimum of the BE under the\r
usual conditions on the step-size parameter. As a true SGD method, this convergence is\r
1For state values there remains a small di↵erence in the treatment of the importance sampling ratio\r
⇢t. In the analagous action-value case (which is the most important case for control algorithms), the\r
residual-gradient algorithm would reduce exactly to the naive version.

11.5. Gradient Descent in the Bellman Error 273\r
robust, applying to both linear and nonlinear function approximators. In the linear case,\r
convergence is always to the unique w that minimizes the BE.\r
However, there remain at least three ways in which the convergence of the residual\u0002gradient method is unsatisfactory. The first of these is that empirically it is slow, much\r
slower that semi-gradient methods. Indeed, proponents of this method have proposed\r
increasing its speed by combining it with faster semi-gradient methods initially, then\r
gradually switching over to residual gradient for the convergence guarantee (Baird and\r
Moore, 1999). The second way in which the residual-gradient algorithm is unsatisfactory\r
is that it still seems to converge to the wrong values. It does get the right values in all\r
tabular cases, such as the A-split example, as for those an exact solution to the Bellman\r
Example 11.3: A-presplit example, a counterexample for the BE\r
A1 B\r
C 0 0\r
0 1\r
A2\r
A\r
Consider the three-state episodic MRP shown to the\r
right: Episodes start in either A1 or A2, with equal\r
probability. These two states look exactly the same to\r
the function approximator, like a single state A whose\r
feature representation is distinct from and unrelated to\r
the feature representation of the other two states, B and\r
C, which are also distinct from each other. Specifically,\r
the parameter of the function approximator has three\r
components, one giving the value of state B, one giving the value of state C, and one\r
giving the value of both states A1 and A2. Other than the selection of the initial\r
state, the system is deterministic. If it starts in A1, then it transitions to B with a\r
reward of 0 and then on to termination with a reward of 1. If it starts in A2, then it\r
transitions to C, and then to termination, with both rewards zero.\r
To a learning algorithm, seeing only the features, the system looks identical to\r
the A-split example. The system seems to always start in A, followed by either\r
B or C with equal probability, and then terminating with a 1 or a 0 depending\r
deterministically on the previous state. As in the A-split example, the true values\r
of B and C are 1 and 0, and the best shared value of A1 and A2 is 1\r
2 , by symmetry.\r
Because this problem appears externally identical to the A-split example, we\r
already know what values will be found by the algorithms. Semi-gradient TD\r
converges to the ideal values just mentioned, while the naive residual-gradient\r
algorithm converges to values of 3\r
4 and 14 for B and C respectively. All state\r
transitions are deterministic, so the non-naive residual-gradient algorithm will also\r
converge to these values (it is the same algorithm in this case). It follows then that\r
this ‘naive’ solution must also be the one that minimizes the BE, and so it is. On a\r
deterministic problem, the Bellman errors and TD errors are all the same, so the\r
BE is always the same as the TDE. Optimizing the BE on this example gives rise to\r
the same failure mode as with the naive residual-gradient algorithm on the A-split\r
example.

274 Chapter 11: O↵-policy Methods with Approximation\r
equation is possible. But if we examine examples with genuine function approximation,\r
then the residual-gradient algorithm, and indeed the BE objective, seem to find the\r
wrong value functions. One of the most telling such examples is the variation on the\r
A-split example known as the A-presplit example, shown on the preceding page, in which\r
the residual-gradient algorithm finds the same poor solution as its naive version. This\r
example shows intuitively that minimizing the BE (which the residual-gradient algorithm\r
surely does) may not be a desirable goal.\r
The third way in which the convergence of the residual-gradient algorithm is not\r
satisfactory is explained in the next section. Like the second way, the third way is also\r
a problem with the BE objective itself rather than with any particular algorithm for\r
achieving it.\r
11.6 The Bellman Error is Not Learnable\r
The concept of learnability that we introduce in this section is di↵erent from that\r
commonly used in machine learning. There, a hypothesis is said to be “learnable” if\r
it is eciently learnable, meaning that it can be learned within a polynomial rather\r
than an exponential number of examples. Here we use the term in a more basic way,\r
to mean learnable at all, with any amount of experience. It turns out many quantities\r
of apparent interest in reinforcement learning cannot be learned even from an infinite\r
amount of experiential data. These quantities are well defined and can be computed\r
given knowledge of the internal structure of the environment, but cannot be computed\r
or estimated from the observed sequence of feature vectors, actions, and rewards.2 We\r
say that they are not learnable. It will turn out that the Bellman error objective (BE)\r
introduced in the last two sections is not learnable in this sense. That the Bellman error\r
objective cannot be learned from the observable data is probably the strongest reason\r
not to seek it.\r
To make the concept of learnability clear, let’s start with some simple examples.\r
Consider the two Markov reward processes3 (MRPs) diagrammed below:\r
0 2 0 2\r
2\r
0\r
w w w\r
Where two edges leave a state, both transitions are assumed to occur with equal probability,\r
and the numbers indicate the reward received. All the states appear the same; they all\r
produce the same single-component feature vector x = 1 and have approximated value\r
w. Thus, the only varying part of the data trajectory is the reward sequence. The left\r
MRP stays in the same state and emits an endless stream of 0s and 2s at random, each\r
with 0.5 probability. The right MRP, on every step, either stays in its current state or\r
2They would of course be estimated if the state sequence were observed rather than only the\r
corresponding feature vectors.\r
3All MRPs can be considered MDPs with a single action in all states; what we conclude about MRPs\r
here applies as well to MDPs.

11.6. The Bellman Error is Not Learnable 275\r
switches to the other, with equal probability. The reward is deterministic in this MRP,\r
always a 0 from one state and always a 2 from the other, but because the each state\r
is equally likely on each step, the observable data is again an endless stream of 0s and\r
2s at random, identical to that produced by the left MRP. (We can assume the right\r
MRP starts in one of two states at random with equal probability.) Thus, even given\r
an infinite amount of data, it would not be possible to tell which of these two MRPs\r
was generating it. In particular, we could not tell if the MRP has one state or two, is\r
stochastic or deterministic. These things are not learnable.\r
This pair of MRPs also illustrates that the VE objective (9.1) is not learnable. If\r
 = 0, then the true values of the three states (in both MRPs), left to right, are 1, 0,\r
and 2. Suppose w = 1. Then the VE is 0 for the left MRP and 1 for the right MRP.\r
Because the VE is di↵erent in the two problems, yet the data generated has the same\r
distribution, the VE cannot be learned. The VE is not a unique function of the data\r
distribution. And if it cannot be learned, then how could the VE possibly be useful as\r
an objective for learning?\r
If an objective cannot be learned, it does indeed draw its utility into question. In\r
the case of the VE, however, there is a way out. Note that the same solution, w = 1,\r
is optimal for both MRPs above (assuming µ is the same for the two indistinguishable\r
states in the right MRP). Is this a coincidence, or could it be generally true that all\r
MDPs with the same data distribution also have the same optimal parameter vector? If\r
this is true—and we will show next that it is—then the VE remains a usable objective.\r
The VE is not learnable, but the parameter that optimizes it is!\r
To understand this, it is useful to bring in another natural objective function, this\r
time one that is clearly learnable. One error that is always observable is that between the\r
value estimate at each time and the return from that time. The mean square return error,\r
denoted RE, is the expectation, under µ, of the square of this error. In the on-policy case\r
the RE can be written\r
RE(w) = E\r
h\r
Gt  vˆ(St,w)\r
2\r
i\r
= VE(w) + E\r
h\r
Gt  v⇡(St)\r
2\r
i\r
. (11.24)\r
Thus, the two objectives are the same except for a variance term that does not depend on\r
the parameter vector. The two objectives must therefore have the same optimal parameter\r
value w⇤. The overall relationships are summarized in the left side of Figure 11.4.\r
⇤\r
Exercise 11.4 Prove (11.24). Hint: Write the RE as an expectation over possible states\r
s of the expectation of the squared error given that St = s. Then add and subtract the\r
true value of state s from the error (before squaring), grouping the subtracted true value\r
with the return and the added true value with the estimated value. Then, if you expand\r
the square, the most complex term will end up being zero, leaving you with (11.24). ⇤\r
Now let us return to the BE. The BE is like the VE in that it can be computed from\r
knowledge of the MDP but is not learnable from data. But it is not like the VE in that its\r
minimum solution is not learnable. The box on the next page presents a counterexample—\r
two MRPs that generate the same data distribution but whose minimizing parameter\r
vector is di↵erent, proving that the optimal parameter vector is not a function of the

276 Chapter 11: O↵-policy Methods with Approximation\r
Example 11.4: Counterexample to the learnability of the Bellman error\r
To show the full range of possibilities we need a slightly more complex pair of Markov\r
reward processes (MRPs) than those considered earlier. Consider the following two\r
MRPs:\r
A B\r
1\r
0\r
-1\r
A B\r
0\r
-1 B\u0001\r
0\r
1 -1\r
Where two edges leave a state, both transitions are assumed to occur with equal\r
probability, and the numbers indicate the reward received. The MRP on the left has\r
two states that are represented distinctly. The MRP on the right has three states,\r
two of which, B and B0, appear the same and must be given the same approximate\r
value. Specifically, w has two components and the value of state A is given by the first\r
component and the value of B and B0 is given by the second. The second MRP has\r
been designed so that equal time is spent in all three states, so we can take µ(s) = 1\r
3 ,\r
for all s.\r
Note that the observable data distribution is identical for the two MRPs. In both\r
cases the agent will see single occurrences of A followed by a 0, then some number\r
of apparent Bs, each followed by a 1 except the last, which is followed by a 1, then\r
we start all over again with a single A and a 0, etc. All the statistical details are the\r
same as well; in both MRPs, the probability of a string of k Bs is 2k.\r
Now suppose w = 0. In the first MRP, this is an exact solution, and the BE is\r
zero. In the second MRP, this solution produces a squared error in both B and B0 of\r
1, such that BE = µ(B)1 + µ(B0)1 = 2\r
3 . These two MRPs, which generate the same\r
data distribution, have di↵erent BEs; the BE is not learnable.\r
Moreover (and unlike the earlier example for the VE) the minimizing value of w\r
is di↵erent for the two MRPs. For the first MRP, w = 0 minimizes the BE for any\r
. For the second MRP, the minimizing w is a complicated function of , but in\r
the limit, as  ! 1, it is (1\r
2 , 0)>. Thus the solution that minimizes BE cannot be\r
estimated from data alone; knowledge of the MRP beyond what is revealed in the\r
data is required. In this sense, it is impossible in principle to pursue the BE as an\r
objective for learning.\r
It may be surprising that in the second MRP the BE-minimizing value of A is so far\r
from zero. Recall that A has a dedicated weight and thus its value is unconstrained\r
by function approximation. A is followed by a reward of 0 and transition to a state\r
with a value of nearly 0, which suggests vw(A) should be 0; why is its optimal\r
value substantially negative rather than 0? The answer is that making vw(A) negative\r
reduces the error upon arriving in A from B. The reward on this deterministic transition\r
is 1, which implies that B should have a value 1 more than A. Because B’s value is\r
approximately zero, A’s value is driven toward 1. The BE-minimizing value of ⇡ 1\r
2\r
for A is a compromise between reducing the errors on leaving and on entering A.

11.6. The Bellman Error is Not Learnable 277\r
MDP1 MDP2\r
MSBE1 MSBE2\r
policy together completely determine the probability distribution over da\r
Assume for the moment that the state, action, and reward sets are all \r
for any finite sequence  = 0, a0, r1,...,rk, k, there is a well defined pr\r
sibly zero) of it occuring as the initial portion of a trajectory, which we \r
P() = Pr{(S0) = 0, A0 = a0, R1 = r1,...,Rk = rk, (Sk) = k}. The \r
then is a complete characterization of a source of data trajectories. To know \r
everything about the statistics of the data, but it is still less than knowin\r
particular, the VE and BE objectives are readily computed from the MDP \r
Section 3, but these cannot be determined from P alone.\r
✓1 ✓2 ✓3 ✓4 BE1 BE2 MDP1 MDP2\r
✓\r
1 ✓2 ✓3 ✓4 BE1 BE2 MDP1 MDP2\r
TDE RE VE\r
The problem can be seen in very simple, POMDP-like examples, in which \r
data produced by two di↵erent MDPs is identical in every respect, yet the \r
In such a case the BE is literally not a function of the data, and thus ther\r
estimate it from data. One of the simplest examples is the pair of MDPs sh\r
A B\r
1\r
0\r
-1\r
A B\r
0\r
-1 \r
0\r
1 \r
These MDPs have only one action (or, equivalently, no actions), so they are in \r
chains. Where two edges leave a state, both possibilities are assumed to oc\r
probability. The numbers on the edges indicate the reward emitted if that ed\r
The MDP on the left has two states that are represented distinctly; each \r
weight so that they can take on any value. The MDP on the right has th\r
of which, B and B\r
, are represented identically and must be given the sam\r
value. We can imagine that the value of state A is given by the first comp\r
the value of B and B is given by the second. Notice that the observable d\r
for the two MDPs. In both cases the agent will see single occurrences of A \r
0, then some number of Bs each followed by a 1, except the last which i\r
1, then we start all over again with a single A and a 0, etc. All the detail\r
as well; in both MDPs, the probability of a string of k Bs is 2k. Now con\r
function v = 0. In the first MDP, this is an exact solution, and the overall \r
the second MDP, this solution produces an error in both B and B of 1, for \r
of pd(B) + d(B\r
), or p2/3 if the three states are equally weighted by d. T\r
which generate the same data, have di↵erent BEs. Thus, the BE cannot be \r
data alone; knowledge of the MDP beyond what is revealed in the data is r\r
Moreover, the two MDPs have di↵erent minimal-BE value functions.2 For \r
the minimal-BE value function is the exact solution v = 0 for any . For th\r
2. This is a critical observation, as it is possible for an error function to be unobservab\r
perfectly satisfactory for use in learning settings because the value that minimizes it c\r
from data. For example, this is what happens with the VE. The VE is not observable \r
policy together completely determine the probability distributio\r
Assume for the moment that the state, action, and reward se\r
for any finite sequence  = 0, a0, r1,...,rk, k, there is a well \r
sibly zero) of it occuring as the initial portion of a trajectory, \r
P() = Pr{(S0) = 0, A0 = a0, R1 = r1,...,Rk = rk, (Sk) = \r
then is a complete characterization of a source of data trajectorie\r
everything about the statistics of the data, but it is still less tha\r
particular, the VE and BE objectives are readily computed from \r
Section 3, but these cannot be determined from P alone.\r
✓1 ✓2 ✓3 ✓4 BE1 BE2 MDP1 MDP2\r
✓\r
1 ✓2 ✓3 ✓4 BE1 BE2 MDP1 MDP2\r
TDE RE VE\r
The problem can be seen in very simple, POMDP-like example\r
data produced by two di↵erent MDPs is identical in every respec\r
In such a case the BE is literally not a function of the data, and \r
estimate it from data. One of the simplest examples is the pair of \r
A B\r
1\r
0\r
-1\r
A \r
0\r
1 \r
These MDPs have only one action (or, equivalently, no actions), so \r
chains. Where two edges leave a state, both possibilities are assu\r
probability. The numbers on the edges indicate the reward emitted \r
The MDP on the left has two states that are represented distin\r
weight so that they can take on any value. The MDP on the ri\r
of which, B and B\r
, are represented identically and must be give\r
value. We can imagine that the value of state A is given by the \r
the value of B and B is given by the second. Notice that the ob\r
for the two MDPs. In both cases the agent will see single occurr\r
0, then some number of Bs each followed by a 1, except the la\r
1, then we start all over again with a single A and a 0, etc. All \r
as well; in both MDPs, the probability of a string of k Bs is 2k\r
function v = 0. In the first MDP, this is an exact solution, and t\r
the second MDP, this solution produces an error in both B and B\r
of pd(B) + d(B\r
), or p2/3 if the three states are equally weighte\r
which generate the same data, have di↵erent BEs. Thus, the BE c\r
data alone; knowledge of the MDP beyond what is revealed in th\r
Moreover, the two MDPs have di↵erent minimal-BE value funct\r
the minimal-BE value function is the exact solution v = 0 for any \r
2. This is a critical observation, as it is possible for an error function to be \r
perfectly satisfactory for use in learning settings because the value that m\r
from data. For example, this is what happens with the VE. The VE is not \r
MSPBE\r
policy together completely determine the probability di\r
Assume for the moment that the state, action, and r\r
for any finite sequence  = 0, a0, r1,...,rk, k, there is \r
sibly zero) of it occuring as the initial portion of a tr\r
P() = Pr{(S0) = 0, A0 = a0, R1 = r1,...,Rk = rk, \r
then is a complete characterization of a source of data tr\r
everything about the statistics of the data, but it is stil\r
particular, the VE and BE objectives are readily comput\r
Section 3, but these cannot be determined from P alone.\r
✓1 ✓2 ✓3 ✓4 BE1 BE2 MDP1 \r
✓\r
1 ✓2 ✓3 ✓4 BE1 BE2 MDP1 \r
TDE RE VE\r
The problem can be seen in very simple, POMDP-like \r
data produced by two di↵erent MDPs is identical in ever\r
In such a case the BE is literally not a function of the d\r
estimate it from data. One of the simplest examples is th\r
A B\r
1\r
0\r
-1\r
A \r
These MDPs have only one action (or, equivalently, no act\r
chains. Where two edges leave a state, both possibilities \r
probability. The numbers on the edges indicate the reward \r
The MDP on the left has two states that are represente\r
weight so that they can take on any value. The MDP o\r
of which, B and B\r
, are represented identically and mus\r
value. We can imagine that the value of state A is given \r
the value of B and B is given by the second. Notice tha\r
for the two MDPs. In both cases the agent will see sing\r
0, then some number of Bs each followed by a 1, excep\r
1, then we start all over again with a single A and a 0, \r
as well; in both MDPs, the probability of a string of k B\r
function v = 0. In the first MDP, this is an exact soluti\r
the second MDP, this solution produces an error in both \r
of pd(B) + d(B\r
), or p2/3 if the three states are equally \r
which generate the same data, have di↵erent BEs. Thus, \r
data alone; knowledge of the MDP beyond what is reveal\r
Moreover, the two MDPs have di↵erent minimal-BE va\r
the minimal-BE value function is the exact solution v = \r
2. This is a critical observation, as it is possible for an error funct\r
perfectly satisfactory for use in learning settings because the val\r
from data. For example, this is what happens with the VE. The \r
20\r
MSTDE\r
policy together completely determine the proba\r
Assume for the moment that the state, action\r
for any finite sequence  = 0, a0, r1,...,rk, k\r
sibly zero) of it occuring as the initial portion \r
P() = Pr{(S0) = 0, A0 = a0, R1 = r1,...,R\r
then is a complete characterization of a source o\r
everything about the statistics of the data, but \r
particular, the VE and BE objectives are readily \r
Section 3, but these cannot be determined from \r
✓1 ✓2 ✓3 ✓4 BE1 BE2 \r
✓\r
1 ✓2 ✓3 ✓4 BE1 BE2 \r
TDE RE VE\r
The problem can be seen in very simple, POM\r
data produced by two di↵erent MDPs is identica\r
In such a case the BE is literally not a function \r
estimate it from data. One of the simplest exam\r
A B\r
1\r
0\r
-1\r
These MDPs have only one action (or, equivalent\r
chains. Where two edges leave a state, both pos\r
probability. The numbers on the edges indicate t\r
The MDP on the left has two states that are r\r
weight so that they can take on any value. The \r
of which, B and B\r
, are represented identically \r
value. We can imagine that the value of state A \r
the value of B and B is given by the second. N\r
for the two MDPs. In both cases the agent will \r
0, then some number of Bs each followed by a \r
1, then we start all over again with a single A a\r
as well; in both MDPs, the probability of a strin\r
function v = 0. In the first MDP, this is an exa\r
the second MDP, this solution produces an erro\r
of pd(B) + d(B\r
), or p2/3 if the three states ar\r
which generate the same data, have di↵erent BE\r
data alone; knowledge of the MDP beyond what \r
Moreover, the two MDPs have di↵erent minim\r
the minimal-BE value function is the exact solut\r
2. This is a critical observation, as it is possible for an \r
perfectly satisfactory for use in learning settings beca\r
from data. For example, this is what happens with th\r
20\r
Data\r
distribution\r
MDP1 MDP2\r
MSVE1 MSVE2\r
MSRE\r
Data\r
distribution \r
mine the probability distribution over data trajectories.\r
e state, action, and reward sets are all finite. Then,\r
0, r1,...,rk, k, there is a well defined probability (pos\u0002\r
initial portion of a trajectory, which we may denoted\r
R1 = r1,...,Rk = rk, (Sk) = k}. The distribution P\r
n of a source of data trajectories. To know P is to know\r
the data, but it is still less than knowing the MDP. In\r
ives are readily computed from the MDP as described in\r
termined from P alone.\r
BE1 BE2 MDP1 MDP2 PBE ✓\r
✓\r
4 BE1 BE2 MDP1 MDP2\r
ry simple, POMDP-like examples, in which the observable\r
DPs is identical in every respect, yet the BE is di↵erent.\r
not a function of the data, and thus there is no way to\r
simplest examples is the pair of MDPs shown below:\r
A B\r
0\r
-1 B\u0001\r
0\r
1 -1\r
(or, equivalently, no actions), so they are in e↵ect Markov\r
state, both possibilities are assumed to occur with equal\r
dges indicate the reward emitted if that edge is traversed.\r
ates that are represented distinctly; each has a separate\r
any value. The MDP on the right has three states, two\r
ed identically and must be given the same approximate\r
value of state A is given by the first component of ✓ and\r
the second. Notice that the observable data is identical\r
the agent will see single occurrences of A followed by a\r
followed by a 1, except the last which is followed by a\r
th a single A and a 0, etc. All the details are the same\r
bility of a string of k Bs is 2k. Now consider the value\r
, this is an exact solution, and the overall BE is zero. In\r
oduces an error in both B and B of 1, for an overall BE\r
three states are equally weighted by d. The two MDPs,\r
ve di↵erent BEs. Thus, the BE cannot be estimated from\r
P beyond what is revealed in the data is required.\r
di↵erent minimal-BE value functions.2 For the first MDP,\r
the exact solution v = 0 for any . For the second MDP,\r
iblfftitbbbld t till b\r
w⇤ w⇤\r
1 w⇤2\r
w⇤\r
3 w⇤4\r
VE1 VE2\r
RE\r
BE1 BE2\r
PBE TDE\r
Monte Carlo\r
objectives Bootstrapping\r
objectives\r
Figure 11.4: Causal relationships among the data distribution, MDPs, and various objectives.\r
Left, Monte Carlo objectives: Two di↵erent MDPs can produce the same data distribution\r
yet also produce di↵erent VEs, proving that the VE objective cannot be determined from data\r
and is not learnable. However, all such VEs must have the same optimal parameter vector, w⇤!\r
Moreover, this same w⇤ can be determined from another objective, the RE, which is uniquely\r
determined from the data distribution. Thus w⇤ and the RE are learnable even though the VEs\r
are not. Right, Bootstrapping objectives: Two di↵erent MDPs can produce the same data\r
distribution yet also produce di↵erent BEs and have di↵erent minimizing parameter vectors;\r
these are not learnable from the data distribution. The PBE and TDE objectives and their\r
(di↵erent) minima can be directly determined from data and thus are learnable.\r
data and thus cannot be learned from it. The other bootstrapping objectives that we\r
have considered, the PBE and TDE, can be determined from data (are learnable) and\r
determine optimal solutions that are in general di↵erent from each other and the BE\r
minimums. The general case is summarized in the right side of Figure 11.4.\r
Thus, the BE is not learnable; it cannot be estimated from feature vectors and other\r
observable data. This limits the BE to model-based settings. There can be no algorithm\r
that minimizes the BE without access to the underlying MDP states beyond the feature\r
vectors. The residual-gradient algorithm is only able to minimize BE because it is allowed\r
to double sample from the same state—not a state that has the same feature vector,\r
but one that is guaranteed to be the same underlying state. We can see now that there\r
is no way around this. Minimizing the BE requires some such access to the nominal,\r
underlying MDP. This is an important limitation of the BE beyond that identified in the\r
A-presplit example on page 273. All this directs more attention toward the PBE.

278 Chapter 11: O↵-policy Methods with Approximation\r
11.7 Gradient-TD Methods\r
We now consider SGD methods for minimizing the PBE. As true SGD methods, these\r
Gradient-TD methods have robust convergence properties even under o↵-policy training\r
and nonlinear function approximation. Remember that in the linear case there is always\r
an exact solution, the TD fixed point wTD, at which the PBE is zero. This solution could\r
be found by least-squares methods (Section 9.8), but only by methods of quadratic O(d2)\r
complexity in the number of parameters. We seek instead an SGD method, which should\r
be O(d) and have robust convergence properties. Gradient-TD methods come close to\r
achieving these goals, at the cost of a rough doubling of computational complexity.\r
To derive an SGD method for the PBE (assuming linear function approximation) we\r
begin by expanding and rewriting the objective (11.22) in matrix terms:\r
PBE(w) = \r
⇧¯w\r
\r
\r
2\r
µ\r
= (⇧¯w)\r
>D⇧¯w (from (11.14))\r
= ¯>\r
w⇧>D⇧¯w\r
= ¯>\r
wDX\r
X>DX1X>D¯w (11.25)\r
(using (11.13) and the identity ⇧>D⇧ = DX X>DX1 X>D)\r
= X>D¯w\r
>\r
X>DX1X>D¯w\r
\r
. (11.26)\r
The gradient with respect to w is\r
rPBE(w)=2r⇥\r
X>D¯w\r
⇤> \r
X>DX1X>D¯w\r
\r
.\r
To turn this into an SGD method, we have to sample something on every time step that\r
has this quantity as its expected value. Let us take µ to be the distribution of states\r
visited under the behavior policy. All three of the factors above can then be written in\r
terms of expectations under this distribution. For example, the last factor can be written\r
X>D¯w = X\r
s\r
µ(s)x(s)¯w(s) = E[⇢ttxt] ,\r
which is just the expectation of the semi-gradient TD(0) update (11.2). The first factor\r
is the transpose of the gradient of this update:\r
rE[⇢ttxt]\r
> = E\r
⇥\r
⇢tr>\r
t x>t\r
⇤\r
= E\r
⇥\r
⇢tr(Rt+1 + w>xt+1  w>xt)\r
>x>\r
t\r
⇤ (using episodic t)\r
= E\r
⇥\r
⇢t(xt+1  xt)x>\r
t\r
⇤\r
.\r
Finally, the middle factor is the inverse of the expected outer-product matrix of the\r
feature vectors:\r
X>DX = X\r
s\r
µ(s)x(s)x(s)\r
> = E\r
⇥\r
xtx>\r
t\r
⇤\r
.

11.7. Gradient-TD Methods 279\r
Substituting these expectations for the three factors in our expression for the gradient of\r
the PBE, we get\r
rPBE(w)=2E\r
⇥\r
⇢t(xt+1  xt)x>\r
t\r
⇤\r
E\r
⇥\r
xtx>\r
t\r
⇤1\r
E[⇢ttxt] . (11.27)\r
It might not be obvious that we have made any progress by writing the gradient in this\r
form. It is a product of three expressions and the first and last are not independent.\r
They both depend on the next feature vector xt+1; we cannot simply sample both of\r
these expectations and then multiply the samples. This would give us a biased estimate\r
of the gradient just as in the residual-gradient algorithm.\r
Another idea would be to estimate the three expectations separately and then combine\r
them to produce an unbiased estimate of the gradient. This would work, but would\r
require a lot of computational resources, particularly to store the first two expectations,\r
which are d ⇥ d matrices, and to compute the inverse of the second. This idea can be\r
improved. If two of the three expectations are estimated and stored, then the third could\r
be sampled and used in conjunction with the two stored quantities. For example, you\r
could store estimates of the second two quantities (using the increment inverse-updating\r
techniques in Section 9.8) and then sample the first expression. Unfortunately, the overall\r
algorithm would still be of quadratic complexity (of order O(d2)).\r
The idea of storing some estimates separately and then combining them with samples\r
is a good one and is also used in Gradient-TD methods. Gradient-TD methods estimate\r
and store the product of the second two factors in (11.27). These factors are a d ⇥ d\r
matrix and a d-vector, so their product is just a d-vector, like w itself. We denote this\r
second learned vector as v:\r
v ⇡ E\r
⇥\r
xtx>\r
t\r
⇤1\r
E[⇢ttxt] . (11.28)\r
This form is familiar to students of linear supervised learning. It is the solution to a linear\r
least-squares problem that tries to approximate ⇢tt from the features. The standard\r
SGD method for incrementally finding the vector v that minimizes the expected squared\r
error v>xt  ⇢tt\r
2 is known as the Least Mean Square (LMS) rule (here augmented\r
with an importance sampling ratio):\r
vt+1\r
.\r
= vt + ⇢t\r
\r
t  v>\r
t xt\r
\r
xt,\r
where  > 0 is another step-size parameter. We can use this method to e↵ectively achieve\r
(11.28) with O(d) storage and per-step computation.\r
Given a stored estimate vt approximating (11.28), we can update our main parameter\r
vector wt using SGD methods based on (11.27). The simplest such rule is\r
wt+1 = wt  1\r
2\r
↵rPBE(wt) (the general SGD rule)\r
= wt  1\r
2\r
↵2E\r
⇥\r
⇢t(xt+1  xt)x>\r
t\r
⇤\r
E\r
⇥\r
xtx>\r
t\r
⇤1\r
E[⇢ttxt] (from (11.27))\r
= wt + ↵E\r
⇥\r
⇢t(xt  xt+1)x>\r
t\r
⇤\r
E\r
⇥\r
xtx>\r
t\r
⇤1\r
E[⇢ttxt] (11.29)\r
⇡ wt + ↵E\r
⇥\r
⇢t(xt  xt+1)x>\r
t\r
⇤\r
vt (based on (11.28))\r
⇡ wt + ↵⇢t (xt  xt+1) x>\r
t vt. (sampling)

280 Chapter 11: O↵-policy Methods with Approximation\r
This algorithm is called GTD2. Note that if the final inner product (x>\r
t vt) is done first,\r
then the entire algorithm is of O(d) complexity.\r
A slightly better algorithm can be derived by doing a few more analytic steps before\r
substituting in vt. Continuing from (11.29):\r
wt+1 = wt + ↵E\r
⇥\r
⇢t(xt  xt+1)x>\r
t\r
⇤\r
E\r
⇥\r
xtx>\r
t\r
⇤1\r
E[⇢ttxt]\r
= wt + ↵ E\r
⇥\r
⇢txtx>\r
t\r
⇤\r
 E\r
⇥\r
⇢txt+1x>\r
t\r
⇤ E⇥\r
xtx>\r
t\r
⇤1\r
E[⇢ttxt]\r
= wt + ↵ E\r
⇥\r
xtx>\r
t\r
⇤\r
 E\r
⇥\r
⇢txt+1x>\r
t\r
⇤ E⇥\r
xtx>\r
t\r
⇤1\r
E[⇢ttxt]\r
= wt + ↵\r
⇣\r
E[⇢ttxt]  E\r
⇥\r
⇢txt+1x>\r
t\r
⇤\r
E\r
⇥\r
xtx>\r
t\r
⇤1\r
E[⇢ttxt]\r
⌘\r
⇡ wt + ↵ E[⇢ttxt]  E\r
⇥\r
⇢txt+1x>\r
t\r
⇤\r
vt\r
 (based on (11.28))\r
⇡ wt + ↵⇢t\r
\r
txt  xt+1x>\r
t vt\r
\r
, (sampling)\r
which again is O(d) if the final product (x>\r
t vt) is done first. This algorithm is known as\r
either TD(0) with gradient correction (TDC) or, alternatively, as GTD(0).\r
Figure 11.5 shows a sample and the expected behavior of TDC on Baird’s counterex\u0002ample. As intended, the PBE falls to zero, but note that the individual components\r
of the parameter vector do not approach zero. In fact, these values are still far from\r
w1– w6\r
w8 w8\r
p\r
VE p\r
VE\r
p\r
PBE p\r
PBE\r
10\r
5\r
2\r
0\r
-2.34\r
0 1000 0 1000 Steps\r
w7\r
w1– w6\r
TDC Expected TDC\r
Sweeps\r
w7\r
Figure 11.5: The behavior of the TDC algorithm on Baird’s counterexample. On the left is\r
shown a typical single run, and on the right is shown the expected behavior of this algorithm if\r
the updates are done synchronously (analogous to (11.9), except for the two TDC parameter\r
vectors). The step sizes were ↵ = 0.005 and  = 0.05.

11.8. Emphatic-TD Methods 281\r
an optimal solution, vˆ(s) = 0, for all s, for which w would have to be proportional to\r
(1, 1, 1, 1, 1, 1, 4, 2)>. After 1000 iterations we are still far from an optimal solution, as\r
we can see from the VE, which remains almost 2. The system is actually converging to\r
an optimal solution, but progress is extremely slow because the PBE is already so close\r
to zero.\r
GTD2 and TDC both involve two learning processes, a primary one for w and a\r
secondary one for v. The logic of the primary learning process relies on the secondary\r
learning process having finished, at least approximately, whereas the secondary learning\r
process proceeds without being influenced by the first. We call this sort of asymmetrical\r
dependence a cascade. In cascades we often assume that the secondary learning process\r
is proceeding faster and thus is always at its asymptotic value, ready and accurate to\r
assist the primary learning process. The convergence proofs for these methods often make\r
this assumption explicitly. These are called two-time-scale proofs. The fast time scale is\r
that of the secondary learning process, and the slower time scale is that of the primary\r
learning process. If ↵ is the step size of the primary learning process, and  is the step\r
size of the secondary learning process, then these convergence proofs will typically require\r
that in the limit  ! 0 and ↵\r
 ! 0.\r
Gradient-TD methods are currently the most well understood and widely used stable\r
o↵-policy methods. There are extensions to action values and control (GQ, Maei et al.,\r
2010), to eligibility traces (GTD() and GQ(), Maei, 2011; Maei and Sutton, 2010), and\r
to nonlinear function approximation (Maei et al., 2009). There have also been proposed\r
hybrid algorithms midway between semi-gradient TD and gradient TD (Hackman, 2012;\r
White and White, 2016). Hybrid-TD algorithms behave like Gradient-TD algorithms in\r
states where the target and behavior policies are very di↵erent, and behave like semi\u0002gradient algorithms in states where the target and behavior policies are the same. Finally,\r
the Gradient-TD idea has been combined with the ideas of proximal methods and control\r
variates to produce more ecient methods (Mahadevan et al., 2014; Du et al., 2017).\r
11.8 Emphatic-TD Methods\r
We turn now to the second major strategy that has been extensively explored for obtaining\r
a cheap and ecient o↵-policy learning method with function approximation. Recall\r
that linear semi-gradient TD methods are ecient and stable when trained under the\r
on-policy distribution, and that we showed in Section 9.4 that this has to do with the\r
positive definiteness of the matrix A (9.11)4 and the match between the on-policy state\r
distribution µ⇡ and the state-transition probabilities p(s|s, a) under the target policy. In\r
o↵-policy learning, we reweight the state transitions using importance sampling so that\r
they become appropriate for learning about the target policy, but the state distribution\r
is still that of the behavior policy. There is a mismatch. A natural idea is to somehow\r
reweight the states, emphasizing some and de-emphasizing others, so as to return the\r
distribution of updates to the on-policy distribution. There would then be a match,\r
and stability and convergence would follow from existing results. This is the idea of\r
4In the o↵-policy case, the matrix A is generally defined as Es⇠b\r
⇥\r
x(s)E\r
⇥\r
x(St+1)>\r
 St =s, At ⇠⇡\r
⇤⇤.

282 Chapter 11: O↵-policy Methods with Approximation\r
Emphatic-TD methods, first introduced for on-policy training in Section 9.11.\r
Actually, the notion of “the on-policy distribution” is not quite right, as there are many\r
on-policy distributions, and any one of these is sucient to guarantee stability. Consider\r
an undiscounted episodic problem. The way episodes terminate is fully determined by the\r
transition probabilities, but there may be several di↵erent ways the episodes might begin.\r
However the episodes start, if all state transitions are due to the target policy, then the\r
state distribution that results is an on-policy distribution. You might start close to the\r
terminal state and visit only a few states with high probability before ending the episode.\r
Or you might start far away and pass through many states before terminating. Both are\r
on-policy distributions, and training on both with a linear semi-gradient method would\r
be guaranteed to be stable. However the process starts, an on-policy distribution results\r
as long as all states encountered are updated up until termination.\r
If there is discounting, it can be treated as partial or probabilistic termination for these\r
purposes. If  = 0.9, then we can consider that with probability 0.1 the process terminates\r
on every time step and then immediately restarts in the state that is transitioned to. A\r
discounted problem is one that is continually terminating and restarting with probability\r
1   on every step. This way of thinking about discounting is an example of a more\r
general notion of pseudo termination—termination that does not a↵ect the sequence of\r
state transitions, but does a↵ect the learning process and the quantities being learned.\r
This kind of pseudo termination is important to o↵-policy learning because the restarting\r
is optional—remember we can start any way we want to—and the termination relieves\r
the need to keep including encountered states within the on-policy distribution. That is,\r
if we don’t consider the new states as restarts, then discounting quickly gives us a limited\r
on-policy distribution.\r
The one-step Emphatic-TD algorithm for learning episodic state values is defined by:\r
t = Rt+1 + vˆ(St+1,wt)  vˆ(St,wt),\r
wt+1 = wt + ↵Mt⇢ttrvˆ(St,wt),\r
Mt = ⇢t1Mt1 + It,\r
with It, the interest, being arbitrary and Mt, the emphasis, being initialized to M1 = 0.\r
How does this algorithm perform on Baird’s counterexample? Figure 11.6 shows the\r
trajectory in expectation of the components of the parameter vector (for the case in\r
which It = 1, for all t). There are some oscillations but eventually everything converges\r
and the VE goes to zero. These trajectories are obtained by iteratively computing the\r
expectation of the parameter vector trajectory without any of the variance due to sampling\r
of transitions and rewards. We do not show the results of applying the Emphatic-TD\r
algorithm directly because its variance on Baird’s counterexample is so high that it is\r
nigh impossible to get consistent results in computational experiments. The algorithm\r
converges to the optimal solution in theory on this problem, but in practice it does\r
not. We turn to the topic of reducing the variance of all these algorithms in the next\r
section.

11.9. Reducing Variance 283\r
Sweeps\r
w1– w6\r
w7\r
w8\r
p\r
VE\r
10\r
5\r
2\r
0\r
-5\r
0 1000\r
Figure 11.6: The behavior of the one-step Emphatic-TD algorithm in expectation on Baird’s\r
counterexample. The step size was ↵ = 0.03.\r
11.9 Reducing Variance\r
O↵-policy learning is inherently of greater variance than on-policy learning. This is not\r
surprising; if you receive data less closely related to a policy, you should expect to learn\r
less about the policy’s values. In the extreme, one may be able to learn nothing. You\r
can’t expect to learn how to drive by cooking dinner, for example. Only if the target and\r
behavior policies are related, if they visit similar states and take similar actions, should\r
one be able to make significant progress in o↵-policy training.\r
On the other hand, any policy has many neighbors, many similar policies with con\u0002siderable overlap in states visited and actions chosen, and yet which are not identical.\r
The raison d’ˆetre of o↵-policy learning is to enable generalization to this vast number\r
of related-but-not-identical policies. The problem remains of how to make the best use\r
of the experience. Now that we have some methods that are stable in expected value\r
(if the step sizes are set right), attention naturally turns to reducing the variance of the\r
estimates. There are many possible ideas, and we can just touch on a few of them in this\r
introductory text.\r
Why is controlling variance especially critical in o↵-policy methods based on importance\r
sampling? As we have seen, importance sampling often involves products of policy ratios.\r
The ratios are always one in expectation (5.13), but their actual values may be very high\r
or as low as zero. Successive ratios are uncorrelated, so their products are also always one\r
in expected value, but they can be of very high variance. Recall that these ratios multiply\r
the step size in SGD methods, so high variance means taking steps that vary greatly in\r
their sizes. This is problematic for SGD because of the occasional very large steps. They\r
must not be so large as to take the parameter to a part of the space with a very di↵erent\r
gradient. SGD methods rely on averaging over multiple steps to get a good sense of\r
the gradient, and if they make large moves from single samples they become unreliable.\r
If the step-size parameter is set small enough to prevent this, then the expected step

284 Chapter 11: O↵-policy Methods with Approximation\r
can end up being very small, resulting in very slow learning. The notions of momentum\r
(Derthick, 1984), of Polyak-Ruppert averaging (Polyak, 1990; Ruppert, 1988; Polyak and\r
Juditsky, 1992), or further extensions of these ideas may significantly help. Methods for\r
adaptively setting separate step sizes for di↵erent components of the parameter vector\r
are also pertinent (e.g., Jacobs, 1988; Sutton, 1992b, c), as are the “importance weight\r
aware” updates of Karampatziakis and Langford (2010).\r
In Chapter 5 we saw how weighted importance sampling is significantly better behaved,\r
with lower variance updates, than ordinary importance sampling. However, adapting\r
weighted importance sampling to function approximation is challenging and can probably\r
only be done approximately with O(d) complexity (Mahmood and Sutton, 2015).\r
The Tree Backup algorithm (Section 7.5) shows that it is possible to perform some\r
o↵-policy learning without using importance sampling. This idea has been extended to\r
the o↵-policy case to produce stable and more ecient methods by Munos, Stepleton,\r
Harutyunyan, and Bellemare (2016) and by Mahmood, Yu and Sutton (2017).\r
Another, complementary strategy is to allow the target policy to be determined in\r
part by the behavior policy, in such a way that it never can be so di↵erent from it to\r
create large importance sampling ratios. For example, the target policy can be defined by\r
reference to the behavior policy, as in the “recognizers” proposed by Precup et al. (2006).\r
11.10 Summary\r
O↵-policy learning is a tempting challenge, testing our ingenuity in designing stable and\r
ecient learning algorithms. Tabular Q-learning makes o↵-policy learning seem easy,\r
and it has natural generalizations to Expected Sarsa and to the Tree Backup algorithm.\r
But as we have seen in this chapter, the extension of these ideas to significant function\r
approximation, even linear function approximation, involves new challenges and forces us\r
to deepen our understanding of reinforcement learning algorithms.\r
Why go to such lengths? One reason to seek o↵-policy algorithms is to give flexibility\r
in dealing with the tradeo↵ between exploration and exploitation. Another is to free\r
behavior from learning, and avoid the tyranny of the target policy. TD learning appears\r
to hold out the possibility of learning about multiple things in parallel, of using one\r
stream of experience to solve many tasks simultaneously. We can certainly do this in\r
special cases, just not in every case that we would like to or as eciently as we would\r
like to.\r
In this chapter we divided the challenge of o↵-policy learning into two parts. The\r
first part, correcting the targets of learning for the behavior policy, is straightforwardly\r
dealt with using the techniques devised earlier for the tabular case, albeit at the cost of\r
increasing the variance of the updates and thereby slowing learning. High variance will\r
probably always remains a challenge for o↵-policy learning.\r
The second part of the challenge of o↵-policy learning emerges as the instability\r
of semi-gradient TD methods that involve bootstrapping. We seek powerful function\r
approximation, o↵-policy learning, and the eciency and flexibility of bootstrapping

11.10. Summary 285\r
TD methods, but it is challenging to combine all three aspects of this deadly triad in\r
one algorithm without introducing the potential for instability. There have been several\r
attempts. The most popular has been to seek to perform true stochastic gradient descent\r
(SGD) in the Bellman error (a.k.a. the Bellman residual). However, our analysis concludes\r
that this is not an appealing goal in many cases, and that anyway it is impossible to\r
achieve with a learning algorithm—the gradient of the BE is not learnable from experience\r
that reveals only feature vectors and not underlying states. Another approach, Gradient\u0002TD methods, performs SGD in the projected Bellman error. The gradient of the PBE\r
is learnable with O(d) complexity, but at the cost of a second parameter vector with a\r
second step size. The newest family of methods, Emphatic-TD methods, refine an old idea\r
for reweighting updates, emphasizing some and de-emphasizing others. In this way they\r
restore the special properties that make on-policy learning stable with computationally\r
simple semi-gradient methods.\r
The whole area of o↵-policy learning is relatively new and unsettled. Which methods\r
are best or even adequate is not yet clear. Are the complexities of the new methods\r
introduced at the end of this chapter really necessary? Which of them can be combined\r
e↵ectively with variance reduction methods? The potential for o↵-policy learning remains\r
tantalizing, the best way to achieve it still a mystery.\r
Bibliographical and Historical Remarks\r
11.1 The first semi-gradient method was linear TD() (Sutton, 1988). The name\r
“semi-gradient” is more recent (Sutton, 2015a). Semi-gradient o↵-policy TD(0)\r
with general importance-sampling ratio may not have been explicitly stated until\r
Sutton, Mahmood, and White (2016), but the action-value forms were introduced\r
by Precup, Sutton, and Singh (2000), who also did eligibility trace forms of these\r
algorithms (see Chapter 12). Their continuing, undiscounted forms have not\r
been significantly explored. The n-step forms given here are new.\r
11.2 The earliest w-to-2w example was given by Tsitsiklis and Van Roy (1996), who\r
also introduced the specific counterexample in the box on page 263. Baird’s\r
counterexample is due to Baird (1995), though the version we present here is\r
slightly modified. Averaging methods for function approximation were developed\r
by Gordon (1995, 1996b). Other examples of instability with o↵-policy DP\r
methods and more complex methods of function approximation are given by\r
Boyan and Moore (1995). Bradtke (1993) gives an example in which Q-learning\r
using linear function approximation in a linear quadratic regulation problem\r
converges to a destabilizing policy.\r
11.3 The deadly triad was first identified by Sutton (1995b) and thoroughly analyzed\r
by Tsitsiklis and Van Roy (1997). The name “deadly triad” is due to Sutton\r
(2015a).\r
11.4 This kind of linear analysis was pioneered by Tsitsiklis and Van Roy (1996; 1997),\r
including the dynamic programming operator. Diagrams like Figure 11.3 were

286 Chapter 11: O↵-policy Methods with Approximation\r
introduced by Lagoudakis and Parr (2003).\r
What we have called the Bellman operator, and denoted B⇡, is more commonly\r
denoted T ⇡ and called a “dynamic programming operator,” while a generalized\r
form, denoted T(), is called the “TD() operator” (Tsitsiklis and Van Roy, 1996,\r
1997).\r
11.5 The BE was first proposed as an objective function for dynamic programming by\r
Schweitzer and Seidmann (1985). Baird (1995, 1999) extended it to TD learning\r
based on stochastic gradient descent. In the literature, BE minimization is often\r
referred to as Bellman residual minimization.\r
The earliest A-split example is due to Dayan (1992). The two forms given here\r
were introduced by Sutton et al. (2009a).\r
11.6 The contents of this section are new to this text.\r
11.7 Gradient-TD methods were introduced by Sutton, Szepesv´ari, and Maei (2009b).\r
The methods highlighted in this section were introduced by Sutton et al. (2009a)\r
and Mahmood et al. (2014). A major extension to proximal TD methods\r
was developed by Mahadevan et al. (2014). The most sensitive empirical\r
investigations to date of Gradient-TD and related methods are given by Geist\r
and Scherrer (2014), Dann, Neumann, and Peters (2014), White (2015), and\r
Ghiassian, Patterson, White, Sutton, and White (2018). Recent developments in\r
the theory of Gradient-TD methods are presented by Yu (2017).\r
11.8 Emphatic-TD methods were introduced by Sutton, Mahmood, and White (2016).\r
Full convergence proofs and other theory were later established by Yu (2015;\r
2016; Yu, Mahmood, and Sutton, 2017), Hallak, Tamar, and Mannor (2015), and\r
Hallak, Tamar, Munos, and Mannor (2016).

Chapter 12\r
Eligibility Traces\r
Eligibility traces are one of the basic mechanisms of reinforcement learning. For example,\r
in the popular TD() algorithm, the  refers to the use of an eligibility trace. Almost any\r
temporal-di↵erence (TD) method, such as Q-learning or Sarsa, can be combined with\r
eligibility traces to obtain a more general method that may learn more eciently.\r
Eligibility traces unify and generalize TD and Monte Carlo methods. When TD\r
methods are augmented with eligibility traces, they produce a family of methods spanning\r
a spectrum that has Monte Carlo methods at one end (= 1) and one-step TD methods\r
at the other ( = 0). In between are intermediate methods that are often better than\r
either extreme method. Eligibility traces also provide a way of implementing Monte Carlo\r
methods online and on continuing problems without episodes.\r
Of course, we have already seen one way of unifying TD and Monte Carlo methods: the\r
n-step TD methods of Chapter 7. What eligibility traces o↵er beyond these is an elegant\r
algorithmic mechanism with significant computational advantages. The mechanism is\r
a short-term memory vector, the eligibility trace zt 2 Rd, that parallels the long-term\r
weight vector wt 2 Rd. The rough idea is that when a component of wt participates in\r
producing an estimated value, then the corresponding component of zt is bumped up and\r
then begins to fade away. Learning will then occur in that component of wt if a nonzero\r
TD error occurs before the trace falls back to zero. The trace-decay parameter  2 [0, 1]\r
determines the rate at which the trace falls.\r
The primary computational advantage of eligibility traces over n-step methods is that\r
only a single trace vector is required rather than a store of the last n feature vectors.\r
Learning also occurs continually and uniformly in time rather than being delayed and\r
then catching up at the end of the episode. In addition learning can occur and a↵ect\r
behavior immediately after a state is encountered rather than being delayed n steps.\r
Eligibility traces illustrate that a learning algorithm can sometimes be implemented in\r
a di↵erent way to obtain computational advantages. Many algorithms are most naturally\r
formulated and understood as an update of a state’s value based on events that follow\r
that state over multiple future time steps. For example, Monte Carlo methods (Chapter 5)\r
update a state based on all the future rewards, and n-step TD methods (Chapter 7)

288 Chapter 12: Eligibility Traces\r
update based on the next n rewards and state n steps in the future. Such formulations,\r
based on looking forward from the updated state, are called forward views. Forward views\r
are always somewhat complex to implement because the update depends on later things\r
that are not available at the time. However, as we show in this chapter it is often possible\r
to achieve nearly the same updates—and sometimes exactly the same updates—with an\r
algorithm that uses the current TD error, looking backward to recently visited states\r
using an eligibility trace. These alternate ways of looking at and implementing learning\r
algorithms are called backward views. Backward views, transformations between forward\r
views and backward views, and equivalences between them, date back to the introduction\r
of temporal di↵erence learning but have become much more powerful and sophisticated\r
since 2014. Here we present the basics of the modern view.\r
As usual, first we fully develop the ideas for state values and prediction, then extend\r
them to action values and control. We develop them first for the on-policy case then\r
extend them to o↵-policy learning. Our treatment pays special attention to the case of\r
linear function approximation, for which the results with eligibility traces are stronger.\r
All these results apply also to the tabular and state aggregation cases because these are\r
special cases of linear function approximation.\r
12.1 The -return\r
In Chapter 7 we defined an n-step return as the sum of the first n rewards plus the\r
estimated value of the state reached in n steps, each appropriately discounted (7.1). The\r
general form of that equation, for any parameterized function approximator, is\r
Gt:t+n\r
.\r
= Rt+1 +Rt+2 +···+n1Rt+n +nvˆ(St+n,wt+n1), 0  t  T n, (12.1)\r
where vˆ(s,w) is the approximate value of state s given weight vector w (Chapter 9), and\r
T is the time of episode termination, if any. We noted in Chapter 7 that each n-step\r
return, for n  1, is a valid update target for a tabular learning update, just as it is for\r
an approximate SGD learning update such as (9.7).\r
Now we note that a valid update can be done not just toward any n-step return, but\r
toward any average of n-step returns for di↵erent ns. For example, an update can be\r
done toward a target that is half of a two-step return and half of a four-step return:\r
1\r
2Gt:t+2 + 12Gt:t+4. Any set of n-step returns can be averaged in this way, even an infinite\r
set, as long as the weights on the component returns are positive and sum to 1. The\r
composite return possesses an error reduction property similar to that of individual n-step\r
returns (7.3) and thus can be used to construct updates with guaranteed convergence\r
properties. Averaging produces a substantial new range of algorithms. For example, one\r
could average one-step and infinite-step returns to obtain another way of interrelating TD\r
and Monte Carlo methods. In principle, one could even average experience-based updates\r
with DP updates to get a simple combination of experience-based and model-based\r
methods (cf. Chapter 8).\r
An update that averages simpler component updates is called a compound update. The\r
backup diagram for a compound update consists of the backup diagrams for each of the\r
component updates with a horizontal line above them and the weighting fractions below.

12.1. The -return 289\r
1\r
2\r
1\r
2\r
For example, the compound update for the case mentioned at the start of\r
this section, mixing half of a two-step return and half of a four-step return,\r
has the diagram shown to the right. A compound update can only be done\r
when the longest of its component updates is complete. The update at the\r
right, for example, could only be done at time t+ 4 for the estimate formed at\r
time t. In general one would like to limit the length of the longest component\r
update because of the corresponding delay in the updates.\r
The TD() algorithm can be understood as one particular way of averaging\r
n-step updates. This average contains all the n-step updates, each weighted\r
proportionally to n1 (where  2 [0, 1)), and is normalized by a factor of\r
1 to ensure that the weights sum to 1 (Figure 12.1). The resulting update\r
is toward a return, called the -return, defined in its state-based form by\r
G\r
t\r
.\r
= (1  )\r
X1\r
n=1\r
n1Gt:t+n. (12.2)\r
Figure 12.2 further illustrates the weighting on the sequence of n-step returns in the\r
-return. The one-step return is given the largest weight, 1  ; the two-step return is\r
given the next largest weight, (1); the three-step return is given the weight (1)2;\r
and so on. The weight fades by  with each additional step. After a terminal state has\r
been reached, all subsequent n-step returns are equal to the conventional return, Gt. If\r
1  \r
(1  )\r
(1  )2\r
T t1\r
···\r
···\r
St\r
At\r
At+1\r
AT 1\r
St+1 Rt+1\r
ST RT\r
···\r
St+2 Rt+2\r
At+2\r
TD()\r
X = 1\r
Figure 12.1: The backup diagram for TD(). If  = 0, then the overall update reduces to its\r
first component, the one-step TD update, whereas if  = 1, then the overall update reduces to\r
its last component, the Monte Carlo update.

290 Chapter 12: Eligibility Traces\r
1!"\r
weight given to\r
the 3-step return\r
decay by "\r
weight given to\r
actual, final return\r
t T\r
Time\r
Weight\r
total area = 1\r
is (1  )2\r
is T t1\r
Weighting\r
Figure 12.2: Weighting given in the -return to each of the n-step returns.\r
we want, we can separate these post-termination terms from the main sum, yielding\r
G\r
t = (1  )\r
T\r
Xt1\r
n=1\r
n1Gt:t+n + T t1Gt, (12.3)\r
as indicated in the figures. This equation makes it clearer what happens when  = 1. In\r
this case the main sum goes to zero, and the remaining term reduces to the conventional\r
return. Thus, for  = 1, updating according to the -return is a Monte Carlo algorithm.\r
On the other hand, if  = 0, then the -return reduces to Gt:t+1, the one-step return.\r
Thus, for  = 0, updating according to the -return is a one-step TD method.\r
Exercise 12.1 Just as the return can be written recursively in terms of the first reward and\r
itself one-step later (3.9), so can the -return. Derive the analogous recursive relationship\r
from (12.2) and (12.1). ⇤\r
Exercise 12.2 The parameter  characterizes how fast the exponential weighting in\r
Figure 12.2 falls o↵, and thus how far into the future the -return algorithm looks in\r
determining its update. But a rate factor such as  is sometimes an awkward way of\r
characterizing the speed of the decay. For some purposes it is better to specify a time\r
constant, or half-life. What is the equation relating  and the half-life, ⌧, the time by\r
which the weighting sequence will have fallen to half of its initial value? ⇤\r
We are now ready to define our first learning algorithm based on the -return: the\r
o↵-line -return algorithm. As an o↵-line algorithm, it makes no changes to the weight\r
vector during the episode. Then, at the end of the episode, a whole sequence of o↵-line\r
updates are made according to our usual semi-gradient rule, using the -return as the\r
target:\r
wt+1\r
.\r
= wt + ↵\r
h\r
G\r
t  vˆ(St,wt)\r
i\r
rvˆ(St,wt), t = 0,...,T  1. (12.4)

12.1. The -return 291\r
The -return gives us an alternative way of moving smoothly between Monte Carlo\r
and one-step TD methods that can be compared with the n-step bootstrapping way\r
developed in Chapter 7. There we assessed e↵ectiveness on a 19-state random walk task\r
(Example 7.1, page 144). Figure 12.3 shows the performance of the o↵-line -return\r
algorithm on this task alongside that of the n-step methods (repeated from Figure 7.2).\r
The experiment was just as described earlier except that for the -return algorithm we\r
varied  instead of n. The performance measure used is the estimated root-mean-square\r
error between the correct and estimated values of each state measured at the end of\r
the episode, averaged over the first 10 episodes and the 19 states. Note that overall\r
performance of the o↵-line -return algorithms is comparable to that of the n-step\r
algorithms. In both cases we get best performance with an intermediate value of the\r
bootstrapping parameter, n for n-step methods and  for the o↵-line -return algorithm.\r
n-step TD methods\r
(from Chapter 7)\r
↵\r
Average\r
RMS error\r
over 19 states\r
and first 10 \r
episodes n=1\r
n=2 n=4 n=8\r
n=16\r
n=32\r
n=32 n=64 128 512256 0.55\r
0.5\r
0.45\r
0.35\r
0.3\r
0.25\r
0.4\r
0 0.2 0.4 0.6 0.8 1\r
0.55\r
0.5\r
0.45\r
0.35\r
0.3\r
0.25\r
0.4\r
0 0.2 0.4 0.6 0.8 1\r
Off line λ-return algorithm\r
↵\r
RMS error\r
at the end \r
of the episode\r
over the first\r
10 episodes λ=0\r
λ=.4\r
λ=.8\r
λ=.9\r
λ=.95\r
λ=.975\r
λ=.99 λ=1\r
λ=.95\r
-\r
Figure 12.3: 19-state Random walk results (Example 7.1): Performance of the o↵-line -return\r
algorithm alongside that of the n-step TD methods. In both case, intermediate values of the\r
bootstrapping parameter ( or n) performed best. The results with the o↵-line -return algorithm\r
are slightly better at the best values of ↵ and , and at high ↵.\r
The approach that we have been taking so far is what we call the theoretical, or\r
forward, view of a learning algorithm. For each state visited, we look forward in time to\r
all the future rewards and decide how best to combine them. We might imagine ourselves\r
riding the stream of states, looking forward from each state to determine its update, as\r
suggested by Figure 12.4. After looking forward from and updating one state, we move\r
on to the next and never have to work with the preceding state again. Future states,\r
on the other hand, are viewed and processed repeatedly, once from each vantage point\r
preceding them.

292 Chapter 12: Eligibility Traces\r
Time\r
r\r
t+3\r
rt+2\r
rt+1\r
r\r
T\r
st+1\r
st+2\r
st+3\r
st\r
St+1\r
St\r
St+2\r
St+3 Rt+3\r
Rt+2\r
Rt+1\r
RT\r
Figure 12.4: The forward view. We decide how to update each state by looking forward to\r
future rewards and states.\r
12.2 TD()\r
TD() is one of the oldest and most widely used algorithms in reinforcement learning.\r
It was the first algorithm for which a formal relationship was shown between a more\r
theoretical forward view and a more computationally-congenial backward view using\r
eligibility traces. Here we will show empirically that it approximates the o↵-line -return\r
algorithm presented in the previous section.\r
TD() improves over the o↵-line -return algorithm in three ways. First it updates\r
the weight vector on every step of an episode rather than only at the end, and thus\r
its estimates may be better sooner. Second, its computations are equally distributed\r
in time rather than all at the end of the episode. And third, it can be applied to\r
continuing problems rather than just to episodic problems. In this section we present the\r
semi-gradient version of TD() with function approximation.\r
With function approximation, the eligibility trace is a vector zt 2 Rd with the same\r
number of components as the weight vector wt. Whereas the weight vector is a long-term\r
memory, accumulating over the lifetime of the system, the eligibility trace is a short-term\r
memory, typically lasting less time than the length of an episode. Eligibility traces assist\r
in the learning process; their only consequence is that they a↵ect the weight vector, and\r
then the weight vector determines the estimated value.\r
In TD(), the eligibility trace vector is initialized to zero at the beginning of the\r
episode, is incremented on each time step by the value gradient, and then fades away by\r
:\r
z1\r
.\r
= 0,\r
zt\r
.\r
= zt1 + rvˆ(St,wt), 0  t  T, (12.5)\r
where  is the discount rate and  is the parameter introduced in the previous section,\r
which we henceforth call the trace-decay parameter. The eligibility trace keeps track\r
of which components of the weight vector have contributed, positively or negatively, to\r
recent state valuations, where “recent” is defined in terms of . (Recall that in linear\r
function approximation, rvˆ(St,wt) is the feature vector, xt, in which case the eligibility\r
trace vector is just a sum of past, fading, input vectors.) The trace is said to indicate\r
the eligibility of each component of the weight vector for undergoing learning changes

12.2. TD() 293\r
should a reinforcing event occur. The reinforcing events we are concerned with are the\r
moment-by-moment one-step TD errors. The TD error for state-value prediction is\r
t\r
.\r
= Rt+1 + vˆ(St+1,wt)  vˆ(St,wt). (12.6)\r
In TD(), the weight vector is updated on each step proportional to the scalar TD error\r
and the vector eligibility trace:\r
wt+1\r
.\r
= wt + ↵t zt. (12.7)\r
Semi-gradient TD() for estimating vˆ ⇡ v⇡\r
Input: the policy ⇡ to be evaluated\r
Input: a di↵erentiable function ˆv : S+ ⇥ Rd ! R such that ˆv(terminal,·)=0\r
Algorithm parameters: step size ↵ > 0, trace decay rate  2 [0, 1]\r
Initialize value-function weights w arbitrarily (e.g., w = 0)\r
Loop for each episode:\r
Initialize S\r
z 0 (a d-dimensional vector)\r
Loop for each step of episode:\r
| Choose A ⇠ ⇡(·|S)\r
| Take action A, observe R, S0\r
| z z + rvˆ(S,w)\r
|  R + vˆ(S0,w)  vˆ(S,w)\r
| w w + ↵z\r
| S S0\r
until S0 is terminal\r
!t et et\r
et\r
et\r
Time\r
st\r
st+1\r
st-1\r
st-2\r
st-3\r
St\r
St+1\r
St-1\r
St-2\r
St-3\r
zt t zt\r
zt\r
zt\r
Figure 12.5: The backward or mechanistic view of TD(). Each update depends on the current\r
TD error combined with the current eligibility traces of past events.

294 Chapter 12: Eligibility Traces\r
TD() is oriented backward in time. At each moment we look at the current TD error\r
and assign it backward to each prior state according to how much that state contributed\r
to the current eligibility trace at that time. We might imagine ourselves riding along the\r
stream of states, computing TD errors, and shouting them back to the previously visited\r
states, as suggested by Figure 12.5. Where the TD error and traces come together, we\r
get the update given by (12.7), changing the values of those past states for when they\r
occur again in the future.\r
To better understand the backward view of TD(), consider what happens at various\r
values of . If  = 0, then by (12.5) the trace at t is exactly the value gradient\r
corresponding to St. Thus the TD() update (12.7) reduces to the one-step semi-gradient\r
TD update treated in Chapter 9 (and, in the tabular case, to the simple TD rule (6.2)).\r
This is why that algorithm was called TD(0). In terms of Figure 12.5, TD(0) is the\r
case in which only the one state preceding the current one is updated by the TD error\r
(other states may have their value estimates changed by generalization due to function\r
approximation). For larger values of , but still  < 1, more of the preceding states are\r
updated, but each more temporally distant state is updated less because the corresponding\r
eligibility trace is smaller, as suggested by the figure. We say that the earlier states are\r
given less credit for the TD error.\r
If  = 1, then the credit given to earlier states falls only by  per step. This turns out\r
to be just the right thing to do to achieve Monte Carlo behavior. For example, remember\r
that the TD error, t, includes an undiscounted term of Rt+1. In passing this back k\r
steps it needs to be discounted, like any reward in a return, by k, which is just what\r
the falling eligibility trace achieves. If  = 1 and  = 1, then the eligibility traces do not\r
decay at all with time. In this case the method behaves like a Monte Carlo method for\r
an undiscounted, episodic task. If  = 1, the algorithm is also known as TD(1).\r
TD(1) is a way of implementing Monte Carlo algorithms that is more general than those\r
presented earlier and that significantly increases their range of applicability. Whereas\r
the earlier Monte Carlo methods were limited to episodic tasks, TD(1) can be applied to\r
discounted continuing tasks as well. Moreover, TD(1) can be performed incrementally\r
and online. One disadvantage of Monte Carlo methods is that they learn nothing from\r
an episode until it is over. For example, if a Monte Carlo control method takes an action\r
that produces a very poor reward but does not end the episode, then the agent’s tendency\r
to repeat the action will be undiminished during the episode. Online TD(1), on the other\r
hand, learns in an n-step TD way from the incomplete ongoing episode, where the n\r
steps are all the way up to the current step. If something unusually good or bad happens\r
during an episode, control methods based on TD(1) can learn immediately and alter their\r
behavior on that same episode.\r
It is revealing to revisit the 19-state random walk example (Example 7.1) to see how\r
well TD() does in approximating the o↵-line -return algorithm. The results for both\r
algorithms are shown in Figure 12.6. For each  value, if ↵ is selected optimally for it (or\r
smaller), then the two algorithms perform virtually identically. If ↵ is chosen larger than\r
is optimal, however, then the -return algorithm is only a little worse whereas TD()\r
is much worse and may even be unstable. This is not catastrophic for TD() on this\r
problem, as these higher parameter values are not what one would want to use anyway,\r
but for other problems it can be a significant weakness.

12.3. n-step Truncated -return Methods 295\r
Off line λ-return algorithm\r
(from the previous section)\r
↵\r
λ=0\r
λ=.4\r
λ=.8\r
λ=.9\r
λ=.95 .975 .99 1\r
TD(λ)\r
↵\r
λ=.8\r
λ=.9\r
RMS error\r
at the end \r
of the episode\r
over the first\r
10 episodes\r
0 0.2 0.4 0.6 0.8 1\r
λ=0\r
λ=.4\r
λ=.8\r
λ=.9\r
λ=.95\r
λ=.975\r
λ=.99 λ=1\r
λ=.95\r
0.55\r
0.5\r
0.45\r
0.35\r
0.3\r
0.25\r
0.4\r
0 0.2 0.4 0.6 0.8 1\r
-\r
Figure 12.6: 19-state Random walk results (Example 7.1): Performance of TD() alongside\r
that of the o↵-line -return algorithm. The two algorithms performed virtually identically at\r
low (less than optimal) ↵ values, but TD() was worse at high ↵ values.\r
Linear TD() has been proved to converge in the on-policy case if the step-size\r
parameter is reduced over time according to the usual conditions (2.7). Just as discussed\r
in Section 9.4, convergence is not to the minimum-error weight vector, but to a nearby\r
weight vector that depends on . The bound on solution quality presented in that section\r
(9.14) can now be generalized to apply for any . For the continuing discounted case,\r
VE(w1) \r
1  \r
1  \r
min\r
w VE(w). (12.8)\r
That is, the asymptotic error is no more than 1\r
1 times the smallest possible error. As\r
 approaches 1, the bound approaches the minimum error (and it is loosest at  = 0).\r
In practice, however,  = 1 is often the poorest choice, as will be illustrated later in\r
Figure 12.14.\r
Exercise 12.3 Some insight into how TD() can closely approximate the o↵-line -return\r
algorithm can be gained by seeing that the latter’s error term (in brackets in (12.4)) can\r
be written as the sum of TD errors (12.6) for a single fixed w. Show this, following the\r
pattern of (6.6), and using the recursive relationship for the -return you obtained in\r
Exercise 12.1. ⇤\r
Exercise 12.4 Use your result from the preceding exercise to show that, if the weight\r
updates over an episode were computed on each step but not actually used to change the\r
weights (w remained fixed), then the sum of TD()’s weight updates would be the same\r
as the sum of the o↵-line -return algorithm’s updates. ⇤\r
12.3 n-step Truncated -return Methods\r
The o↵-line -return algorithm is an important ideal, but it is of limited utility because\r
it uses the -return (12.2), which is not known until the end of the episode. In the

296 Chapter 12: Eligibility Traces\r
continuing case, the -return is technically never known, as it depends on n-step returns\r
for arbitrarily large n, and thus on rewards arbitrarily far in the future. However, the\r
dependence becomes weaker for longer-delayed rewards, falling by  for each step of\r
delay. A natural approximation, then, would be to truncate the sequence after some\r
number of steps. Our existing notion of n-step returns provides a natural way to do this\r
in which the missing rewards are replaced with estimated values.\r
In general, we define the truncated -return for time t, given data only up to some\r
later horizon, h, as\r
G\r
t:h\r
.\r
= (1  )\r
h\r
Xt1\r
n=1\r
n1Gt:t+n + ht1Gt:h, 0  t<h  T. (12.9)\r
If you compare this equation with the -return (12.3), it is clear that the horizon h is\r
playing the same role as was previously played by T, the time of termination. Whereas\r
in the -return there is a residual weight given to the conventional return Gt, here it is\r
given to the longest available n-step return, Gt:h (Figure 12.2).\r
The truncated -return immediately gives rise to a family of n-step -return algorithms\r
similar to the n-step methods of Chapter 7. In all of these algorithms, updates are\r
delayed by n steps and only take into account the first n rewards, but now all the k-step\r
returns are included for 1  k  n (whereas the earlier n-step algorithms used only the\r
n-step return), weighted geometrically as in Figure 12.2. In the state-value case, this\r
family of algorithms is known as Truncated TD(), or TTD(). The compound backup\r
diagram, shown in Figure 12.7, is similar to that for TD() (Figure 12.1) except that the\r
longest component update is at most n steps rather than always going all the way to the\r
1  \r
(1  )\r
(1  )2 T t1\r
or,\r
if t + n T ··· ···\r
···\r
n1\r
St\r
At\r
At+1\r
AT 1\r
St+n Rt+n\r
St+1 Rt+1\r
ST RT\r
At+n1\r
n-step truncated TD()\r
Figure 12.7: The backup diagram for Truncated TD().

12.4. Redoing Updates: Online -return Algorithm 297\r
end of the episode. TTD() is defined by (cf. (9.15)):\r
wt+n\r
.\r
= wt+n1 + ↵ ⇥G\r
t:t+n  vˆ(St,wt+n1)\r
⇤\r
rvˆ(St,wt+n1), 0  t < T.\r
This algorithm can be implemented eciently so that per-step computation does not scale\r
with n (though of course memory must). Much as in n-step TD methods, no updates are\r
made on the first n  1 time steps of each episode, and n  1 additional updates are made\r
upon termination. Ecient implementation relies on the fact that the k-step -return\r
can be written exactly as\r
G\r
t:t+k = ˆv(St,wt1) +\r
t+\r
X\r
k1\r
i=t\r
()\r
it\r
0\r
i, (12.10)\r
where\r
0\r
t\r
.\r
= Rt+1 + vˆ(St+1,wt)  vˆ(St,wt1).\r
Exercise 12.5 Several times in this book (often in exercises) we have established that\r
returns can be written as sums of TD errors if the value function is held constant. Why\r
is (12.10) another instance of this? Prove (12.10). ⇤\r
12.4 Redoing Updates: Online -return Algorithm\r
Choosing the truncation parameter n in Truncated TD() involves a tradeo↵. n should\r
be large so that the method closely approximates the o↵-line -return algorithm, but it\r
should also be small so that the updates can be made sooner and can influence behavior\r
sooner. Can we get the best of both? Well, yes, in principle we can, albeit at the cost of\r
computational complexity.\r
The idea is that, on each time step as you gather a new increment of data, you go back\r
and redo all the updates since the beginning of the current episode. The new updates\r
will be better than the ones you previously made because now they can take into account\r
the time step’s new data. That is, the updates are always towards an n-step truncated\r
-return target, but they always use the latest horizon. In each pass over that episode\r
you can use a slightly longer horizon and obtain slightly better results. Recall that the\r
truncated -return is defined in (12.9) as\r
G\r
t:h\r
.\r
= (1  )\r
h\r
Xt1\r
n=1\r
n1Gt:t+n + ht1Gt:h.\r
Let us step through how this target could ideally be used if computational complexity was\r
not an issue. The episode begins with an estimate at time 0 using the weights w0 from\r
the end of the previous episode. Learning begins when the data horizon is extended to\r
time step 1. The target for the estimate at step 0, given the data up to horizon 1, could\r
only be the one-step return G0:1, which includes R1 and bootstraps from the estimate\r
vˆ(S1,w0). Note that this is exactly what G\r
0:1 is, with the sum in the first term of the

298 Chapter 12: Eligibility Traces\r
equation degenerating to zero. Using this update target, we construct w1. Then, after\r
advancing the data horizon to step 2, what do we do? We have new data in the form of\r
R2 and S2, as well as the new w1, so now we can construct a better update target G\r
0:2\r
for the first update from S0 as well as a better update target G\r
1:2 for the second update\r
from S1. Using these improved targets, we redo the updates at S1 and S2, starting again\r
from w0, to produce w2. Now we advance the horizon to step 3 and repeat, going all the\r
way back to produce three new targets, redoing all updates starting from the original w0\r
to produce w3, and so on. Each time the horizon is advanced, all the updates are redone\r
starting from w0 using the weight vector from the preceding horizon.\r
This conceptual algorithm involves multiple passes over the episode, one at each\r
horizon, each generating a di↵erent sequence of weight vectors. To describe it clearly we\r
have to distinguish between the weight vectors computed at the di↵erent horizons. Let us\r
use wh\r
t to denote the weights used to generate the value at time t in the sequence up to\r
horizon h. The first weight vector wh\r
0 in each sequence is that inherited from the previous\r
episode (so they are the same for all h), and the last weight vector wh\r
h in each sequence\r
defines the ultimate weight-vector sequence of the algorithm. At the final horizon h = T\r
we obtain the final weights wT\r
T which will be passed on to form the initial weights of the\r
next episode. With these conventions, the three first sequences described in the previous\r
paragraph can be given explicitly:\r
h =1: w1\r
1\r
.\r
= w1\r
0 + ↵ ⇥\r
G\r
0:1  vˆ(S0,w10)\r
⇤\r
rvˆ(S0,w1\r
0),\r
h =2: w2\r
1\r
.\r
= w2\r
0 + ↵ ⇥\r
G\r
0:2  vˆ(S0,w20)\r
⇤\r
rvˆ(S0,w2\r
0),\r
w2\r
2\r
.\r
= w2\r
1 + ↵ ⇥\r
G\r
1:2  vˆ(S1,w21)\r
⇤\r
rvˆ(S1,w2\r
1),\r
h =3: w3\r
1\r
.\r
= w3\r
0 + ↵ ⇥\r
G\r
0:3  vˆ(S0,w30)\r
⇤\r
rvˆ(S0,w3\r
0),\r
w3\r
2\r
.\r
= w3\r
1 + ↵ ⇥\r
G\r
1:3  vˆ(S1,w31)\r
⇤\r
rvˆ(S1,w3\r
1),\r
w3\r
3\r
.\r
= w3\r
2 + ↵ ⇥\r
G\r
2:3  vˆ(S2,w32)\r
⇤\r
rvˆ(S2,w3\r
2).\r
The general form for the update is\r
wh\r
t+1\r
.\r
= wh\r
t + ↵ ⇥\r
G\r
t:h  vˆ(St,wht )\r
⇤\r
rvˆ(St,wh\r
t ), 0  t<h  T.\r
This update, together with wt\r
.\r
= wt\r
t defines the online -return algorithm.\r
The online -return algorithm is fully online, determining a new weight vector wt\r
at each step t during an episode, using only information available at time t. Its main\r
drawback is that it is computationally complex, passing over the portion of the episode\r
experienced so far on every step. Note that it is strictly more complex than the o↵-line\r
-return algorithm, which passes through all the steps at the time of termination but does\r
not make any updates during the episode. In return, the online algorithm can be expected\r
to perform better than the o↵-line one, not only during the episode when it makes an\r
update while the o↵-line algorithm makes none, but also at the end of the episode because\r
the weight vector used in bootstrapping (in G\r
t:h) has had a larger number of informative

12.5. True Online TD() 299\r
updates. This e↵ect can be seen if one looks carefully at Figure 12.8, which compares the\r
two algorithms on the 19-state random walk task.\r
Off line λ-return algorithm\r
(from Section 12.1)\r
↵ ↵\r
RMS error\r
at the end \r
of the episode\r
over the first\r
10 episodes\r
0 0.2 0.4 0.6 0.8 1\r
λ=0\r
λ=.4\r
λ=.8\r
λ=.9\r
λ=.95\r
λ=.975\r
λ=.99 λ=1\r
λ=.95\r
0.55\r
0.5\r
0.45\r
0.35\r
0.3\r
0.25\r
0.4\r
0 0.2 0.4 0.6 0.8 1\r
On-line λ-return algorithm\r
= true online TD(λ)\r
λ=0\r
λ=.4 λ=.8\r
λ=.9\r
λ=.95\r
λ=.975\r
λ=.99\r
λ=1\r
λ=.95\r
-\r
Figure 12.8: 19-state Random walk results (Example 7.1): Performance of online and o↵-line\r
-return algorithms. The performance measure here is the VE at the end of the episode, which\r
should be the best case for the o↵-line algorithm. Nevertheless, the online algorithm performs\r
subtly better. For comparison, the = 0 line is the same for both methods.\r
12.5 True Online TD()\r
The online -return algorithm just presented is currently the best performing temporal\u0002di↵erence algorithm. It is an ideal which online TD() only approximates. As presented,\r
however, the online -return algorithm is very complex. Is there a way to invert this\r
forward-view algorithm to produce an ecient backward-view algorithm using eligibility\r
traces? It turns out that there is indeed an exact computationally congenial implementa\u0002tion of the online -return algorithm for the case of linear function approximation. This\r
implementation is known as the true online TD() algorithm because it is “truer” to the\r
ideal of the online -return algorithm than the TD() algorithm is.\r
The derivation of true online TD() is a little too complex to present here (see the\r
next section and the appendix to the paper by van Seijen et al., 2016) but its strategy is\r
simple. The sequence of weight vectors produced by the online -return algorithm can\r
be arranged in a triangle:\r
w0\r
0\r
w1\r
0 w11\r
w2\r
0 w21 w22\r
w3\r
0 w31 w32 w33\r
.\r
.\r
. .\r
.\r
. .\r
.\r
. .\r
.\r
. ...\r
wT\r
0 wT1 wT2 wT3 ··· wT\r
T\r
One row of this triangle is produced on each time step. It turns out that the weight vectors\r
on the diagonal, the wt\r
t, are the only ones really needed. The first, w0\r
0, is the initial weight

300 Chapter 12: Eligibility Traces\r
vector of the episode, the last, wT\r
T , is the final weight vector, and each weight vector\r
along the way, wt\r
t, plays a role in bootstrapping in the n-step returns of the updates.\r
In the final algorithm the diagonal weight vectors are renamed without a superscript,\r
wt\r
.\r
= wt\r
t. The strategy then is to find a compact, ecient way of computing each wtt\r
from the one before. If this is done, for the linear case in which vˆ(s,w) = w>x(s), then\r
we arrive at the true online TD() algorithm:\r
wt+1\r
.\r
= wt + ↵t zt + ↵ w>\r
t xt  w>t1xt\r
\r
(zt  xt),\r
where we have used the shorthand xt\r
.\r
= x(St), t is defined as in TD() (12.6), and zt is\r
defined by\r
zt\r
.\r
= zt1 + 1  ↵z>\r
t1xt\r
\r
xt. (12.11)\r
This algorithm has been proven to produce exactly the same sequence of weight vectors,\r
wt, 0  t  T, as the online -return algorithm (van Seijen et al. 2016). Thus the results\r
on the random walk task on the left of Figure 12.8 are also its results on that task. Now,\r
however, the algorithm is much less expensive. The memory requirements of true online\r
TD() are identical to those of conventional TD(), while the per-step computation is\r
increased by about 50% (there is one more inner product in the eligibility-trace update).\r
Overall, the per-step computational complexity remains of O(d), the same as TD().\r
Pseudocode for the complete algorithm is given in the box.\r
True online TD() for estimating w>x ⇡ v⇡\r
Input: the policy ⇡ to be evaluated\r
Input: a feature function x : S+ ! Rd such that x(terminal, ·) = 0\r
Algorithm parameters: step size ↵ > 0, trace decay rate  2 [0, 1]\r
Initialize value-function weights w 2 Rd (e.g., w = 0)\r
Loop for each episode:\r
Initialize state and obtain initial feature vector x\r
z 0 (a d-dimensional vector)\r
Vold 0 (a temporary scalar variable)\r
Loop for each step of episode:\r
| Choose A ⇠ ⇡\r
| Take action A, observe R, x0 (feature vector of the next state)\r
| V w>x\r
| V 0 w>x0\r
|  R + V 0  V\r
| z z + 1  ↵z>x\r
\r
x\r
| w w + ↵( + V  Vold)z  ↵(V  Vold)x\r
| Vold V 0\r
| x x0\r
until x0 = 0 (signaling arrival at a terminal state)\r
The eligibility trace (12.11) used in true online TD() is called a dutch trace to\r
distinguish it from the trace (12.5) used in TD(), which is called an accumulating trace.

12.6. *Dutch Traces in Monte Carlo Learning 301\r
Earlier work often used a third kind of trace called the replacing trace, defined only for\r
the tabular case or for binary feature vectors such as those produced by tile coding. The\r
replacing trace is defined on a component-by-component basis depending on whether the\r
component of the feature vector was 1 or 0:\r
zi,t\r
.\r
=\r
⇢ 1 if xi,t = 1\r
zi,t1 otherwise. (12.12)\r
Nowadays, we see replacing traces as crude approximations to dutch traces, which largely\r
supersede them. Dutch traces usually perform better than replacing traces and have a\r
clearer theoretical basis. Accumulating traces remain of interest for nonlinear function\r
approximations where dutch traces are not available.\r
12.6 *Dutch Traces in Monte Carlo Learning\r
Although eligibility traces are closely associated historically with TD learning, in fact\r
they have nothing to do with it. In fact, eligibility traces arise even in Monte Carlo\r
learning, as we show in this section. We show that the linear MC algorithm (Chapter 9),\r
taken as a forward view, can be used to derive an equivalent yet computationally cheaper\r
backward-view algorithm using dutch traces. This is the only equivalence of forward- and\r
backward-views that we explicitly demonstrate in this book. It gives some of the flavor\r
of the proof of equivalence of true online TD() and the online -return algorithm, but is\r
much simpler.\r
The linear version of the gradient Monte Carlo prediction algorithm (page 202) makes\r
the following sequence of updates, one for each time step of the episode:\r
wt+1\r
.\r
= wt + ↵ ⇥G  w>\r
t xt\r
⇤\r
xt, 0  t < T. (12.13)\r
To simplify the example, we assume here that the return G is a single reward received at\r
the end of the episode (this is why G is not subscripted by time) and that there is no\r
discounting. In this case the update is also known as the Least Mean Square (LMS) rule.\r
As a Monte Carlo algorithm, all the updates depend on the final reward/return, so none\r
can be made until the end of the episode. The MC algorithm is an o↵-line algorithm and\r
we do not seek to improve this aspect of it. Rather we seek merely an implementation of\r
this algorithm with computational advantages. We will still update the weight vector\r
only at the end of the episode, but we will do some computation during each step of the\r
episode and less at its end. This will give a more equal distribution of computation—O(d)\r
per step—and also remove the need to store the feature vectors at each step for use later\r
at the end of each episode. Instead, we will introduce an additional vector memory, the\r
eligibility trace, keeping in it a summary of all the feature vectors seen so far. This will\r
be sucient to eciently recreate exactly the same overall update as the sequence of MC

302 Chapter 12: Eligibility Traces\r
updates (12.13), by the end of the episode:\r
wT = wT 1 + ↵ G  w>\r
T 1xT 1\r
\r
xT 1\r
= wT 1 + ↵xT 1\r
\r
x>\r
T 1wT 1\r
\r
+ ↵GxT 1\r
= I  ↵xT 1x>\r
T 1\r
\r
wT 1 + ↵GxT 1\r
= FT 1wT 1 + ↵GxT 1\r
where Ft\r
.\r
= I  ↵xtx>\r
t is a forgetting, or fading, matrix. Now, recursing,\r
= FT 1 (FT 2wT 2 + ↵GxT 2) + ↵GxT 1\r
= FT 1FT 2wT 2 + ↵G (FT 1xT 2 + xT 1)\r
= FT 1FT 2 (FT 3wT 3 + ↵GxT 3) + ↵G (FT 1xT 2 + xT 1)\r
= FT 1FT 2FT 3wT 3 + ↵G (FT 1FT 2xT 3 + FT 1xT 2 + xT 1)\r
.\r
.\r
.\r
= FT 1FT 2 ··· F0w0 | {z } aT 1+ ↵G\r
T\r
X1\r
k=0\r
FT 1FT 2 ··· Fk+1xk\r
| {z } zT 1\r
= aT 1 + ↵GzT 1 , (12.14)\r
where aT 1 and zT 1 are the values at time T  1 of two auxiliary memory vectors that\r
can be updated incrementally without knowledge of G and with O(d) complexity per time\r
step. The zt vector is in fact a dutch-style eligibility trace. It is initialized to z0 = x0\r
and then updated according to\r
zt\r
.\r
= Xt\r
k=0\r
FtFt1 ··· Fk+1xk, 1  t<T\r
= Xt1\r
k=0\r
FtFt1 ··· Fk+1xk + xt\r
= Ft\r
Xt1\r
k=0\r
Ft1Ft2 ··· Fk+1xk + xt\r
= Ftzt1 + xt\r
= I  ↵xtx>\r
t\r
\r
zt1 + xt\r
= zt1  ↵xtx>\r
t zt1 + xt\r
= zt1  ↵ z>\r
t1xt\r
\r
xt + xt\r
= zt1 + 1  ↵z>\r
t1xt\r
\r
xt,\r
which is the dutch trace for the case of = 1 (cf. Eq. 12.11). The at auxiliary vector is\r
initialized to a0 = w0 and then updated according to\r
at\r
.\r
= FtFt1 ··· F0w0 = Ftat1 = at1  ↵xtx>\r
t at1, 1  t < T.

12.7. Sarsa() 303\r
The auxiliary vectors, at and zt, are updated on each time step t<T and then, at time\r
T when G is observed, they are used in (12.14) to compute wT . In this way we achieve\r
exactly the same final result as the MC/LMS algorithm that has poor computational\r
properties (12.13), but now with an incremental algorithm whose time and memory\r
complexity per step is O(d). This is surprising and intriguing because the notion of\r
an eligibility trace (and the dutch trace in particular) has arisen in a setting without\r
temporal-di↵erence (TD) learning (in contrast to van Seijen and Sutton, 2014). It seems\r
eligibility traces are not specific to TD learning at all; they are more fundamental than\r
that. The need for eligibility traces seems to arise whenever one tries to learn long-term\r
predictions in an ecient manner.\r
12.7 Sarsa()\r
Very few changes in the ideas already presented in this chapter are required in order to\r
extend eligibility-traces to action-value methods. To learn approximate action values,\r
qˆ(s, a, w), rather than approximate state values, vˆ(s,w), we need to use the action-value\r
form of the n-step return, from Chapter 10:\r
Gt:t+n\r
.\r
= Rt+1 + ··· + n1Rt+n + nqˆ(St+n, At+n, wt+n1), t + n < T,\r
with Gt:t+n\r
.\r
= Gt if t + n  T. Using this, we can form the action-value form of the\r
-return, which is otherwise identical to the state-value form (12.3). The action-value\r
form of the o↵-line -return algorithm (12.4) simply uses ˆq rather than ˆv:\r
wt+1\r
.\r
= wt + ↵\r
h\r
G\r
t  qˆ(St, At, wt)\r
i\r
rqˆ(St, At, wt), t = 0,...,T  1, (12.15)\r
where G\r
t\r
.\r
= G\r
t:1. The compound backup diagram for this forward view is shown in\r
Figure 12.9. Notice the similarity to the diagram of the TD() algorithm (Figure 12.1).\r
The first update looks ahead one full step, to the next state–action pair, the second looks\r
ahead two steps, to the second state–action pair, and so on. A final update is based on\r
the complete return. The weighting of each n-step update in the -return is just as in\r
TD() and the -return algorithm (12.3).\r
The temporal-di↵erence method for action values, known as Sarsa(), approximates\r
this forward view. It has the same update rule as given earlier for TD():\r
wt+1\r
.\r
= wt + ↵t zt,\r
except, naturally, using the action-value form of the TD error:\r
t\r
.\r
= Rt+1 + qˆ(St+1, At+1, wt)  qˆ(St, At, wt), (12.16)\r
and the action-value form of the eligibility trace:\r
z1\r
.\r
= 0,\r
zt\r
.\r
= zt1 + rqˆ(St, At, wt), 0  t  T.

304 Chapter 12: Eligibility Traces\r
1  \r
(1  )\r
(1  )2\r
T t1\r
···\r
···\r
St At\r
At+1\r
AT 1\r
St+1 Rt+1\r
ST RT\r
···\r
St+2 Rt+2\r
At+2\r
X = 1\r
Sarsa()\r
Figure 12.9: Sarsa()’s backup diagram. Compare with Figure 12.1.\r
Complete pseudocode for Sarsa() with linear function approximation, binary features,\r
and either accumulating or replacing traces is given in the box on the next page. This\r
pseudocode highlights a few optimizations possible in the special case of binary features\r
(features are either active (=1) or inactive (=0)).\r
Example 12.1: Traces in Gridworld The use of eligibility traces can substantially\r
increase the eciency of control algorithms over one-step methods and even over n-step\r
methods. The reason for this is illustrated by the gridworld example below.\r
Path taken\r
Action values increased\r
by one-step Sarsa\r
Action values increased\r
by Sarsa( ) with =0.9\r
G\r
G\r
G\r
Path taken\r
Action values increased\r
by one-step Sarsa\r
Action values increased\r
by Sarsa( by 10-step Sarsa \u0004 ) with \u0004 =0.9\r
G G G\r
Path taken\r
Action values increased\r
by one-step Sarsa\r
Action values increased\r
by 10-step Sarsa\r
G G G\r
h h\r
The first panel shows the path taken by an agent in a single episode. The initial estimated\r
values were zero, and all rewards were zero except for a positive reward at the goal\r
location marked by G. The arrows in the other panels show, for various algorithms, which\r
action-values would be increased, and by how much, upon reaching the goal. A one-step\r
method would increment only the last action value, whereas an n-step method would\r
equally increment the last n actions’ values (assuming  = 1), and an eligibility trace\r
method would update all the action values up to the beginning of the episode, to di↵erent\r
degrees, fading with recency. The fading strategy is often the best.

12.7. Sarsa() 305\r
Sarsa() with binary features and linear function approximation\r
for estimating w>x ⇡ q⇡ or q⇤\r
Input: a function F(s, a) returning the set of (indices of) active features for s, a\r
Input: a policy ⇡\r
Algorithm parameters: step size ↵ > 0, trace decay rate  2 [0, 1], small " > 0\r
Initialize: w = (w1,...,wd)> 2 Rd (e.g., w = 0), z = (z1,...,zd)> 2 Rd\r
Loop for each episode:\r
Initialize S\r
Choose A ⇠ ⇡(·|S) or "-greedy according to ˆq(S, ·, w)\r
z 0\r
Loop for each step of episode:\r
Take action A, observe R, S0\r
 R\r
Loop for i in F(S, A):\r
   wi\r
zi zi + 1 (accumulating traces)\r
or zi 1 (replacing traces)\r
If S0 is terminal then:\r
w w + ↵z\r
Go to next episode\r
Choose A0 ⇠ ⇡(·|S0) or "-greedy according to ˆq(S0, ·, w)\r
Loop for i in F(S0, A0):   + wi\r
w w + ↵z\r
z z\r
S S0; A A0\r
Exercise 12.6 Modify the pseudocode for Sarsa() to use dutch traces (12.11) without the\r
other distinctive features of a true online algorithm. Assume linear function approximation\r
and binary features. ⇤\r
Example 12.2: Sarsa() on Mountain Car Figure 12.10 (left) on the next page\r
shows results with Sarsa() on the Mountain Car task introduced in Example 10.1. The\r
function approximation, action selection, and environmental details were exactly as in\r
Chapter 10, and thus it is appropriate to numerically compare these results with the\r
Chapter 10 results for n-step Sarsa (right side of the figure). The earlier results varied the\r
update length n whereas here for Sarsa() we vary the trace parameter , which plays\r
a similar role. The fading-trace bootstrapping strategy of Sarsa() appears to result in\r
more ecient learning on this problem.\r
There is also an action-value version of our ideal TD method, the online -return algo\u0002rithm (Section 12.4) and its ecient implementation as true online TD() (Section 12.5).\r
Everything in Section 12.4 goes through without change other than to use the action-value\r
form of the n-step return given at the beginning of the current section. The analyses in\r
Sections 12.5 and 12.6 also carry through for action values, the only change being the use

306 Chapter 12: Eligibility Traces\r
220\r
240\r
260\r
300\r
0 0.5 1 1.5\r
Mountain Car\r
Steps per episode\r
averaged over\r
first 50 episodes\r
and 100 runs\r
280\r
200\r
180\r
↵ × number of tilings (8)\r
λ=.96\r
λ=.92\r
λ=.99\r
λ=.84\r
λ=.68\r
λ=0\r
220\r
240\r
260\r
300\r
0 0.5 1 1.5\r
280\r
0 0.5 1 1.5\r
n=1 n=2\r
n=4\r
n=8\r
n=16\r
n=8\r
n=4\r
n=2\r
n=16\r
n=1\r
λ=.98\r
↵ × number of tilings (8)\r
Sarsa(λ) with replacing traces n-step Sarsa\r
λ=.92\r
λ=.84\r
Figure 12.10: Early performance on the Mountain Car task of Sarsa() with replacing traces\r
and n-step Sarsa (copied from Figure 10.4) as a function of the step size, ↵.\r
of state–action feature vectors xt = x(St, At) instead of state feature vectors xt = x(St).\r
Pseudocode for the resulting ecient algorithm, called true online Sarsa() is given in\r
the box on the next page. The figure below compares the performance of various versions\r
of Sarsa() on the Mountain Car example.\r
True Online TD()\r
1.5 0 0.5 1 1.5 0\r
0.2\r
0.4\r
0.6\r
0.8\r
1\r
step−size\r
RMS \r
error \r
TD(λ), replacing traces − Task 1\r
λ = 0\r
λ = 1\r
0 0.5 1 1.5 0\r
0.2\r
0.4\r
0.6\r
0.8\r
1\r
step−size\r
RMS \r
error \r
true online TD(λ) − Task 1\r
λ = 0\r
λ = 1\r
1.5 \r
0 0.5 1 1.5 0\r
0.2\r
0.4\r
0.6\r
0.8\r
1\r
step−size\r
RMS \r
error \r
TD(λ), replacing traces − Task 2\r
0 0.5 1 1.5 0\r
0.2\r
0.4\r
0.6\r
0.8\r
1\r
step−size\r
RMS \r
error \r
true online TD(λ) − Task 2\r
λ = 1\r
λ = 0\r
he end of each episode, averaged over the first 10 episodes, as well as 100 independent runs, for\r
() with the traditional\r
dard mountain car task\r
gs of each 10⇥10 tiles.\r
↵ = ↵0/10, for ↵0 from\r
ring/no clearing refers\r
selected actions are set\r
), in case of replacing\r
true online principle is\r
nline version of the for\u0002\r
ical and intuitive foun\u0002\r
addition, we have pre\u0002\r
with the same compu\u0002\r
l algorithm, which we\r
that true online TD()\r
ew exactly, in contrast\r
only approximates its\r
monstrated empirically\r
conventional TD() on\r
ms, by adhering more\r
matching an intuitively\r
ne case—that we have\r
0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2 −550\r
−500\r
−450\r
−400\r
−350\r
−300\r
−250\r
−200\r
−150\r
α0\r
return\r
Sarsa(λ), replacing, clearing\r
Sarsa(λ), replacing, no clearing\r
Sarsa(λ), accumulating\r
Figure 4. Average return over first 20 episodes on mountain car\r
task for  = 0.9 and different ↵0. Results are averaged over 100\r
independent runs.\r
Acknowledgements\r
The authors thank Hado van Hasselt and Rupam Mahmood\r
Mountain Car\r
Reward per episode\r
averaged over\r
first 20 episodes\r
and 100 runs\r
↵ × number of tilings (8)\r
7UXH\u0003RQOLQH\u00036DUVD\u000Bλ\f\r
Sarsa(λ) with replacing traces\r
Sarsa(λ) with replacing traces\r
and clearing the traces of other actions\r
Sarsa(λ) with accumulating traces\r
Figure 12.11: Summary comparison of Sarsa() algorithms on the Mountain Car task. True\r
online Sarsa() performed better than regular Sarsa() with both accumulating and replacing\r
traces. Also included is a version of Sarsa() with replacing traces in which, on each time step,\r
the traces for the state and the actions not selected were set to zero.

12.8. Variable  and  307\r
True online Sarsa() for estimating w>x ⇡ q⇡ or q⇤\r
Input: a feature function x : S+ ⇥ A ! Rd such that x(terminal, ·) = 0\r
Input: a policy ⇡ (if estimating q⇡)\r
Algorithm parameters: step size ↵ > 0, trace decay rate  2 [0, 1], small " > 0\r
Initialize: w 2 Rd (e.g., w = 0)\r
Loop for each episode:\r
Initialize S\r
Choose A ⇠ ⇡(·|S) or "-greedy according to ˆq(S, ·, w)\r
x x(S, A)\r
z 0\r
Qold 0\r
Loop for each step of episode:\r
| Take action A, observe R, S0\r
| Choose A0 ⇠ ⇡(·|S0) or "-greedy according to ˆq(S0, ·, w)\r
| x0 x(S0, A0)\r
| Q w>x\r
| Q0 w>x0\r
|  R + Q0  Q\r
| z z + 1  ↵z>x\r
\r
x\r
| w w + ↵( + Q  Qold)z  ↵(Q  Qold)x\r
| Qold Q0\r
| x x0\r
| A A0\r
until S0 is terminal\r
Finally, there is also a truncated version of Sarsa(), called forward Sarsa() (van\r
Seijen, 2016), which appears to be a particularly e↵ective model-free control method for\r
use in conjunction with multi-layer artificial neural networks.\r
12.8 Variable  and \r
We are starting now to reach the end of our development of fundamental TD learning\r
algorithms. To present the final algorithms in their most general forms, it is useful to\r
generalize the degree of bootstrapping and discounting beyond constant parameters to\r
functions potentially dependent on the state and action. That is, each time step will have\r
a di↵erent  and , denoted t and t. We change notation now so that  : S⇥A ! [0, 1]\r
is now a function from states and actions to the unit interval such that t\r
.\r
= (St, At), and\r
similarly,  : S ! [0, 1] is a function from states to the unit interval such that t\r
.\r
= (St).\r
Introducing the function , the termination function, is particularly significant because\r
it changes the return, the fundamental random variable whose expectation we seek to

308 Chapter 12: Eligibility Traces\r
estimate. Now the return is defined more generally as\r
Gt\r
.\r
= Rt+1 + t+1Gt+1\r
= Rt+1 + t+1Rt+2 + t+1t+2Rt+3 + t+1t+2t+3Rt+4 + ···\r
= X1\r
k=t\r
 Y\r
k\r
i=t+1\r
i\r
!\r
Rk+1, (12.17)\r
where, to assure the sums are finite, we require that Q1\r
k=t k = 0 with probability one for\r
all t. One convenient aspect of this definition is that it enables the episodic setting and\r
its algorithms to be presented in terms of a single stream of experience, without special\r
terminal states, start distributions, or termination times. An erstwhile terminal state\r
becomes a state at which (s) = 0 and which transitions to the start distribution. In that\r
way (and by choosing (·) as a constant in all other states) we can recover the classical\r
episodic setting as a special case. State dependent termination includes other prediction\r
cases such as pseudo termination, in which we seek to predict a quantity without altering\r
the flow of the Markov process. Discounted returns can be thought of as such a quantity,\r
in which case state-dependent termination unifies the episodic and discounted-continuing\r
cases. (The undiscounted-continuing case still needs some special treatment.)\r
The generalization to variable bootstrapping is not a change in the problem, like\r
discounting, but a change in the solution strategy. The generalization a↵ects the -\r
returns for states and actions. The new state-based -return can be written recursively\r
as\r
Gs\r
t\r
.\r
= Rt+1 + t+1 (1  t+1)ˆv(St+1,wt) + t+1Gs\r
t+1\r
, (12.18)\r
where now we have added the “s” to the superscript  to remind us that this is a return\r
that bootstraps from state values, distinguishing it from returns that bootstrap from\r
action values, which we present below with “a” in the superscript. This equation says\r
that the -return is the first reward, undiscounted and una↵ected by bootstrapping, plus\r
possibly a second term to the extent that we are not discounting at the next state (that\r
is, according to t+1; recall that this is zero if the next state is terminal). To the extent\r
that we aren’t terminating at the next state, we have a second term which is itself divided\r
into two cases depending on the degree of bootstrapping in the state. To the extent we\r
are bootstrapping, this term is the estimated value at the state, whereas, to the extent\r
that we are not bootstrapping, the term is the -return for the next time step. The\r
action-based -return is either the Sarsa form\r
Ga\r
t\r
.\r
= Rt+1 + t+1⇣(1  t+1)ˆq(St+1, At+1, wt) + t+1Ga\r
t+1⌘\r
, (12.19)\r
or the Expected Sarsa form,\r
Ga\r
t\r
.\r
= Rt+1 + t+1⇣(1  t+1)V¯t(St+1) + t+1Ga\r
t+1⌘\r
, (12.20)\r
where (7.8) is generalized to function approximation as\r
V¯t(s) .= X\r
a\r
⇡(a|s)ˆq(s, a, wt). (12.21)

12.9. O↵-policy Traces with Control Variates 309\r
Exercise 12.7 Generalize the three recursive equations above to their truncated versions,\r
defining Gs\r
t:h and Gat:h. ⇤\r
12.9 O↵-policy Traces with Control Variates\r
The final step is to incorporate importance sampling. For methods using non-truncated\r
-returns, there is not a practical option in which the importance-sampling weighting is\r
applied to the target return (as there is for n-step methods as explained in Section 7.3).\r
Instead, we move directly to the bootstrapping generalization of per-decision importance\r
sampling with control variates (Section 7.4).\r
In the state case, our final definition of the -return generalizes (12.18), after the model\r
of (7.13), to\r
Gs\r
t\r
.\r
= ⇢t\r
⇣\r
Rt+1 +t+1(1t+1)ˆv(St+1,wt)+t+1Gs\r
t+1⌘\r
+ (1⇢t)ˆv(St,wt), (12.22)\r
where ⇢t = ⇡(At|St)\r
b(At|St) is the usual single-step importance sampling ratio. Much like the\r
other returns we have seen in this book, this final -return can be approximated simply\r
in terms of sums of the state-based TD error,\r
s\r
t\r
.\r
= Rt+1 + t+1vˆ(St+1,wt)  vˆ(St,wt), (12.23)\r
as\r
Gs\r
t ⇡ vˆ(St,wt) + ⇢t\r
X1\r
k=t\r
s\r
k\r
Y\r
k\r
i=t+1\r
ii⇢i, (12.24)\r
with the approximation becoming exact if the approximate value function does not change.\r
Exercise 12.8 Prove that (12.24) becomes exact if the value function does not change.\r
To save writing, consider the case of t = 0, and use the notation Vk\r
.\r
= ˆv(Sk,w). ⇤\r
Exercise 12.9 The truncated version of the general o↵-policy return is denoted Gs\r
t:h.\r
Guess the correct equation, based on (12.24). ⇤\r
The above form of the -return (12.24) is convenient to use in a forward-view update,\r
wt+1 = wt + ↵ Gs\r
t  vˆ(St,wt)\r
\r
rvˆ(St,wt)\r
⇡ wt + ↵⇢t\r
 X1\r
k=t\r
s\r
k\r
Y\r
k\r
i=t+1\r
ii⇢i\r
!\r
rvˆ(St,wt),\r
which to the experienced eye looks like an eligibility-based TD update—the product is\r
like an eligibility trace and it is multiplied by TD errors. But this is just one time step of\r
a forward view. The relationship that we are looking for is that the forward-view update,\r
summed over time, is approximately equal to a backward-view update, summed over\r
time (this relationship is only approximate because again we ignore changes in the value

310 Chapter 12: Eligibility Traces\r
function). The sum of the forward-view update over time is\r
X1\r
t=0\r
(wt+1  wt) ⇡ X1\r
t=0\r
X1\r
k=t\r
↵⇢ts\r
krvˆ(St,wt) Y\r
k\r
i=t+1\r
ii⇢i\r
= X1\r
k=0\r
X\r
k\r
t=0\r
↵⇢trvˆ(St,wt)s\r
k\r
Y\r
k\r
i=t+1\r
ii⇢i\r
(using the summation rule: Py\r
t=x\r
Py\r
k=t = Pyk=x\r
Pk\r
t=x)\r
= X1\r
k=0\r
↵s\r
k\r
X\r
k\r
t=0\r
⇢trvˆ(St,wt) Y\r
k\r
i=t+1\r
ii⇢i,\r
which would be in the form of the sum of a backward-view TD update if the entire\r
expression from the second sum on could be written and updated incrementally as an\r
eligibility trace, which we now show can be done. That is, we show that if this expression\r
was the trace at time k, then we could update it from its value at time k  1 by:\r
zk = X\r
k\r
t=0\r
⇢trvˆ(St,wt) Y\r
k\r
i=t+1\r
ii⇢i\r
=\r
k\r
X1\r
t=0\r
⇢trvˆ(St,wt) Y\r
k\r
i=t+1\r
ii⇢i + ⇢krvˆ(Sk,wk)\r
= kk⇢k\r
k\r
X1\r
t=0\r
⇢trvˆ(St,wt)\r
k\r
Y1\r
i=t+1\r
ii⇢i\r
| {z } zk1\r
+ ⇢krvˆ(Sk,wk)\r
= ⇢k\r
\r
kkzk1 + rvˆ(Sk,wk)\r
\r
,\r
which, changing the index from k to t, is the general accumulating trace update for state\r
values:\r
zt\r
.\r
= ⇢t\r
\r
ttzt1 + rvˆ(St,wt)\r
\r
, (12.25)\r
This eligibility trace, together with the usual semi-gradient parameter-update rule for\r
TD() (12.7), forms a general TD() algorithm that can be applied to either on-policy or\r
o↵-policy data. In the on-policy case, the algorithm is exactly TD() because ⇢t is alway\r
1 and (12.25) becomes the usual accumulating trace (12.5) (extended to variable  and\r
). In the o↵-policy case, the algorithm often works well but, as a semi-gradient method,\r
is not guaranteed to be stable. In the next few sections we will consider extensions of it\r
that do guarantee stability.\r
A very similar series of steps can be followed to derive the o↵-policy eligibility traces\r
for action-value methods and corresponding general Sarsa() algorithms. One could start\r
with either recursive form for the general action-based -return, (12.19) or (12.20), but\r
the latter (the Expected Sarsa form) works out to be simpler. We extend (12.20) to the

12.9. O↵-policy Traces with Control Variates 311\r
o↵-policy case after the model of (7.14) to produce\r
Ga\r
t\r
.\r
= Rt+1+t+1⇣(1t+1)V¯t(St+1)+t+1⇥⇢t+1Ga\r
t+1+V¯t(St+1)⇢t+1qˆ(St+1, At+1, wt)\r
⇤\r
⌘\r
= Rt+1 + t+1⇣V¯t(St+1) + t+1⇢t+1 ⇥Ga\r
t+1  qˆ(St+1, At+1, wt)\r
⇤⌘\r
(12.26)\r
where V¯t(St+1) is as given by (12.21). Again the -return can be written approximately\r
as the sum of TD errors,\r
Ga\r
t ⇡ qˆ(St, At, wt) +X1\r
k=t\r
a\r
k\r
Y\r
k\r
i=t+1\r
ii⇢i, (12.27)\r
using the expectation form of the action-based TD error:\r
a\r
t = Rt+1 + t+1V¯t(St+1)  qˆ(St, At, wt). (12.28)\r
As before, the approximation becomes exact if the approximate value function does not\r
change.\r
Exercise 12.10 Prove that (12.27) becomes exact if the value function does not change.\r
To save writing, consider the case of t = 0, and use the notation Qk = qˆ(Sk, Ak, w). Hint:\r
Start by writing out a\r
0 and Ga0 , then Ga0  Q0. ⇤\r
Exercise 12.11 The truncated version of the general o↵-policy return is denoted Ga\r
t:h.\r
Guess the correct equation for it, based on (12.27). ⇤\r
Using steps entirely analogous to those for the state case, one can write a forward-view\r
update based on (12.27), transform the sum of the updates using the summation rule,\r
and finally derive the following form for the eligibility trace for action values:\r
zt\r
.\r
= tt⇢tzt1 + rqˆ(St, At, wt). (12.29)\r
This eligibility trace, together with the expectation-based TD error (12.28) and the usual\r
semi-gradient parameter-update rule (12.7), forms an elegant, ecient Expected Sarsa()\r
algorithm that can be applied to either on-policy or o↵-policy data. It is probably the\r
best algorithm of this type at the current time (though of course it is not guaranteed to\r
be stable until combined in some way with one of the methods presented in the following\r
sections). In the on-policy case with constant  and , and the usual state–action TD\r
error (12.16), the algorithm would be identical to the Sarsa() algorithm presented in\r
Section 12.7.\r
Exercise 12.12 Show in detail the steps outlined above for deriving (12.29) from (12.27).\r
Start with the update (12.15), substitute Ga\r
t from (12.26) for Gt , then follow similar\r
steps as led to (12.25). ⇤\r
At  = 1, these algorithms become closely related to corresponding Monte Carlo\r
algorithms. One might expect that an exact equivalence would hold for episodic problems\r
and o↵-line updating, but in fact the relationship is subtler and slightly weaker than that.\r
Under these most favorable conditions still there is not an episode by episode equiva\u0002lence of updates, only of their expectations. This should not be surprising as these methods

312 Chapter 12: Eligibility Traces\r
make irrevocable updates as a trajectory unfolds, whereas true Monte Carlo methods\r
would make no update for a trajectory if any action within it has zero probability under\r
the target policy. In particular, all of these methods, even at  = 1, still bootstrap in\r
the sense that their targets depend on the current value estimates—it’s just that the\r
dependence cancels out in expected value. Whether this is a good or bad property in\r
practice is another question. Recently, methods have been proposed that do achieve an\r
exact equivalence (Sutton, Mahmood, Precup and van Hasselt, 2014). These methods\r
require an additional vector of “provisional weights” that keep track of updates which\r
have been made but may need to be retracted (or emphasized) depending on the actions\r
taken later. The state and state–action versions of these methods are called PTD() and\r
PQ() respectively, where the ‘P’ stands for Provisional.\r
The practical consequences of all these new o↵-policy methods have not yet been\r
established. Undoubtedly, issues of high variance will arise as they do in all o↵-policy\r
methods using importance sampling (Section 11.9).\r
If  < 1, then all these o↵-policy algorithms involve bootstrapping and the deadly\r
triad applies (Section 11.3), meaning that they can be guaranteed stable only for the\r
tabular case, for state aggregation, and for other limited forms of function approximation.\r
For linear and more-general forms of function approximation the parameter vector may\r
diverge to infinity as in the examples in Chapter 11. As we discussed there, the challenge\r
of o↵-policy learning has two parts. O↵-policy eligibility traces deal e↵ectively with the\r
first part of the challenge, correcting for the expected value of the targets, but not at\r
all with the second part of the challenge, having to do with the distribution of updates.\r
Algorithmic strategies for meeting the second part of the challenge of o↵-policy learning\r
with eligibility traces are summarized in Section 12.11.\r
Exercise 12.13 What are the dutch-trace and replacing-trace versions of o↵-policy\r
eligibility traces for state-value and action-value methods? ⇤\r
12.10 Watkins’s Q() to Tree-Backup()\r
Several methods have been proposed over the years to extend Q-learning to eligibility\r
traces. The original is Watkins’s Q(), which decays its eligibility traces in the usual way\r
as long as a greedy action was taken, then cuts the traces to zero after the first non-greedy\r
action. The backup diagram for Watkins’s Q() is shown in Figure 12.12. In Chapter 6,\r
we unified Q-learning and Expected Sarsa in the o↵-policy version of the latter, which\r
includes Q-learning as a special case, and generalizes it to arbitrary target policies, and\r
in the previous section of this chapter we completed our treatment of Expected Sarsa by\r
generalizing it to o↵-policy eligibility traces. In Chapter 7, however, we distinguished\r
n-step Expected Sarsa from n-step Tree Backup, where the latter retained the property\r
of not using importance sampling. It remains then to present the eligibility trace version\r
of Tree Backup, which we will call Tree-Backup(), or TB() for short. This is arguably\r
the true successor to Q-learning because it retains its appealing absence of importance\r
sampling even though it can be applied to o↵-policy data.

12.10. Watkins’s Q() to Tree-Backup() 313\r
1  \r
(1  )\r
(1  )2\r
T t1\r
···\r
St At\r
At+1\r
St+1 Rt+1\r
ST RT\r
···\r
St+2 Rt+2\r
At+2\r
X = 1\r
OR\r
···\r
···\r
St+n Rt+n\r
First non-greedy action\r
n1\r
Watkins’s Q()\r
Figure 12.12: The backup diagram for Watkins’s Q(). The series of component updates ends\r
either with the end of the episode or with the first nongreedy action, whichever comes first.\r
The concept of TB() is straightforward. As shown in its backup diagram in Fig\u0002ure 12.13, the tree-backup updates of each length (from Section 7.5) are weighted in the\r
usual way dependent on the bootstrapping parameter . To get the detailed equations,\r
with the right indices on the general bootstrapping and discounting parameters, it is\r
best to start with a recursive form (12.20) for the -return using action values, and then\r
expand the bootstrapping case of the target after the model of (7.16):\r
Ga\r
t\r
.\r
= Rt+1+t+1✓(1t+1)V¯t(St+1)+t+1hX\r
a6=At+1\r
⇡(a|St+1)ˆq(St+1, a, wt)+⇡(At+1|St+1)Ga\r
t+1i◆\r
= Rt+1 + t+1✓V¯t(St+1) + t+1⇡(At+1|St+1)\r
⇣\r
Ga\r
t+1  qˆ(St+1, At+1, wt)\r
⌘◆\r
As per the usual pattern, it can also be written approximately (ignoring changes in the\r
approximate value function) as a sum of TD errors,\r
Ga\r
t ⇡ qˆ(St, At, wt) +X1\r
k=t\r
a\r
k\r
Y\r
k\r
i=t+1\r
ii⇡(Ai|Si),\r
using the expectation form of the action-based TD error (12.28).\r
Following the same steps as in the previous section, we arrive at a special eligibility\r
trace update involving the target-policy probabilities of the selected actions,\r
zt\r
.\r
= tt⇡(At|St)zt1 + rqˆ(St, At, wt).

314 Chapter 12: Eligibility Traces\r
1  \r
(1  )\r
(1  )2\r
T t1\r
···\r
St At\r
At+1\r
AT 1\r
St+1 Rt+1\r
ST RT\r
···\r
St+2 Rt+2\r
At+2\r
X = 1\r
···\r
Tree Backup()\r
ST 1\r
Figure 12.13: The backup diagram for the  version of the Tree Backup algorithm.\r
This, together with the usual parameter-update rule (12.7), defines the TB() algorithm.\r
Like all semi-gradient algorithms, TB() is not guaranteed to be stable when used with\r
o↵-policy data and with a powerful function approximator. To obtain those assurances,\r
TB() would have to be combined with one of the methods presented in the next section.\r
⇤\r
Exercise 12.14 How might Double Expected Sarsa be extended to eligibility traces? ⇤\r
12.11 Stable O↵-policy Methods with Traces\r
Several methods using eligibility traces have been proposed that achieve guarantees\r
of stability under o↵-policy training, and here we present four of the most important\r
using this book’s standard notation, including general bootstrapping and discounting\r
functions. All are based on either the Gradient-TD or the Emphatic-TD ideas presented\r
in Sections 11.7 and 11.8. All the algorithms assume linear function approximation,\r
though extensions to nonlinear function approximation can also be found in the literature.\r
GTD() is the eligibility-trace algorithm analogous to TDC, the better of the two\r
state-value Gradient-TD prediction algorithms discussed in Section 11.7. Its goal is to\r
learn a parameter wt such that vˆ(s,w) .= w>\r
t x(s) ⇡ v⇡(s), even from data that is due to\r
following another policy b. Its update is\r
wt+1\r
.\r
= wt + ↵s\r
t zt  ↵t+1(1  t+1)\r
\r
z>\r
t vt\r
\r
xt+1,

12.11. Stable O↵-policy Methods with Traces 315\r
with s\r
t , zt, and ⇢t defined in the usual ways for state values (12.23) (12.25) (11.1), and\r
vt+1\r
.\r
= vt + s\r
t zt  \r
\r
v>\r
t xt\r
\r
xt, (12.30)\r
where, as in Section 11.7, v 2 Rd is a vector of the same dimension as w, initialized to\r
v0 = 0, and  > 0 is a second step-size parameter.\r
GQ() is the Gradient-TD algorithm for action values with eligibility traces. Its goal\r
is to learn a parameter wt such that qˆ(s, a, wt) .= w>\r
t x(s, a) ⇡ q⇡(s, a) from o↵-policy\r
data. If the target policy is "-greedy, or otherwise biased toward the greedy policy for qˆ,\r
then GQ() can be used as a control algorithm. Its update is\r
wt+1\r
.\r
= wt + ↵a\r
t zt  ↵t+1(1  t+1)\r
\r
z>\r
t vt\r
\r
x¯t+1,\r
where x¯t is the average feature vector for St under the target policy,\r
x¯t\r
.\r
= X\r
a\r
⇡(a|St)x(St, a),\r
a\r
t is the expectation form of the TD error, which can be written\r
a\r
t\r
.\r
= Rt+1 + t+1w>\r
t x¯t+1  w>t xt,\r
zt is defined in the usual way for action values (12.29), and the rest is as in GTD(),\r
including the update for vt (12.30).\r
HTD() is a hybrid state-value algorithm combining aspects of GTD() and TD(). Its\r
most appealing feature is that it is a strict generalization of TD() to o↵-policy learning,\r
meaning that if the behavior policy happens to be the same as the target policy, then\r
HTD() becomes the same as TD(), which is not true for GTD(). This is appealing\r
because TD() is often faster than GTD() when both algorithms converge, and TD()\r
requires setting only a single step size. HTD() is defined by\r
wt+1\r
.\r
= wt + ↵s\r
t zt + ↵ \r
(zt  zb\r
t )\r
>vt\r
\r
(xt  t+1xt+1),\r
vt+1\r
.\r
= vt + s\r
t zt  \r
⇣\r
zb\r
t\r
>vt\r
⌘\r
(xt  t+1xt+1), with v0\r
.\r
= 0,\r
zt\r
.\r
= ⇢t\r
\r
ttzt1 + xt\r
\r
, with z1\r
.\r
= 0,\r
zb\r
t\r
.\r
= ttzb\r
t1 + xt, with zb1\r
.\r
= 0,\r
where  > 0 again is a second step-size parameter. In addition to the second set of\r
weights, vt, HTD() also has a second set of eligibility traces, zb\r
t . These are conventional\r
accumulating eligibility traces for the behavior policy and become equal to zt if all the ⇢t\r
are 1, which causes the last term in the wt update to be zero and the overall update to\r
reduce to TD().\r
Emphatic TD() is the extension of the one-step Emphatic-TD algorithm (Sections\r
9.11 and 11.8) to eligibility traces. The resultant algorithm retains strong o↵-policy\r
convergence guarantees while enabling any degree of bootstrapping, albeit at the cost of

316 Chapter 12: Eligibility Traces\r
high variance and potentially slow convergence. Emphatic TD() is defined by\r
wt+1\r
.\r
= wt + ↵tzt\r
t\r
.\r
= Rt+1 + t+1w>\r
t xt+1  w>t xt\r
zt\r
.\r
= ⇢t\r
\r
ttzt1 + Mtxt\r
\r
, with z1\r
.\r
= 0,\r
Mt\r
.\r
= t It + (1  t)Ft\r
Ft\r
.\r
= ⇢t1tFt1 + It, with F0\r
.\r
= i(S0),\r
where Mt  0 is the general form of emphasis, Ft  0 is termed the followon trace, and\r
It  0 is the interest, as described in Section 11.8. Note that Mt, like t, is not really an\r
additional memory variable. It can be removed from the algorithm by substituting its\r
definition into the eligibility-trace equation. Pseudocode and software for the true online\r
version of Emphatic-TD() are available on the web (Sutton, 2015b).\r
In the on-policy case (⇢t = 1, for all t), Emphatic-TD() is similar to conventional\r
TD(), but still significantly di↵erent. In fact, whereas Emphatic-TD() is guaranteed\r
to converge for all state-dependent  functions, TD() is not. TD() is guaranteed\r
convergent only for all constant . See Yu’s counterexample (Ghiassian, Rafiee, and\r
Sutton, 2016).\r
12.12 Implementation Issues\r
It might at first appear that tabular methods using eligibility traces are much more\r
complex than one-step methods. A naive implementation would require every state (or\r
state–action pair) to update both its value estimate and its eligibility trace on every time\r
step. This would not be a problem for implementations on single-instruction, multiple\u0002data, parallel computers or in plausible artificial neural network (ANN) implementations,\r
but it is a problem for implementations on conventional serial computers. Fortunately,\r
for typical values of  and  the eligibility traces of almost all states are almost always\r
nearly zero; only those states that have recently been visited will have traces significantly\r
greater than zero and only these few states need to be updated to closely approximate\r
these algorithms.\r
In practice, then, implementations on conventional computers may keep track of and\r
update only the few traces that are significantly greater than zero. Using this trick, the\r
computational expense of using traces in tabular methods is typically just a few times\r
that of a one-step method. The exact multiple of course depends on  and  and on the\r
expense of the other computations. Note that the tabular case is in some sense the worst\r
case for the computational complexity of eligibility traces. When function approximation\r
is used, the computational advantages of not using traces generally decrease. For example,\r
if ANNs and backpropagation are used, then eligibility traces generally cause only a\r
doubling of the required memory and computation per step. Truncated -return methods\r
(Section 12.3) can be computationally ecient on conventional computers though they\r
always require some additional memory.

12.13. Conclusions 317\r
12.13 Conclusions\r
Eligibility traces in conjunction with TD errors provide an ecient, incremental way of\r
shifting and choosing between Monte Carlo and TD methods. The n-step methods of\r
Chapter 7 also enabled this, but eligibility trace methods are more general, often faster to\r
learn, and o↵er di↵erent computational complexity tradeo↵s. This chapter has o↵ered an\r
introduction to the elegant, emerging theoretical understanding of eligibility traces for on\u0002and o↵-policy learning and for variable bootstrapping and discounting. One aspect of this\r
elegant theory is true online methods, which exactly reproduce the behavior of expensive\r
ideal methods while retaining the computational congeniality of conventional TD methods.\r
Another aspect is the possibility of derivations that automatically convert from intuitive\r
forward-view methods to more ecient incremental backward-view algorithms. We\r
illustrated this general idea in a derivation that started with a classical, expensive Monte\r
Carlo algorithm and ended with a cheap incremental non-TD implementation using the\r
same novel eligibility trace used in true online TD methods.\r
As we mentioned in Chapter 5, Monte Carlo methods may have advantages in non\u0002Markov tasks because they do not bootstrap. Because eligibility traces make TD methods\r
more like Monte Carlo methods, they also can have advantages in these cases. If one\r
wants to use TD methods because of their other advantages, but the task is at least\r
partially non-Markov, then the use of an eligibility trace method is indicated. Eligibility\r
traces are the first line of defense against both long-delayed rewards and non-Markov\r
tasks.\r
By adjusting , we can place eligibility trace methods anywhere along a continuum\r
from Monte Carlo to one-step TD methods. Where shall we place them? We do not yet\r
have a good theoretical answer to this question, but a clear empirical answer appears to\r
be emerging. On tasks with many steps per episode, or many steps within the half-life of\r
discounting, it appears significantly better to use eligibility traces than not to (e.g., see\r
Figure 12.14). On the other hand, if the traces are so long as to produce a pure Monte\r
Carlo method, or nearly so, then performance degrades sharply. An intermediate mixture\r
appears to be the best choice. Eligibility traces should be used to bring us toward Monte\r
Carlo methods, but not all the way there. In the future it may be possible to more finely\r
vary the trade-o↵ between TD and Monte Carlo methods by using variable , but at\r
present it is not clear how this can be done reliably and usefully.\r
Methods using eligibility traces require more computation than one-step methods, but\r
in return they o↵er significantly faster learning, particularly when rewards are delayed by\r
many steps. Thus it often makes sense to use eligibility traces when data are scarce and\r
cannot be repeatedly processed, as is often the case in online applications. On the other\r
hand, in o↵-line applications in which data can be generated cheaply, perhaps from an\r
inexpensive simulation, then it often does not pay to use eligibility traces. In these cases\r
the objective is not to get more out of a limited amount of data, but simply to process as\r
much data as possible as quickly as possible. In these cases the speedup per datum due to\r
traces is typically not worth their computational cost, and one-step methods are favored.

318 Chapter 12: Eligibility Traces\r
accumulating\r
traces\r
0.2\r
0.3\r
0.4\r
0.5\r
0 0.2 0.4 0.6 0.8 1\r
!\r
RANDOM WALK\r
50\r
100\r
150\r
200\r
250\r
300\r
Failures per\r
100,000 steps\r
0 0.2 0.4 0.6 0.8 1\r
!\r
CART AND POLE\r
400\r
450\r
500\r
550\r
600\r
650\r
700\r
Steps per\r
episode\r
0 0.2 0.4 0.6 0.8 1\r
!\r
MOUNTAIN CAR\r
replacing\r
traces\r
150\r
160\r
170\r
180\r
190\r
200\r
210\r
220\r
230\r
240\r
Cost per\r
episode\r
0 0.2 0.4 0.6 0.8 1\r
!\r
PUDDLE WORLD\r
replacing\r
traces\r
accumulating\r
traces \r
replacing\r
traces\r
accumulating\r
traces\r
RMS error\r
Figure 12.14: The e↵ect of  on reinforcement learning performance in four di↵erent test\r
problems. In all cases, performance is generally best (a lower number in the graph) at an\r
intermediate value of . The two left panels are applications to simple continuous-state control\r
tasks using the Sarsa() algorithm and tile coding, with either replacing or accumulating traces\r
(Sutton, 1996). The upper-right panel is for policy evaluation on a random walk task using TD()\r
(Singh and Sutton, 1996). The lower right panel is unpublished data for the pole-balancing task\r
(Example 3.4) from an earlier study (Sutton, 1984).

12.13. Conclusions 319\r
Bibliographical and Historical Remarks\r
Eligibility traces came into reinforcement learning via the fecund ideas of Klopf (1972).\r
Our use of eligibility traces is based on Klopf’s work (Sutton, 1978a, 1978b, 1978c; Barto\r
and Sutton, 1981a, 1981b; Sutton and Barto, 1981a; Barto, Sutton, and Anderson, 1983;\r
Sutton, 1984). We may have been the first to use the term “eligibility trace” (Sutton\r
and Barto, 1981a). The idea that stimuli produce after e↵ects in the nervous system\r
that are important for learning is very old (see Chapter 14). Some of the earliest uses of\r
eligibility traces were in the actor–critic methods discussed in Chapter 13 (Barto, Sutton,\r
and Anderson, 1983; Sutton, 1984).\r
12.1 Compound updates were called “complex backups” in the first edition of this\r
book.\r
The -return and its error-reduction properties were introduced by Watkins (1989)\r
and further developed by Jaakkola, Jordan, and Singh (1994). The random walk\r
results in this and subsequent sections are new to this text, as are the terms\r
“forward view” and “backward view.” The notion of a -return algorithm was\r
introduced in the first edition of this text. The more refined treatment presented\r
here was developed in conjunction with Harm van Seijen (e.g., van Seijen and\r
Sutton, 2014).\r
12.2 TD() with accumulating traces was introduced by Sutton (1988, 1984). Con\u0002vergence in the mean was proved by Dayan (1992), and with probability 1 by\r
many researchers, including Peng (1993), Dayan and Sejnowski (1994), Tsitsiklis\r
(1994), and Gurvits, Lin, and Hanson (1994). The bound on the error of the\r
asymptotic -dependent solution of linear TD() is due to Tsitsiklis and Van\r
Roy (1997).\r
12.3 Truncated TD methods were developed by Cichosz (1995) and van Seijen (2016).\r
12.4 The idea of redoing updates was extensively developed by van Seijen, originally\r
under the name “best-match learning” (van Seijen, 2011; van Seijen, Whiteson,\r
van Hasselt, and Weiring, 2011).\r
12.5 True online TD() is primarily due to Harm van Seijen (van Seijen and Sutton,\r
2014; van Seijen et al., 2016) though some of its key ideas were discovered\r
independently by Hado van Hasselt (personal communication). The name “dutch\r
traces” is in recognition of the contributions of both scientists. Replacing traces\r
are due to Singh and Sutton (1996).\r
12.6 The material in this section is from van Hasselt and Sutton (2015).\r
12.7 Sarsa() with accumulating traces was first explored as a control method by\r
Rummery and Niranjan (1994; Rummery, 1995). True Online Sarsa() was

320 Chapter 12: Eligibility Traces\r
introduced by van Seijen and Sutton (2014). The algorithm on page 307 was\r
adapted from van Seijen et al. (2016). The Mountain Car results were made for\r
this text, except for Figure 12.11 which is adapted from van Seijen and Sutton\r
(2014).\r
12.8 Perhaps the first published discussion of variable  was by Watkins (1989), who\r
pointed out that the cutting o↵ of the update sequence (Figure 12.12) in his\r
Q() when a nongreedy action was selected could be implemented by temporarily\r
setting  to 0.\r
Variable  was introduced in the first edition of this text. The roots of variable \r
are in the work on options (Sutton, Precup, and Singh, 1999) and its precursors\r
(Sutton, 1995a), becoming explicit in the GQ() paper (Maei and Sutton, 2010),\r
which also introduced some of these recursive forms for the -returns.\r
A di↵erent notion of variable  has been developed by Yu (2012).\r
12.9 O↵-policy eligibility traces were introduced by Precup et al. (2000, 2001), then\r
further developed by Bertsekas and Yu (2009), Maei (2011; Maei and Sutton,\r
2010), Yu (2012), and by Sutton, Mahmood, Precup, and van Hasselt (2014).\r
The last reference in particular gives a powerful forward view for o↵-policy TD\r
methods with general state-dependent  and . The presentation here seems to\r
be new.\r
This section ends with an elegant Expected Sarsa() algorithm. Although it is\r
a natural algorithm, to our knowledge it has not previously been described or\r
tested in the literature.\r
12.10 Watkins’s Q() is due to Watkins (1989). The tabular, episodic, o↵-line version\r
has been proven convergent by Munos, Stepleton, Harutyunyan, and Bellemare\r
(2016). Alternative Q() algorithms were proposed by Peng and Williams (1994,\r
1996) and by Sutton, Mahmood, Precup, and van Hasselt (2014). Tree Backup()\r
is due to Precup, Sutton, and Singh (2000).\r
12.11 GTD() is due to Maei (2011). GQ() is due to Maei and Sutton (2010).\r
HTD() is due to White and White (2016) based on the one-step HTD algorithm\r
introduced by Hackman (2012). The latest developments in the theory of\r
Gradient-TD methods are by Yu (2017). Emphatic TD() was introduced by\r
Sutton, Mahmood, and White (2016), who proved its stability. Yu (2015, 2016)\r
proved its convergence, and the algorithm was developed further by Hallak et\r
al. (2015, 2016).

Chapter 13\r
Policy Gradient Methods\r
In this chapter we consider something new. So far in this book almost all the methods\r
have been action-value methods; they learned the values of actions and then selected\r
actions based on their estimated action values1; their policies would not even exist without\r
the action-value estimates. In this chapter we consider methods that instead learn a\r
parameterized policy that can select actions without consulting a value function. A value\r
function may still be used to learn the policy parameter, but is not required for action\r
selection. We use the notation ✓ 2 Rd0for the policy’s parameter vector. Thus we write\r
⇡(a|s, ✓) = Pr{At =a | St =s, ✓t =✓} for the probability that action a is taken at time t\r
given that the environment is in state s at time t with parameter ✓. If a method uses a\r
learned value function as well, then the value function’s weight vector is denoted w 2 Rd\r
as usual, as in ˆv(s,w).\r
In this chapter we consider methods for learning the policy parameter based on the\r
gradient of some scalar performance measure J(✓) with respect to the policy parameter.\r
These methods seek to maximize performance, so their updates approximate gradient\r
ascent in J:\r
✓t+1 = ✓t + ↵r\\J(✓t), (13.1)\r
where r\\J(✓t) 2 Rd0is a stochastic estimate whose expectation approximates the gradient\r
of the performance measure with respect to its argument ✓t. All methods that follow\r
this general schema we call policy gradient methods, whether or not they also learn an\r
approximate value function. Methods that learn approximations to both policy and value\r
functions are often called actor–critic methods, where ‘actor’ is a reference to the learned\r
policy, and ‘critic’ refers to the learned value function, usually a state-value function.\r
First we treat the episodic case, in which performance is defined as the value of the start\r
state under the parameterized policy, before going on to consider the continuing case, in\r
1The lone exception is the gradient bandit algorithms of Section 2.8. In fact, that section goes through\r
many of the same steps, in the single-state bandit case, as we go through here for full MDPs. Reviewing\r
that section would be good preparation for fully understanding this chapter.

322 Chapter 13: Policy Gradient Methods\r
which performance is defined as the average reward rate, as in Section 10.3. In the end,\r
we are able to express the algorithms for both cases in very similar terms.\r
13.1 Policy Approximation and its Advantages\r
In policy gradient methods, the policy can be parameterized in any way, as long as\r
⇡(a|s, ✓) is di↵erentiable with respect to its parameters, that is, as long as r⇡(a|s, ✓) (the\r
column vector of partial derivatives of ⇡(a|s, ✓) with respect to the components of ✓) exists\r
and is finite for all s 2 S, a 2 A(s), and ✓ 2 Rd0. In practice, to ensure exploration we\r
generally require that the policy never becomes deterministic (i.e., that ⇡(a|s, ✓) 2 (0, 1),\r
for all s, a, ✓). In this section we introduce the most common parameterization for\r
discrete action spaces and point out the advantages it o↵ers over action-value methods.\r
Policy-based methods also o↵er useful ways of dealing with continuous action spaces, as\r
we describe later in Section 13.7.\r
If the action space is discrete and not too large, then a natural and common kind of\r
parameterization is to form parameterized numerical preferences h(s, a, ✓) 2 R for each\r
state–action pair. The actions with the highest preferences in each state are given the\r
highest probabilities of being selected, for example, according to an exponential soft-max\r
distribution:\r
⇡(a|s, ✓) .= eh(s,a,✓)\r
P\r
b eh(s,b,✓) , (13.2)\r
where e ⇡ 2.71828 is the base of the natural logarithm. Note that the denominator here\r
is just what is required so that the action probabilities in each state sum to one. We call\r
this kind of policy parameterization soft-max in action preferences.\r
The action preferences themselves can be parameterized arbitrarily. For example, they\r
might be computed by a deep artificial neural network (ANN), where ✓ is the vector\r
of all the connection weights of the network (as in the AlphaGo system described in\r
Section 16.6). Or the preferences could simply be linear in features,\r
h(s, a, ✓) = ✓>x(s, a), (13.3)\r
using feature vectors x(s, a) 2 Rd0constructed by any of the methods described in\r
Section 9.5.\r
One advantage of parameterizing policies according to the soft-max in action preferences\r
is that the approximate policy can approach a deterministic policy, whereas with "-greedy\r
action selection over action values there is always an " probability of selecting a random\r
action. Of course, one could select according to a soft-max distribution based on action\r
values, but this alone would not allow the policy to approach a deterministic policy.\r
Instead, the action-value estimates would converge to their corresponding true values,\r
which would di↵er by a finite amount, translating to specific probabilities other than 0 and\r
1. If the soft-max distribution included a temperature parameter, then the temperature\r
could be reduced over time to approach determinism, but in practice it would be dicult\r
to choose the reduction schedule, or even the initial temperature, without more prior\r
knowledge of the true action values than we would like to assume. Action preferences

13.1. Policy Approximation and its Advantages 323\r
are di↵erent because they do not approach specific values; instead they are driven to\r
produce the optimal stochastic policy. If the optimal policy is deterministic, then the\r
preferences of the optimal actions will be driven infinitely higher than all suboptimal\r
actions (if permitted by the parameterization).\r
A second advantage of parameterizing policies according to the soft-max in action\r
preferences is that it enables the selection of actions with arbitrary probabilities. In\r
problems with significant function approximation, the best approximate policy may be\r
stochastic. For example, in card games with imperfect information the optimal play is\r
often to do two di↵erent things with specific probabilities, such as when blung in Poker.\r
Action-value methods have no natural way of finding stochastic optimal policies, whereas\r
policy approximating methods can, as shown in Example 13.1.\r
Example 13.1 Short corridor with switched actions\r
Consider the small corridor gridworld shown inset in the graph below. The reward\r
is 1 per step, as usual. In each of the three nonterminal states there are only\r
two actions, right and left. These actions have their usual consequences in the\r
first and third states (left causes no movement in the first state), but in the\r
second state they are reversed, so that right moves to the left and left moves to\r
the right. The problem is dicult because all the states appear identical under\r
the function approximation. In particular, we define x(s,right) = [1, 0]> and\r
x(s, left) = [0, 1]>, for all s. An action-value method with "-greedy action selection\r
is forced to choose between just two policies: choosing right with high probability\r
1  "/2 on all steps or choosing left with the same high probability on all time\r
steps. If " = 0.1, then these two policies achieve a value (at the start state)\r
of less than 44 and 82, respectively, as shown in the graph. A method can\r
do significantly better if it can learn a specific probability with which to select\r
right. The best probability is about 0.59, which achieves a value of about 11.6.\r
probability of right action\r
-11.6\r
0.1 0.2\r
-20\r
-40\r
-60\r
-80\r
-100\r
0 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1\r
-greedy left \r
-greedy right \r
optimal\r
stochastic\r
policy \r
J(✓) = v⇡✓ (S)\r
S G

324 Chapter 13: Policy Gradient Methods\r
Perhaps the simplest advantage that policy parameterization may have over action\u0002value parameterization is that the policy may be a simpler function to approximate.\r
Problems vary in the complexity of their policies and action-value functions. For some,\r
the action-value function is simpler and thus easier to approximate. For others, the policy\r
is simpler. In the latter case a policy-based method will typically learn faster and yield a\r
superior asymptotic policy (as in Tetris; see S¸im¸sek, Alg´orta, and Kothiyal, 2016).\r
Finally, we note that the choice of policy parameterization is sometimes a good way\r
of injecting prior knowledge about the desired form of the policy into the reinforcement\r
learning system. This is often the most important reason for using a policy-based learning\r
method.\r
Exercise 13.1 Use your knowledge of the gridworld and its dynamics to determine an\r
exact symbolic expression for the optimal probability of selecting the right action in\r
Example 13.1. ⇤\r
13.2 The Policy Gradient Theorem\r
In addition to the practical advantages of policy parameterization over "-greedy action\r
selection, there is also an important theoretical advantage. With continuous policy\r
parameterization the action probabilities change smoothly as a function of the learned\r
parameter, whereas in "-greedy selection the action probabilities may change dramatically\r
for an arbitrarily small change in the estimated action values, if that change results in a\r
di↵erent action having the maximal value. Largely because of this, stronger convergence\r
guarantees are available for policy-gradient methods than for action-value methods. In\r
particular, it is the continuity of the policy dependence on the parameters that enables\r
policy-gradient methods to approximate gradient ascent (13.1).\r
The episodic and continuing cases define the performance measure, J(✓), di↵erently\r
and thus have to be treated separately to some extent. Nevertheless, we will try to\r
present both cases uniformly, and we develop a notation so that the major theoretical\r
results can be described with a single set of equations.\r
In this section we treat the episodic case, for which we define the performance measure\r
as the value of the start state of the episode. We can simplify the notation without\r
losing any meaningful generality by assuming that every episode starts in some particular\r
(non-random) state s0. Then, in the episodic case we define performance as\r
J(✓) .= v⇡✓ (s0), (13.4)\r
where v⇡✓ is the true value function for ⇡✓, the policy determined by ✓. From here on in\r
our discussion we will assume no discounting ( = 1) for the episodic case, although for\r
completeness we do include the possibility of discounting in the boxed algorithms.\r
With function approximation it may seem challenging to change the policy parameter\r
in a way that ensures improvement. The problem is that performance depends on both\r
the action selections and the distribution of states in which those selections are made,\r
and that both of these are a↵ected by the policy parameter. Given a state, the e↵ect of\r
the policy parameter on the actions, and thus on reward, can be computed in a relatively\r
straightforward way from knowledge of the parameterization. But the e↵ect of the policy

13.2. The Policy Gradient Theorem 325\r
Proof of the Policy Gradient Theorem (episodic case)\r
With just elementary calculus and re-arranging of terms, we can prove the policy\r
gradient theorem from first principles. To keep the notation simple, we leave it\r
implicit in all cases that ⇡ is a function of ✓, and all gradients are also implicitly\r
with respect to ✓. First note that the gradient of the state-value function can be\r
written in terms of the action-value function as\r
rv⇡(s) = r\r
"\r
X\r
a\r
⇡(a|s)q⇡(s, a)\r
#\r
, for all s 2 S (Exercise 3.18)\r
= X\r
a\r
h\r
r⇡(a|s)q⇡(s, a) + ⇡(a|s)rq⇡(s, a)\r
i\r
(product rule of calculus)\r
= X\r
a\r
h\r
r⇡(a|s)q⇡(s, a) + ⇡(a|s)rX\r
s0,r\r
p(s0, r|s, a)\r
\r
r + v⇡(s0)\r
i\r
(Exercise 3.19 and Equation 3.2)\r
= X\r
a\r
h\r
r⇡(a|s)q⇡(s, a) + ⇡(a|s)\r
X\r
s0\r
p(s0|s, a)rv⇡(s0)\r
i\r
(Eq. 3.4)\r
= X\r
a\r
h\r
r⇡(a|s)q⇡(s, a) + ⇡(a|s)\r
X\r
s0\r
p(s0|s, a) (unrolling)\r
X\r
a0\r
⇥\r
r⇡(a0|s0)q⇡(s0, a0) + ⇡(a0|s0)\r
X\r
s00\r
p(s00 |s0, a0)rv⇡(s00)\r
⇤i\r
= X\r
x2S\r
X1\r
k=0\r
Pr(s!x, k, ⇡)\r
X\r
a\r
r⇡(a|x)q⇡(x, a),\r
after repeated unrolling, where Pr(s!x, k, ⇡) is the probability of transitioning\r
from state s to state x in k steps under policy ⇡. It is then immediate that\r
rJ(✓) = rv⇡(s0)\r
= X\r
s\r
 X1\r
k=0\r
Pr(s0!s, k, ⇡)\r
!X\r
a\r
r⇡(a|s)q⇡(s, a)\r
= X\r
s\r
⌘(s)\r
X\r
a\r
r⇡(a|s)q⇡(s, a) (box page 199)\r
= X\r
s0\r
⌘(s0)\r
X\r
s\r
⌘(s)\r
P\r
s0 ⌘(s0\r
)\r
X\r
a\r
r⇡(a|s)q⇡(s, a)\r
= X\r
s0\r
⌘(s0)\r
X\r
s\r
µ(s)\r
X\r
a\r
r⇡(a|s)q⇡(s, a) (Eq. 9.3)\r
/ X\r
s\r
µ(s)\r
X\r
a\r
r⇡(a|s)q⇡(s, a) (Q.E.D.)

326 Chapter 13: Policy Gradient Methods\r
on the state distribution is a function of the environment and is typically unknown. How\r
can we estimate the performance gradient with respect to the policy parameter when the\r
gradient depends on the unknown e↵ect of policy changes on the state distribution?\r
Fortunately, there is an excellent theoretical answer to this challenge in the form of\r
the policy gradient theorem, which provides an analytic expression for the gradient of\r
performance with respect to the policy parameter (which is what we need to approximate\r
for gradient ascent (13.1)) that does not involve the derivative of the state distribution.\r
The policy gradient theorem for the episodic case establishes that\r
rJ(✓) / X\r
s\r
µ(s)\r
X\r
a\r
q⇡(s, a)r⇡(a|s, ✓), (13.5)\r
where the gradients are column vectors of partial derivatives with respect to the compo\u0002nents of ✓, and ⇡ denotes the policy corresponding to parameter vector ✓. The symbol /\r
here means “proportional to”. In the episodic case, the constant of proportionality is the\r
average length of an episode, and in the continuing case it is 1, so that the relationship is\r
actually an equality. The distribution µ here (as in Chapters 9 and 10) is the on-policy\r
distribution under ⇡ (see page 199). The policy gradient theorem is proved for the\r
episodic case in the box on the previous page.\r
13.3 REINFORCE: Monte Carlo Policy Gradient\r
We are now ready to derive our first policy-gradient learning algorithm. Recall our overall\r
strategy of stochastic gradient ascent (13.1), which requires a way to obtain samples such\r
that the expectation of the sample gradient is proportional to the actual gradient of the\r
performance measure as a function of the parameter. The sample gradients need only be\r
proportional to the gradient because any constant of proportionality can be absorbed\r
into the step size ↵, which is otherwise arbitrary. The policy gradient theorem gives an\r
exact expression proportional to the gradient; all that is needed is some way of sampling\r
whose expectation equals or approximates this expression. Notice that the right-hand\r
side of the policy gradient theorem is a sum over states weighted by how often the states\r
occur under the target policy ⇡; if ⇡ is followed, then states will be encountered in these\r
proportions. Thus\r
rJ(✓) / X\r
s\r
µ(s)\r
X\r
a\r
q⇡(s, a)r⇡(a|s, ✓)\r
= E⇡\r
"\r
X\r
a\r
q⇡(St, a)r⇡(a|St, ✓)\r
#\r
. (13.6)\r
We could stop here and instantiate our stochastic gradient-ascent algorithm (13.1) as\r
✓t+1\r
.\r
= ✓t + ↵\r
X\r
a\r
qˆ(St, a, w)r⇡(a|St, ✓), (13.7)\r
where qˆ is some learned approximation to q⇡. This algorithm, which has been called\r
an all-actions method because its update involves all of the actions, is promising and

13.3. REINFORCE: Monte Carlo Policy Gradient 327\r
deserving of further study, but our current interest is the classical REINFORCE algorithm\r
(Willams, 1992) whose update at time t involves just At, the one action actually taken at\r
time t.\r
We continue our derivation of REINFORCE by introducing At in the same way as we\r
introduced St in (13.6)—by replacing a sum over the random variable’s possible values\r
by an expectation under ⇡, and then sampling the expectation. Equation (13.6) involves\r
an appropriate sum over actions, but each term is not weighted by ⇡(a|St, ✓) as is needed\r
for an expectation under ⇡. So we introduce such a weighting, without changing the\r
equality, by multiplying and then dividing the summed terms by ⇡(a|St, ✓). Continuing\r
from (13.6), we have\r
rJ(✓) / E⇡\r
"\r
X\r
a\r
⇡(a|St, ✓)q⇡(St, a)\r
r⇡(a|St, ✓)\r
⇡(a|St, ✓)\r
#\r
= E⇡\r
\r
q⇡(St, At)\r
r⇡(At|St, ✓)\r
⇡(At|St, ✓)\r
\r
(replacing a by the sample At ⇠ ⇡)\r
= E⇡\r
\r
Gt\r
r⇡(At|St, ✓)\r
⇡(At|St, ✓)\r
\r
, (because E⇡[Gt|St, At] = q⇡(St, At))\r
where Gt is the return as usual. The final expression in brackets is exactly what is needed,\r
a quantity that can be sampled on each time step whose expectation is proportional\r
to the gradient. Using this sample to instantiate our generic stochastic gradient ascent\r
algorithm (13.1) yields the REINFORCE update:\r
✓t+1\r
.\r
= ✓t + ↵Gt\r
r⇡(At|St, ✓t)\r
⇡(At|St, ✓t) . (13.8)\r
This update has an intuitive appeal. Each increment is proportional to the product of a\r
return Gt and a vector, the gradient of the probability of taking the action actually taken\r
divided by the probability of taking that action. The vector is the direction in parameter\r
space that most increases the probability of repeating the action At on future visits\r
to state St. The update increases the parameter vector in this direction proportional\r
to the return, and inversely proportional to the action probability. The former makes\r
sense because it causes the parameter to move most in the directions that favor actions\r
that yield the highest return. The latter makes sense because otherwise actions that are\r
selected frequently are at an advantage (the updates will be more often in their direction)\r
and might win out even if they do not yield the highest return.\r
Note that REINFORCE uses the complete return from time t, which includes all\r
future rewards up until the end of the episode. In this sense REINFORCE is a Monte\r
Carlo algorithm and is well defined only for the episodic case with all updates made in\r
retrospect after the episode is completed (like the Monte Carlo algorithms in Chapter 5).\r
This is shown explicitly in the boxed algorithm on the next page.\r
Notice that the update in the last line of pseudocode appears rather di↵erent from\r
the REINFORCE update rule (13.8). One di↵erence is that the pseudocode uses the\r
compact expression r ln ⇡(At|St, ✓t) for the fractional vector r⇡(At|St,✓t)\r
⇡(At|St,✓t) in (13.8). That\r
these two expressions for the vector are equivalent follows from the identity r ln x = rx\r
x .

328 Chapter 13: Policy Gradient Methods\r
This vector has been given several names and notations in the literature; we will refer\r
to it simply as the eligibility vector. Note that it is the only place that the policy\r
parameterization appears in the algorithm.\r
REINFORCE: Monte-Carlo Policy-Gradient Control (episodic) for ⇡⇤\r
Input: a di↵erentiable policy parameterization ⇡(a|s, ✓)\r
Algorithm parameter: step size ↵ > 0\r
Initialize policy parameter ✓ 2 Rd0(e.g., to 0)\r
Loop forever (for each episode):\r
Generate an episode S0, A0, R1,...,ST 1, AT 1, RT , following ⇡(·|·, ✓)\r
Loop for each step of the episode t = 0, 1,...,T  1:\r
G PT\r
k=t+1 kt1Rk (Gt)\r
✓ ✓ + ↵tGr ln ⇡(At|St, ✓)\r
The second di↵erence between the pseudocode update and the REINFORCE update\r
equation (13.8) is that the former includes a factor of t. This is because, as mentioned\r
earlier, in the text we are treating the non-discounted case ( = 1) while in the boxed\r
algorithms we are giving the algorithms for the general discounted case. All of the ideas\r
go through in the discounted case with appropriate adjustments (including to the box on\r
page 199) but involve additional complexity that distracts from the main ideas.\r
⇤\r
Exercise 13.2 Generalize the box on page 199, the policy gradient theorem (13.5), the\r
proof of the policy gradient theorem (page 325), and the steps leading to the REINFORCE\r
update equation (13.8), so that (13.8) ends up with a factor of t and thus aligns with\r
the general algorithm given in the pseudocode. ⇤\r
Figure 13.1 shows the performance of REINFORCE on the short-corridor gridworld\r
from Example 13.1.\r
↵ = 213\r
↵ = 212\r
Episode\r
1 200 400 600 800 1000\r
-80\r
-90\r
-60\r
-40\r
-20\r
-10\r
Total reward\r
on episode\r
averaged over 100 runs\r
G0\r
v⇤(s0)\r
↵ = 214\r
Figure 13.1: REINFORCE on the short-corridor gridworld (Example 13.1). With a good step\r
size, the total reward per episode approaches the optimal value of the start state.

13.4. REINFORCE with Baseline 329\r
As a stochastic gradient method, REINFORCE has good theoretical convergence\r
properties. By construction, the expected update over an episode is in the same direction\r
as the performance gradient. This assures an improvement in expected performance for\r
suciently small ↵, and convergence to a local optimum under standard stochastic approx\u0002imation conditions for decreasing ↵. However, as a Monte Carlo method REINFORCE\r
may be of high variance and thus produce slow learning.\r
Exercise 13.3 In Section 13.1 we considered policy parameterizations using the soft-max in\r
action preferences (13.2) with linear action preferences (13.3). For this parameterization,\r
prove that the eligibility vector is\r
r ln ⇡(a|s, ✓) = x(s, a) X\r
b\r
⇡(b|s, ✓)x(s, b), (13.9)\r
using the definitions and elementary calculus. ⇤\r
13.4 REINFORCE with Baseline\r
The policy gradient theorem (13.5) can be generalized to include a comparison of the\r
action value to an arbitrary baseline b(s):\r
rJ(✓) / X\r
s\r
µ(s)\r
X\r
a\r
⇣\r
q⇡(s, a)  b(s)\r
⌘\r
r⇡(a|s, ✓). (13.10)\r
The baseline can be any function, even a random variable, as long as it does not vary\r
with a; the equation remains valid because the subtracted quantity is zero:\r
X\r
a\r
b(s)r⇡(a|s, ✓) = b(s)rX\r
a\r
⇡(a|s, ✓) = b(s)r1=0.\r
The policy gradient theorem with baseline (13.10) can be used to derive an update\r
rule using similar steps as in the previous section. The update rule that we end up with\r
is a new version of REINFORCE that includes a general baseline:\r
✓t+1\r
.\r
= ✓t + ↵\r
⇣\r
Gt  b(St)\r
⌘r⇡(At|St, ✓t)\r
⇡(At|St, ✓t) . (13.11)\r
Because the baseline could be uniformly zero, this update is a strict generalization of\r
REINFORCE. In general, the baseline leaves the expected value of the update unchanged,\r
but it can have a large e↵ect on its variance. For example, we saw in Section 2.8 that an\r
analogous baseline can significantly reduce the variance (and thus speed the learning) of\r
gradient bandit algorithms. In the bandit algorithms the baseline was just a number (the\r
average of the rewards seen so far), but for MDPs the baseline should vary with state.\r
In some states all actions have high values and we need a high baseline to di↵erentiate\r
the higher valued actions from the less highly valued ones; in other states all actions will\r
have low values and a low baseline is appropriate.\r
One natural choice for the baseline is an estimate of the state value, vˆ(St,w), where\r
w 2 Rd is a weight vector learned by one of the methods presented in previous chapters.

330 Chapter 13: Policy Gradient Methods\r
Because REINFORCE is a Monte Carlo method for learning the policy parameter, ✓,\r
it seems natural to also use a Monte Carlo method to learn the state-value weights, w.\r
A complete pseudocode algorithm for REINFORCE with baseline using such a learned\r
state-value function as the baseline is given in the box below.\r
REINFORCE with Baseline (episodic), for estimating ⇡✓ ⇡ ⇡⇤\r
Input: a di↵erentiable policy parameterization ⇡(a|s, ✓)\r
Input: a di↵erentiable state-value function parameterization ˆv(s,w)\r
Algorithm parameters: step sizes ↵✓ > 0, ↵w > 0\r
Initialize policy parameter ✓ 2 Rd0and state-value weights w 2 Rd (e.g., to 0)\r
Loop forever (for each episode):\r
Generate an episode S0, A0, R1,...,ST 1, AT 1, RT , following ⇡(·|·, ✓)\r
Loop for each step of the episode t = 0, 1,...,T  1:\r
G PT\r
k=t+1 kt1Rk (Gt)\r
 G  vˆ(St,w)\r
w w + ↵w rvˆ(St,w)\r
✓ ✓ + ↵✓ t r ln ⇡(At|St, ✓)\r
This algorithm has two step sizes, denoted ↵✓ and ↵w (where ↵✓ is the ↵ in (13.11)).\r
Choosing the step size for values (here ↵w) is relatively easy; in the linear case we have\r
rules of thumb for setting it, such as ↵w = 0.1/E\r
⇥\r
krvˆ(St,w)k\r
2\r
µ\r
⇤\r
(see Section 9.6). It is\r
much less clear how to set the step size for the policy parameters, ↵✓, whose best value\r
depends on the range of variation of the rewards and on the policy parameterization.\r
↵ = 213\r
Episode\r
1 200 400 600 800 1000\r
-80\r
-90\r
-60\r
-40\r
-20\r
-10 v⇤(s0)\r
REINFORCE\r
REINFORCE with baseline\r
↵ = 29\r
↵✓ = 29, ↵w = 26\r
Total reward\r
on episode\r
averaged over 100 runs\r
G0\r
Figure 13.2: Adding a baseline to REINFORCE can make it learn much faster, as illus\u0002trated here on the short-corridor gridworld (Example 13.1). The step size used here for plain\r
REINFORCE is that at which it performs best (to the nearest power of two; see Figure 13.1).

13.5. Actor–Critic Methods 331\r
Figure 13.2 compares the behavior of REINFORCE with and without a baseline on\r
the short-corridor gridword (Example 13.1). Here the approximate state-value function\r
used in the baseline is ˆv(s,w) = w. That is, w is a single component, w.\r
13.5 Actor–Critic Methods\r
In REINFORCE with baseline, the learned state-value function estimates the value of\r
the first state of each state transition. This estimate sets a baseline for the subsequent\r
return, but is made prior to the transition’s action and thus cannot be used to assess that\r
action. In actor–critic methods, on the other hand, the state-value function is applied\r
also to the second state of the transition. The estimated value of the second state, when\r
discounted and added to the reward, constitutes the one-step return, Gt:t+1, which is a\r
useful estimate of the actual return and thus is a way of assessing the action. As we have\r
seen in the TD learning of value functions throughout this book, the one-step return is\r
often superior to the actual return in terms of its variance and computational congeniality,\r
even though it introduces bias. We also know how we can flexibly modulate the extent\r
of the bias with n-step returns and eligibility traces (Chapters 7 and 12). When the\r
state-value function is used to assess actions in this way it is called a critic, and the\r
overall policy-gradient method is termed an actor–critic method. Note that the bias in\r
the gradient estimate is not due to bootstrapping as such; the actor would be biased even\r
if the critic was learned by a Monte Carlo method.\r
First consider one-step actor–critic methods, the analog of the TD methods introduced\r
in Chapter 6 such as TD(0), Sarsa(0), and Q-learning. The main appeal of one-step\r
methods is that they are fully online and incremental, yet avoid the complexities of\r
eligibility traces. They are a special case of the eligibility trace methods, but easier\r
to understand. One-step actor–critic methods replace the full return of REINFORCE\r
(13.11) with the one-step return (and use a learned state-value function as the baseline)\r
as follows:\r
✓t+1\r
.\r
= ✓t + ↵\r
⇣\r
Gt:t+1  vˆ(St,w)\r
⌘r⇡(At|St, ✓t)\r
⇡(At|St, ✓t) (13.12)\r
= ✓t + ↵\r
⇣\r
Rt+1 + vˆ(St+1,w)  vˆ(St,w)\r
⌘r⇡(At|St, ✓t)\r
⇡(At|St, ✓t) (13.13)\r
= ✓t + ↵t\r
r⇡(At|St, ✓t)\r
⇡(At|St, ✓t) . (13.14)\r
The natural state-value-function learning method to pair with this is semi-gradient TD(0).\r
Pseudocode for the complete algorithm is given in the box at the top of the next page.\r
Note that it is now a fully online, incremental algorithm, with states, actions, and rewards\r
processed as they occur and then never revisited.

332 Chapter 13: Policy Gradient Methods\r
One-step Actor–Critic (episodic), for estimating ⇡✓ ⇡ ⇡⇤\r
Input: a di↵erentiable policy parameterization ⇡(a|s, ✓)\r
Input: a di↵erentiable state-value function parameterization ˆv(s,w)\r
Parameters: step sizes ↵✓ > 0, ↵w > 0\r
Initialize policy parameter ✓ 2 Rd0and state-value weights w 2 Rd (e.g., to 0)\r
Loop forever (for each episode):\r
Initialize S (first state of episode)\r
I 1\r
Loop while S is not terminal (for each time step):\r
A ⇠ ⇡(·|S, ✓)\r
Take action A, observe S0, R\r
 R +  vˆ(S0,w)  vˆ(S,w) (if S0 is terminal, then ˆv(S0,w) .= 0)\r
w w + ↵w rvˆ(S,w)\r
✓ ✓ + ↵✓ I r ln ⇡(A|S, ✓)\r
I I\r
S S0\r
The generalizations to the forward view of n-step methods and then to a -return\r
algorithm are straightforward. The one-step return in (13.12) is merely replaced by Gt:t+n\r
or G\r
t respectively. The backward view of the -return algorithm is also straightforward,\r
using separate eligibility traces for the actor and critic, each after the patterns in\r
Chapter 12. Pseudocode for the complete algorithm is given in the box below.\r
Actor–Critic with Eligibility Traces (episodic), for estimating ⇡✓ ⇡ ⇡⇤\r
Input: a di↵erentiable policy parameterization ⇡(a|s, ✓)\r
Input: a di↵erentiable state-value function parameterization ˆv(s,w)\r
Parameters: trace-decay rates ✓ 2 [0, 1], w 2 [0, 1]; step sizes ↵✓ > 0, ↵w > 0\r
Initialize policy parameter ✓ 2 Rd0and state-value weights w 2 Rd (e.g., to 0)\r
Loop forever (for each episode):\r
Initialize S (first state of episode)\r
z✓ 0 (d0-component eligibility trace vector)\r
zw 0 (d-component eligibility trace vector)\r
I 1\r
Loop while S is not terminal (for each time step):\r
A ⇠ ⇡(·|S, ✓)\r
Take action A, observe S0, R\r
 R +  vˆ(S0,w)  vˆ(S,w) (if S0 is terminal, then ˆv(S0,w) .= 0)\r
zw wzw + rvˆ(S,w)\r
z✓ ✓z✓ + I r ln ⇡(A|S, ✓)\r
w w + ↵w zw\r
✓ ✓ + ↵✓ z✓\r
I I\r
S S0

13.6. Policy Gradient for Continuing Problems 333\r
13.6 Policy Gradient for Continuing Problems\r
As discussed in Section 10.3, for continuing problems without episode boundaries we need\r
to define performance in terms of the average rate of reward per time step:\r
J(✓) .= r(⇡) .= lim\r
h!1\r
1\r
h\r
X\r
h\r
t=1\r
E[Rt | S0, A0:t1 ⇠⇡] (13.15)\r
= limt!1 E[Rt | S0, A0:t1 ⇠⇡]\r
= X\r
s\r
µ(s)\r
X\r
a\r
⇡(a|s)\r
X\r
s0,r\r
p(s0, r|s, a)r,\r
where µ is the steady-state distribution under ⇡, µ(s) .= limt!1 Pr{St =s|A0:t ⇠⇡},\r
which is assumed to exist and to be independent of S0 (an ergodicity assumption).\r
Remember that this is the special distribution under which, if you select actions according\r
to ⇡, you remain in the same distribution:\r
X\r
s\r
µ(s)\r
X\r
a\r
⇡(a|s, ✓)p(s0|s, a) = µ(s0), for all s0 2 S. (13.16)\r
Complete pseudocode for the actor–critic algorithm in the continuing case (backward\r
view) is given in the box below.\r
Actor–Critic with Eligibility Traces (continuing), for estimating ⇡✓ ⇡ ⇡⇤\r
Input: a di↵erentiable policy parameterization ⇡(a|s, ✓)\r
Input: a di↵erentiable state-value function parameterization ˆv(s,w)\r
Algorithm parameters: w 2 [0, 1], ✓ 2 [0, 1], ↵w > 0, ↵✓ > 0, ↵R¯ > 0\r
Initialize R¯ 2 R (e.g., to 0)\r
Initialize state-value weights w 2 Rd and policy parameter ✓ 2 Rd0(e.g., to 0)\r
Initialize S 2 S (e.g., to s0)\r
zw 0 (d-component eligibility trace vector)\r
z✓ 0 (d0-component eligibility trace vector)\r
Loop forever (for each time step):\r
A ⇠ ⇡(·|S, ✓)\r
Take action A, observe S0, R\r
 R  R¯ + ˆv(S0,w)  vˆ(S,w)\r
R¯ R¯ + ↵R¯ \r
zw wzw + rvˆ(S,w)\r
z✓ ✓z✓ + r ln ⇡(A|S, ✓)\r
w w + ↵w zw\r
✓ ✓ + ↵✓ z✓\r
S S0

334 Chapter 13: Policy Gradient Methods\r
Naturally, in the continuing case, we define values, v⇡(s) .= E⇡[Gt|St =s] and q⇡(s, a) .=\r
E⇡[Gt|St =s, At =a], with respect to the di↵erential return:\r
Gt\r
.\r
= Rt+1  r(⇡) + Rt+2  r(⇡) + Rt+3  r(⇡) + ··· . (13.17)\r
With these alternate definitions, the policy gradient theorem as given for the episodic\r
case (13.5) remains true for the continuing case. A proof is given in the box on the next\r
page. The forward and backward view equations also remain the same.\r
Proof of the Policy Gradient Theorem (continuing case)\r
The proof of the policy gradient theorem for the continuing case begins similarly\r
to the episodic case. Again we leave it implicit in all cases that ⇡ is a function\r
of ✓ and that the gradients are with respect to ✓. Recall that in the continuing\r
case J(✓) = r(⇡) (13.15) and that v⇡ and q⇡ denote values with respect to the\r
di↵erential return (13.17). The gradient of the state-value function can be written,\r
for any s 2 S, as\r
rv⇡(s) = r\r
"\r
X\r
a\r
⇡(a|s)q⇡(s, a)\r
#\r
, for all s 2 S (Exercise 3.18)\r
= X\r
a\r
h\r
r⇡(a|s)q⇡(s, a) + ⇡(a|s)rq⇡(s, a)\r
i\r
(product rule of calculus)\r
= X\r
a\r
h\r
r⇡(a|s)q⇡(s, a) + ⇡(a|s)rX\r
s0,r\r
p(s0, r|s, a)\r
\r
r  r(✓) + v⇡(s0)\r
i\r
= X\r
a\r
h\r
r⇡(a|s)q⇡(s, a) + ⇡(a|s)\r
⇥\r
rr(✓) +X\r
s0\r
p(s0|s, a)rv⇡(s0)\r
⇤i\r
.\r
After re-arranging terms, we obtain\r
rr(✓) = X\r
a\r
h\r
r⇡(a|s)q⇡(s, a) + ⇡(a|s)\r
X\r
s0\r
p(s0|s, a)rv⇡(s0)\r
i\r
 rv⇡(s).\r
Notice that the left-hand side can be written rJ(✓), and that it does not depend\r
on s. Thus the right-hand side does not depend on s either, and we can safely sum\r
it over all s 2 S, weighted by µ(s), without changing it (because P\r
s µ(s) = 1):\r
rJ(✓) = X\r
s\r
µ(s)\r
 X\r
a\r
h\r
r⇡(a|s)q⇡(s, a) + ⇡(a|s)\r
X\r
s0\r
p(s0|s, a)rv⇡(s0)\r
i\r
 rv⇡(s)\r
!\r
= X\r
s\r
µ(s)\r
X\r
a\r
r⇡(a|s)q⇡(s, a)\r
+X\r
s\r
µ(s)\r
X\r
a\r
⇡(a|s)\r
X\r
s0\r
p(s0|s, a)rv⇡(s0) X\r
s\r
µ(s)rv⇡(s)

13.7. Policy Parameterization for Continuous Actions 335\r
= X\r
s\r
µ(s)\r
X\r
a\r
r⇡(a|s)q⇡(s, a)\r
+X\r
s0\r
X\r
s\r
µ(s)\r
X\r
a\r
⇡(a|s)p(s0|s, a)\r
| {z } µ(s0) (13.16)\r
rv⇡(s0) X\r
s\r
µ(s)rv⇡(s)\r
= X\r
s\r
µ(s)\r
X\r
a\r
r⇡(a|s)q⇡(s, a) +X\r
s0\r
µ(s0)rv⇡(s0) X\r
s\r
µ(s)rv⇡(s)\r
= X\r
s\r
µ(s)\r
X\r
a\r
r⇡(a|s)q⇡(s, a). Q.E.D.\r
13.7 Policy Parameterization for Continuous Actions\r
Policy-based methods o↵er practical ways of dealing with large action spaces, even\r
continuous spaces with an infinite number of actions. Instead of computing learned\r
probabilities for each of the many actions, we instead learn statistics of the probability\r
distribution. For example, the action set might be the real numbers, with actions chosen\r
from a normal (Gaussian) distribution.\r
The probability density function for the normal distribution is conventionally written\r
p(x) .= 1\r
\r
p2⇡ exp✓\r
(x  µ)2\r
22\r
◆\r
, (13.18)\r
p(x) .= 1\r
\r
p2⇡ exp ✓\r
(x  µ)2\r
22\r
◆\r
φµ,σ 2 ( 0.8\r
0.6\r
0.4\r
0.2\r
0.0\r
−5 −3 1 3 5\r
x\r
1.0\r
−4 −2 −1 0 2 4\r
x)\r
µ= 0,\r
µ= 0,\r
µ= 0,\r
µ= −2,\r
2 σ = 0.2, 2 σ = 1.0,\r
2 σ = 5.0,\r
2 σ = 0.5,\r
p(x) .= 1\r
\r
p2⇡ exp ✓\r
(x  µ)2\r
22\r
◆\r
where µ and  here are the mean and stan\u0002dard deviation of the normal distribution,\r
and of course ⇡ here is just the number\r
⇡ ⇡ 3.14159. The probability density func\u0002tions for several di↵erent means and stan\u0002dard deviations are shown to the right. The\r
value p(x) is the density of the probability\r
at x, not the probability. It can be greater\r
than 1; it is the total area under p(x) that\r
must sum to 1. In general, one can take\r
the integral under p(x) for any range of x\r
values to get the probability of x falling\r
within that range.\r
To produce a policy parameterization, the policy can be defined as the normal proba\u0002bility density over a real-valued scalar action, with mean and standard deviation given\r
by parametric function approximators that depend on the state. That is,\r
⇡(a|s, ✓) .= 1\r
(s, ✓)\r
p2⇡ exp✓\r
(a  µ(s, ✓))2\r
2(s, ✓)2\r
◆\r
, (13.19)\r
where µ : S⇥Rd0! R and  : S⇥Rd0! R+ are two parameterized function approximators.

336 Chapter 13: Policy Gradient Methods\r
To complete the example we need only give a form for these approximators. For this we\r
divide the policy’s parameter vector into two parts, ✓ = [✓µ, ✓]\r
>, one part to be used\r
for the approximation of the mean and one part for the approximation of the standard\r
deviation. The mean can be approximated as a linear function. The standard deviation\r
must always be positive and is better approximated as the exponential of a linear function.\r
Thus\r
µ(s, ✓) .= ✓µ\r
>xµ(s) and (s, ✓) .\r
= exp⇣✓\r
>x(s)\r
⌘\r
, (13.20)\r
where xµ(s) and x(s) are state feature vectors perhaps constructed by one of the methods\r
described in Section 9.5. With these definitions, all the algorithms described in the rest\r
of this chapter can be applied to learn to select real-valued actions.\r
Exercise 13.4 Show that for the Gaussian policy parameterization (Equations 13.19 and\r
13.20) the eligibility vector has the following two parts:\r
r ln ⇡(a|s, ✓µ) = r⇡(a|s, ✓µ)\r
⇡(a|s, ✓) = 1(s, ✓)2\r
\r
a  µ(s, ✓)\r
\r
xµ(s), and\r
r ln ⇡(a|s, ✓) = r⇡(a|s, ✓)\r
⇡(a|s, ✓) =\r
 \r
a  µ(s, ✓)\r
2\r
(s, ✓)2  1\r
!\r
x(s). ⇤\r
Exercise 13.5 A Bernoulli-logistic unit is a stochastic neuron-like unit used in some ANNs\r
(Section 9.7). Its input at time t is a feature vector x(St); its output, At, is a random\r
variable having two values, 0 and 1, with Pr{At = 1} = Pt and Pr{At = 0} = 1  Pt (the\r
Bernoulli distribution). Let h(s, 0, ✓) and h(s, 1, ✓) be the preferences in state s for the\r
unit’s two actions given policy parameter ✓. Assume that the di↵erence between the\r
action preferences is given by a weighted sum of the unit’s input vector, that is, assume\r
that h(s, 1, ✓)  h(s, 0, ✓) = ✓>x(s), where ✓ is the unit’s weight vector.\r
(a) Show that if the exponential soft-max distribution (13.2) is used to convert action\r
preferences to policies, then Pt = ⇡(1|St, ✓t)=1/(1 + exp(✓>\r
t x(St))) (the logistic\r
function).\r
(b) What is the Monte-Carlo REINFORCE update of ✓t to ✓t+1 upon receipt of return\r
Gt?\r
(c) Express the eligibility r ln ⇡(a|s, ✓) for a Bernoulli-logistic unit, in terms of a, x(s),\r
and ⇡(a|s, ✓) by calculating the gradient.\r
Hint for part (c): Define P = ⇡(1|s, ✓) and compute the derivative of the logarithm, for\r
each action, using the chain rule on P. Combine the two results into one expression that\r
depends on a and P, and then use the chain rule again, this time on ✓>x(s), noting that\r
the derivative of the logistic function f(x)=1/(1 + ex) is f(x)(1  f(x)). ⇤

13.8. Summary 337\r
13.8 Summary\r
Prior to this chapter, this book focused on action-value methods—meaning methods that\r
learn action values and then use them to determine action selections. In this chapter, on\r
the other hand, we considered methods that learn a parameterized policy that enables\r
actions to be taken without consulting action-value estimates. In particular, we have\r
considered policy-gradient methods—meaning methods that update the policy parameter\r
on each step in the direction of an estimate of the gradient of performance with respect\r
to the policy parameter.\r
Methods that learn and store a policy parameter have many advantages. They can\r
learn specific probabilities for taking the actions. They can learn appropriate levels\r
of exploration and approach deterministic policies asymptotically. They can naturally\r
handle continuous action spaces. All these things are easy for policy-based methods but\r
awkward or impossible for "-greedy methods and for action-value methods in general. In\r
addition, on some problems the policy is just simpler to represent parametrically than\r
the value function; these problems are more suited to parameterized policy methods.\r
Parameterized policy methods also have an important theoretical advantage over\r
action-value methods in the form of the policy gradient theorem, which gives an exact\r
formula for how performance is a↵ected by the policy parameter that does not involve\r
derivatives of the state distribution. This theorem provides a theoretical foundation for\r
all policy gradient methods.\r
The REINFORCE method follows directly from the policy gradient theorem. Adding\r
a state-value function as a baseline reduces REINFORCE’s variance without introducing\r
bias. If the state-value function is also used to assess—or criticize—the policy’s action\r
selections, then the value function is called a critic and the policy is called an actor ;\r
the overall method is called an actor–critic method. The critic introduces bias into the\r
actor’s gradient estimates, but is often desirable for the same reason that bootstrapping\r
TD methods are often superior to Monte Carlo methods (substantially reduced variance).\r
Overall, policy-gradient methods provide a significantly di↵erent set of strengths and\r
weaknesses than action-value methods. Today they are less well understood in some\r
respects, but a subject of excitement and ongoing research.\r
Bibliographical and Historical Remarks\r
Methods that we now see as related to policy gradients were actually some of the earliest\r
to be studied in reinforcement learning (Witten, 1977; Barto, Sutton, and Anderson,\r
1983; Sutton, 1984; Williams, 1987, 1992) and in predecessor fields (see Phansalkar\r
and Thathachar, 1995). They were largely supplanted in the 1990s by the action-value\r
methods that are the focus of the other chapters of this book. In recent years, however,\r
attention has returned to actor–critic methods and to policy-gradient methods in general.\r
Among the further developments beyond what we cover here are natural-gradient methods\r
(Amari, 1998; Kakade, 2002, Peters, Vijayakumar and Schaal, 2005; Peters and Schaal,\r
2008; Park, Kim and Kang, 2005; Bhatnagar, Sutton, Ghavamzadeh and Lee, 2009; see\r
Grondman, Busoniu, Lopes and Babuska, 2012), deterministic policy-gradient methods

338 Chapter 13: Policy Gradient Methods\r
(Silver et al., 2014), o↵-policy policy-gradient methods (Degris, White, and Sutton, 2012;\r
Maei, 2018), and entropy regularization (see Schulman, Chen, and Abbeel, 2017). Major\r
applications include acrobatic helicopter autopilots and AlphaGo (Section 16.6).\r
Our presentation in this chapter is based primarily on that by Sutton, McAllester,\r
Singh, and Mansour (2000), who introduced the term “policy gradient methods.” A\r
useful overview is provided by Bhatnagar et al. (2009). One of the earliest related works\r
is by Aleksandrov, Sysoyev, and Shemeneva (1968). Thomas (2014) first realized that\r
the factor of t, as specified in the boxed algorithms of this chapter, was needed in the\r
case of discounted episodic problems.\r
13.1 Example 13.1 and the results with it in this chapter were developed with Eric\r
Graves.\r
13.2 The policy gradient theorem here and on page 334 was first obtained by Marbach\r
and Tsitsiklis (1998, 2001) and then independently by Sutton et al. (2000). A\r
similar expression was obtained by Cao and Chen (1997). Other early results\r
are due to Konda and Tsitsiklis (2000, 2003), Baxter and Bartlett (2001), and\r
Baxter, Bartlett, and Weaver (2001). Some additional results are developed by\r
Sutton, Singh, and McAllester (2000).\r
13.3 REINFORCE is due to Williams (1987, 1992). Phansalkar and Thathachar\r
(1995) proved both local and global convergence theorems for modified versions\r
of REINFORCE algorithms.\r
The all-actions algorithm was first presented in an unpublished but widely\r
circulated incomplete paper (Sutton, Singh, and McAllester, 2000) and then\r
developed further by Ciosek and Whiteson (2017, 2018), who termed it “expected\r
policy gradients,” and by Asadi, Allen, Roderick, Mohamed, Konidaris, and\r
Littman (2017), who called it “mean actor critic.”\r
13.4 The baseline was introduced in Williams’s (1987, 1992) original work. Greensmith,\r
Bartlett, and Baxter (2004) analyzed an arguably better baseline (see Dick, 2015).\r
Thomas and Brunskill (2017) argue that an action-dependent baseline can be\r
used without incurring bias.\r
13.5–6 Actor–critic methods were among the earliest to be investigated in reinforcement\r
learning (Witten, 1977; Barto, Sutton, and Anderson, 1983; Sutton, 1984). The\r
algorithms presented here are based on the work of Degris, White, and Sutton\r
(2012). Actor–critic methods are sometimes referred to as advantage actor–critic\r
(“A2C”) methods in the literature.\r
13.7 The first to show how continuous actions could be handled this way appears\r
to have been Williams (1987, 1992). The figure on page 335 is adapted from\r
Wikipedia.

Part III: Looking Deeper\r
In this last part of the book we look beyond the standard reinforcement learning ideas\r
presented in the first two parts of the book to briefly survey their relationships with\r
psychology and neuroscience, a sampling of reinforcement learning applications, and some\r
of the active frontiers for future reinforcement learning research.

Chapter 14\r
Psychology\r
In previous chapters we developed ideas for algorithms based on computational con\u0002siderations alone. In this chapter we look at some of these algorithms from another\r
perspective: the perspective of psychology and its study of how animals learn. The goals\r
of this chapter are, first, to discuss ways that reinforcement learning ideas and algorithms\r
correspond to what psychologists have discovered about animal learning, and second, to\r
explain the influence reinforcement learning is having on the study of animal learning.\r
The clear formalism provided by reinforcement learning that systemizes tasks, returns,\r
and algorithms is proving to be enormously useful in making sense of experimental data,\r
in suggesting new kinds of experiments, and in pointing to factors that may be critical to\r
manipulate and to measure. The idea of optimizing return over the long term that is\r
at the core of reinforcement learning is contributing to our understanding of otherwise\r
puzzling features of animal learning and behavior.\r
Some of the correspondences between reinforcement learning and psychological theories\r
are not surprising because the development of reinforcement learning drew inspiration\r
from psychological learning theories. However, as developed in this book, reinforcement\r
learning explores idealized situations from the perspective of an artificial intelligence\r
researcher or engineer, with the goal of solving computational problems with ecient\r
algorithms, rather than to replicate or explain in detail how animals learn. As a result,\r
some of the correspondences we describe connect ideas that arose independently in their\r
respective fields. We believe these points of contact are specially meaningful because they\r
expose computational principles important to learning, whether it is learning by artificial\r
or by natural systems.\r
For the most part, we describe correspondences between reinforcement learning and\r
learning theories developed to explain how animals like rats, pigeons, and rabbits learn\r
in controlled laboratory experiments. Thousands of these experiments were conducted\r
throughout the 20th century, and many are still being conducted today. Although\r
sometimes dismissed as irrelevant to wider issues in psychology, these experiments probe\r
subtle properties of animal learning, often motivated by precise theoretical questions.\r
As psychology shifted its focus to more cognitive aspects of behavior, that is, to mental

342 Chapter 14: Psychology\r
processes such as thought and reasoning, animal learning experiments came to play\r
less of a role in psychology than they once did. But this experimentation led to the\r
discovery of learning principles that are elemental and widespread throughout the animal\r
kingdom, principles that should not be neglected in designing artificial learning systems.\r
In addition, as we shall see, some aspects of cognitive processing connect naturally to the\r
computational perspective provided by reinforcement learning.\r
This chapter’s final section includes references relevant to the connections we discuss as\r
well as to connections we neglect. We hope this chapter encourages readers to probe all\r
of these connections more deeply. Also included in this final section is a discussion of how\r
the terminology used in reinforcement learning relates to that of psychology. Many of\r
the terms and phrases used in reinforcement learning are borrowed from animal learning\r
theories, but the computational/engineering meanings of these terms and phrases do not\r
always coincide with their meanings in psychology.\r
14.1 Prediction and Control\r
The algorithms we describe in this book fall into two broad categories: algorithms\r
for prediction and algorithms for control.\r
1 These categories arise naturally in solution\r
methods for the reinforcement learning problem presented in Chapter 3. In many ways\r
these categories respectively correspond to categories of learning extensively studied\r
by psychologists: classical, or Pavlovian, conditioning and instrumental, or operant,\r
conditioning. These correspondences are not completely accidental because of psychology’s\r
influence on reinforcement learning, but they are nevertheless striking because they connect\r
ideas arising from di↵erent objectives.\r
The prediction algorithms presented in this book estimate quantities that depend\r
on how features of an agent’s environment are expected to unfold over the future. We\r
specifically focus on estimating the amount of reward an agent can expect to receive over\r
the future while it interacts with its environment. In this role, prediction algorithms are\r
policy evaluation algorithms, which are integral components of algorithms for improving\r
policies. But prediction algorithms are not limited to predicting future reward; they can\r
predict any feature of the environment (see, for example, Modayil, White, and Sutton,\r
2014). The correspondence between prediction algorithms and classical conditioning rests\r
on their common property of predicting upcoming stimuli, whether or not those stimuli\r
are rewarding (or punishing).\r
The situation in an instrumental, or operant, conditioning experiment is di↵erent.\r
Here, the experimental apparatus is set up so that an animal is given something it likes\r
(a reward) or something it dislikes (a penalty) depending on what the animal did. The\r
animal learns to increase its tendency to produce rewarded behavior and to decrease its\r
tendency to produce penalized behavior. The reinforcing stimulus is said to be contingent\r
on the animal’s behavior, whereas in classical conditioning it is not (although it is dicult\r
to remove all behavior contingencies in a classical conditioning experiment). Instrumental\r
1What control means for us is di↵erent from what it typically means in animal learning theories; there\r
the environment controls the agent instead of the other way around. See our comments on terminology\r
at the end of this chapter.

14.2. Classical Conditioning 343\r
conditioning experiments are like those that inspired Thorndike’s Law of E↵ect that\r
we briefly discuss in Chapter 1. Control is at the core of this form of learning, which\r
corresponds to the operation of reinforcement learning’s policy-improvement algorithms.\r
Thinking of classical conditioning in terms of prediction, and instrumental conditioning\r
in terms of control, is a starting point for connecting our computational view of rein\u0002forcement learning to animal learning, but in reality, the situation is more complicated\r
than this. There is more to classical conditioning than prediction; it also involves action,\r
and so is a mode of control, sometimes called Pavlovian control. Further, classical and\r
instrumental conditioning interact in interesting ways, with both sorts of learning likely\r
being engaged in most experimental situations. Despite these complications, aligning the\r
classical/instrumental distinction with the prediction/control distinction is a convenient\r
first approximation in connecting reinforcement learning to animal learning.\r
In psychology, the term reinforcement is used to describe learning in both classical and\r
instrumental conditioning. Originally referring only to the strengthening of a pattern of\r
behavior, it is frequently also used for the weakening of a pattern of behavior. A stimulus\r
considered to be the cause of the change in behavior is called a reinforcer, whether or\r
not it is contingent on the animal’s previous behavior. At the end of this chapter we\r
discuss this terminology in more detail and how it relates to terminology used in machine\r
learning.\r
14.2 Classical Conditioning\r
While studying the activity of the digestive system, the celebrated Russian physiologist\r
Ivan Pavlov found that an animal’s innate responses to certain triggering stimuli can\r
come to be triggered by other stimuli that are quite unrelated to the inborn triggers. His\r
experimental subjects were dogs that had undergone minor surgery to allow the intensity\r
of their salivary reflex to be accurately measured. In one case he describes, the dog did\r
not salivate under most circumstances, but about 5 seconds after being presented with\r
food it produced about six drops of saliva over the next several seconds. After several\r
repetitions of presenting another stimulus, one not related to food, in this case the sound\r
of a metronome, shortly before the introduction of food, the dog salivated in response to\r
the sound of the metronome in the same way it did to the food. “The activity of the\r
salivary gland has thus been called into play by impulses of sound—a stimulus quite\r
alien to food” (Pavlov, 1927, p. 22). Summarizing the significance of this finding, Pavlov\r
wrote:\r
It is pretty evident that under natural conditions the normal animal must\r
respond not only to stimuli which themselves bring immediate benefit or\r
harm, but also to other physical or chemical agencies—waves of sound, light,\r
and the like—which in themselves only signal the approach of these stimuli;\r
though it is not the sight and sound of the beast of prey which is in itself\r
harmful to the smaller animal, but its teeth and claws. (Pavlov, 1927, p. 14)\r
Connecting new stimuli to innate reflexes in this way is now called classical, or Pavlovian,\r
conditioning. Pavlov (or more exactly, his translators) called inborn responses (e.g.,

344 Chapter 14: Psychology\r
salivation in his demonstration described above) “unconditioned responses” (URs), their\r
natural triggering stimuli (e.g., food) “unconditioned stimuli” (USs), and new responses\r
triggered by predictive stimuli (e.g., here also salivation) “conditioned responses” (CRs).\r
A stimulus that is initially neutral, meaning that it does not normally elicit strong\r
responses (e.g., the metronome sound), becomes a “conditioned stimulus” (CS) as the\r
animal learns that it predicts the US and so comes to produce a CR in response to the CS.\r
These terms are still used in describing classical conditioning experiments (though better\r
translations would have been “conditional” and “unconditional” instead of conditioned\r
and unconditioned). The US is called a reinforcer because it reinforces producing a CR\r
in response to the CS.\r
t\r
Trace Conditioning\r
Delay Conditioning\r
CS\r
US\r
CS\r
US\r
ISI\r
The arrangement of stimuli in two\r
common types of classical condition\u0002ing experiments is shown to the right.\r
In delay conditioning, the CS extends\r
throughout the interstimulus interval, or\r
ISI, which is the time interval between\r
the CS onset and the US onset (with\r
the CS ending when the US ends in a\r
common version shown here). In trace\r
conditioning, the US begins after the CS\r
ends, and the time interval between CS\r
o↵set and US onset is called the trace\r
interval.\r
The salivation of Pavlov’s dogs to the\r
sound of a metronome is just one exam\u0002ple of classical conditioning, which has\r
been intensively studied across many response systems of many species of animals. URs\r
are often preparatory in some way, like the salivation of Pavlov’s dog, or protective in\r
some way, like an eye blink in response to something irritating to the eye, or freezing\r
in response to seeing a predator. Experiencing the CS-US predictive relationship over\r
a series of trials causes the animal to learn that the CS predicts the US so that the\r
animal can respond to the CS with a CR that prepares the animal for, or protects it\r
from, the predicted US. Some CRs are similar to the UR but begin earlier and di↵er in\r
ways that increase their e↵ectiveness. In one intensively studied type of experiment, for\r
example, a tone CS reliably predicts a pu↵ of air (the US) to a rabbit’s eye, triggering a\r
UR consisting of the closure of a protective inner eyelid called the nictitating membrane.\r
After one or more trials, the tone comes to trigger a CR consisting of membrane closure\r
that begins before the air pu↵ and eventually becomes timed so that peak closure occurs\r
just when the air pu↵ is likely to occur. This CR, being initiated in anticipation of the\r
air pu↵ and appropriately timed, o↵ers better protection than simply initiating closure\r
as a reaction to the irritating US. The ability to act in anticipation of important events\r
by learning about predictive relationships among stimuli is so beneficial that it is widely\r
present across the animal kingdom.

14.2. Classical Conditioning 345\r
14.2.1 Blocking and Higher-order Conditioning\r
Many interesting properties of classical conditioning have been observed in experiments.\r
Beyond the anticipatory nature of CRs, two widely observed properties figured prominently\r
in the development of classical conditioning models: blocking and higher-order conditioning.\r
Blocking occurs when an animal fails to learn a CR when a potential CS is presented along\r
with another CS that had been used previously to condition the animal to produce that\r
CR. For example, in the first stage of a blocking experiment involving rabbit nictitating\r
membrane conditioning, a rabbit is first conditioned with a tone CS and an air pu↵ US\r
to produce the CR of closing its nictitating membrane in anticipation of the air pu↵. The\r
experiment’s second stage consists of additional trials in which a second stimulus, say\r
a light, is added to the tone to form a compound tone/light CS followed by the same\r
air pu↵ US. In the experiment’s third phase, the second stimulus alone—the light—is\r
presented to the rabbit to see if the rabbit has learned to respond to it with a CR. It\r
turns out that the rabbit produces very few, or no, CRs in response to the light: learning\r
to the light had been blocked by the previous learning to the tone.2 Blocking results like\r
this challenged the idea that conditioning depends only on simple temporal contiguity,\r
that is, that a necessary and sucient condition for conditioning is that a US frequently\r
follows a CS closely in time. In the next section we describe the Rescorla–Wagner model\r
(Rescorla and Wagner, 1972) that o↵ered an influential explanation for blocking.\r
Higher-order conditioning occurs when a previously-conditioned CS acts as a US\r
in conditioning another initially neutral stimulus. Pavlov described an experiment in\r
which his assistant first conditioned a dog to salivate to the sound of a metronome that\r
predicted a food US, as described above. After this stage of conditioning, a number of\r
trials were conducted in which a black square, to which the dog was initially indi↵erent,\r
was placed in the dog’s line of vision followed by the sound of the metronome—and\r
this was not followed by food. In just ten trials, the dog began to salivate merely upon\r
seeing the black square, despite the fact that the sight of it had never been followed by\r
food. The sound of the metronome itself acted as a US in conditioning a salivation CR\r
to the black square CS. This was second-order conditioning. If the black square had\r
been used as a US to establish salivation CRs to another otherwise neutral CS, it would\r
have been third-order conditioning, and so on. Higher-order conditioning is dicult to\r
demonstrate, especially above the second order, in part because a higher-order reinforcer\r
loses its reinforcing value due to not being repeatedly followed by the original US during\r
higher-order conditioning trials. But under the right conditions, such as intermixing\r
first-order trials with higher-order trials or by providing a general energizing stimulus,\r
higher-order conditioning beyond the second order can be demonstrated. As we describe\r
below, the TD model of classical conditioning uses the bootstrapping idea that is central\r
to our approach to extend the Rescorla–Wagner model’s account of blocking to include\r
both the anticipatory nature of CRs and higher-order conditioning.\r
2Comparison with a control group is necessary to show that the previous conditioning to the tone is\r
responsible for blocking learning to the light. This is done by trials with the tone/light CS but with no\r
prior conditioning to the tone. Learning to the light in this case is unimpaired. Moore and Schmajuk\r
(2008) give a full account of this procedure.

346 Chapter 14: Psychology\r
Higher-order instrumental conditioning occurs as well. In this case, a stimulus that\r
consistently predicts primary reinforcement becomes a reinforcer itself, where reinforce\u0002ment is primary if its rewarding or penalizing quality has been built into the animal by\r
evolution. The predicting stimulus becomes a secondary reinforcer, or more generally, a\r
higher-order or conditioned reinforcer—the latter being a better term when the predicted\r
reinforcing stimulus is itself a secondary, or an even higher-order, reinforcer. A condi\u0002tioned reinforcer delivers conditioned reinforcement: conditioned reward or conditioned\r
penalty. Conditioned reinforcement acts like primary reinforcement in increasing an\r
animal’s tendency to produce behavior that leads to conditioned reward, and to decrease\r
an animal’s tendency to produce behavior that leads to conditioned penalty. (See our\r
comments at the end of this chapter that explain how our terminology sometimes di↵ers,\r
as it does here, from terminology used in psychology.)\r
Conditioned reinforcement is a key phenomenon that explains, for instance, why we\r
work for the conditioned reinforcer money, whose worth derives solely from what is\r
predicted by having it. In actor–critic methods described in Section 13.5 (and discussed\r
in the context of neuroscience in Sections 15.7 and 15.8), the critic uses a TD method\r
to evaluate the actor’s policy, and its value estimates provide conditioned reinforcement\r
to the actor, allowing the actor to improve its policy. This analog of higher-order\r
instrumental conditioning helps address the credit-assignment problem mentioned in\r
Section 1.7 because the critic gives moment-by-moment reinforcement to the actor when\r
the primary reward signal is delayed. We discuss this more below in Section 14.4.\r
14.2.2 The Rescorla–Wagner Model\r
Rescorla and Wagner created their model mainly to account for blocking. The core\r
idea of the Rescorla–Wagner model is that an animal only learns when events violate\r
its expectations, in other words, only when the animal is surprised (although without\r
necessarily implying any conscious expectation or emotion). We first present Rescorla and\r
Wagner’s model using their terminology and notation before shifting to the terminology\r
and notation we use to describe the TD model.\r
Here is how Rescorla and Wagner described their model. The model adjusts the\r
“associative strength” of each component stimulus of a compound CS, which is a number\r
representing how strongly or reliably that component is predictive of a US. When\r
a compound CS consisting of several component stimuli is presented in a classical\r
conditioning trial, the associative strength of each component stimulus changes in a way\r
that depends on an associative strength associated with the entire stimulus compound,\r
called the “aggregate associative strength,” and not just on the associative strength of\r
each component itself.\r
Rescorla and Wagner considered a compound CS AX, consisting of component stimuli\r
A and X, where the animal may have already experienced stimulus A, and stimulus X\r
might be new to the animal. Let VA, VX, and VAX respectively denote the associative\r
strengths of stimuli A, X, and the compound AX. Suppose that on a trial the compound\r
CS AX is followed by a US, which we label stimulus Y. Then the associative strengths of

14.2. Classical Conditioning 347\r
the stimulus components change according to these expressions:\r
VA = ↵AY(RY  VAX)\r
VX = ↵XY(RY  VAX),\r
where ↵AY and ↵XY are the step-size parameters, which depend on the identities of\r
the CS components and the US, and RY is the asymptotic level of associative strength\r
that the US Y can support. (Rescorla and Wagner used  here instead of R, but we\r
use R to avoid confusion with our use of  and because we usually think of this as the\r
magnitude of a reward signal, with the caveat that the US in classical conditioning is not\r
necessarily rewarding or penalizing.) A key assumption of the model is that the aggregate\r
associative strength VAX is equal to VA + VX. The associative strengths as changed by\r
these s become the associative strengths at the beginning of the next trial.\r
To be complete, the model needs a response-generation mechanism, which is a way\r
of mapping values of V s to CRs. Because this mapping would depend on details of\r
the experimental situation, Rescorla and Wagner did not specify a mapping but simply\r
assumed that larger V s would produce stronger or more likely CRs, and that negative\r
V s would mean that there would be no CRs.\r
The Rescorla–Wagner model accounts for the acquisition of CRs in a way that explains\r
blocking. As long as the aggregate associative strength, VAX, of the stimulus compound\r
is below the asymptotic level of associative strength, RY, that the US Y can support, the\r
prediction error RY VAX is positive. This means that over successive trials the associative\r
strengths VA and VX of the component stimuli increase until the aggregate associative\r
strength VAX equals RY, at which point the associative strengths stop changing (unless the\r
US changes). When a new component is added to a compound CS to which the animal has\r
already been conditioned, further conditioning with the augmented compound produces\r
little or no increase in the associative strength of the added CS component because the\r
error has already been reduced to zero, or to a low value. The occurrence of the US is\r
already predicted nearly perfectly, so little or no error—or surprise—is introduced by the\r
new CS component. Prior learning blocks learning to the new component.\r
To transition from Rescorla and Wagner’s model to the TD model of classical condi\u0002tioning (which we just call the TD model), we first recast their model in terms of the\r
concepts that we are using throughout this book. Specifically, we match the notation\r
we use for learning with linear function approximation (Section 9.4), and we think of\r
the conditioning process as one of learning to predict the “magnitude of the US” on a\r
trial on the basis of the compound CS presented on that trial, where the magnitude of\r
a US Y is the RY of the Rescorla–Wagner model as given above. We also introduce\r
states. Because the Rescorla–Wagner model is a trial-level model, meaning that it deals\r
with how associative strengths change from trial to trial without considering any details\r
about what happens within and between trials, we do not have to consider how states\r
change during a trial until we present the full TD model in the following section. Instead,\r
here we simply think of a state as a way of labeling a trial in terms of the collection of\r
component CSs that are present on the trial.\r
Therefore, assume that trial-type, or state, s is described by a real-valued vector of\r
features x(s)=(x1(s), x2(s),...,xd(s))> where xi(s) = 1 if CSi, the i\r
th component of a

348 Chapter 14: Psychology\r
compound CS, is present on the trial and 0 otherwise. Then if the d-dimensional vector\r
of associative strengths is w, the aggregate associative strength for trial-type s is\r
vˆ(s,w) = w>x(s). (14.1)\r
This corresponds to a value estimate in reinforcement learning, and we think of it as the\r
US prediction.\r
Now temporally let t denote the number of a complete trial and not its usual meaning\r
as a time step (we revert to t’s usual meaning when we extend this to the TD model\r
below), and assume that St is the state corresponding to trial t. Conditioning trial t\r
updates the associative strength vector wt to wt+1 as follows:\r
wt+1 = wt + ↵tx(St), (14.2)\r
where ↵ is the step-size parameter, and—because here we are describing the Rescorla–\r
Wagner model—t is the prediction error\r
t = Rt  vˆ(St,wt). (14.3)\r
Rt is the target of the prediction on trial t, that is, the magnitude of the US, or in Rescorla\r
and Wagner’s terms, the associative strength that the US on the trial can support. Note\r
that because of the factor x(St) in (14.2), only the associative strengths of CS components\r
present on a trial are adjusted as a result of that trial. You can think of the prediction\r
error as a measure of surprise, and the aggregate associative strength as the animal’s\r
expectation that is violated when it does not match the target US magnitude.\r
From the perspective of machine learning, the Rescorla–Wagner model is an error\u0002correction supervised learning rule. It is essentially the same as the Least Mean Square\r
(LMS), or Widrow-Ho↵, learning rule (Widrow and Ho↵, 1960) that finds the weights—\r
here the associative strengths—that make the average of the squares of all the errors as\r
close to zero as possible. It is a “curve-fitting,” or regression, algorithm that is widely\r
used in engineering and scientific applications (see Section 9.4).3\r
The Rescorla–Wagner model was very influential in the history of animal learning\r
theory because it showed that a “mechanistic” theory could account for the main facts\r
about blocking without resorting to more complex cognitive theories involving, for\r
example, an animal’s explicit recognition that another stimulus component had been\r
added and then scanning its short-term memory backward to reassess the predictive\r
relationships involving the US. The Rescorla–Wagner model showed how traditional\r
contiguity theories of conditioning—that temporal contiguity of stimuli was a necessary\r
and sucient condition for learning—could be adjusted in a simple way to account for\r
blocking (Moore and Schmajuk, 2008).\r
The Rescorla–Wagner model provides a simple account of blocking and some other\r
features of classical conditioning, but it is not a complete or perfect model of classical\r
3The only di↵erences between the LMS rule and the Rescorla–Wagner model are that for LMS the\r
input vectors xt can have any real numbers as components, and—at least in the simplest version of the\r
LMS rule—the step-size parameter ↵ does not depend on the input vector or the identity of the stimulus\r
setting the prediction target.

14.2. Classical Conditioning 349\r
conditioning. Di↵erent ideas account for a variety of other observed e↵ects, and progress\r
is still being made toward understanding the many subtleties of classical conditioning.\r
The TD model, which we describe next, though also not a complete or perfect model\r
model of classical conditioning, extends the Rescorla–Wagner model to address how\r
within-trial and between-trial timing relationships among stimuli can influence learning\r
and how higher-order conditioning might arise.\r
14.2.3 The TD Model\r
The TD model is a real-time model, as opposed to a trial-level model like the Rescorla–\r
Wagner model. A single step t in the Rescorla–Wagner model represents an entire\r
conditioning trial. The model does not apply to details about what happens during the\r
time a trial is taking place, or what might happen between trials. Within each trial an\r
animal might experience various stimuli whose onsets occur at particular times and that\r
have particular durations. These timing relationships strongly influence learning. The\r
Rescorla–Wagner model also does not include a mechanism for higher-order conditioning,\r
whereas for the TD model, higher-order conditioning is a natural consequence of the\r
bootstrapping idea that is at the base of TD algorithms.\r
To describe the TD model we begin with the formulation of the Rescorla–Wagner\r
model above, but t now labels time steps within or between trials instead of complete\r
trials. Think of the time between t and t + 1 as a small time interval, say .01 second, and\r
think of a trial as a sequences of states, one associated with each time step, where the\r
state at step t now represents details of how stimuli are represented at t instead of just\r
a label for the CS components present on a trial. In fact, we can completely abandon\r
the idea of trials. From the point of view of the animal, a trial is just a fragment of its\r
continuing experience interacting with its world. Following our usual view of an agent\r
interacting with its environment, imagine that the animal is experiencing an endless\r
sequence of states s, each represented by a feature vector x(s). That said, it is still often\r
convenient to refer to trials as fragments of time during which patterns of stimuli repeat\r
in an experiment.\r
State features are not restricted to describing the external stimuli that an animal\r
experiences; they can describe neural activity patterns that external stimuli produce in\r
an animal’s brain, and these patterns can be history-dependent, meaning that they can\r
be persistent patterns produced by sequences of external stimuli. Of course, we do not\r
know exactly what these neural activity patterns are, but a real-time model like the TD\r
model allows one to explore the consequences on learning of di↵erent hypotheses about\r
the internal representations of external stimuli. For these reasons, the TD model does\r
not commit to any particular state representation. In addition, because the TD model\r
includes discounting and eligibility traces that span time intervals between stimuli, the\r
model also makes it possible to explore how discounting and eligibility traces interact with\r
stimulus representations in making predictions about the results of classical conditioning\r
experiments.\r
Below we describe some of the state representations that have been used with the\r
TD model and some of their implications, but for the moment we stay agnostic about

350 Chapter 14: Psychology\r
the representation and just assume that each state s is represented by a feature vector\r
x(s)=(x1(s), x2(s),...,xn(s))>. Then the aggregate associative strength corresponding\r
to a state s is given by (14.1), the same as for the Rescorla-Wagner model, but the TD\r
model updates the associative strength vector, w, di↵erently. With t now labeling a time\r
step instead of a complete trial, the TD model governs learning according to this update:\r
wt+1 = wt + ↵t zt, (14.4)\r
which replaces xt(St) in the Rescorla–Wagner update (14.2) with zt, a vector of eligibility\r
traces, and instead of the t of (14.3), here t is a TD error:\r
t = Rt+1 + vˆ(St+1,wt)  vˆ(St,wt), (14.5)\r
where  is a discount factor (between 0 and 1), Rt is the prediction target at time t, and\r
vˆ(St+1,wt) and vˆ(St,wt) are aggregate associative strengths at t + 1 and t as defined by\r
(14.1).\r
Each component i of the eligibility-trace vector zt increments or decrements according\r
to the component xi(St) of the feature vector x(St), and otherwise decays with a rate\r
determined by :\r
zt = zt1 + x(St). (14.6)\r
Here  is the usual eligibility trace decay parameter.\r
Note that if  = 0, the TD model reduces to the Rescorla–Wagner model with the\r
exceptions that: the meaning of t is di↵erent in each case (a trial number for the\r
Rescorla–Wagner model and a time step for the TD model), and in the TD model there\r
is a one-time-step lead in the prediction target R. The TD model is equivalent to the\r
backward view of the semi-gradient TD() algorithm with linear function approximation\r
(Chapter 12), except that Rt in the model does not have to be a reward signal as it does\r
when the TD algorithm is used to learn a value function for policy-improvement.\r
14.2.4 TD Model Simulations\r
Real-time conditioning models like the TD model are interesting primarily because they\r
make predictions for a wide range of situations that cannot be represented by trial-level\r
models. These situations involve the timing and durations of conditionable stimuli, the\r
timing of these stimuli in relation to the timing of the US, and the timing and shapes\r
of CRs. For example, the US generally must begin after the onset of a neutral stimulus\r
for conditioning to occur, with the rate and e↵ectiveness of learning depending on the\r
inter-stimulus interval, or ISI, the interval between the onsets of the CS and the US. When\r
CRs appear, they generally begin before the appearance of the US and their temporal\r
profiles change during learning. In conditioning with compound CSs, the component\r
stimuli of the compound CSs may not all begin and end at the same time, sometimes\r
forming what is called a serial compound in which the component stimuli occur in a\r
sequence over time. Timing considerations like these make it important to consider how

14.2. Classical Conditioning 351\r
Presence Complete Serial \r
Compound\r
Stimulus \r
Representation\r
Microstimuli\r
CS\r
US\r
Figure 14.1: Three stimulus representations (in columns) sometimes used with the TD model.\r
Each row represents one element of the stimulus representation. The three representations vary\r
along a temporal generalization gradient, with no generalization between nearby time points in\r
the complete serial compound (left column) and complete generalization between nearby time\r
points in the presence representation (right column). The microstimulus representation occupies\r
a middle ground. The degree of temporal generalization determines the temporal granularity\r
with which US predictions are learned. Adapted with minor changes from Learning & Behavior,\r
Evaluating the TD Model of Classical Conditioning, volume 40, 2012, p. 311, E. A. Ludvig, R. S.\r
Sutton, E. J. Kehoe. With permission of Springer.\r
stimuli are represented, how these representations unfold over time during and between\r
trials, and how they interact with discounting and eligibility traces.\r
Figure 14.1 shows three of the stimulus representations that have been used in exploring\r
the behavior of the TD model: the complete serial compound (CSC), the microstimulus\r
(MS), and the presence representations (Ludvig, Sutton, and Kehoe, 2012). These\r
representations di↵er in the degree to which they force generalization among nearby time\r
points during which a stimulus is present.\r
The simplest of the representations shown in Figure 14.1 is the presence representation\r
in the figure’s right column. This representation has a single feature for each component\r
CS present on a trial, where the feature has value 1 whenever that component is present,\r
and 0 otherwise.4 The presence representation is not a realistic hypothesis about how\r
stimuli are represented in an animal’s brain, but as we describe below, the TD model\r
with this representation can produce many of the timing phenomena seen in classical\r
conditioning.\r
4In our formalism, there is a di↵erent state, St, for each time step t during a trial, and for a trial\r
in which a compound CS consists of n component CSs of various durations occurring at various times\r
throughout the trial, there is a feature, xi, for each component CSi, i = 1,...,n, where xi(St) = 1 for\r
all times t when the CSi is present, and equals zero otherwise.

352 Chapter 14: Psychology\r
For the CSC representation (left column of Figure 14.1), the onset of each external\r
stimulus initiates a sequence of precisely-timed short-duration internal signals that\r
continues until the external stimulus ends.5 This is like assuming the animal’s nervous\r
system has a clock that keeps precise track of time during stimulus presentations; it is\r
what engineers call a “tapped delay line.” Like the presence representation, the CSC\r
representation is unrealistic as a hypothesis about how the brain internally represents\r
stimuli, but Ludvig et al. (2012) call it a “useful fiction” because it can reveal details of\r
how the TD model works when relatively unconstrained by the stimulus representation.\r
The CSC representation is also used in most TD models of dopamine-producing neurons\r
in the brain, a topic we take up in Chapter 15. The CSC representation is often viewed\r
as an essential part of the TD model, although this view is mistaken.\r
The MS representation (center column of Figure 14.1) is like the CSC representation\r
in that each external stimulus initiates a cascade of internal stimuli, but in this case the\r
internal stimuli—the microstimuli—are not of such limited and non-overlapping form;\r
they are extended over time and overlap. As time elapses from stimulus onset, di↵erent\r
sets of microstimuli become more or less active, and each subsequent microstimulus\r
becomes progressively wider in time and reaches a lower maximal level. Of course, there\r
are many MS representations depending on the nature of the microstimuli, and a number\r
of examples of MS representations have been studied in the literature, in some cases along\r
with proposals for how an animal’s brain might generate them (see the Bibliographic and\r
Historical Comments at the end of this chapter). MS representations are more realistic\r
than the presence or CSC representations as hypotheses about neural representations of\r
stimuli, and they allow the behavior of the TD model to be related to a broader collection\r
of phenomena observed in animal experiments. In particular, by assuming that cascades\r
of microstimuli are initiated by USs as well as by CSs, and by studying the significant\r
e↵ects on learning of interactions between microstimuli, eligibility traces, and discounting,\r
the TD model is helping to frame hypotheses to account for many of the subtle phenomena\r
of classical conditioning and how an animal’s brain might produce them. We say more\r
about this below, particularly in Chapter 15 where we discuss reinforcement learning and\r
neuroscience.\r
Even with the simple presence representation, however, the TD model produces all the\r
basic properties of classical conditioning that are accounted for by the Rescorla–Wagner\r
model, plus features of conditioning that are beyond the scope of trial-level models. For\r
example, as we have already mentioned, a conspicuous feature of classical conditioning is\r
that the US generally must begin after the onset of a neutral stimulus for conditioning\r
to occur, and that after conditioning, the CR begins before the appearance of the US.\r
In other words, conditioning generally requires a positive ISI, and the CR generally\r
anticipates the US. How the strength of conditioning (e.g., the percentage of CRs elicited\r
by a CS) depends on the ISI varies substantially across species and response systems, but\r
it typically has the following properties: it is negligible for a zero or negative ISI, i.e., when\r
5In our formalism, for each CS component CSi present on a trial, and for each time step t during a\r
trial, there is a separate feature xt\r
i , where xti (St0 ) = 1 if t = t0 for any t0 at which CSi is present, and\r
equals 0 otherwise. This is di↵erent from the CSC representation in Sutton and Barto (1990) in which\r
there are the same distinct features for each time step but no reference to external stimuli; hence the\r
name complete serial compound.

14.2. Classical Conditioning 353\r
the US onset occurs simultaneously with, or earlier than, the CS onset (although research\r
has found that associative strengths sometimes increase slightly or become negative with\r
negative ISIs); it increases to a maximum at a positive ISI where conditioning is most\r
e↵ective; and it then decreases to zero after an interval that varies widely with response\r
systems. The precise shape of this dependency for the TD model depends on the values\r
of its parameters and details of the stimulus representation, but these basic features of\r
ISI-dependency are core properties of the TD model.\r
Facilitation of remote associations in the TD model\r
One of the theoretical issues arising\r
with serial-compound conditioning, that\r
is, conditioning with a compound CS\r
whose components occur in a sequence,\r
concerns the facilitation of remote asso\u0002ciations. It has been found that if the\r
empty trace interval between a first CS\r
(CSA) and the US is filled with a second\r
CS (CSB) to form a serial-compound\r
stimulus, then conditioning to CSA is\r
facilitated. Shown to the right is the\r
behavior of the TD model with the pres\u0002ence representation in a simulation of\r
such an experiment whose timing details\r
are shown above. Consistent with the\r
experimental results (Kehoe, 1982), the\r
model shows facilitation of both the rate\r
of conditioning and the asymptotic level\r
of conditioning of the first CS due to the\r
presence of the second CS.\r
wCSB\r
The Egger-Miller e↵ect in the TD model\r
A well-known demonstration of the\r
e↵ects on conditioning of temporal re\u0002lationships among stimuli within a trial\r
is an experiment by Egger and Miller\r
(1962) that involved two overlapping\r
CSs in a delay configuration as shown\r
to the right (top). Although CSB was\r
in a better temporal relationship with\r
the US, the presence of CSA substan\u0002tially reduced conditioning to CSB as\r
compared to controls in which CSA was\r
absent. Directly to the right is shown\r
the same result being generated by the\r
TD model in a simulation of this exper\u0002iment with the presence representation.\r
The TD model accounts for blocking\r
because it is an error-correcting learning

354 Chapter 14: Psychology\r
rule like the Rescorla–Wagner model. Beyond accounting for basic blocking results,\r
however, the TD model predicts (with the presence representation and more complex\r
representations as well) that blocking is reversed if the blocked stimulus is moved earlier\r
wCSB\r
Figure 14.2: Temporal primacy overriding\r
blocking in the TD model.\r
in time (like CSA in the diagram to the\r
right) so that its onset occurs before the\r
onset of the blocking stimulus. This feature\r
of the TD model’s behavior deserves atten\u0002tion because it had not been observed at\r
the time of the model’s introduction. Recall\r
that in blocking, if an animal has already\r
learned that one CS predicts a US, then\r
learning that a newly-added second CS also\r
predicts the US is much reduced, i.e., is\r
blocked. But if the newly-added second CS\r
begins earlier than the pretrained CS, then—\r
according to the TD model—learning to the\r
newly-added CS is not blocked. In fact, as\r
training continues and the newly-added CS\r
gains associative strength, the pretrained\r
CS loses associative strength. The behavior\r
of the TD model under these conditions\r
is shown in the lower part of Figure 14.2.\r
This simulation experiment di↵ered from the Egger-Miller experiment (bottom of the\r
preceding page) in that the shorter CS with the later onset was given prior training\r
until it was fully associated with the US. This surprising prediction led Kehoe, Schreurs,\r
and Graham (1987) to conduct the experiment using the well-studied rabbit nictitating\r
membrane preparation. Their results confirmed the model’s prediction, and they noted\r
that non-TD models have considerable diculty explaining their data.\r
With the TD model, an earlier predictive stimulus takes precedence over a later\r
predictive stimulus because, like all the prediction methods described in this book, the\r
TD model is based on the backing-up or bootstrapping idea: updates to associative\r
strengths shift the strengths at a particular state toward the strength at later states.\r
Another consequence of bootstrapping is that the TD model provides an account of higher\u0002order conditioning, a feature of classical conditioning that is beyond the scope of the\r
Rescorla-Wagner and similar models. As we described above, higher-order conditioning\r
is the phenomenon in which a previously-conditioned CS can act as a US in conditioning\r
another initially neutral stimulus. Figure 14.3 shows the behavior of the TD model (again\r
with the presence representation) in a higher-order conditioning experiment—in this case\r
it is second-order conditioning. In the first phase (not shown in the figure), CSB is trained\r
to predict a US so that its associative strength increases, here to 1.65. In the second\r
phase, CSA is paired with CSB in the absence of the US, in the sequential arrangement\r
shown at the top of the figure. CSA acquires associative strength even though it is never\r
paired with the US. With continued training, CSA’s associative strength reaches a peak\r
and then decreases because the associative strength of CSB, the secondary reinforcer,\r
decreases so that it loses its ability to provide secondary reinforcement. CSB’s associative

14.2. Classical Conditioning 355\r
ww\r
Figure 14.3: Second-order conditioning with\r
the TD model.\r
strength decreases because the US does not\r
occur in these higher-order conditioning tri\u0002als. These are extinction trials for CSB\r
because its predictive relationship to the\r
US is disrupted so that its ability to act as\r
a reinforcer decreases. This same pattern\r
is seen in animal experiments. This extinc\u0002tion of conditioned reinforcement in higher\u0002order conditioning trials makes it dicult\r
to demonstrate higher-order conditioning\r
unless the original predictive relationships\r
are periodically refreshed by occasionally\r
inserting first-order trials.\r
The TD model produces an analog of\r
second- and higher-order conditioning be\u0002cause vˆ(St+1,wt)  vˆ(St,wt) appears in\r
the TD error t (14.5). Due to the first\r
phase of learning, vˆ(St+1,wt) may di↵er\r
from vˆ(St,wt), making t non-zero (a temporal di↵erence). This di↵erence has the same\r
status as Rt+1 in (14.5), implying that as far as learning is concerned there is no di↵erence\r
between a temporal di↵erence and the occurrence of a US. In fact, this feature of the\r
TD algorithm is one of the major reasons for its development, which we now understand\r
through its connection to dynamic programming as described in Chapter 6. Bootstrapping\r
values is intimately related to second-order, and higher-order, conditioning.\r
In the examples of the TD model’s behavior described above, we examined only the\r
changes in the associative strengths of the CS components; we did not look at what\r
the model predicts about properties of an animal’s conditioned responses (CRs): their\r
timing, shape, and how they develop over conditioning trials. These properties depend\r
on the species, the response system being observed, and parameters of the conditioning\r
trials, but in many experiments with di↵erent animals and di↵erent response systems, the\r
magnitude of the CR, or the probability of a CR, increases as the expected time of the\r
US approaches. For example, in classical conditioning of a rabbit’s nictitating membrane\r
response that we mentioned above, over conditioning trials the delay from CS onset to\r
when the nictitating membrane begins to move across the eye decreases over trials, and\r
the amplitude of this anticipatory closure gradually increases over the interval between\r
the CS and the US until the membrane reaches maximal closure at the expected time of\r
the US. The timing and shape of this CR is critical to its adaptive significance—covering\r
the eye too early reduces vision (even though the nictitating membrane is translucent),\r
while covering it too late is of little protective value. Capturing CR features like these is\r
challenging for models of classical conditioning.\r
The TD model does not include as part of its definition any mechanism for translat\u0002ing the time course of the US prediction, vˆ(St,wt), into a profile that can be compared

356 Chapter 14: Psychology\r
with the properties of an animal’s CR. The simplest choice is to let the time course of\r
a simulated CR equal the time course of the US prediction. In this case, features of\r
simulated CRs and how they change over trials depend only on the stimulus representation\r
chosen and the values of the model’s parameters ↵, , and .\r
Figure 14.4 shows the time courses of US predictions at di↵erent points during learning\r
with the three representations shown in Figure 14.1. For these simulations the US\r
occurred 25 time steps after the onset of the CS, and ↵ = .05,  = .95 and  = .97.\r
With the CSC representation (Figure 14.4 left), the curve of the US prediction formed\r
by the TD model increases exponentially throughout the interval between the CS and\r
the US until it reaches a maximum exactly when the US occurs (at time step 25). This\r
exponential increase is the result of discounting in the TD model learning rule. With the\r
presence representation (Figure 14.4 middle), the US prediction is nearly constant while\r
the stimulus is present because there is only one weight, or associative strength, to be\r
learned for each stimulus. Consequently, the TD model with the presence representation\r
cannot recreate many features of CR timing. With an MS representation (Figure 14.4\r
right), the development of the TD model’s US prediction is more complicated. After 200\r
trials the prediction’s profile is a reasonable approximation of the US prediction curve\r
produced with the CSC representation.\r
ˆv\r
ˆv\r
ˆv\r
Figure 14.4: Time course of US prediction over the course of acquisition for the TD model\r
with three di↵erent stimulus representations. Left: With the complete serial compound (CSC),\r
the US prediction increases exponentially through the interval, peaking at the time of the US.\r
At asymptote (trial 200), the US prediction peaks at the US intensity (1 in these simulations).\r
Middle: With the presence representation, the US prediction converges to an almost constant\r
level. This constant level is determined by the US intensity and the length of the CS–US interval.\r
Right: With the microstimulus representation, at asymptote, the TD model approximates the\r
exponentially increasing time course depicted with the CSC through a linear combination of the\r
di↵erent microstimuli. Adapted with minor changes from Learning & Behavior, Evaluating the\r
TD Model of Classical Conditioning, volume 40, 2012, E. A. Ludvig, R. S. Sutton, E. J. Kehoe.\r
With permission of Springer.\r
The US prediction curves shown in Figure 14.4 were not intended to precisely match\r
profiles of CRs as they develop during conditioning in any particular animal experiment,\r
but they illustrate the strong influence that the stimulus representation has on predictions\r
derived from the TD model. Further, although we can only mention it here, how the

14.3. Instrumental Conditioning 357\r
stimulus representation interacts with discounting and eligibility traces is important\r
in determining properties of the US prediction profiles produced by the TD model.\r
Another dimension beyond what we can discuss here is the influence of di↵erent response\u0002generation mechanisms that translate US predictions into CR profiles; the profiles shown\r
in Figure 14.4 are “raw” US prediction profiles. Even without any special assumption\r
about how an animal’s brain might produce overt responses from US predictions, however,\r
the profiles in Figure 14.4 for the CSC and MS representations increase as the time of the\r
US approaches and reach a maximum at the time of the US, as is seen in many animal\r
conditioning experiments.\r
The TD model, when combined with particular stimulus representations and response\u0002generation mechanisms, is able to account for a surprisingly wide range of phenomena\r
observed in animal classical conditioning experiments, but it is far from being a perfect\r
model. To generate other details of classical conditioning the model needs to be extended,\r
perhaps by adding model-based elements and mechanisms for adaptively altering some of\r
its parameters. Other approaches to modeling classical conditioning depart significantly\r
from the Rescorla–Wagner-style error-correction process. Bayesian models, for example,\r
work within a probabilistic framework in which experience revises probability estimates.\r
All of these models usefully contribute to our understanding of classical conditioning.\r
Perhaps the most notable feature of the TD model is that it is based on a theory—the\r
theory we have described in this book—that suggests an account of what an animal’s\r
nervous system is trying to do while undergoing conditioning: it is trying to form accurate\r
long-term predictions, consistent with the limitations imposed by the way stimuli are\r
represented and how the nervous system works. In other words, it suggests a normative\r
account of classical conditioning in which long-term, instead of immediate, prediction is a\r
key feature.\r
The development of the TD model of classical conditioning is one instance in which the\r
explicit goal was to model some of the details of animal learning behavior. In addition to\r
its standing as an algorithm, then, TD learning is also the basis of this model of aspects\r
of biological learning. As we discuss in Chapter 15, TD learning has also turned out\r
to underlie an influential model of the activity of neurons that produce dopamine, a\r
chemical in the brain of mammals that is deeply involved in reward processing. These\r
are instances in which reinforcement learning theory makes detailed contact with animal\r
behavioral and neural data.\r
We now turn to considering correspondences between reinforcement learning and animal\r
behavior in instrumental conditioning experiments, the other major type of laboratory\r
experiment studied by animal learning psychologists.\r
14.3 Instrumental Conditioning\r
In instrumental conditioning experiments learning depends on the consequences of be\u0002havior: the delivery of a reinforcing stimulus is contingent on what the animal does.\r
In classical conditioning experiments, in contrast, the reinforcing stimulus—the US—is\r
delivered independently of the animal’s behavior. Instrumental conditioning is usually\r
considered to be the same as operant conditioning, the term B. F. Skinner (1938, 1963)

358 Chapter 14: Psychology\r
introduced for experiments with behavior-contingent reinforcement, though the experi\u0002ments and theories of those who use these two terms di↵er in a number of ways, some of\r
which we touch on below. We will exclusively use the term instrumental conditioning for\r
experiments in which reinforcement is contingent upon behavior. The roots of instrumen\u0002tal conditioning go back to experiments performed by the American psychologist Edward\r
Thorndike one hundred years before publication of the first edition of this book.\r
One of Thorndike’s puzzle boxes.\r
Reprinted from Thorndike, Animal Intelligence: An\r
Experimental Study of the Associative Processes in\r
Animals, The Psychological Review, Series of Mono\u0002graph Supplements II(4), Macmillan, New York, 1898.\r
Thorndike observed the behavior of cats\r
when they were placed in “puzzle boxes,”\r
such as the one at the right, from which\r
they could escape by appropriate actions.\r
For example, a cat could open the door\r
of one box by performing a sequence of\r
three separate actions: depressing a plat\u0002form at the back of the box, pulling a string\r
by clawing at it, and pushing a bar up or\r
down. When first placed in a puzzle box,\r
with food visible outside, all but a few of\r
Thorndike’s cats displayed “evident signs\r
of discomfort” and extraordinarily vigorous\r
activity “to strive instinctively to escape\r
from confinement” (Thorndike, 1898).\r
In experiments with di↵erent cats and\r
boxes with di↵erent escape mechanisms, Thorndike recorded the amounts of time each\r
cat took to escape over multiple experiences in each box. He observed that the time\r
almost invariably decreased with successive experiences, for example, from 300 seconds\r
to 6 or 7 seconds. He described cats’ behavior in a puzzle box like this:\r
The cat that is clawing all over the box in her impulsive struggle will probably\r
claw the string or loop or button so as to open the door. And gradually all the\r
other non-successful impulses will be stamped out and the particular impulse\r
leading to the successful act will be stamped in by the resulting pleasure,\r
until, after many trials, the cat will, when put in the box, immediately claw\r
the button or loop in a definite way. (Thorndike 1898, p. 13)\r
These and other experiments (some with dogs, chicks, monkeys, and even fish) led\r
Thorndike to formulate a number of “laws” of learning, the most influential being the\r
Law of E↵ect. This law describes what is generally known as learning by trial and\r
error. As we mentioned in Chapter 1, many aspects of the Law of E↵ect have generated\r
controversy, and its details have been modified over the years. Still the law—in one form\r
or another—expresses an enduring principle of learning.\r
Essential features of reinforcement learning algorithms correspond to features of animal\r
learning described by the Law of E↵ect. First, reinforcement learning algorithms are\r
selectional, meaning that they try alternatives and select among them by comparing their\r
consequences. Second, reinforcement learning algorithms are associative, meaning that\r
the alternatives found by selection are associated with particular situations, or states,\r
to form the agent’s policy. Like learning described by the Law of E↵ect, reinforcement

14.3. Instrumental Conditioning 359\r
learning is not just the process of finding actions that produce a lot of reward, but also\r
of connecting these actions to situations or states. Thorndike used the phrase learning\r
by “selecting and connecting” (Hilgard, 1956). Natural selection in evolution is a prime\r
example of a selectional process, but it is not associative (at least as it is commonly\r
understood); supervised learning is associative, but it is not selectional because it relies\r
on instructions that directly tell the agent how to change its behavior.\r
In computational terms, the Law of E↵ect describes an elementary way of combining\r
search and memory: search in the form of trying and selecting among many actions\r
in each situation, and memory in the form of associations linking situations with the\r
actions found—so far—to work best in those situations. Search and memory are essential\r
components of all reinforcement learning algorithms, whether memory takes the form of\r
an agent’s policy, value function, or environment model.\r
A reinforcement learning algorithm’s need to search means that it has to explore in\r
some way. Animals clearly explore as well, and early animal learning researchers disagreed\r
about the degree of guidance an animal uses in selecting its actions in situations like\r
Thorndike’s puzzle boxes. Are actions the result of “absolutely random, blind groping”\r
(Woodworth, 1938, p. 777), or is there some degree of guidance, either from prior learning,\r
reasoning, or other means? Although some thinkers, including Thorndike, seem to have\r
taken the former position, others favored more deliberate exploration. Reinforcement\r
learning algorithms allow wide latitude for how much guidance an agent can employ in\r
selecting actions. The forms of exploration we have used in the algorithms presented\r
in this book, such as "-greedy and upper-confidence-bound action selection, are merely\r
among the simplest. More sophisticated methods are possible, with the only stipulation\r
being that there has to be some form of exploration for the algorithms to work e↵ectively.\r
The feature of our treatment of reinforcement learning allowing the set of actions\r
available at any time to depend on the environment’s current state echoes something\r
Thorndike observed in his cats’ puzzle-box behaviors. The cats selected actions from\r
those that they instinctively perform in their current situation, which Thorndike called\r
their “instinctual impulses.” First placed in a puzzle box, a cat instinctively scratches,\r
claws, and bites with great energy: a cat’s instinctual responses to finding itself in a\r
confined space. Successful actions are selected from these and not from every possible\r
action or activity. This is like the feature of our formalism where the action selected\r
from a state s belongs to a set of admissible actions, A(s). Specifying these sets is an\r
important aspect of reinforcement learning because it can radically simplify learning.\r
They are like an animal’s instinctual impulses. On the other hand, Thorndike’s cats might\r
have been exploring according to an instinctual context-specific ordering over actions\r
rather than by just selecting from a set of instinctual impulses. This is another way to\r
make reinforcement learning easier.\r
Among the most prominent animal learning researchers influenced by the Law of E↵ect\r
were Clark Hull (e.g., Hull, 1943) and B. F. Skinner (e.g., Skinner, 1938). At the center\r
of their research was the idea of selecting behavior on the basis of its consequences.\r
Reinforcement learning has features in common with Hull’s theory, which included\r
eligibility-like mechanisms and secondary reinforcement to account for the ability to learn\r
when there is a significant time interval between an action and the consequent reinforcing

360 Chapter 14: Psychology\r
stimulus (see Section 14.4). Randomness also played a role in Hull’s theory through what\r
he called “behavioral oscillation” to introduce exploratory behavior.\r
Skinner did not fully subscribe to the memory aspect of the Law of E↵ect. Being averse\r
to the idea of associative linkages, he instead emphasized selection from spontaneously\u0002emitted behavior. He introduced the term “operant” to emphasize the key role of an\r
action’s e↵ects on an animal’s environment. Unlike the experiments of Thorndike and\r
others, which consisted of sequences of separate trials, Skinner’s operant conditioning\r
experiments allowed animal subjects to behave for extended periods of time without\r
interruption. He invented the operant conditioning chamber, now called a “Skinner box,”\r
the most basic version of which contains a lever or key that an animal can press to obtain\r
a reward, such as food or water, which would be delivered according to a well-defined rule,\r
called a reinforcement schedule. By recording the cumulative number of lever presses\r
as a function of time, Skinner and his followers could investigate the e↵ect of di↵erent\r
reinforcement schedules on the animal’s rate of lever-pressing. Modeling results from\r
experiments likes these using the reinforcement learning principles we present in this\r
book is not well developed, but we mention some exceptions in the Bibliographic and\r
Historical Remarks section at the end of this chapter.\r
Another of Skinner’s contributions resulted from his recognition of the e↵ectiveness of\r
training an animal by reinforcing successive approximations of the desired behavior, a\r
process he called shaping. Although this technique had been used by others, including\r
Skinner himself, its significance was impressed upon him when he and colleagues were\r
attempting to train a pigeon to bowl by swiping a wooden ball with its beak. After\r
waiting for a long time without seeing any swipe that they could reinforce, they\r
... decided to reinforce any response that had the slightest resemblance to\r
a swipe—perhaps, at first, merely the behavior of looking at the ball—and\r
then to select responses which more closely approximated the final form. The\r
result amazed us. In a few minutes, the ball was caroming o↵ the walls of\r
the box as if the pigeon had been a champion squash player. (Skinner, 1958,\r
p. 94)\r
Not only did the pigeon learn a behavior that is unusual for pigeons, it learned quickly\r
through an interactive process in which its behavior and the reinforcement contingencies\r
changed in response to each other. Skinner compared the process of altering reinforcement\r
contingencies to the work of a sculptor shaping clay into a desired form. Shaping is a\r
powerful technique for computational reinforcement learning systems as well. When it is\r
dicult for an agent to receive any non-zero reward signal at all, either due to sparseness\r
of rewarding situations or their inaccessibility given initial behavior, starting with an\r
easier problem and incrementally increasing its diculty as the agent learns can be an\r
e↵ective, and sometimes indispensable, strategy.\r
A concept from psychology that is especially relevant in the context of instrumental\r
conditioning is motivation, which refers to processes that influence the direction and\r
strength, or vigor, of behavior. Thorndike’s cats, for example, were motivated to escape\r
from puzzle boxes because they wanted the food that was sitting just outside. Obtaining\r
this goal was rewarding to them and reinforced the actions allowing them to escape. It\r
is dicult to link the concept of motivation, which has many dimensions, in a precise

14.4. Delayed Reinforcement 361\r
way to reinforcement learning’s computational perspective, but there are clear links with\r
some of its dimensions.\r
In one sense, a reinforcement learning agent’s reward signal is at the base of its\r
motivation: the agent is motivated to maximize the total reward it receives over the long\r
run. A key facet of motivation, then, is what makes an agent’s experience rewarding. In\r
reinforcement learning, reward signals depend on the state of the reinforcement learning\r
agent’s environment and the agent’s actions. Further, as pointed out in Chapter 1, the\r
state of the agent’s environment not only includes information about what is external to\r
the machine, like an organism or a robot, that houses the agent, but also what is internal\r
to this machine. Some internal state components correspond to what psychologists call\r
an animal’s motivational state, which influences what is rewarding to the animal. For\r
example, an animal will be more rewarded by eating when it is hungry than when it has\r
just finished a satisfying meal. The concept of state dependence is broad enough to allow\r
for many types of modulating influences on the generation of reward signals.\r
Value functions provide a further link to psychologists’ concept of motivation. If the\r
most basic motive for selecting an action is to obtain as much reward as possible, for a\r
reinforcement learning agent that selects actions using a value function, a more proximal\r
motive is to ascend the gradient of its value function, that is, to select actions expected\r
to lead to the most highly-valued next states (or what is essentially the same thing, to\r
select actions with the greatest action-values). For these agents, value functions are the\r
main driving force determining the direction of their behavior.\r
Another dimension of motivation is that an animal’s motivational state not only\r
influences learning, but also influences the strength, or vigor, of the animal’s behavior\r
after learning. For example, after learning to find food in the goal box of a maze, a hungry\r
rat will run faster to the goal box than one that is not hungry. This aspect of motivation\r
does not link so cleanly to the reinforcement learning framework we present here, but\r
in the Bibliographical and Historical Remarks section at the end of this chapter we cite\r
several publications that propose theories of behavioral vigor based on reinforcement\r
learning.\r
We turn now to the subject of learning when reinforcing stimuli occur well after the\r
events they reinforce. The mechanisms used by reinforcement learning algorithms to\r
enable learning with delayed reinforcement—eligibility traces and TD learning—closely\r
correspond to psychologists’ hypotheses about how animals can learn under these condi\u0002tions.\r
14.4 Delayed Reinforcement\r
The Law of E↵ect requires a backward e↵ect on connections, and some early critics of the\r
law could not conceive of how the present could a↵ect something that was in the past. This\r
concern was amplified by the fact that learning can even occur when there is a considerable\r
delay between an action and the consequent reward or penalty. Similarly, in classical\r
conditioning, learning can occur when US onset follows CS o↵set by a non-negligible time\r
interval. We call this the problem of delayed reinforcement, which is related to what\r
Minsky (1961) called the “credit-assignment problem for learning systems”: how do you

362 Chapter 14: Psychology\r
distribute credit for success among the many decisions that may have been involved in\r
producing it? The reinforcement learning algorithms presented in this book include two\r
basic mechanisms for addressing this problem. The first is the use of eligibility traces,\r
and the second is the use of TD methods to learn value functions that provide nearly\r
immediate evaluations of actions (in tasks like instrumental conditioning experiments) or\r
that provide immediate prediction targets (in tasks like classical conditioning experiments).\r
Both of these methods correspond to similar mechanisms proposed in theories of animal\r
learning.\r
Pavlov (1927) pointed out that every stimulus must leave a trace in the nervous system\r
that persists for some time after the stimulus ends, and he proposed that stimulus traces\r
make learning possible when there is a temporal gap between the CS o↵set and the\r
US onset. To this day, conditioning under these conditions is called trace conditioning\r
(page 344). Assuming a trace of the CS remains when the US arrives, learning occurs\r
through the simultaneous presence of the trace and the US. We discuss some proposals\r
for trace mechanisms in the nervous system in Chapter 15.\r
Stimulus traces were also proposed as a means for bridging the time interval between\r
actions and consequent rewards or penalties in instrumental conditioning. In Hull’s\r
influential learning theory, for example, “molar stimulus traces” accounted for what\r
he called an animal’s goal gradient, a description of how the maximum strength of an\r
instrumentally-conditioned response decreases with increasing delay of reinforcement\r
(Hull, 1932, 1943). Hull hypothesized that an animal’s actions leave internal stimuli whose\r
traces decay exponentially as functions of time since an action was taken. Looking at the\r
animal learning data available at the time, he hypothesized that the traces e↵ectively\r
reach zero after 30 to 40 seconds.\r
The eligibility traces used in the algorithms described in this book are like Hull’s\r
traces: they are decaying traces of past state visitations, or of past state–action pairs.\r
Eligibility traces were introduced by Klopf (1972) in his neuronal theory in which they\r
are temporally-extended traces of past activity at synapses, the connections between\r
neurons. Klopf’s traces are more complex than the exponentially-decaying traces our\r
algorithms use, and we discuss this more when we take up his theory in Section 15.9.\r
To account for goal gradients that extend over longer time periods than spanned\r
by stimulus traces, Hull (1943) proposed that longer gradients result from conditioned\r
reinforcement passing backwards from the goal, a process acting in conjunction with\r
his molar stimulus traces. Animal experiments showed that if conditions favor the\r
development of conditioned reinforcement during a delay period, learning does not\r
decrease with increased delay as much as it does under conditions that obstruct secondary\r
reinforcement. Conditioned reinforcement is favored if there are stimuli that regularly\r
occur during the delay interval. Then it is as if reward is not actually delayed because\r
there is more immediate conditioned reinforcement. Hull therefore envisioned that there\r
is a primary gradient based on the delay of the primary reinforcement mediated by\r
stimulus traces, and that this is progressively modified, and lengthened, by conditioned\r
reinforcement.\r
Algorithms presented in this book that use both eligibility traces and value functions\r
to enable learning with delayed reinforcement correspond to Hull’s hypothesis about how\r
animals are able to learn under these conditions. The actor–critic architecture discussed

14.5. Cognitive Maps 363\r
in Sections 13.5, 15.7, and 15.8 illustrates this correspondence most clearly. The critic uses\r
a TD algorithm to learn a value function associated with the system’s current behavior,\r
that is, to predict the current policy’s return. The actor updates the current policy based\r
on the critic’s predictions, or more exactly, on changes in the critic’s predictions. The\r
TD error produced by the critic acts as a conditioned reinforcement signal for the actor,\r
providing an immediate evaluation of performance even when the primary reward signal\r
itself is considerably delayed. Algorithms that estimate action-value functions, such as\r
Q-learning and Sarsa, similarly use TD learning principles to enable learning with delayed\r
reinforcement by means of conditioned reinforcement. The close parallel between TD\r
learning and the activity of dopamine producing neurons that we discuss in Chapter 15\r
lends additional support to links between reinforcement learning algorithms and this\r
aspect of Hull’s learning theory.\r
14.5 Cognitive Maps\r
Model-based reinforcement learning algorithms use environment models that have elements\r
in common with what psychologists call cognitive maps. Recall from our discussion of\r
planning and learning in Chapter 8 that by an environment model we mean anything\r
an agent can use to predict how its environment will respond to its actions in terms of\r
state transitions and rewards, and by planning we mean any process that computes a\r
policy from such a model. Environment models consist of two parts: the state-transition\r
part encodes knowledge about the e↵ect of actions on state changes, and the reward\u0002model part encodes knowledge about the reward signals expected for each state or each\r
state–action pair. A model-based algorithm selects actions by using a model to predict\r
the consequences of possible courses of action in terms of future states and the reward\r
signals expected to arise from those states. The simplest kind of planning is to compare\r
the predicted consequences of collections of “imagined” sequences of decisions.\r
Questions about whether or not animals use environment models, and if so, what are the\r
models like and how are they learned, have played influential roles in the history of animal\r
learning research. Some researchers challenged the then-prevailing stimulus-response\r
(S–R) view of learning and behavior, which corresponds to the simplest model-free way\r
of learning policies, by demonstrating latent learning. In the earliest latent learning\r
experiment, two groups of rats were run in a maze. For the experimental group, there\r
was no reward during the first stage of the experiment, but food was suddenly introduced\r
into the goal box of the maze at the start of the second stage. For the control group, food\r
was in the goal box throughout both stages. The question was whether or not rats in the\r
experimental group would have learned anything during the first stage in the absence\r
of food reward. Although the experimental rats did not appear to learn much during\r
the first, unrewarded, stage, as soon as they discovered the food that was introduced\r
in the second stage, they rapidly caught up with the rats in the control group. It was\r
concluded that “during the non-reward period, the rats [in the experimental group] were\r
developing a latent learning of the maze which they were able to utilize as soon as reward\r
was introduced” (Blodgett, 1929).

364 Chapter 14: Psychology\r
Latent learning is most closely associated with the psychologist Edward Tolman, who\r
interpreted this result, and others like it, as showing that animals could learn a “cognitive\r
map of the environment” in the absence of rewards or penalties, and that they could use\r
the map later when they were motivated to reach a goal (Tolman, 1948). A cognitive map\r
could also allow a rat to plan a route to the goal that was di↵erent from the route the rat\r
had used in its initial exploration. Explanations of results like these led to the enduring\r
controversy lying at the heart of the behaviorist/cognitive dichotomy in psychology. In\r
modern terms, cognitive maps are not restricted to models of spatial layouts but are\r
more generally environment models, or models of an animal’s “task space” (e.g., Wilson,\r
Takahashi, Schoenbaum, and Niv, 2014). The cognitive map explanation of latent learning\r
experiments is analogous to the claim that animals use model-based algorithms, and that\r
environment models can be learned even without explicit rewards or penalties. Models\r
are then used for planning when the animal is motivated by the appearance of rewards or\r
penalties.\r
Tolman’s account of how animals learn cognitive maps was that they learn stimulus\u0002stimulus, or S–S, associations by experiencing successions of stimuli as they explore an\r
environment. In psychology this is called expectancy theory: given S–S associations, the\r
occurrence of a stimulus generates an expectation about the stimulus to come next. This\r
is much like what control engineers call system identification, in which a model of a\r
system with unknown dynamics is learned from labeled training examples. In the simplest\r
discrete-time versions, training examples are S–S0 pairs, where S is a state and S0, the\r
subsequent state, is the label. When S is observed, the model creates the “expectation”\r
that S0 will be observed next. Models more useful for planning involve actions as well,\r
so that examples look like SA–S0, where S0 is expected when action A is executed in\r
state S. It is also useful to learn how the environment generates rewards. In this case,\r
examples are of the form S–R or SA–R, where R is a reward signal associated with S or\r
the SA pair. These are all forms of supervised learning by which an agent can acquire\r
cognitive-like maps whether or not it receives any non-zero reward signals while exploring\r
its environment.\r
14.6 Habitual and Goal-directed Behavior\r
The distinction between model-free and model-based reinforcement learning algorithms\r
corresponds to the distinction psychologists make between habitual and goal-directed\r
control of learned behavioral patterns. Habits are behavior patterns triggered by appro\u0002priate stimuli and then performed more-or-less automatically. Goal-directed behavior,\r
according to how psychologists use the phrase, is purposeful in the sense that it is con\u0002trolled by knowledge of the value of goals and the relationship between actions and their\r
consequences. Habits are sometimes said to be controlled by antecedent stimuli, whereas\r
goal-directed behavior is said to be controlled by its consequences (Dickinson, 1980,\r
1985). Goal-directed control has the advantage that it can rapidly change an animal’s\r
behavior when the environment changes its way of reacting to the animal’s actions. While\r
habitual behavior responds quickly to input from an accustomed environment, it is unable

14.6. Habitual and Goal-directed Behavior 365\r
to quickly adjust to changes in the environment. The development of goal-directed\r
behavioral control was likely a major advance in the evolution of animal intelligence.\r
Figure 14.5 illustrates the di↵erence between model-free and model-based decision\r
strategies in a hypothetical task in which a rat has to navigate a maze that has distinctive\r
goal boxes, each delivering an associated reward of the magnitude shown (Figure 14.5\r
top). Starting at S1, the rat has to first select left (L) or right (R) and then has to select\r
L or R again at S2 or S3 to reach one of the goal boxes. The goal boxes are the terminal\r
states of each episode of the rat’s episodic task. A model-free strategy (Figure 14.5 lower\r
left) relies on stored values for state–action pairs. These action values are estimates of\r
the highest return the rat can expect for each action taken from each (nonterminal) state.\r
They are obtained over many trials of running the maze from start to finish. When the\r
action values have become good enough estimates of the optimal returns, the rat just has\r
S1, L\r
S2, L\r
S2\r
, R\r
S3\r
, L\r
S3\r
, R\r
S1\r
, R\r
4\r
3\r
0\r
4\r
3\r
2\r
Model-Free\r
= 4\r
= 0\r
= 2\r
= 3\r
Reward\r
Model-Based\r
S1\r
L\r
R\r
S2\r
S3\r
L\r
R\r
L\r
R\r
(4)\r
S2 S3\r
S1\r
0 4 2 3\r
Figure 14.5: Model-based and model-free strategies to solve a hypothetical sequential action\u0002selection problem. Top: a rat navigates a maze with distinctive goal boxes, each associated\r
with a reward having the value shown. Lower left: a model-free strategy relies on stored action\r
values for all the state–action pairs obtained over many learning trials. To make decisions the\r
rat just has to select at each state the action with the largest action value for that state. Lower\r
right: in a model-based strategy, the rat learns an environment model, consisting of knowledge\r
of state–action-next-state transitions and a reward model consisting of knowledge of the reward\r
associated with each distinctive goal box. The rat can decide which way to turn at each state\r
by using the model to simulate sequences of action choices to find a path yielding the highest\r
return. Adapted from Trends in Cognitive Science, volume 10, number 8, Y. Niv, D. Joel, and P.\r
Dayan, A Normative Perspective on Motivation, p. 376, 2006, with permission from Elsevier.

366 Chapter 14: Psychology\r
to select at each state the action with the largest action value in order to make optimal\r
decisions. In this case, when the action-value estimates become accurate enough, the\r
rat selects L from S1 and R from S2 to obtain the maximum return of 4. A di↵erent\r
model-free strategy might simply rely on a cached policy instead of action values, making\r
direct links from S1 to L and from S2 to R. In neither of these strategies do decisions\r
rely on an environment model. There is no need to consult a state-transition model, and\r
no connection is required between the features of the goal boxes and the rewards they\r
deliver.\r
Figure 14.5 (lower right) illustrates a model-based strategy. It uses an environment\r
model consisting of a state-transition model and a reward model. The state-transition\r
model is shown as a decision tree, and the reward model associates the distinctive features\r
of the goal boxes with the rewards to be found in each. (The rewards associated with\r
states S1, S2, and S3 are also part of the reward model, but here they are zero and are\r
not shown.) A model-based agent can decide which way to turn at each state by using\r
the model to simulate sequences of action choices to find a path yielding the highest\r
return. In this case the return is the reward obtained from the outcome at the end of\r
the path. Here, with a suciently accurate model, the rat would select L and then R\r
to obtain a return of 4. Comparing the predicted returns of simulated paths is a simple\r
form of planning, which can be done in a variety of ways as discussed in Chapter 8.\r
When the environment of a model-free agent changes the way it reacts to the agent’s\r
actions, the agent has to acquire new experience in the changed environment during\r
which it can update its policy and/or value function. In the model-free strategy shown\r
in Figure 14.5 (lower left), for example, if one of the goal boxes were to somehow shift\r
to delivering a di↵erent reward, the rat would have to traverse the maze, possibly many\r
times, to experience the new reward upon reaching that goal box, all the while updating\r
either its policy or its action-value function (or both) based on this experience. The key\r
point is that for a model-free agent to change the action its policy specifies for a state, or\r
to change an action value associated with a state, it has to move to that state, act from\r
it, possibly many times, and experience the consequences of its actions.\r
A model-based agent can accommodate changes in its environment without this kind\r
of ‘personal experience’ with the states and actions a↵ected by the change. A change in\r
its model automatically (through planning) changes its policy. Planning can determine\r
the consequences of changes in the environment that have never been linked together in\r
the agent’s own experience. For example, again referring to the maze task of Figure 14.5,\r
imagine that a rat with a previously learned transition and reward model is placed directly\r
in the goal box to the right of S2 to find that the reward available there now has value 1\r
instead of 4. The rat’s reward model will change even though the action choices required\r
to find that goal box in the maze were not involved. The planning process will bring\r
knowledge of the new reward to bear on maze running without the need for additional\r
experience in the maze; in this case changing the policy to right turns at both S1 and S3\r
to obtain a return of 3.\r
Exactly this logic is the basis of outcome-devaluation experiments with animals. Results\r
from these experiments provide insight into whether an animal has learned a habit or if\r
its behavior is under goal-directed control. Outcome-devaluation experiments are like\r
latent-learning experiments in that the reward changes from one stage to the next. After

14.6. Habitual and Goal-directed Behavior 367\r
an initial rewarded stage of learning, the reward value of an outcome is changed, including\r
being shifted to zero or even to a negative value.\r
An early important experiment of this type was conducted by Adams and Dickinson\r
(1981). They trained rats via instrumental conditioning until the rats energetically pressed\r
a lever for sucrose pellets in a training chamber. The rats were then placed in the same\r
chamber with the lever retracted and allowed non-contingent food, meaning that pellets\r
were made available to them independently of their actions. After 15-minutes of this\r
free-access to the pellets, rats in one group were injected with the nausea-inducing poison\r
lithium chloride. This was repeated for three sessions, in the last of which none of the\r
injected rats consumed any of the non-contingent pellets, indicating that the reward\r
value of the pellets had been decreased—the pellets had been devalued. In the next stage\r
taking place a day later, the rats were again placed in the chamber and given a session of\r
extinction training, meaning that the response lever was back in place but disconnected\r
from the pellet dispenser so that pressing it did not release pellets. The question was\r
whether the rats that had the reward value of the pellets decreased would lever-press\r
less than rats that did not have the reward value of the pellets decreased, even without\r
experiencing the devalued reward as a result of lever-pressing. It turned out that the\r
injected rats had significantly lower response rates than the non-injected rats right from\r
the start of the extinction trials.\r
Adams and Dickinson concluded that the injected rats associated lever pressing with\r
consequent nausea by means of a cognitive map linking lever pressing to pellets, and\r
pellets to nausea. Hence, in the extinction trials, the rats “knew” that the consequences\r
of pressing the lever would be something they did not want, and so they reduced their\r
lever-pressing right from the start. The important point is that they reduced lever-pressing\r
without ever having experienced lever-pressing directly followed by being sick: no lever\r
was present when they were made sick. They seemed able to combine knowledge of the\r
outcome of a behavioral choice (pressing the lever will be followed by getting a pellet)\r
with the reward value of the outcome (pellets are to be avoided) and hence could alter\r
their behavior accordingly. Not every psychologist agrees with this “cognitive” account\r
of this kind of experiment, and it is not the only possible way to explain these results,\r
but the model-based planning explanation is widely accepted.\r
Nothing prevents an agent from using both model-free and model-based algorithms, and\r
there are good reasons for using both. We know from our own experience that with enough\r
repetition, goal-directed behavior tends to turn into habitual behavior. Experiments show\r
that this happens for rats too. Adams (1982) conducted an experiment to see if extended\r
training would convert goal-directed behavior into habitual behavior. He did this by\r
comparing the e↵ect of outcome devaluation on rats that experienced di↵erent amounts\r
of training. If extended training made the rats less sensitive to devaluation compared to\r
rats that received less training, this would be evidence that extended training made the\r
behavior more habitual. Adams’ experiment closely followed the Adams and Dickinson\r
(1981) experiment just described. Simplifying a bit, rats in one group were trained until\r
they made 100 rewarded lever-presses, and rats in the other group—the overtrained\r
group—were trained until they made 500 rewarded lever-presses. After this training,\r
the reward value of the pellets was decreased (using lithium chloride injections) for rats

368 Chapter 14: Psychology\r
in both groups. Then both groups of rats were given a session of extinction training.\r
Adams’ question was whether devaluation would e↵ect the rate of lever-pressing for the\r
overtrained rats less than it would for the non-overtrained rats, which would be evidence\r
that extended training reduces sensitivity to outcome devaluation. It turned out that\r
devaluation strongly decreased the lever-pressing rate of the non-overtrained rats. For\r
the overtrained rats, in contrast, devaluation had little e↵ect on their lever-pressing; in\r
fact, if anything, it made it more vigorous. (The full experiment included control groups\r
showing that the di↵erent amounts of training did not by themselves significantly e↵ect\r
lever-pressing rates after learning.) This result suggested that while the non-overtrained\r
rats were acting in a goal-directed manner sensitive to their knowledge of the outcome of\r
their actions, the overtrained rats had developed a lever-pressing habit.\r
Viewing this and other results like it from a computational perspective provides insight\r
as to why one might expect animals to behave habitually in some circumstances but in a\r
goal-directed way in others, and why they shift from one mode of control to another as\r
they continue to learn. While animals undoubtedly use algorithms that do not exactly\r
match those we have presented in this book, one can gain insight into animal behavior by\r
considering the tradeo↵s that various reinforcement learning algorithms imply. An idea\r
developed by computational neuroscientists Daw, Niv, and Dayan (2005) is that animals\r
use both model-free and model-based processes. Each process proposes an action, and\r
the action chosen for execution is the one proposed by the process judged to be the more\r
trustworthy of the two as determined by measures of confidence that are maintained\r
throughout learning. Early in learning the planning process of a model-based system is\r
more trustworthy because it chains together short-term predictions which can become\r
accurate with less experience than long-term predictions of the model-free process. But\r
with continued experience, the model-free process becomes more trustworthy because\r
planning is prone to making mistakes due to model inaccuracies and short-cuts necessary\r
to make planning feasible, such as various forms of “tree-pruning”: the removal of\r
unpromising search tree branches. According to this idea one would expect a shift from\r
goal-directed behavior to habitual behavior as more experience accumulates. Other ideas\r
have been proposed for how animals arbitrate between goal-directed and habitual control,\r
and both behavioral and neuroscience research continues to examine this and related\r
questions.\r
The distinction between model-free and model-based algorithms is proving to be useful\r
for this research. One can examine the computational implications of these types of\r
algorithms in abstract settings that expose basic advantages and limitations of each\r
type. This serves both to suggest and to sharpen questions that guide the design\r
of experiments necessary for increasing psychologists’ understanding of habitual and\r
goal-directed behavioral control.\r
14.7 Summary\r
Our goal in this chapter has been to discuss correspondences between reinforcement\r
learning and the experimental study of animal learning in psychology. We emphasized\r
at the outset that reinforcement learning as described in this book is not intended

14.7. Summary 369\r
to model details of animal behavior. It is an abstract computational framework that\r
explores idealized situations from the perspective of artificial intelligence and engineering.\r
But many of the basic reinforcement learning algorithms were inspired by psychological\r
theories, and in some cases, these algorithms have contributed to the development of\r
new animal learning models. This chapter described the most conspicuous of these\r
correspondences.\r
The distinction in reinforcement learning between algorithms for prediction and al\u0002gorithms for control parallels animal learning theory’s distinction between classical, or\r
Pavlovian, conditioning and instrumental conditioning. The key di↵erence between\r
instrumental and classical conditioning experiments is that in the former the reinforcing\r
stimulus is contingent upon the animal’s behavior, whereas in the latter it is not. Learning\r
to predict via a TD algorithm corresponds to classical conditioning, and we described\r
the TD model of classical conditioning as one instance in which reinforcement learning\r
principles account for some details of animal learning behavior. This model generalizes\r
the influential Rescorla–Wagner model by including the temporal dimension where events\r
within individual trials influence learning, and it provides an account of second-order\r
conditioning, where predictors of reinforcing stimuli become reinforcing themselves. It\r
also is the basis of an influential view of the activity of dopamine neurons in the brain,\r
something we take up in Chapter 15.\r
Learning by trial and error is at the base of the control aspect of reinforcement learning.\r
We presented some details about Thorndike’s experiments with cats and other animals that\r
led to his Law of E↵ect, which we discussed here and in Chapter 1 (page 15). We pointed\r
out that in reinforcement learning, exploration does not have to be limited to “blind\r
groping”; trials can be generated by sophisticated methods using innate and previously\r
learned knowledge as long as there is some exploration. We discussed the training method\r
B. F. Skinner called shaping in which reward contingencies are progressively altered to\r
train an animal to successively approximate a desired behavior. Shaping is not only\r
indispensable for animal training, it is also an e↵ective tool for training reinforcement\r
learning agents. There is also a connection to the idea of an animal’s motivational state,\r
which influences what an animal will approach or avoid and what events are rewarding\r
or punishing for the animal.\r
The reinforcement learning algorithms presented in this book include two basic mecha\u0002nisms for addressing the problem of delayed reinforcement: eligibility traces and value\r
functions learned via TD algorithms. Both mechanisms have antecedents in theories of\r
animal learning. Eligibility traces are similar to stimulus traces of early theories, and\r
value functions correspond to the role of secondary reinforcement in providing nearly\r
immediate evaluative feedback.\r
The next correspondence the chapter addressed is that between reinforcement learning’s\r
environment models and what psychologists call cognitive maps. Experiments conducted\r
in the mid 20th century purported to demonstrate the ability of animals to learn cognitive\r
maps as alternatives to, or as additions to, state–action associations, and later use them\r
to guide behavior, especially when the environment changes unexpectedly. Environment\r
models in reinforcement learning are like cognitive maps in that they can be learned by\r
supervised learning methods without relying on reward signals, and then they can be\r
used later to plan behavior.

370 Chapter 14: Psychology\r
Reinforcement learning’s distinction between model-free and model-based algorithms\r
corresponds to the distinction in psychology between habitual and goal-directed behavior.\r
Model-free algorithms make decisions by accessing information that has been stored in a\r
policy or an action-value function, whereas model-based methods select actions as the\r
result of planning ahead using a model of the agent’s environment. Outcome-devaluation\r
experiments provide information about whether an animal’s behavior is habitual or under\r
goal-directed control. Reinforcement learning theory has helped clarify thinking about\r
these issues.\r
Animal learning clearly informs reinforcement learning, but as a type of machine\r
learning, reinforcement learning is directed toward designing and understanding e↵ective\r
learning algorithms, not toward replicating or explaining details of animal behavior.\r
We focused on aspects of animal learning that relate in clear ways to methods for\r
solving prediction and control problems, highlighting the fruitful two-way flow of ideas\r
between reinforcement learning and psychology without venturing deeply into many of the\r
behavioral details and controversies that have occupied the attention of animal learning\r
researchers. Future development of reinforcement learning theory and algorithms will\r
likely exploit links to many other features of animal learning as the computational utility\r
of these features becomes better appreciated. We expect that a flow of ideas between\r
reinforcement learning and psychology will continue to bear fruit for both disciplines.\r
Many connections between reinforcement learning and areas of psychology and other\r
behavioral sciences are beyond the scope of this chapter. We largely omit discussing\r
links to the psychology of decision making, which focuses on how actions are selected,\r
or how decisions are made, after learning has taken place. We also do not discuss links\r
to ecological and evolutionary aspects of behavior studied by ethologists and behavioral\r
ecologists: how animals relate to one another and to their physical surroundings, and how\r
their behavior contributes to evolutionary fitness. Optimization, MDPs, and dynamic\r
programming figure prominently in these fields, and our emphasis on agent interaction\r
with dynamic environments connects to the study of agent behavior in complex “ecologies.”\r
Multi-agent reinforcement learning, omitted in this book, has connections to social aspects\r
of behavior. Despite the lack of treatment here, reinforcement learning should by no means\r
be interpreted as dismissing evolutionary perspectives. Nothing about reinforcement\r
learning implies a tabula rasa view of learning and behavior. Indeed, experience with\r
engineering applications has highlighted the importance of building into reinforcement\r
learning systems knowledge that is analogous to what evolution provides to animals.

14.7. Summary 371\r
Bibliographical and Historical Remarks\r
Ludvig, Bellemare, and Pearson (2011) and Shah (2012) review reinforcement learning in\r
the contexts of psychology and neuroscience. These publications are useful companions\r
to this chapter and the following chapter on reinforcement learning and neuroscience.\r
14.1 Dayan, Niv, Seymour, and Daw (2006) focused on interactions between clas\u0002sical and instrumental conditioning, particularly situations where classically\u0002conditioned and instrumental responses are in conflict. They proposed a Q\u0002learning framework for modeling aspects of this interaction. Modayil and Sutton\r
(2014) used a mobile robot to demonstrate the e↵ectiveness of a control method\r
combining a fixed response with online prediction learning. Calling this Pavlo\u0002vian control, they emphasized that it di↵ers from the usual control methods of\r
reinforcement learning, being based on predictively executing fixed responses\r
and not on reward maximization. The electro-mechanical machine of Ross (1933)\r
and especially the learning version of Walter’s turtle (Walter, 1951) were very\r
early illustrations of Pavlovian control.\r
14.2.1 Kamin (1968) first reported blocking, now commonly known as Kamin blocking,\r
in classical conditioning. Moore and Schmajuk (2008) provide an excellent\r
summary of the blocking phenomenon, the research it stimulated, and its lasting\r
influence on animal learning theory. Gibbs, Cool, Land, Kehoe, and Gormezano\r
(1991) describe second-order conditioning of the rabbit’s nictitating membrane\r
response and its relationship to conditioning with serial-compound stimuli. Finch\r
and Culler (1934) reported obtaining fifth-order conditioning of a dog’s foreleg\r
withdrawal “when the motivation of the animal is maintained through the various\r
orders.”\r
14.2.2 The idea built into the Rescorla–Wagner model that learning occurs when animals\r
are surprised is derived from Kamin (1969). Models of classical conditioning\r
other than Rescorla and Wagner’s include the models of Klopf (1988), Grossberg\r
(1975), Mackintosh (1975), Moore and Stickney (1980), Pearce and Hall (1980),\r
and Courville, Daw, and Touretzky (2006). Schmajuk (2008) reviews models of\r
classical conditioning. Wagner (2008) provides a modern psychological perspective\r
on the Rescorla-Wagner model and similar elemental theories of learning.\r
14.2.3 An early version of the TD model of classical conditioning appeared in Sutton and\r
Barto (1981a), which also included the early model’s prediction that temporal\r
primacy overrides blocking, later shown by Kehoe, Schreurs, and Graham (1987)\r
to occur in the rabbit nictitating membrane preparation. Sutton and Barto\r
(1981a) contains the earliest recognition of the near identity between the Rescorla–\r
Wagner model and the Least-Mean-Square (LMS), or Widrow-Ho↵, learning\r
rule (Widrow and Ho↵, 1960). This early model was revised following Sutton’s\r
development of the TD algorithm (Sutton, 1984, 1988) and was first presented as\r
the TD model in Sutton and Barto (1987) and more completely in Sutton and\r
Barto (1990), upon which this section is largely based. Additional exploration

372 Chapter 14: Psychology\r
of the TD model and its possible neural implementation was conducted by\r
Moore and colleagues (Moore, Desmond, Berthier, Blazis, Sutton, and Barto,\r
1986; Moore and Blazis, 1989; Moore, Choi, and Brunzell, 1998; Moore, Marks,\r
Castagna, and Polewan, 2001). Klopf’s (1988) drive-reinforcement theory of\r
classical conditioning extends the TD model to address additional experimental\r
details, such as the S-shape of acquisition curves. In some of these publications\r
TD is taken to mean Time Derivative instead of Temporal Di↵erence.\r
14.2.4 Ludvig, Sutton, and Kehoe (2012) evaluated the performance of the TD model\r
in previously unexplored tasks involving classical conditioning and examined\r
the influence of various stimulus representations, including the microstimulus\r
representation that they introduced earlier (Ludvig, Sutton, and Kehoe, 2008).\r
Earlier investigations of the influence of various stimulus representations and their\r
possible neural implementations on response timing and topography in the context\r
of the TD model are those of Moore and colleagues cited above. Although not in\r
the context of the TD model, representations like the microstimulus representation\r
of Ludvig et al. (2012) have been proposed and studied by Grossberg and\r
Schmajuk (1989), Brown, Bullock, and Grossberg (1999), Buhusi and Schmajuk\r
(1999), and Machado (1997). The figures on pages 353–355 are adapted from\r
Sutton and Barto (1990).\r
14.3 Section 1.7 includes comments on the history of trial-and-error learning and\r
the Law of E↵ect. The idea that Thorndike’s cats might have been exploring\r
according to an instinctual context-specific ordering over actions rather than by\r
just selecting from a set of instinctual impulses was suggested by Peter Dayan\r
(personal communication). Selfridge, Sutton, and Barto (1985) illustrated the\r
e↵ectiveness of shaping in a pole-balancing reinforcement learning task. Other\r
examples of shaping in reinforcement learning are Gullapalli and Barto (1992),\r
Mahadevan and Connell (1992), Mataric (1994), Dorigo and Colombette (1994),\r
Saksida, Raymond, and Touretzky (1997), and Randløv and Alstrøm (1998). Ng\r
(2003) and Ng, Harada, and Russell (1999) used the term shaping in a sense\r
somewhat di↵erent from Skinner’s, focusing on the problem of how to alter the\r
reward signal without altering the set of optimal policies.\r
Dickinson and Balleine (2002) discuss the complexity of the interaction between\r
learning and motivation. Wise (2004) provides an overview of reinforcement\r
learning and its relation to motivation. Daw and Shohamy (2008) link motivation\r
and learning to aspects of reinforcement learning theory. See also McClure,\r
Daw, and Montague (2003), Niv, Joel, and Dayan (2006), Rangel, Camerer, and\r
Montague (2008), and Dayan and Berridge (2014). McClure et al. (2003), Niv,\r
Daw, and Dayan (2006), and Niv, Daw, Joel, and Dayan (2007) present theories\r
of behavioral vigor related to the reinforcement learning framework.\r
14.4 Spence, Hull’s student and collaborator at Yale, elaborated the role of higher\u0002order reinforcement in addressing the problem of delayed reinforcement (Spence,\r
1947). Learning over very long delays, as in taste-aversion conditioning with

14.7. Summary 373\r
delays up to several hours, led to interference theories as alternatives to decaying\u0002trace theories (e.g., Revusky and Garcia, 1970; Boakes and Costa, 2014). Other\r
views of learning under delayed reinforcement invoke roles for awareness and\r
working memory (e.g., Clark and Squire, 1998; Seo, Barraclough, and Lee, 2007).\r
14.5 Thistlethwaite (1951) provides an extensive review of latent learning experiments\r
up to the time of its publication. Ljung (1998) provides an overview of model\r
learning, or system identification, techniques in engineering. Gopnik, Glymour,\r
Sobel, Schulz, Kushnir, and Danks (2004) present a Bayesian theory about how\r
children learn models.\r
14.6 Connections between habitual and goal-directed behavior and model-free and\r
model-based reinforcement learning were first proposed by Daw, Niv, and Dayan\r
(2005). The hypothetical maze task used to explain habitual and goal-directed\r
behavioral control is based on the explanation of Niv, Joel, and Dayan (2006).\r
Dolan and Dayan (2013) review four generations of experimental research related\r
to this issue and discuss how it can move forward on the basis of reinforcement\r
learning’s model-free/model-based distinction. Dickinson (1980, 1985) and Dick\u0002inson and Balleine (2002) discuss experimental evidence related to this distinction.\r
Donahoe and Burgos (2000) alternatively argue that model-free processes can\r
account for the results of outcome-devaluation experiments. Dayan and Berridge\r
(2014) argue that classical conditioning involves model-based processes. Rangel,\r
Camerer, and Montague (2008) review many of the outstanding issues involving\r
habitual, goal-directed, and Pavlovian modes of control.\r
Comments on Terminology— The traditional meaning of reinforcement in psychol\u0002ogy is the strengthening of a pattern of behavior (by increasing either its intensity or\r
frequency) as a result of an animal receiving a stimulus (or experiencing the omission\r
of a stimulus) in an appropriate temporal relationship with another stimulus or with a\r
response. Reinforcement produces changes that remain in future behavior. Sometimes in\r
psychology reinforcement refers to the process of producing lasting changes in behavior,\r
whether the changes strengthen or weaken a behavior pattern (Mackintosh, 1983). Letting\r
reinforcement refer to weakening in addition to strengthening is at odds with the everyday\r
meaning of reinforce, and its traditional use in psychology, but it is a useful extension\r
that we have adopted here. In either case, a stimulus considered to be the cause of the\r
behavioral change is called a reinforcer.\r
Psychologists do not generally use the specific phrase reinforcement learning as we\r
do. Animal learning pioneers probably regarded reinforcement and learning as being\r
synonymous, so it would be redundant to use both words. Our use of the phrase follows\r
its use in computational and engineering research, influenced mostly by Minsky (1961).\r
But the phrase is lately gaining currency in psychology and neuroscience, likely because\r
strong parallels have surfaced between reinforcement learning algorithms and animal\r
learning—parallels described in this chapter and the next.\r
According to common usage, a reward is an object or event that an animal will\r
approach and work for. A reward may be given to an animal in recognition of its ‘good’

374 Chapter 14: Psychology\r
behavior, or given in order to make the animal’s behavior ‘better.’ Similarly, a penalty is\r
an object or event that the animal usually avoids and that is given as a consequence of\r
‘bad’ behavior, usually in order to change that behavior. Primary reward is reward due\r
to machinery built into an animal’s nervous system by evolution to improve its chances\r
of survival and reproduction, for example, reward produced by the taste of nourishing\r
food, sexual contact, successful escape, and many other stimuli and events that predicted\r
reproductive success over the animal’s ancestral history. As explained in Section 14.2.1,\r
higher-order reward is reward delivered by stimuli that predict primary reward, either\r
directly or indirectly by predicting other stimuli that predict primary reward. Reward is\r
secondary if its rewarding quality is the result of directly predicting primary reward.\r
In this book we call Rt the ‘reward signal at time t,’ or sometimes just the ‘reward\r
at time t,’ but we do not think of it as an object or event in the agent’s environment.\r
Because Rt is a number—not an object or an event—it is more like a reward signal in\r
neuroscience, which is a signal internal to the brain, like the activity of neurons, that\r
influences decision making and learning. This signal might be triggered when the animal\r
perceives an attractive (or an aversive) object, but it can also be triggered by things that\r
do not physically exist in the animal’s external environment, such as memories, ideas, or\r
hallucinations. Because our Rt can be positive, negative, or zero, it might be better to\r
call a negative Rt a penalty, and an Rt equal to zero a neutral signal, but for simplicity\r
we generally avoid these terms.\r
In reinforcement learning, the process that generates all the Rts defines the problem\r
the agent is trying to solve. The agent’s objective is to keep the magnitude of Rt as large\r
as possible over time. In this respect, Rt is like primary reward for an animal if we think\r
of the problem the animal faces as the problem of obtaining as much primary reward as\r
possible over its lifetime (and thereby, through the prospective “wisdom” of evolution,\r
improve its chances of solving its real problem, which is to pass its genes on to future\r
generations). However, as we suggest in Chapter 15, it is unlikely that there is a single\r
“master” reward signal like Rt in an animal’s brain.\r
Not all reinforcers are rewards or penalties. Sometimes reinforcement is not the result\r
of an animal receiving a stimulus that evaluates its behavior by labeling the behavior\r
good or bad. A behavior pattern can be reinforced by a stimulus that arrives to an animal\r
no matter how the animal behaved. As described in Section 14.1, whether the delivery of\r
reinforcement depends, or does not depend, on preceding behavior is the defining di↵erence\r
between instrumental, or operant, conditioning experiments and classical, or Pavlovian,\r
conditioning experiments. Reinforcement is at work in both types of experiments, but\r
only in the former is it feedback that evaluates past behavior. (Though it has often been\r
pointed out that even when the reinforcing US in a classical conditioning experiment is\r
not contingent on the subject’s preceding behavior, its reinforcing value can be influenced\r
by this behavior, an example being that a closed eye makes an air pu↵ to the eye less\r
aversive.)\r
The distinction between reward signals and reinforcement signals is a crucial point\r
when we discuss neural correlates of these signals in the next chapter. Like a reward signal,\r
for us, the reinforcement signal at any specific time is a positive or negative number, or\r
zero. A reinforcement signal is the major factor directing changes a learning algorithm

14.7. Summary 375\r
makes in an agent’s policy, value estimates, or environment models. The definition that\r
makes the most sense to us is that a reinforcement signal at any time is a number that\r
multiplies (possibly along with some constants) a vector to determine parameter updates\r
in some learning algorithm.\r
For some algorithms, the reward signal alone is the critical multiplier in the parameter\u0002update equation. For these algorithms the reinforcement signal is the same as the\r
reward signal. But for most of the algorithms we discuss in this book, reinforcement\r
signals include terms in addition to the reward signal, an example being a TD error\r
t = Rt+1 + V (St+1)  V (St), which is the reinforcement signal for TD state-value\r
learning (and analogous TD errors for action-value learning). In this reinforcement signal,\r
Rt+1 is the primary reinforcement contribution, and the temporal di↵erence in predicted\r
values, V (St+1)  V (St) (or an analogous temporal di↵erence for action values), is\r
the conditioned reinforcement contribution. Thus, whenever V (St+1)  V (St) = 0, t\r
signals ‘pure’ primary reinforcement; and whenever Rt+1 = 0, it signals ‘pure’ conditioned\r
reinforcement, but it often signals a mixture of these. Note as we mentioned in Section 6.1,\r
this t is not available until time t + 1. We therefore think of t as the reinforcement\r
signal at time t + 1, which is fitting because it reinforces predictions and/or actions made\r
earlier at step t.\r
A possible source of confusion is the terminology used by the famous psychologist\r
B. F. Skinner and his followers. For Skinner, positive reinforcement occurs when the\r
consequences of an animal’s behavior increase the frequency of that behavior; punishment\r
occurs when the behavior’s consequences decrease that behavior’s frequency. Negative\r
reinforcement occurs when behavior leads to the removal of an aversive stimulus (that is,\r
a stimulus the animal does not like), thereby increasing the frequency of that behavior.\r
Negative punishment, on the other hand, occurs when behavior leads to the removal of an\r
appetitive stimulus (that is, a stimulus the animal likes), thereby decreasing the frequency\r
of that behavior. We find no critical need for these distinctions because our approach\r
is more abstract than this, with both reward and reinforcement signals allowed to take\r
on both positive and negative values. (But note especially that when our reinforcement\r
signal is negative, it is not the same as Skinner’s negative reinforcement.)\r
On the other hand, it has often been pointed out that using a single number as a\r
reward or a penalty signal, depending only on its sign, is at odds with the fact that\r
animals’ appetitive and aversive systems have qualitatively di↵erent properties and\r
involve di↵erent brain mechanisms. This points to a direction in which the reinforcement\r
learning framework might be developed in the future to exploit computational advantages\r
of separate appetitive and aversive systems, but for now we are passing over these\r
possibilities.\r
Another discrepancy in terminology is how we use the word action. To many cognitive\r
scientists, an action is purposeful in the sense of being the result of an animal’s knowledge\r
about the relationship between the behavior in question and the consequences of that\r
behavior. An action is goal-directed and the result of a decision, whereas a response\r
is triggered by a stimulus and is the result of a reflex or a habit. We use the word\r
action without di↵erentiating among what others call actions, decisions, and responses.\r
These are important distinctions, but for us they are encompassed by di↵erences between

376 Chapter 14: Psychology\r
model-free and model-based reinforcement learning algorithms, which we discussed above\r
in relation to habitual and goal-directed behavior in Section 14.6. Dickinson (1985)\r
discusses the distinction between responses and actions.\r
A term used a lot in this book is control. What we mean by control is entirely di↵erent\r
from what it means to animal learning psychologists. By control we mean that an agent\r
influences its environment to bring about states or events that the agent prefers: the agent\r
exerts control over its environment. This is the sense of control used by control engineers.\r
In psychology, on the other hand, control typically means that an animal’s behavior is\r
influenced by—is controlled by—the stimuli the animal receives (stimulus control) or\r
the reinforcement schedule it experiences. Here the environment is controlling the agent.\r
Control in this sense is the basis of behavior modification therapy. Of course, both of\r
these directions of control are at play when an agent interacts with its environment,\r
but our focus is on the agent as controller, not the environment as controller. A view\r
equivalent to ours, and perhaps more illuminating, is that the agent is actually controlling\r
the input it receives from its environment (Powers, 1973). This is not what psychologists\r
mean by stimulus control.\r
Sometimes reinforcement learning is understood to refer solely to learning policies\r
directly from rewards (and penalties) without the involvement of value functions or\r
environment models. This is what psychologists call stimulus-response, or S-R, learning.\r
But for us, along with most of today’s psychologists, reinforcement learning is much\r
broader than this, including in addition to S-R learning, methods involving value functions,\r
environment models, planning, and other processes that are commonly thought to belong\r
to the more cognitive side of mental functioning.

Chapter 15\r
Neuroscience\r
Neuroscience is the multidisciplinary study of nervous systems: how they regulate bodily\r
functions; control behavior; change over time as a result of development, learning, and\r
aging; and how cellular and molecular mechanisms make these functions possible. One\r
of the most exciting aspects of reinforcement learning is the mounting evidence from\r
neuroscience that the nervous systems of humans and many other animals implement\r
algorithms that correspond in striking ways to reinforcement learning algorithms. The\r
main objective of this chapter is to explain these parallels and what they suggest about\r
the neural basis of reward-related learning in animals.\r
The most remarkable point of contact between reinforcement learning and neuroscience\r
involves dopamine, a chemical deeply involved in reward processing in the brains of mam\u0002mals. Dopamine appears to convey temporal-di↵erence (TD) errors to brain structures\r
where learning and decision making take place. This parallel is expressed by the reward\r
prediction error hypothesis of dopamine neuron activity, a hypothesis that resulted from\r
the convergence of computational reinforcement learning and results of neuroscience\r
experiments. In this chapter we discuss this hypothesis, the neuroscience findings that\r
led to it, and why it is a significant contribution to understanding brain reward systems.\r
We also discuss parallels between reinforcement learning and neuroscience that are less\r
striking than this dopamine/TD-error parallel but that provide useful conceptual tools\r
for thinking about reward-based learning in animals. Other elements of reinforcement\r
learning have the potential to impact the study of nervous systems, but their connections\r
to neuroscience are still relatively undeveloped. We discuss several of these evolving\r
connections that we think will grow in importance over time.\r
As we outlined in the history section of this book’s introductory chapter (Section 1.7),\r
many aspects of reinforcement learning were influenced by neuroscience. A second\r
objective of this chapter is to acquaint readers with ideas about brain function that have\r
contributed to our approach to reinforcement learning. Some elements of reinforcement\r
learning are easier to understand when seen in light of theories of brain function. This\r
is particularly true for the idea of the eligibility trace, one of the basic mechanisms\r
of reinforcement learning, that originated as a conjectured property of synapses, the\r
structures by which nerve cells—neurons—communicate with one another.

378 Chapter 15: Neuroscience\r
In this chapter we do not delve very deeply into the enormous complexity of the neural\r
systems underlying reward-based learning in animals: this chapter is too short, and we are\r
not neuroscientists. We do not try to describe—or even to name—the very many brain\r
structures and pathways, or any of the molecular mechanisms, believed to be involved in\r
these processes. We also do not do justice to hypotheses and models that are alternatives\r
to those that align so well with reinforcement learning. It should not be surprising that\r
there are di↵ering views among experts in the field. We can only provide a glimpse into\r
this fascinating and developing story. We hope, though, that this chapter convinces\r
you that a very fruitful channel has emerged connecting reinforcement learning and its\r
theoretical underpinnings to the neuroscience of reward-based learning in animals.\r
Many excellent publications cover links between reinforcement learning and neuro\u0002science, some of which we cite in this chapter’s final section. Our treatment di↵ers from\r
most of these because we assume familiarity with reinforcement learning as presented\r
in the earlier chapters of this book, but we do not assume knowledge of neuroscience.\r
We begin with a brief introduction to the neuroscience concepts needed for a basic\r
understanding of what is to follow.\r
15.1 Neuroscience Basics\r
Some basic information about nervous systems is helpful for following what we cover in\r
this chapter. Terms that we refer to later are italicized. Skipping this section will not be\r
a problem if you already have an elementary knowledge of neuroscience.\r
Neurons, the main components of nervous systems, are cells specialized for processing\r
and transmitting information using electrical and chemical signals. They come in many\r
forms, but a neuron typically has a cell body, dendrites, and a single axon. Dendrites\r
are structures that branch from the cell body to receive input from other neurons (or to\r
also receive external signals in the case of sensory neurons). A neuron’s axon is a fiber\r
that carries the neuron’s output to other neurons (or to muscles or glands). A neuron’s\r
output consists of sequences of electrical pulses called action potentials that travel along\r
the axon. Action potentials are also called spikes, and a neuron is said to fire when it\r
generates a spike. In models of neural networks it is common to use real numbers to\r
represent a neuron’s firing rate, the average number of spikes per some unit of time.\r
A neuron’s axon can branch widely so that the neuron’s action potentials reach\r
many targets. The branching structure of a neuron’s axon is called the neuron’s axonal\r
arbor. Because the conduction of an action potential is an active process, not unlike the\r
burning of a fuse, when an action potential reaches an axonal branch point it “lights\r
up” action potentials on all of the outgoing branches (although propagation to a branch\r
can sometimes fail). As a result, the activity of a neuron with a large axonal arbor can\r
influence many target sites.

15.1. Neuroscience Basics 379\r
A synapse is a structure generally at the termination of an axon branch that mediates\r
the communication of one neuron to another. A synapse transmits information from\r
the presynaptic neuron’s axon to a dendrite or cell body of the postsynaptic neuron.\r
With a few exceptions, synapses release a chemical neurotransmitter upon the arrival\r
of an action potential from the presynaptic neuron. (The exceptions are cases of direct\r
electric coupling between neurons, but these will not concern us here.) Neurotransmitter\r
molecules released from the presynaptic side of the synapse di↵use across the synaptic\r
cleft, the very small space between the presynaptic ending and the postsynaptic neuron,\r
and then bind to receptors on the surface of the postsynaptic neuron to excite or inhibit\r
its spike-generating activity, or to modulate its behavior in other ways. A particular\r
neurotransmitter may bind to several di↵erent types of receptors, with each producing a\r
di↵erent e↵ect on the postsynaptic neuron. For example, there are at least five di↵erent\r
receptor types by which the neurotransmitter dopamine can a↵ect a postsynaptic neuron.\r
Many di↵erent chemicals have been identified as neurotransmitters in animal nervous\r
systems.\r
A neuron’s background activity is its level of activity, usually its firing rate, when the\r
neuron does not appear to be driven by synaptic input related to the task of interest\r
to the experimenter, for example, when the neuron’s activity is not correlated with a\r
stimulus delivered to a subject as part of an experiment. Background activity can be\r
irregular due to input from the wider network, or due to noise within the neuron or its\r
synapses. Sometimes background activity is the result of dynamic processes intrinsic to\r
the neuron. A neuron’s phasic activity, in contrast to its background activity, consists of\r
bursts of spiking activity usually caused by synaptic input. Activity that varies slowly\r
and often in a graded manner, whether as background activity or not, is called a neuron’s\r
tonic activity.\r
The strength or e↵ectiveness by which the neurotransmitter released at a synapse\r
influences the postsynaptic neuron is the synapse’s ecacy. One way a nervous system\r
can change through experience is through changes in synaptic ecacies as a result of\r
combinations of the activities of the presynaptic and postsynaptic neurons, and sometimes\r
by the presence of a neuromodulator, which is a neurotransmitter having e↵ects other\r
than, or in addition to, direct fast excitation or inhibition.\r
Brains contain several di↵erent neuromodulation systems consisting of clusters of\r
neurons with widely branching axonal arbors, with each system using a di↵erent neuro\u0002transmitter. Neuromodulation can alter the function of neural circuits, mediate motivation,\r
arousal, attention, memory, mood, emotion, sleep, and body temperature. Important\r
here is that a neuromodulatory system can distribute something like a scalar signal, such\r
as a reinforcement signal, to alter the operation of synapses in widely distributed sites\r
critical for learning.\r
The ability of synaptic ecacies to change is called synaptic plasticity. It is one of the\r
primary mechanisms responsible for learning. The parameters, or weights, adjusted by\r
learning algorithms correspond to synaptic ecacies. As we detail below, modulation of\r
synaptic plasticity via the neuromodulator dopamine is a plausible mechanism for how\r
the brain might implement learning algorithms like many of those described in this book.

380 Chapter 15: Neuroscience\r
15.2 Reward Signals, Reinforcement Signals, Values,\r
and Prediction Errors\r
Links between neuroscience and computational reinforcement learning begin as parallels\r
between signals in the brain and signals playing prominent roles in reinforcement learning\r
theory and algorithms. In Chapter 3 we said that any problem of learning goal-directed\r
behavior can be reduced to the three signals representing actions, states, and rewards.\r
However, to explain links that have been made between neuroscience and reinforcement\r
learning, we have to be less abstract than this and consider other reinforcement learning\r
signals that correspond, in certain ways, to signals in the brain. In addition to reward\r
signals, these include reinforcement signals (which we argue are di↵erent from reward\r
signals), value signals, and signals conveying prediction errors. When we label a signal by\r
its function in this way, we are doing it in the context of reinforcement learning theory\r
in which the signal corresponds to a term in an equation or an algorithm. On the other\r
hand, when we refer to a signal in the brain, we mean a physiological event such as\r
a burst of action potentials or the secretion of a neurotransmitter. Labeling a neural\r
signal by its function, for example calling the phasic activity of a dopamine neuron a\r
reinforcement signal, means that the neural signal behaves like, and is conjectured to\r
function like, the corresponding theoretical signal.\r
Uncovering evidence for these correspondences involves many challenges. Neural\r
activity related to reward processing can be found in nearly every part of the brain,\r
and it is dicult to interpret results unambiguously because representations of di↵erent\r
reward-related signals tend to be highly correlated with one another. Experiments need to\r
be carefully designed to allow one type of reward-related signal to be distinguished with\r
any degree of certainty from others—or from an abundance of other signals not related to\r
reward processing. Despite these diculties, many experiments have been conducted with\r
the aim of reconciling aspects of reinforcement learning theory and algorithms with neural\r
signals, and some compelling links have been established. To prepare for examining these\r
links, in the rest of this section we remind the reader of what various reward-related\r
signals mean according to reinforcement learning theory.\r
In our Comments on Terminology at the end of the previous chapter, we said that\r
Rt is like a reward signal in an animal’s brain and not an object or event in the\r
animal’s environment. In reinforcement learning, the reward signal (along with an agent’s\r
environment) defines the problem a reinforcement learning agent is trying to solve. In\r
this respect, Rt is like a signal in an animal’s brain that distributes primary reward to\r
sites throughout the brain. But it is unlikely that a unitary master reward signal like Rt\r
exists in an animal’s brain. It is best to think of Rt as an abstraction summarizing the\r
overall e↵ect of a multitude of neural signals generated by many systems in the brain\r
that assess the rewarding or punishing qualities of sensations and states.\r
Reinforcement signals in reinforcement learning are di↵erent from reward signals. The\r
function of a reinforcement signal is to direct the changes a learning algorithm makes in\r
an agent’s policy, value estimates, or environment models. For a TD method, for instance,\r
the reinforcement signal at time t is the TD error t1 = Rt + V (St)  V (St1).1 The\r
1As we mentioned in Section 6.1, t in our notation is defined to be Rt+1 + V (St+1)  V (St), so t

15.3. The Reward Prediction Error Hypothesis 381\r
reinforcement signal for some algorithms could be just the reward signal, but for most\r
of the algorithms we consider the reinforcement signal is the reward signal adjusted by\r
other information, such as the value estimates in TD errors.\r
Estimates of state values or of action values, that is, V or Q, specify what is good or\r
bad for the agent over the long run. They are predictions of the total reward an agent can\r
expect to accumulate over the future. Agents make good decisions by selecting actions\r
leading to states with the largest estimated state values, or by selecting actions with the\r
largest estimated action values.\r
Prediction errors measure discrepancies between expected and actual signals or sensa\u0002tions. Reward prediction errors (RPEs) specifically measure discrepancies between the\r
expected and the received reward signal, being positive when the reward signal is greater\r
than expected, and negative otherwise. TD errors like (6.5) are special kinds of RPEs\r
that signal discrepancies between current and earlier expectations of reward over the\r
long-term. When neuroscientists refer to RPEs they generally (though not always) mean\r
TD RPEs, which we simply call TD errors throughout this chapter. Also in this chapter,\r
a TD error is generally one that does not depend on actions, as opposed to TD errors\r
used in learning action-values by algorithms like Sarsa and Q-learning. This is because\r
the most well-known links to neuroscience are stated in terms of action-free TD errors,\r
but we do not mean to rule out possible similar links involving action-dependent TD\r
errors. (TD errors for predicting signals other than rewards are useful too, but that case\r
will not concern us here. See, for example, Modayil, White, and Sutton, 2014.)\r
One can ask many questions about links between neuroscience data and these theoretically\u0002defined signals. Is an observed signal more like a reward signal, a value signal, a prediction\r
error, a reinforcement signal, or something altogether di↵erent? And if it is an error\r
signal, is it an RPE, a TD error, or a simpler error like the Rescorla–Wagner error (14.3)?\r
And if it is a TD error, does it depend on actions like the TD error of Q-learning or\r
Sarsa? As indicated above, probing the brain to answer questions like these is extremely\r
dicult. But experimental evidence suggests that one neurotransmitter, specifically\r
the neurotransmitter dopamine, signals RPEs, and further, that the phasic activity of\r
dopamine-producing neurons in fact conveys TD errors (see Section 15.1 for a definition of\r
phasic activity). This evidence led to the reward prediction error hypothesis of dopamine\r
neuron activity, which we describe next.\r
15.3 The Reward Prediction Error Hypothesis\r
The reward prediction error hypothesis of dopamine neuron activity proposes that one\r
of the functions of the phasic activity of dopamine-producing neurons in mammals is to\r
deliver an error between an old and a new estimate of expected future reward to target\r
areas throughout the brain. This hypothesis (though not in these exact words) was\r
first explicitly stated by Montague, Dayan, and Sejnowski (1996), who showed how the\r
TD error concept from reinforcement learning accounts for many features of the phasic\r
is not available until time t + 1. The TD error available at t is actually t1 = Rt + V (St)  V (St1).\r
Because we are thinking of time steps as very small, or even infinitesimal, time intervals, one should not\r
attribute undue importance to this one-step time shift.

382 Chapter 15: Neuroscience\r
activity of dopamine neurons in mammals. The experiments that led to this hypothesis\r
were performed in the 1980s and early 1990s in the laboratory of neuroscientist Wolfram\r
Schultz. Section 15.5 describes these influential experiments, Section 15.6 explains how the\r
results of these experiments align with TD errors, and the Bibliographical and Historical\r
Remarks section at the end of this chapter includes a guide to the literature surrounding\r
the development of this influential hypothesis.\r
Montague et al. (1996) compared the TD errors of the TD model of classical conditioning\r
with the phasic activity of dopamine-producing neurons during classical conditioning\r
experiments. Recall from Section 14.2 that the TD model of classical conditioning is\r
basically the semi-gradient-descent TD() algorithm with linear function approximation.\r
Montague et al. made several assumptions to set up this comparison. First, because a\r
TD error can be negative but neurons cannot have a negative firing rate, they assumed\r
that the quantity corresponding to dopamine neuron activity is t1 + bt, where bt is the\r
background firing rate of the neuron. A negative TD error corresponds to a drop in a\r
dopamine neuron’s firing rate below its background rate.2\r
A second assumption was needed about the states visited in each classical conditioning\r
trial and how they are represented as inputs to the learning algorithm. This is the same\r
issue we discussed in Section 14.2.4 for the TD model. Montague et al. chose a complete\r
serial compound (CSC) representation as shown in the left column of Figure 14.1, but\r
where the sequence of short-duration internal signals continues until the onset of the US,\r
which here is the arrival of a non-zero reward signal. This representation allows the TD\r
error to mimic the fact that dopamine neuron activity not only predicts a future reward,\r
but that it is also sensitive to when after a predictive cue that reward is expected to\r
arrive. There has to be some way to keep track of the time between sensory cues and\r
the arrival of reward. If a stimulus initiates a sequence of internal signals that continues\r
after the stimulus ends, and if there is a di↵erent signal for each time step following the\r
stimulus, then each time step after the stimulus is represented by a distinct state. Thus,\r
the TD error, being state-dependent, can be sensitive to the timing of events within a\r
trial.\r
In simulated trials with these assumptions about background firing rate and input\r
representation, TD errors of the TD model are remarkably similar to dopamine neuron\r
phasic activity. Previewing our description of details about these similarities in Section 15.5\r
below, the TD errors parallel the following features of dopamine neuron activity: (1) the\r
phasic response of a dopamine neuron only occurs when a rewarding event is unpredicted;\r
(2) early in learning, neutral cues that precede a reward do not cause substantial phasic\r
dopamine responses, but with continued learning these cues gain predictive value and\r
come to elicit phasic dopamine responses; (3) if an even earlier cue reliably precedes a\r
cue that has already acquired predictive value, the phasic dopamine response shifts to\r
the earlier cue, ceasing for the later cue; and (4) if after learning, the predicted rewarding\r
event is omitted, a dopamine neuron’s response decreases below its baseline level shortly\r
after the expected time of the rewarding event.\r
2In the literature relating TD errors to the activity of dopamine neurons, their t is the same as our\r
t1 = Rt + V (St)  V (St1).

15.4. Dopamine 383\r
Although not every dopamine neuron monitored in the experiments of Schultz and\r
colleagues behaved in all of these ways, the striking correspondence between the ac\u0002tivities of most of the monitored neurons and TD errors lends strong support to the\r
reward prediction error hypothesis. There are situations, however, in which predictions\r
based on the hypothesis do not match what is observed in experiments. The choice\r
of input representation is critical to how closely TD errors match some of the details\r
of dopamine neuron activity, particularly details about the timing of dopamine neuron\r
responses. Di↵erent ideas, some of which we discuss below, have been proposed about\r
input representations and other features of TD learning to make the TD errors fit the data\r
better, though the main parallels appear with the CSC representation that Montague et\r
al. used. Overall, the reward prediction error hypothesis has received wide acceptance\r
among neuroscientists studying reward-based learning, and it has proven to be remarkably\r
resilient in the face of accumulating results from neuroscience experiments.\r
To prepare for our description of the neuroscience experiments supporting the reward\r
prediction error hypothesis, and to provide some context so that the significance of the\r
hypothesis can be appreciated, we next present some of what is known about dopamine,\r
the brain structures it influences, and how it is involved in reward-based learning.\r
15.4 Dopamine\r
Dopamine is produced as a neurotransmitter by neurons whose cell bodies lie mainly\r
in two clusters of neurons in the midbrain of mammals: the substantia nigra pars\r
compacta (SNpc) and the ventral tegmental area (VTA). Dopamine plays essential roles\r
in many processes in the mammalian brain. Prominent among these are motivation,\r
learning, action-selection, most forms of addiction, and the disorders schizophrenia and\r
Parkinson’s disease. Dopamine is called a neuromodulator because it performs many\r
functions other than direct fast excitation or inhibition of targeted neurons. Although\r
much remains unknown about dopamine’s functions and details of its cellular e↵ects, it is\r
clear that it is fundamental to reward processing in the mammalian brain. Dopamine\r
is not the only neuromodulator involved in reward processing, and its role in aversive\r
situations—punishment—remains controversial. Dopamine also can function di↵erently in\r
non-mammals. But no one doubts that dopamine is essential for reward-related processes\r
in mammals, including humans.\r
An early, traditional view is that dopamine neurons broadcast a reward signal to\r
multiple brain regions implicated in learning and motivation. This view followed from a\r
famous 1954 paper by James Olds and Peter Milner that described the e↵ects of electrical\r
stimulation on certain areas of a rat’s brain. They found that electrical stimulation to\r
particular regions acted as a very powerful reward in controlling the rat’s behavior: “...the\r
control exercised over the animal’s behavior by means of this reward is extreme, possibly\r
exceeding that exercised by any other reward previously used in animal experimentation”\r
(Olds and Milner, 1954). Later research revealed that the sites at which stimulation\r
was most e↵ective in producing this rewarding e↵ect excited dopamine pathways, either\r
directly or indirectly, that ordinarily are excited by natural rewarding stimuli. E↵ects\r
similar to these were also observed with human subjects. These observations strongly\r
suggested that dopamine neuron activity signals reward.

384 Chapter 15: Neuroscience\r
But if the reward prediction error hypothesis is correct—even if it accounts for only\r
some features of a dopamine neuron’s activity—this traditional view of dopamine neuron\r
activity is not entirely correct: phasic responses of dopamine neurons signal reward\r
prediction errors, not reward itself. In reinforcement learning’s terms, a dopamine\r
neuron’s phasic response at a time t corresponds to t1 = Rt + V (St)  V (St1), not\r
to Rt.\r
Reinforcement learning theory and algorithms help reconcile the reward-prediction\u0002error view with the conventional notion that dopamine signals reward. In many of the\r
algorithms we discuss in this book,  functions as a reinforcement signal, meaning that it\r
is the main driver of learning. For example,  is the critical factor in the TD model of\r
classical conditioning, and  is the reinforcement signal for learning both a value function\r
and a policy in an actor–critic architecture (Sections 13.5 and 15.7). Action-dependent\r
forms of  are reinforcement signals for Q-learning and Sarsa. The reward signal Rt is\r
a crucial component of t1, but it is not the complete determinant of its reinforcing\r
e↵ect in these algorithms. The additional term V (St)  V (St1) is the higher-order\r
reinforcement part of t1, and even if reward occurs (Rt 6= 0), the TD error can be silent\r
if the reward is fully predicted (which is fully explained in Section 15.6 below).\r
A closer look at Olds’ and Milner’s 1954 paper, in fact, reveals that it is mainly\r
about the reinforcing e↵ect of electrical stimulation in an instrumental conditioning task.\r
Electrical stimulation not only energized the rats’ behavior—through dopamine’s e↵ect on\r
motivation—it also led to the rats quickly learning to stimulate themselves by pressing a\r
lever, which they would do frequently for long periods of time. The activity of dopamine\r
neurons triggered by electrical stimulation reinforced the rats’ lever pressing.\r
More recent experiments using optogenetic methods clinch the role of phasic responses\r
of dopamine neurons as reinforcement signals. These methods allow neuroscientists to\r
precisely control the activity of selected neuron types at a millisecond timescale in awake\r
behaving animals. Optogenetic methods introduce light-sensitive proteins into selected\r
neuron types so that these neurons can be activated or silenced by means of flashes of\r
laser light. The first experiment using optogenetic methods to study dopamine neurons\r
showed that optogenetic stimulation producing phasic activation of dopamine neurons\r
in mice was enough to condition the mice to prefer the side of a chamber where they\r
received this stimulation as compared to the chamber’s other side where they received\r
no, or lower-frequency, stimulation (Tsai et al. 2009). In another example, Steinberg\r
et al. (2013) used optogenetic activation of dopamine neurons to create artificial bursts\r
of dopamine neuron activity in rats at the times when rewarding stimuli were expected\r
but omitted—times when dopamine neuron activity normally pauses. With these pauses\r
replaced by artificial bursts, responding was sustained when it would ordinarily decrease\r
due to lack of reinforcement (in extinction trials), and learning was enabled when it would\r
ordinarily be blocked due to the reward being already predicted (the blocking paradigm;\r
Section 14.2.1).\r
Additional evidence for the reinforcing function of dopamine comes from optogenetic\r
experiments with fruit flies, except in these animals dopamine’s e↵ect is the opposite of\r
its e↵ect in mammals: optically triggered bursts of dopamine neuron activity act just\r
like electric foot shock in reinforcing avoidance behavior, at least for the population

15.4. Dopamine 385\r
of dopamine neurons activated (Claridge-Chang et al. 2009). Although none of these\r
optogenetic experiments showed that phasic dopamine neuron activity is specifically\r
like a TD error, they convincingly demonstrated that phasic dopamine neuron activity\r
acts just like  acts (or perhaps like minus  acts in fruit flies) as the reinforcement\r
signal in algorithms for both prediction (classical conditioning) and control (instrumental\r
conditioning).\r
Axonal arbor of a single neuron producing\r
dopamine as a neurotransmitter. These\r
axons make synaptic contacts with a huge\r
number of dendrites of neurons in targeted\r
brain areas.\r
Adapted from The Journal of Neuroscience,\r
Matsuda, Furuta, Nakamura, Hioki, Fujiyama,\r
Arai, and Kaneko, volume 29, 2009, page 451.\r
Dopamine neurons are particularly well suited\r
to broadcasting a reinforcement signal to many\r
areas of the brain. These neurons have huge\r
axonal arbors, each releasing dopamine at 100 to\r
1,000 times more synaptic sites than reached by\r
the axons of typical neurons. Shown to the right\r
is the axonal arbor of a single dopamine neuron\r
whose cell body is in the SNpc of a rat’s brain.\r
Each axon of a SNpc or VTA dopamine neuron\r
makes roughly 500,000 synaptic contacts on the\r
dendrites of neurons in targeted brain areas.\r
If dopamine neurons broadcast a reinforce\u0002ment signal like reinforcement learning’s , then\r
because this is a scalar signal, i.e., a single num\u0002ber, all dopamine neurons in both the SNpc\r
and VTA would be expected to activate more\u0002or-less identically so that they would act in near\r
synchrony to send the same signal to all of the\r
sites their axons target. Although it has been\r
a common belief that dopamine neurons do act\r
together like this, modern evidence is pointing\r
to the more complicated picture that di↵erent\r
subpopulations of dopamine neurons respond to\r
input di↵erently depending on the structures to\r
which they send their signals and the di↵erent\r
ways these signals act on their target structures. Dopamine has functions other than\r
signaling RPEs, and even for dopamine neurons that do signal RPEs, it can make sense\r
to send di↵erent RPEs to di↵erent structures depending on the roles these structures\r
play in producing reinforced behavior. This is beyond what we treat in any detail in this\r
book, but vector-valued RPE signals make sense from the perspective of reinforcement\r
learning when decisions can be decomposed into separate sub-decisions, or more generally,\r
as a way to address the structural version of the credit assignment problem: How do\r
you distribute credit for success (or blame for failure) of a decision among the many\r
component structures that could have been involved in producing it? We say a bit more\r
about this in Section 15.10 below.\r
The axons of most dopamine neurons make synaptic contact with neurons in the frontal\r
cortex and the basal ganglia, areas of the brain involved in voluntary movement, decision\r
making, learning, and cognitive functions such as planning. Because most ideas relating

386 Chapter 15: Neuroscience\r
dopamine to reinforcement learning focus on the basal ganglia, and the connections from\r
dopamine neurons are particularly dense there, we focus on the basal ganglia here. The\r
basal ganglia are a collection of neuron groups, or nuclei, lying at the base of the forebrain.\r
The main input structure of the basal ganglia is called the striatum. Essentially all of the\r
cerebral cortex, among other structures, provides input to the striatum. The activity of\r
cortical neurons conveys a wealth of information about sensory input, internal states, and\r
motor activity. The axons of cortical neurons make synaptic contacts on the dendrites of\r
the main input/output neurons of the striatum, called medium spiny neurons. Output\r
from the striatum loops back via other basal ganglia nuclei and the thalamus to frontal\r
areas of cortex, and to motor areas, making it possible for the striatum to influence\r
movement, abstract decision processes, and reward processing. Two main subdivisions\r
of the striatum are important for reinforcement learning: the dorsal striatum, primarily\r
implicated in influencing action selection, and the ventral striatum, thought to be critical\r
for di↵erent aspects of reward processing, including the assignment of a↵ective value to\r
sensations.\r
The dendrites of medium spiny neurons are covered with spines on whose tips the\r
axons of neurons in the cortex make synaptic contact. Also making synaptic contact with\r
these spines—in this case contacting the spine stems—are axons of dopamine neurons\r
(Figure 15.1). This arrangement brings together presynaptic activity of cortical neurons,\r
Figure 15.1: Spine of a striatal neuron showing input from both cortical and dopamine neurons.\r
Axons of cortical neurons influence striatal neurons via corticostriatal synapses releasing the\r
neurotransmitter glutamate at the tips of spines covering the dendrites of striatal neurons.\r
An axon of a VTA or SNpc dopamine neuron is shown passing by the spine (from the lower\r
right). “Dopamine varicosities” on this axon release dopamine at or near the spine stem, in an\r
arrangement that brings together presynaptic input from cortex, postsynaptic activity of the\r
striatal neuron, and dopamine, making it possible that several types of learning rules govern the\r
plasticity of corticostriatal synapses. Each axon of a dopamine neuron makes synaptic contact\r
with the stems of roughly 500,000 spines. Some of the complexity omitted from our discussion\r
is shown here by other neurotransmitter pathways and multiple receptor types, such as D1 an\r
D2 dopamine receptors by which dopamine can produce di↵erent e↵ects at spines and other\r
postsynaptic sites. From Journal of Neurophysiology, W. Schultz, vol. 80, 1998, page 10.

15.5. Experimental Support for the Reward Prediction Error Hypothesis 387\r
postsynaptic activity of medium spiny neurons, and input from dopamine neurons. What\r
actually occurs at these spines is complex and not completely understood. Figure 15.1\r
hints at the complexity by showing two types of receptors for dopamine, receptors for\r
glutamate—the neurotransmitter of the cortical inputs—and multiple ways that the\r
various signals can interact. But evidence is mounting that changes in the ecacies of\r
the synapses on the pathway from the cortex to the striatum, which neuroscientists call\r
corticostriatal synapses, depend critically on appropriately-timed dopamine signals.\r
15.5 Experimental Support\r
for the Reward Prediction Error Hypothesis\r
Dopamine neurons respond with bursts of activity to intense, novel, or unexpected visual\r
and auditory stimuli that trigger eye and body movements, but very little of their activity is\r
related to the movements themselves. This is surprising because degeneration of dopamine\r
neurons is a cause of Parkinson’s disease, whose symptoms include motor disorders,\r
particularly deficits in self-initiated movement. Motivated by the weak relationship\r
between dopamine neuron activity and stimulus-triggered eye and body movements,\r
Romo and Schultz (1990) and Schultz and Romo (1990) took the first steps toward the\r
reward prediction error hypothesis by recording the activity of dopamine neurons and\r
muscle activity while monkeys moved their arms.\r
They trained two monkeys to reach from a resting hand position into a bin containing\r
a bit of apple, a piece of cookie, or a raisin, when the monkey saw and heard the bin’s\r
door open. The monkey could then grab and bring the food to its mouth. After a monkey\r
became good at this, it was trained on two additional tasks. The purpose of the first\r
task was to see what dopamine neurons do when movements are self-initiated. The bin\r
was left open but covered from above so that the monkey could not see inside but could\r
reach in from below. No triggering stimuli were presented, and after the monkey reached\r
for and ate the food morsel, the experimenter usually (though not always), silently and\r
unseen by the monkey, replaced food in the bin by sticking it onto a rigid wire. Here too,\r
the activity of the dopamine neurons Romo and Schultz monitored was not related to the\r
monkey’s movements, but a large percentage of these neurons produced phasic responses\r
whenever the monkey first touched a food morsel. These neurons did not respond when\r
the monkey touched just the wire or explored the bin when no food was there. This was\r
good evidence that the neurons were responding to the food and not to other aspects of\r
the task.\r
The purpose of Romo and Schultz’s second task was to see what happens when\r
movements are triggered by stimuli. This task used a di↵erent bin with a movable cover.\r
The sight and sound of the bin opening triggered reaching movements to the bin. In this\r
case, Romo and Schultz found that after some period of training, the dopamine neurons\r
no longer responded to the touch of the food but instead responded to the sight and sound\r
of the opening cover of the food bin. The phasic responses of these neurons had shifted\r
from the reward itself to stimuli predicting the availability of the reward. In a followup\r
study, Romo and Schultz found that most of the dopamine neurons whose activity they

388 Chapter 15: Neuroscience\r
monitored did not respond to the sight and sound of the bin opening outside the context\r
of the behavioral task. These observations suggested that the dopamine neurons were\r
responding neither to the initiation of a movement nor to the sensory properties of the\r
stimuli, but were rather signaling an expectation of reward.\r
Schultz’s group conducted many additional studies involving both SNpc and VTA\r
dopamine neurons. A particular series of experiments was influential in suggesting that\r
the phasic responses of dopamine neurons correspond to TD errors and not to simpler\r
errors like those in the Rescorla–Wagner model (14.3). In the first of these experiments\r
(Ljungberg, Apicella, and Schultz, 1992), monkeys were trained to depress a lever after\r
a light was illuminated as a ‘trigger cue’ to obtain a drop of apple juice. As Romo\r
and Schultz had observed earlier, many dopamine neurons initially responded to the\r
reward—the drop of juice (Figure 15.2, top panel). But many of these neurons lost that\r
reward response as training continued and developed responses instead to the illumination\r
of the light that predicted the reward (Figure 15.2, middle panel). With continued\r
training, lever pressing became faster while the number of dopamine neurons responding\r
to the trigger cue decreased.\r
Following this study, the same monkeys were trained on a new task (Schultz, Apicella,\r
and Ljungberg, 1993). Here the monkeys faced two levers, each with a light above it.\r
Illuminating one of these lights was an ‘instruction cue’ indicating which of the two levers\r
Figure 15.2: The response of dopamine neurons shifts from initial responses to primary reward\r
to earlier predictive stimuli. These are plots of the number of action potentials produced\r
by monitored dopamine neurons within small time intervals, averaged over all the monitored\r
dopamine neurons (ranging from 23 to 44 neurons for these data). Top: dopamine neurons are\r
activated by the unpredicted delivery of drop of apple juice. Middle: with learning, dopamine\r
neurons developed responses to the reward-predicting trigger cue and lost responsiveness to the\r
delivery of reward. Bottom: with the addition of an instruction cue preceding the trigger cue by\r
1 second, dopamine neurons shifted their responses from the trigger cue to the earlier instruction\r
cue. From Schultz et al. (1995), MIT Press.

15.5. Experimental Support for the Reward Prediction Error Hypothesis 389\r
Figure 15.3: The response of dopamine neurons drops\r
below baseline shortly after the time when an expected\r
reward fails to occur. Top: dopamine neurons are\r
activated by the unpredicted delivery of a drop of\r
apple juice. Middle: dopamine neurons respond to a\r
conditioned stimulus (CS) that predicts reward and\r
do not respond to the reward itself. Bottom: when\r
the reward predicted by the CS fails to occur, the\r
activity of dopamine neurons drops below baseline\r
shortly after the time the reward is expected to occur.\r
At the top of each of these panels is shown the average\r
number of action potentials produced by monitored\r
dopamine neurons within small time intervals around\r
the indicated times. The raster plots below show the\r
activity patterns of the individual dopamine neurons\r
that were monitored; each dot represents an action\r
potential. From Schultz, Dayan, and Montague, A\r
Neural Substrate of Prediction and Reward, Science,\r
vol. 275, issue 5306, pages 1593-1598, March 14, 1997.\r
Reprinted with permission from AAAS.\r
would produce a drop of apple juice.\r
In this task, the instruction cue pre\u0002ceded the trigger cue of the previ\u0002ous task by a fixed interval of 1 sec\u0002ond. The monkeys learned to with\u0002hold reaching until seeing the trig\u0002ger cue, and dopamine neuron activ\u0002ity increased, but now the responses\r
of the monitored dopamine neurons\r
occurred almost exclusively to the\r
earlier instruction cue and not to\r
the trigger cue (Figure 15.2, bot\u0002tom panel). Here again the num\u0002ber of dopamine neurons responding\r
to the instruction cue was much re\u0002duced when the task was well learned.\r
During learning across these tasks,\r
dopamine neuron activity shifted\r
from initially responding to the re\u0002ward to responding to the earlier\r
predictive stimuli, first progressing\r
to the trigger stimulus then to the\r
still earlier instruction cue. As re\u0002sponding moved earlier in time it\r
disappeared from the later stimuli.\r
This shifting of responses to earlier\r
reward predictors, while losing re\u0002sponses to later predictors is a hall\u0002mark of TD learning (see, for exam\u0002ple, Figure 14.2).\r
The task just described revealed\r
another property of dopamine neu\u0002ron activity shared with TD learn\u0002ing. The monkeys sometimes pressed\r
the wrong key, that is, the key other\r
than the instructed one, and conse\u0002quently received no reward. In these\r
trials, many of the dopamine neu\u0002rons showed a sharp decrease in their\r
firing rates below baseline shortly af\u0002ter the reward’s usual time of deliv\u0002ery, and this happened without the\r
availability of any external cue to\r
mark the usual time of reward de\u0002livery (Figure 15.3). Somehow the

390 Chapter 15: Neuroscience\r
monkeys were internally keeping track of the timing of the reward. (Response timing is\r
one area where the simplest version of TD learning needs to be modified to account for\r
some of the details of the timing of dopamine neuron responses. We consider this issue in\r
the following section.)\r
The observations from the studies described above led Schultz and his group to conclude\r
that dopamine neurons respond to unpredicted rewards, to the earliest predictors of\r
reward, and that dopamine neuron activity decreases below baseline if a reward, or a\r
predictor of reward, does not occur at its expected time. Researchers familiar with\r
reinforcement learning were quick to recognize that these results are strikingly similar\r
to how the TD error behaves as the reinforcement signal in a TD algorithm. The next\r
section explores this similarity by working through a specific example in detail.\r
15.6 TD Error/Dopamine Correspondence\r
This section explains the correspondence between the TD error  and the phasic responses\r
of dopamine neurons observed in the experiments just described. We examine how \r
changes over the course of learning in a task something like the one described above\r
where a monkey first sees an instruction cue and then a fixed time later has to respond\r
correctly to a trigger cue in order to obtain reward. We use a simple idealized version of\r
this task, but we go into a lot more detail than is usual because we want to emphasize\r
the theoretical basis of the parallel between TD errors and dopamine neuron activity.\r
The first simplifying assumption is that the agent has already learned the actions\r
required to obtain reward. Then its task is just to learn accurate predictions of future\r
reward for the sequence of states it experiences. This is then a prediction task, or\r
more technically, a policy-evaluation task: learning the value function for a fixed policy\r
(Sections 4.1 and 6.1). The value function to be learned assigns to each state a value that\r
predicts the return that will follow that state if the agent selects actions according to the\r
given policy, where the return is the (possibly discounted) sum of all the future rewards.\r
This is unrealistic as a model of the monkey’s situation because the monkey would likely\r
learn these predictions at the same time that it is learning to act correctly (as would a\r
reinforcement learning algorithm that learns policies as well as value functions, such as\r
an actor–critic algorithm), but this scenario is simpler to describe than one in which a\r
policy and a value function are learned simultaneously.\r
Now imagine that the agent’s experience divides into multiple trials, in each of which\r
the same sequence of states repeats, with a distinct state occurring on each time step\r
during the trial. Further imagine that the return being predicted is limited to the return\r
over a trial, which makes a trial analogous to a reinforcement learning episode as we have\r
defined it. In reality, of course, the returns being predicted are not confined to single\r
trials, and the time interval between trials is an important factor in determining what an\r
animal learns. This is true for TD learning as well, but here we assume that returns do\r
not accumulate over multiple trials. Given this, then, a trial in experiments like those\r
conducted by Schultz and colleagues is equivalent to an episode of reinforcement learning.\r
(Though in this discussion, we will use the term trial instead of episode to relate better\r
to the experiments.)

15.6. TD Error/Dopamine Correspondence 391\r
As usual, we also need to make an assumption about how states are represented as\r
inputs to the learning algorithm, an assumption that influences how closely the TD error\r
corresponds to dopamine neuron activity. We discuss this issue later, but for now we\r
assume the same CSC representation used by Montague et al. (1996) in which there is a\r
separate internal stimulus for each state visited at each time step in a trial. This reduces\r
the process to the tabular case covered in the first part of this book. Finally, we assume\r
that the agent uses TD(0) to learn a value function, V , stored in a lookup table initialized\r
to be zero for all the states. We also assume that this is a deterministic task and that\r
the discount factor, , is very nearly one so that we can ignore it.\r
Figure 15.4 shows the time courses of R, V , and  at several stages of learning in this\r
policy-evaluation task. The time axes represent the time interval over which a sequence\r
of states is visited in a trial (where for clarity we omit showing individual states). The\r
reward signal is zero throughout each trial except when the agent reaches the rewarding\r
state, shown near the right end of the time line, when the reward signal becomes some\r
positive number, say R?. The goal of TD learning is to predict the return for each\r
state visited in a trial, which in this undiscounted case and given our assumption that\r
predictions are confined to individual trials, is simply R? for each state.\r
R\r
RR\r
R\r
Rt\r
R\r
t1 = Rt + Vt  Vt1\r
R\r
V\r
V\r
\r
\r
\r
early in\r
learning\r
learning\r
complete\r
R omitted\r
regular predictors of over this interval R\r
R?\r
Figure 15.4: The behavior of the TD error  during TD learning is consistent with features of\r
the phasic activation of dopamine neurons. (Here  is the TD error available at time t, i.e., t1).\r
Top: a sequence of states, shown as an interval of regular predictors, is followed by a non-zero\r
reward R?. Early in learning: the initial value function, V , and initial , which at first is equal\r
to R?. Learning complete: the value function accurately predicts future reward,  is positive at\r
the earliest predictive state, and  = 0 at the time of the non-zero reward. R? omitted: at the\r
time the predicted reward is omitted,  becomes negative. See text for a complete explanation\r
of why this happens.

392 Chapter 15: Neuroscience\r
Preceding the rewarding state is a sequence of reward-predicting states, with the\r
earliest reward-predicting state shown near the left end of the time line. This is like\r
the state near the start of a trial, for example like the state marked by the instruction\r
cue in a trial of the monkey experiment of Schultz et al. (1993) described above. It is\r
the first state in a trial that reliably predicts that trial’s reward. (Of course, in reality\r
states visited on preceding trials are even earlier reward-predicting states, but because\r
we are confining predictions to individual trials, these do not qualify as predictors of this\r
trial’s reward. Below we give a more satisfactory, though more abstract, description of an\r
earliest reward-predicting state.) The latest reward-predicting state in a trial is the state\r
immediately preceding the trial’s rewarding state. This is the state near the far right end\r
of the time line in Figure 15.4. Note that the rewarding state of a trial does not predict\r
the return for that trial: the value of this state would come to predict the return over all\r
the following trials, which here we are assuming to be zero in this episodic formulation.\r
Figure 15.4 shows the first-trial time courses of V and  as the graphs labeled ‘early in\r
learning.’ Because the reward signal is zero throughout the trial except when the rewarding\r
state is reached, and all the V -values are zero, the TD error is also zero until it becomes\r
R? at the rewarding state. This follows because t1 = Rt + Vt  Vt1 = Rt + 0  0 = Rt,\r
which is zero until it equals R? when the reward occurs. Here Vt and Vt1 are respectively\r
the estimated values of the states visited at times t and t  1 in a trial. The TD error at\r
this stage of learning is analogous to a dopamine neuron responding to an unpredicted\r
reward (e.g., a drop of apple juice) at the start of training.\r
Throughout this first trial and all successive trials, TD(0) updates occur at each\r
state transition as described in Chapter 6. This successively increases the values of the\r
reward-predicting states, with the increases spreading backwards from the rewarding\r
state, until the values converge to the correct return predictions. In this case (because\r
we are assuming no discounting) the correct predictions are equal to R? for all the\r
reward-predicting states. This can be seen in Figure 15.4 as the graph of V labeled\r
‘learning complete’ where the values of all the states from the earliest to the latest\r
reward-predicting states all equal R?. The values of the states preceding the earliest\r
reward-predicting state remain low (which Figure 15.4 shows as zero) because they are\r
not reliable predictors of reward.\r
When learning is complete, that is, when V attains its correct values, the TD errors\r
associated with transitions from any reward-predicting state are zero because the predic\u0002tions are now accurate. This is because for a transition from a reward-predicting state to\r
another reward-predicting state, we have t1 = Rt + Vt  Vt1 =0+ R?  R? = 0, and\r
for the transition from the latest reward-predicting state to the rewarding state, we have\r
t1 = Rt +Vt Vt1 = R? + 0R? = 0. On the other hand, the TD error on a transition\r
from any state to the earliest reward-predicting state is positive because of the mismatch\r
between this state’s low value and the larger value of the following reward-predicting\r
state. Indeed, if the value of a state preceding the earliest reward-predicting state were\r
zero, then after the transition to the earliest reward-predicting state, we would have\r
that t1 = Rt + Vt  Vt1 =0+ R?  0 = R?. The ‘learning complete’ graph of  in\r
Figure 15.4 shows this positive value at the earliest reward-predicting state, and zeros\r
everywhere else.

15.6. TD Error/Dopamine Correspondence 393\r
The positive TD error upon transitioning to the earliest reward-predicting state is\r
analogous to the persistence of dopamine responses to the earliest stimuli predicting\r
reward. By the same token, when learning is complete, a transition from the latest\r
reward-predicting state to the rewarding state produces a zero TD error because the\r
latest reward-predicting state’s value, being correct, cancels the reward. This parallels the\r
observation that fewer dopamine neurons generate a phasic response to a fully predicted\r
reward than to an unpredicted reward.\r
After learning, if the reward is suddenly omitted, the TD error goes negative at the\r
usual time of reward because the value of the latest reward-predicting state is then too\r
high: t1 = Rt + Vt  Vt1 =0+0  R? = R?, as shown at the right end of the ‘R\r
omitted’ graph of  in Figure 15.4. This is like dopamine neuron activity decreasing\r
below baseline at the time an expected reward is omitted as seen in the experiment of\r
Schultz et al. (1993) described above and shown in Figure 15.3.\r
The idea of an earliest reward-predicting state deserves more attention. In the scenario\r
described above, because experience is divided into trials, and we assumed that predictions\r
are confined to individual trials, the earliest reward-predicting state is always the first\r
state of a trial. Clearly this is artificial. A more general way to think of an earliest\r
reward-predicting state is that it is an unpredicted predictor of reward, and there can\r
be many such states. In an animal’s life, many di↵erent states may precede an earliest\r
reward-predicting state. However, because these states are more often followed by other\r
states that do not predict reward, their reward-predicting powers, that is, their values,\r
remain low. A TD algorithm, if operating throughout the animal’s life, would update the\r
values of these states too, but the updates would not consistently accumulate because, by\r
assumption, none of these states reliably precedes an earliest reward-predicting state. If\r
any of them did, they would be reward-predicting states as well. This might explain why\r
with overtraining, dopamine responses decrease to even the earliest reward-predicting\r
stimulus in a trial. With overtraining one would expect that even a formerly-unpredicted\r
predictor state would become predicted by stimuli associated with earlier states: the\r
animal’s interaction with its environment both inside and outside of an experimental\r
task would become commonplace. Upon breaking this routine with the introduction of a\r
new task, however, one would see TD errors reappear, as indeed is observed in dopamine\r
neuron activity.\r
The example described above explains why the TD error shares key features with\r
the phasic activity of dopamine neurons when the animal is learning in a task similar\r
to the idealized task of our example. But not every property of the phasic activity of\r
dopamine neurons coincides so neatly with properties of . One of the most troubling\r
discrepancies involves what happens when a reward occurs earlier than expected. We\r
have seen that the omission of an expected reward produces a negative prediction error\r
at the reward’s expected time, which corresponds to the activity of dopamine neurons\r
decreasing below baseline when this happens. If the reward arrives later than expected,\r
it is then an unexpected reward and generates a positive prediction error. This happens\r
with both TD errors and dopamine neuron responses. But when reward arrives earlier\r
than expected, dopamine neurons do not do what the TD error does—at least with the\r
CSC representation used by Montague et al. (1996) and by us in our example. Dopamine

394 Chapter 15: Neuroscience\r
neurons do respond to the early reward, which is consistent with a positive TD error\r
because the reward is not predicted to occur then. However, at the later time when the\r
reward is expected but omitted, the TD error is negative whereas, in contrast to this\r
prediction, dopamine neuron activity does not drop below baseline in the way the TD\r
model predicts (Hollerman and Schultz, 1998). Something more complicated is going on\r
in the animal’s brain than simply TD learning with a CSC representation.\r
Some of the mismatches between the TD error and dopamine neuron activity can\r
be addressed by selecting suitable parameter values for the TD algorithm and by using\r
stimulus representations other than the CSC representation. For instance, to address\r
the early-reward mismatch just described, Suri and Schultz (1999) proposed a CSC\r
representation in which the sequences of internal signals initiated by earlier stimuli\r
are cancelled by the occurrence of a reward. Another proposal by Daw, Courville,\r
and Touretzky (2006) is that the brain’s TD system uses representations produced by\r
statistical modeling carried out in sensory cortex rather than simpler representations\r
based on raw sensory input. Ludvig, Sutton, and Kehoe (2008) found that TD learning\r
with a microstimulus (MS) representation (Figure 14.1) fits the activity of dopamine\r
neurons in the early-reward and other situations better than when a CSC representation\r
is used. Pan, Schmidt, Wickens, and Hyland (2005) found that even with the CSC\r
representation, prolonged eligibility traces improve the fit of the TD error to some aspects\r
of dopamine neuron activity. In general, many fine details of TD-error behavior depend\r
on subtle interactions between eligibility traces, discounting, and stimulus representations.\r
Findings like these elaborate and refine the reward prediction error hypothesis without\r
refuting its core claim that the phasic activity of dopamine neurons is well characterized\r
as signaling TD errors.\r
On the other hand, there are other discrepancies between the TD theory and exper\u0002imental data that are not so easily accommodated by selecting parameter values and\r
stimulus representations (we mention some of these discrepancies in the Bibliographical\r
and Historical Remarks section at the end of this chapter), and more mismatches are\r
likely to be discovered as neuroscientists conduct ever more refined experiments. But the\r
reward prediction error hypothesis has been functioning very e↵ectively as a catalyst for\r
improving our understanding of how the brain’s reward system works. Intricate experi\u0002ments have been designed to validate or refute predictions derived from the hypothesis,\r
and experimental results have, in turn, led to refinement and elaboration of the TD\r
error/dopamine hypothesis.\r
A remarkable aspect of these developments is that the reinforcement learning algo\u0002rithms and theory that connect so well with properties of the dopamine system were\r
developed from a computational perspective in total absence of any knowledge about the\r
relevant properties of dopamine neurons—remember, TD learning and its connections\r
to optimal control and dynamic programming were developed many years before any of\r
the experiments were conducted that revealed the TD-like nature of dopamine neuron\r
activity. This unplanned correspondence, despite not being perfect, suggests that the TD\r
error/dopamine parallel captures something significant about brain reward processes.\r
In addition to accounting for many features of the phasic activity of dopamine neurons,\r
the reward prediction error hypothesis links neuroscience to other aspects of reinforcement

15.7. Neural Actor–Critic 395\r
learning, in particular, to learning algorithms that use TD errors as reinforcement signals.\r
Neuroscience is still far from reaching complete understanding of the circuits, molecular\r
mechanisms, and functions of the phasic activity of dopamine neurons, but evidence\r
supporting the reward prediction error hypothesis, along with evidence that phasic\r
dopamine responses are reinforcement signals for learning, suggest that the brain might\r
implement something like an actor–critic algorithm in which TD errors play critical roles.\r
Other reinforcement learning algorithms are plausible candidates too, but actor–critic\r
algorithms fit the anatomy and physiology of the mammalian brain particularly well, as\r
we describe in the following two sections.\r
15.7 Neural Actor–Critic\r
Actor–critic algorithms learn both policies and value functions. The ‘actor’ is the\r
component that learns policies, and the ‘critic’ is the component that learns about\r
whatever policy is currently being followed by the actor in order to ‘criticize’ the actor’s\r
action choices. The critic uses a TD algorithm to learn the state-value function for the\r
actor’s current policy. The value function allows the critic to critique the actor’s action\r
choices by sending TD errors, , to the actor. A positive  means that the action was\r
‘good’ because it led to a state with a better-than-expected value; a negative  means\r
that the action was ‘bad’ because it led to a state with a worse-than-expected value.\r
Based on these critiques, the actor continually updates its policy.\r
Two distinctive features of actor–critic algorithms are responsible for thinking that the\r
brain might implement an algorithm like this. First, the two components of an actor–critic\r
algorithm—the actor and the critic—suggest that two parts of the striatum—the dorsal\r
and ventral subdivisions (Section 15.4), both critical for reward-based learning—may\r
function respectively something like an actor and a critic. A second property of actor–\r
critic algorithms that suggests a brain implementation is that the TD error has the dual\r
role of being the reinforcement signal for both the actor and the critic, though it has a\r
di↵erent influence on learning in each of these components. This fits well with several\r
properties of the neural circuitry: axons of dopamine neurons target both the dorsal\r
and ventral subdivisions of the striatum; dopamine appears to be critical for modulating\r
synaptic plasticity in both structures; and how a neuromodulator such as dopamine\r
acts on a target structure depends on properties of the target structure and not just on\r
properties of the neuromodulator.\r
Section 13.5 presents actor–critic algorithms as policy gradient methods, but the actor–\r
critic algorithm of Barto, Sutton, and Anderson (1983) was simpler and was presented as\r
an artificial neural network (ANN). Here we describe an ANN implementation something\r
like that of Barto et al., and we follow Takahashi, Schoenbaum, and Niv (2008) in giving\r
a schematic proposal for how this ANN might be implemented by real neural networks in\r
the brain. We postpone discussion of the actor and critic learning rules until Section 15.8,\r
where we present them as special cases of the policy-gradient formulation and discuss\r
what they suggest about how dopamine might modulate synaptic plasticity.

396 Chapter 15: Neuroscience\r
Figure 15.5a shows an implementation of an actor–critic algorithm as an ANN with\r
component networks implementing the actor and the critic. The critic consists of a single\r
neuron-like unit, V , whose output activity represents state values, and a component\r
shown as the diamond labeled TD that computes TD errors by combining V ’s output\r
with reward signals and with previous state values (as suggested by the loop from the\r
TD diamond to itself). The actor network has a single layer of k actor units labeled Ai,\r
i = 1,...,k. The output of each actor unit is a component of a k-dimensional action\r
vector. An alternative is that there are k separate actions, one commanded by each actor\r
unit, that compete with one another to be executed, but here we will think of the entire\r
A-vector as an action.\r
Reward\r
Actor\r
. TD\r
. \r
.\r
. \r
. \r
.\r
Critic\r
. \r
. \r
.\r
δ\r
Actions\r
States/Stimuli\r
TD error\r
Environment\r
Actor\r
VTA\r
SNc\r
S2\r
S1\r
Sn\r
. \r
. \r
.\r
. \r
. \r
.\r
Critic\r
. \r
. \r
.\r
S1\r
Sn\r
S2\r
States/\r
Actions\r
Stimuli\r
Dopamine\r
Ventral\r
striatum Dorsal striatum\r
Cortex (multiple areas)\r
Environment\r
Reward\r
1\r
2\r
n\r
1\r
2\r
n\r
A1\r
A2\r
A3\r
Ak\r
V\r
\r
(a) (b)\r
x1\r
x2\r
xn\r
x1\r
x2\r
xn\r
Figure 15.5: Actor–critic ANN and a hypothetical neural implementation. a) Actor–critic\r
algorithm as an ANN. The actor adjusts a policy based on the TD error  it receives from the\r
critic; the critic adjusts state-value parameters using the same . The critic produces a TD error\r
from the reward signal, R, and the current change in its estimate of state values. The actor does\r
not have direct access to the reward signal, and the critic does not have direct access to the\r
action. b) Hypothetical neural implementation of an actor–critic algorithm. The actor and the\r
value-learning part of the critic are respectively placed in the dorsal and ventral subdivisions\r
of the striatum. The TD error is transmitted by dopamine neurons located in the VTA and\r
SNpc to modulate changes in synaptic ecacies of input from cortical areas to the ventral and\r
dorsal striatum. Adapted from Frontiers in Neuroscience, vol. 2(1), 2008, Y. Takahashi, G.\r
Schoenbaum, and Y. Niv, Silencing the critics: Understanding the e↵ects of cocaine sensitization\r
on dorsolateral and ventral striatum in the context of an Actor/Critic model.

15.7. Neural Actor–Critic 397\r
Both the critic and actor networks receive input consisting of multiple features repre\u0002senting the state of the agent’s environment. (Recall from Chapter 1 that the environment\r
of a reinforcement learning agent includes components both inside and outside of the\r
‘organism’ containing the agent.) The figure shows these features as the circles labeled\r
x1, x2,...,xn, shown twice just to keep the figure simple. A weight representing the\r
ecacy of a synapse is associated with each connection from each feature xi to the\r
critic unit, V , and to each of the action units, Ai. The weights in the critic network\r
parameterize the value function, and the weights in the actor network parameterize the\r
policy. The networks learn as these weights change according to the critic and actor\r
learning rules that we describe in the following section.\r
The TD error produced by circuitry in the critic is the reinforcement signal for changing\r
the weights in both the critic and the actor networks. This is shown in Figure 15.5a by\r
the line labeled ‘TD error ’ extending across all of the connections in the critic and\r
actor networks. This aspect of the network implementation, together with the reward\r
prediction error hypothesis and the fact that the activity of dopamine neurons is so\r
widely distributed by the extensive axonal arbors of these neurons, suggests that an\r
actor–critic network something like this may not be too farfetched as a hypothesis about\r
how reward-related learning might happen in the brain.\r
Figure 15.5b suggests—very schematically—how the ANN on the figure’s left might\r
map onto structures in the brain according to the hypothesis of Takahashi et al. (2008).\r
The hypothesis puts the actor and the value-learning part of the critic respectively in the\r
dorsal and ventral subdivisions of the striatum, the input structure of the basal ganglia.\r
Recall from Section 15.4 that the dorsal striatum is primarily implicated in influencing\r
action selection, and the ventral striatum is thought to be critical for di↵erent aspects of\r
reward processing, including the assignment of a↵ective value to sensations. The cerebral\r
cortex, along with other structures, sends input to the striatum conveying information\r
about stimuli, internal states, and motor activity.\r
In this hypothetical actor–critic brain implementation, the ventral striatum sends value\r
information to the VTA and SNpc, where dopamine neurons in these nuclei combine it\r
with information about reward to generate activity corresponding to TD errors (though\r
exactly how dopaminergic neurons calculate these errors is not yet understood). The ‘TD\r
error ’ line in Figure 15.5a becomes the line labeled ‘Dopamine’ in Figure 15.5b, which\r
represents the widely branching axons of dopamine neurons whose cell bodies are in the\r
VTA and SNpc. Referring back to Figure 15.1, these axons make synaptic contact with\r
the spines on the dendrites of medium spiny neurons, the main input/output neurons of\r
both the dorsal and ventral divisions of the striatum. Axons of the cortical neurons that\r
send input to the striatum make synaptic contact on the tips of these spines. According\r
to the hypothesis, it is at these spines where changes in the ecacies of the synapses\r
from cortical regions to the striatum are governed by learning rules that critically depend\r
on a reinforcement signal supplied by dopamine.\r
An important implication of the hypothesis illustrated in Figure 15.5b is that the\r
dopamine signal is not the ‘master’ reward signal like the scalar Rt of reinforcement\r
learning. In fact, the hypothesis implies that one should not necessarily be able to\r
probe the brain and record any signal like Rt in the activity of any single neuron.

398 Chapter 15: Neuroscience\r
Many interconnected neural systems generate reward-related information, with di↵erent\r
structures being recruited depending on di↵erent types of rewards. Dopamine neurons\r
receive information from many di↵erent brain areas, so the input to the SNpc and\r
VTA labeled ‘Reward’ in Figure 15.5b should be thought of as vector of reward-related\r
information arriving to neurons in these nuclei along multiple input channels. What the\r
theoretical scalar reward signal Rt might correspond to, then, is the net contribution of\r
all reward-related information to dopamine neuron activity. It is the result of a pattern\r
of activity across many neurons in di↵erent areas of the brain.\r
Although the actor–critic neural implementation illustrated in Figure 15.5b may be\r
correct on some counts, it clearly needs to be refined, extended, and modified to qualify\r
as a full-fledged model of the function of the phasic activity of dopamine neurons. The\r
Historical and Bibliographic Remarks section at the end of this chapter cites publications\r
that discuss in more detail both empirical support for this hypothesis and places where it\r
falls short. We now look in detail at what the actor and critic learning algorithms suggest\r
about the rules governing changes in synaptic ecacies of corticostriatal synapses.\r
15.8 Actor and Critic Learning Rules\r
If the brain does implement something like the actor–critic algorithm—and assuming\r
populations of dopamine neurons broadcast a common reinforcement signal to the corti\u0002costriatal synapses of both the dorsal and ventral striatum as illustrated in Figure 15.5b\r
(which is likely an oversimplification as we mentioned above)—then this reinforcement\r
signal a↵ects the synapses of these two structures in di↵erent ways. The learning rules for\r
the critic and the actor use the same reinforcement signal, the TD error , but its e↵ect\r
on learning is di↵erent for these two components. The TD error (combined with eligibility\r
traces) tells the actor how to update action probabilities in order to reach higher-valued\r
states. Learning by the actor is like instrumental conditioning using a Law-of-E↵ect-type\r
learning rule (Section 1.7): the actor works to keep  as positive as possible. On the\r
other hand, the TD error (when combined with eligibility traces) tells the critic the\r
direction and magnitude in which to change the parameters of the value function in order\r
to improve its predictive accuracy. The critic works to reduce ’s magnitude to be as\r
close to zero as possible using a learning rule like the TD model of classical conditioning\r
(Section 14.2). The di↵erence between the critic and actor learning rules is relatively\r
simple, but this di↵erence has a profound e↵ect on learning and is essential to how the\r
actor–critic algorithm works. The di↵erence lies solely in the eligibility traces each type\r
of learning rule uses.\r
More than one set of learning rules can be used in actor–critic neural networks like\r
those in Figure 15.5b but, to be specific, here we focus on the actor–critic algorithm for\r
continuing problems with eligibility traces presented in Section 13.6. On each transition\r
from state St to state St+1, taking action At and receiving reward Rt+1, that algorithm\r
computes the TD error () and then updates the eligibility trace vectors (zw\r
t and z✓t ) and

15.8. Actor and Critic Learning Rules 399\r
the parameters for the critic and actor (w and ✓), according to\r
t = Rt+1 +  vˆ(St+1,w)  vˆ(St,w),\r
zw\r
t = wzwt1 + rvˆ(St,w),\r
z✓\r
t = ✓z✓t1 + r ln ⇡(At|St, ✓),\r
w w + ↵w t zw\r
t ,\r
✓ ✓ + ↵✓ z✓\r
t ,\r
where  2 [0, 1) is a discount-rate parameter, w 2 [0, 1] and ✓ 2 [0, 1] are bootstrapping\r
parameters for the critic and the actor respectively, and ↵w > 0 and ↵✓ > 0 are analogous\r
step-size parameters.\r
Think of the approximate value function vˆ as the output of a single linear neuron-like\r
unit, called the critic unit and labeled V in Figure 15.5a. Then the value function is a\r
linear function of the feature-vector representation of state s, x(s)=(x1(s),...,xn(s))>,\r
parameterized by a weight vector w = (w1,...,wn)>:\r
vˆ(s,w) = w>x(s). (15.1)\r
Each xi(s) is like the presynaptic signal to a neuron’s synapse whose ecacy is wi. The\r
weights of the critic are incremented according to the rule above by ↵wtzw\r
t , where the\r
reinforcement signal, t, corresponds to a dopamine signal being broadcast to all of the\r
critic unit’s synapses. The eligibility trace vector, zw\r
t , for the critic unit is a trace (average\r
of recent values) of rvˆ(St,w). Because vˆ(s,w) is linear in the weights, rvˆ(St,w) = x(St).\r
In neural terms, this means that each synapse has its own eligibility trace, which is\r
one component of the vector zw\r
t . A synapse’s eligibility trace accumulates according to\r
the level of activity arriving at that synapse, that is, the level of presynaptic activity,\r
represented here by the component of the feature vector x(St) arriving at that synapse.\r
The trace otherwise decays toward zero at a rate governed by the fraction w. A synapse\r
is eligible for modification as long as its eligibility trace is non-zero. How the synapse’s\r
ecacy is actually modified depends on the reinforcement signals that arrive while the\r
synapse is eligible. We call eligibility traces like these of the critic unit’s synapses non\u0002contingent eligibility traces because they only depend on presynaptic activity and are not\r
contingent in any way on postsynaptic activity.\r
The non-contingent eligibility traces of the critic unit’s synapses mean that the critic\r
unit’s learning rule is essentially the TD model of classical conditioning described in\r
Section 14.2. With the definition we have given above of the critic unit and its learning\r
rule, the critic in Figure 15.5a is the same as the critic in the ANN actor–critic of Barto\r
et al. (1983). Clearly, a critic like this consisting of just one linear neuron-like unit is the\r
simplest starting point; this critic unit is a proxy for a more complicated neural network\r
able to learn value functions of greater complexity.\r
The actor in Figure 15.5a is a one-layer network of k neuron-like actor units, each\r
receiving at time t the same feature vector, x(St), that the critic unit receives. Each\r
actor unit j, j = 1,...,k, has its own weight vector, ✓j , but because the actor units are\r
all identical, we describe just one of the units and omit the subscript. One way for these

400 Chapter 15: Neuroscience\r
units to follow the actor–critic algorithm given in the equations above is for each to be a\r
Bernoulli-logistic unit. This means that the output of each actor unit at each time is\r
a random variable, At, taking value 0 or 1. Think of value 1 as the neuron firing, that\r
is, emitting an action potential. The weighted sum, ✓>x(St), of a unit’s input vector\r
determines the unit’s action probabilities via the exponential soft-max distribution (13.2),\r
which for two actions is the logistic function:\r
⇡(1|s, ✓)=1  ⇡(0|s, ✓) = 1\r
1 + exp(✓>x(s)). (15.2)\r
The weights of each actor unit are incremented, as above, by: ✓ ✓ + ↵✓ t z✓\r
t , where\r
 again corresponds to the dopamine signal: the same reinforcement signal that is sent to\r
all the critic unit’s synapses. Figure 15.5a shows t being broadcast to all the synapses\r
of all the actor units (which makes this actor network a team of reinforcement learning\r
agents, something we discuss in Section 15.10 below). The actor eligibility trace vector\r
z✓\r
t is a trace (average of recent values) of r ln ⇡(At|St, ✓). To understand this eligibility\r
trace refer to Exercise 13.5, which defines this kind of unit and asks you to give a learning\r
rule for it. That exercise asked you to express r ln ⇡(a|s, ✓) in terms of a, x(s), and\r
⇡(a|s, ✓) (for arbitrary state s and action a) by calculating the gradient. For the action\r
and state actually occurring at time t, the answer is\r
r ln ⇡(At|St, ✓) = At  ⇡(1|St, ✓)\r
\r
x(St). (15.3)\r
Unlike the non-contingent eligibility trace of a critic synapse that only accumulates\r
the presynaptic activity x(St), the eligibility trace of an actor unit’s synapse in addition\r
depends on the activity of the actor unit itself. We call this a contingent eligibility\r
trace because it is contingent on this postsynaptic activity. The eligibility trace at each\r
synapse continually decays, but increments or decrements depending on the activity of\r
the presynaptic neuron and whether or not the postsynaptic neuron fires. The factor\r
At ⇡(1|St, ✓) in (15.3) is positive when At = 1 and negative otherwise. The postsynaptic\r
contingency in the eligibility traces of actor units is the only di↵erence between the critic\r
and actor learning rules. By keeping information about what actions were taken in\r
what states, contingent eligibility traces allow credit for reward (positive ), or blame for\r
punishment (negative ), to be apportioned among the policy parameters (the ecacies\r
of the actor units’ synapses) according to the contributions these parameters made to the\r
units’ outputs that could have influenced later values of . Contingent eligibility traces\r
mark the synapses as to how they should be modified to alter the units’ future responses\r
to favor positive values of .\r
What do the critic and actor learning rules suggest about how ecacies of corticostriatal\r
synapses change? Both learning rules are related to Donald Hebb’s classic proposal that\r
whenever a presynaptic signal participates in activating the postsynaptic neuron, the\r
synapse’s ecacy increases (Hebb, 1949). The critic and actor learning rules share with\r
Hebb’s proposal the idea that changes in a synapse’s ecacy depend on the interaction\r
of several factors. In the critic learning rule the interaction is between the reinforcement\r
signal  and eligibility traces that depend only on presynaptic signals. Neuroscientists\r
call this a two-factor learning rule because the interaction is between two signals or

15.8. Actor and Critic Learning Rules 401\r
quantities. The actor learning rule, on the other hand, is a three-factor learning rule\r
because, in addition to depending on , its eligibility traces depend on both presynaptic\r
and postsynaptic activity. Unlike Hebb’s proposal, however, the relative timing of the\r
factors is critical to how synaptic ecacies change, with eligibility traces intervening to\r
allow the reinforcement signal to a↵ect synapses that were active in the recent past.\r
Some subtleties about signal timing for the actor and critic learning rules deserve closer\r
attention. In defining the neuron-like actor and critic units, we ignored the small amount\r
of time it takes synaptic input to e↵ect the firing of a real neuron. When an action\r
potential from the presynaptic neuron arrives at a synapse, neurotransmitter molecules\r
are released that di↵use across the synaptic cleft to the postsynaptic neuron, where\r
they bind to receptors on the postsynaptic neuron’s surface; this activates molecular\r
machinery that causes the postsynaptic neuron to fire (or to inhibit its firing in the case of\r
inhibitory synaptic input). This process can take several tens of milliseconds. According\r
to (15.1) and (15.2), though, the input to a critic and actor unit instantaneously produces\r
the unit’s output. Ignoring activation time like this is common in abstract models of\r
Hebbian-style plasticity in which synaptic ecacies change according to a simple product\r
of simultaneous pre- and postsynaptic activity. More realistic models must take activation\r
time into account.\r
Activation time is especially important for a more realistic actor unit because it\r
influences how contingent eligibility traces have to work in order to properly apportion\r
credit for reinforcement to the appropriate synapses. The expression At⇡(1|St, ✓)\r
\r
x(St)\r
defining contingent eligibility traces for the actor unit’s learning rule given above includes\r
the postsynaptic factor At  ⇡(1|St, ✓)\r
\r
and the presynaptic factor x(St). This works\r
because by ignoring activation time, the presynaptic activity x(St) participates in causing\r
the postsynaptic activity appearing in At⇡(1|St, ✓)\r
\r
. To assign credit for reinforcement\r
correctly, the presynaptic factor defining the eligibility trace must be a cause of the\r
postsynaptic factor that also defines the trace. Contingent eligibility traces for a more\r
realistic actor unit would have to take activation time into account. (Activation time\r
should not be confused with the time required for a neuron to receive a reinforcement\r
signal influenced by that neuron’s activity. The function of eligibility traces is to span\r
this time interval which is generally much longer than the activation time. We discuss\r
this further in the following section.)\r
There are hints from neuroscience for how this process might work in the brain.\r
Neuroscientists have discovered a form of Hebbian plasticity called spike-timing-dependent\r
plasticity (STDP) that lends plausibility to the existence of actor-like synaptic plasticity\r
in the brain. STDP is a Hebbian-style plasticity, but changes in a synapse’s ecacy\r
depend on the relative timing of presynaptic and postsynaptic action potentials. The\r
dependence can take di↵erent forms, but in the one most studied, a synapse increases in\r
strength if spikes incoming via that synapse arrive shortly before the postsynaptic neuron\r
fires. If the timing relation is reversed, with a presynaptic spike arriving shortly after the\r
postsynaptic neuron fires, then the strength of the synapse decreases. STDP is a type of\r
Hebbian plasticity that takes the activation time of a neuron into account, which is one\r
of the ingredients needed for actor-like learning.

402 Chapter 15: Neuroscience\r
The discovery of STDP has led neuroscientists to investigate the possibility of a three\u0002factor form of STDP in which neuromodulatory input must follow appropriately-timed\r
pre- and postsynaptic spikes. This form of synaptic plasticity, called reward-modulated\r
STDP, is much like the actor learning rule discussed here. Synaptic changes that would\r
be produced by regular STDP only occur if there is neuromodulatory input within a time\r
window after a presynaptic spike is closely followed by a postsynaptic spike. Evidence\r
is accumulating that reward-modulated STDP occurs at the spines of medium spiny\r
neurons of the dorsal striatum, with dopamine providing the neuromodulatory factor—\r
the sites where actor learning takes place in the hypothetical neural implementation of\r
an actor–critic algorithm illustrated in Figure 15.5b. Experiments have demonstrated\r
reward-modulated STDP in which lasting changes in the ecacies of corticostriatal\r
synapses occur only if a neuromodulatory pulse arrives within a time window that can\r
last up to 10 seconds after a presynaptic spike is closely followed by a postsynaptic\r
spike (Yagishita et al. 2014). Although the evidence is indirect, these experiments point\r
to the existence of contingent eligibility traces having prolonged time courses. The\r
molecular mechanisms producing these traces, as well as the much shorter traces that\r
likely underly STDP, are not yet understood, but research focusing on time-dependent\r
and neuromodulator-dependent synaptic plasticity is continuing.\r
The neuron-like actor unit that we have described here, with its Law-of-E↵ect-style\r
learning rule, appeared in somewhat simpler form in the actor–critic network of Barto et\r
al. (1983). That network was inspired by the “hedonistic neuron” hypothesis proposed\r
by physiologist A. H. Klopf (1972, 1982). Not all the details of Klopf’s hypothesis are\r
consistent with what has been learned about synaptic plasticity, but the discovery of\r
STDP and the growing evidence for a reward-modulated form of STDP suggest that\r
Klopf’s ideas may not have been far o↵ the mark. We discuss Klopf’s hedonistic neuron\r
hypothesis next.\r
15.9 Hedonistic Neurons\r
In his hedonistic neuron hypothesis, Klopf (1972, 1982) conjectured that individual\r
neurons seek to maximize the di↵erence between synaptic input treated as rewarding\r
and synaptic input treated as punishing by adjusting the ecacies of their synapses\r
on the basis of rewarding or punishing consequences of their own action potentials. In\r
other words, individual neurons can be trained with response-contingent reinforcement\r
like an animal can be trained in an instrumental conditioning task. His hypothesis\r
included the idea that rewards and punishments are conveyed to a neuron via the same\r
synaptic input that excites or inhibits the neuron’s spike-generating activity. (Had Klopf\r
known what we know today about neuromodulatory systems, he might have assigned the\r
reinforcing role to neuromodulatory input, but he wanted to avoid any centralized source\r
of training information.) Synaptically-local traces of past pre- and postsynaptic activity\r
had the key function in Klopf’s hypothesis of making synapses eligible—the term he\r
introduced—for modification by later reward or punishment. He conjectured that these\r
traces are implemented by molecular mechanisms local to each synapse and therefore\r
di↵erent from the electrical activity of both the pre- and the postsynaptic neurons. In

15.9. Hedonistic Neurons 403\r
the Bibliographical and Historical Remarks section of this chapter we bring attention to\r
some similar proposals made by others.\r
Klopf specifically conjectured that synaptic ecacies change in the following way. When\r
a neuron fires an action potential, all of its synapses that were active in contributing to\r
that action potential become eligible to undergo changes in their ecacies. If the action\r
potential is followed within an appropriate time period by an increase of reward, the\r
ecacies of all the eligible synapses increase. Symmetrically, if the action potential is\r
followed within an appropriate time period by an increase of punishment, the ecacies\r
of eligible synapses decrease. This is implemented by triggering an eligibility trace at a\r
synapse upon a coincidence of presynaptic and postsynaptic activity (or more exactly,\r
upon pairing of presynaptic activity with the postsynaptic activity that that presynaptic\r
activity participates in causing)—what we call a contingent eligibility trace. This is\r
essentially the three-factor learning rule of an actor unit described in the previous section.\r
The shape and time course of an eligibility trace in Klopf’s theory reflects the durations\r
of the many feedback loops in which the neuron is embedded, some of which lie entirely\r
within the brain and body of the organism, while others extend out through the organism’s\r
external environment as mediated by its motor and sensory systems. His idea was that\r
the shape of a synaptic eligibility trace is like a histogram of the durations of the feedback\r
loops in which the neuron is embedded. The peak of an eligibility trace would then occur\r
at the duration of the most prevalent feedback loops in which that neuron participates.\r
The eligibility traces used by algorithms described in this book are simplified versions\r
of Klopf’s original idea, being exponentially (or geometrically) decreasing functions\r
controlled by the parameters  and . This simplifies simulations as well as theory, but\r
we regard these simple eligibility traces as a placeholders for traces closer to Klopf’s\r
original conception, which would have computational advantages in complex reinforcement\r
learning systems by refining the credit-assignment process.\r
Klopf’s hedonistic neuron hypothesis is not as implausible as it may at first appear.\r
A well-studied example of a single cell that seeks some stimuli and avoids others is the\r
bacterium Escherichia coli. The movement of this single-cell organism is influenced by\r
chemical stimuli in its environment, behavior known as chemotaxis. It swims in its liquid\r
environment by rotating hairlike structures called flagella attached to its surface. (Yes,\r
it rotates them!) Molecules in the bacterium’s environment bind to receptors on its\r
surface. Binding events modulate the frequency with which the bacterium reverses flagellar\r
rotation. Each reversal causes the bacterium to tumble in place and then head o↵ in a\r
random new direction. A little chemical memory and computation causes the frequency\r
of flagellar reversal to decrease when the bacterium swims toward higher concentrations\r
of molecules it needs to survive (attractants) and increase when the bacterium swims\r
toward higher concentrations of molecules that are harmful (repellants). The result is\r
that the bacterium tends to persist in swimming up attractant gradients and tends to\r
avoid swimming up repellant gradients.\r
The chemotactic behavior just described is called klinokinesis. It is a kind of trial\u0002and-error behavior, although it is unlikely that learning is involved: the bacterium needs\r
a modicum of short-term memory to detect molecular concentration gradients, but it\r
probably does not maintain long-term memories. Artificial intelligence pioneer Oliver

404 Chapter 15: Neuroscience\r
Selfridge called this strategy “run and twiddle,” pointing out its utility as a basic adaptive\r
strategy: “keep going in the same way if things are getting better, and otherwise move\r
around” (Selfridge, 1978, 1984). Similarly, one might think of a neuron “swimming” (not\r
literally of course) in a medium composed of the complex collection of feedback loops\r
in which it is embedded, acting to obtain one type of input signal and to avoid others.\r
Unlike the bacterium, however, the neuron’s synaptic strengths retain information about\r
its past trial-and-error behavior. If this view of the behavior of a neuron (or just one type\r
of neuron) is plausible, then the closed-loop nature of how the neuron interacts with its\r
environment is important for understanding its behavior, where the neuron’s environment\r
consists of the rest of the animal together with the environment with which the animal\r
as a whole interacts.\r
Klopf’s hedonistic neuron hypothesis extended beyond the idea that individual neurons\r
are reinforcement learning agents. He argued that many aspects of intelligent behavior\r
can be understood as the result of the collective behavior of a population of self-interested\r
hedonistic neurons interacting with one another in an immense society or economic system\r
making up an animal’s nervous system. Whether or not this view of nervous systems\r
is useful, the collective behavior of reinforcement learning agents has implications for\r
neuroscience. We take up this subject next.\r
15.10 Collective Reinforcement Learning\r
The behavior of populations of reinforcement learning agents is deeply relevant to the\r
study of social and economic systems, and if anything like Klopf’s hedonistic neuron\r
hypothesis is correct, to neuroscience as well. The hypothesis described above about how\r
an actor–critic algorithm might be implemented in the brain only narrowly addresses\r
the implications of the fact that the dorsal and ventral subdivisions of the striatum, the\r
respective locations of the actor and the critic according to the hypothesis, each contain\r
millions of medium spiny neurons whose synapses undergo change modulated by phasic\r
bursts of dopamine neuron activity.\r
The actor in Figure 15.5a is a single-layer network of k actor units. The actions\r
produced by this network are vectors (A1, A2, ··· , Ak)> presumed to drive the animal’s\r
behavior. Changes in the ecacies of the synapses of all of these units depend on the\r
reinforcement signal . Because actor units attempt to make  as large as possible, \r
e↵ectively acts as a reward signal for them (so in this case reinforcement is the same\r
as reward). Thus, each actor unit is itself a reinforcement learning agent—a hedonistic\r
neuron if you will. Now, to make the situation as simple as possible, assume that each\r
of these units receives the same reward signal at the same time (although, as indicated\r
above, the assumption that dopamine is released at all the corticostriatal synapses under\r
the same conditions and at the same times is likely an oversimplification).\r
What can reinforcement learning theory tell us about what happens when all members\r
of a population of reinforcement learning agents learn according to a common reward\r
signal? The field of multi-agent reinforcement learning considers many aspects of learning\r
by populations of reinforcement learning agents. Although this field is beyond the scope\r
of this book, we believe that some of its basic concepts and results are relevant to thinking

15.10. Collective Reinforcement Learning 405\r
about the brain’s di↵use neuromodulatory systems. In multi-agent reinforcement learning\r
(and in game theory), the scenario in which all the agents try to maximize a common\r
reward signal that they simultaneously receive is known as a cooperative game or a team\r
problem.\r
What makes a team problem interesting and challenging is that the common reward\r
signal sent to each agent evaluates the pattern of activity produced by the entire population,\r
that is, it evaluates the collective action of the team members. This means that any\r
individual agent has only limited ability to a↵ect the reward signal because any single\r
agent contributes just one component of the collective action evaluated by the common\r
reward signal. E↵ective learning in this scenario requires addressing a structural credit\r
assignment problem: which team members, or groups of team members, deserve credit for\r
a favorable reward signal, or blame for an unfavorable reward signal? It is a cooperative\r
game, or a team problem, because the agents are united in seeking to increase the same\r
reward signal: there are no conflicts of interest among the agents. The scenario would be\r
a competitive game if di↵erent agents receive di↵erent reward signals, where each reward\r
signal again evaluates the collective action of the population, and the objective of each\r
agent is to increase its own reward signal. In this case there might be conflicts of interest\r
among the agents, meaning that actions that are good for some agents are bad for others.\r
Even deciding what the best collective action should be is a non-trivial aspect of game\r
theory. This competitive setting might be relevant to neuroscience too (for example, to\r
account for heterogeneity of dopamine neuron activity), but here we focus only on the\r
cooperative, or team, case.\r
How can each reinforcement learning agent in a team learn to “do the right thing” so\r
that the collective action of the team is highly rewarded? An interesting result is that\r
if each agent can learn e↵ectively despite its reward signal being corrupted by a large\r
amount of noise, and despite its lack of access to complete state information, then the\r
population as a whole will learn to produce collective actions that improve as evaluated by\r
the common reward signal, even when the agents cannot communicate with one another.\r
Each agent faces its own reinforcement learning task in which its influence on the reward\r
signal is deeply buried in the noise created by the influences of other agents. In fact, for\r
any agent, all the other agents are part of its environment because its input, both the part\r
conveying state information and the reward part, depends on how all the other agents\r
are behaving. Furthermore, lacking access to the actions of the other agents, indeed\r
lacking access to the parameters determining their policies, each agent can only partially\r
observe the state of its environment. This makes each team member’s learning task very\r
dicult, but if each uses a reinforcement learning algorithm able to increase a reward\r
signal even under these dicult conditions, teams of reinforcement learning agents can\r
learn to produce collective actions that improve over time as evaluated by the team’s\r
common reward signal.\r
If the team members are neuron-like units, then each unit has to have the goal of\r
increasing the amount of reward it receives over time, as the actor unit does that we\r
described in Section 15.8. Each unit’s learning algorithm has to have two essential features.\r
First, it has to use contingent eligibility traces. Recall that a contingent eligibility trace,\r
in neural terms, is initiated (or increased) at a synapse when its presynaptic input

406 Chapter 15: Neuroscience\r
participates in causing the postsynaptic neuron to fire. A non-contingent eligibility trace,\r
in contrast, is initiated or increased by presynaptic input independently of what the\r
postsynaptic neuron does. As explained in Section 15.8, by keeping information about\r
what actions were taken in what states, contingent eligibility traces allow credit for\r
reward, or blame for punishment, to be apportioned to an agent’s policy parameters\r
according to the contribution the values of these parameters made in determining the\r
agent’s action. By similar reasoning, a team member must remember its recent action so\r
that it can either increase or decrease the likelihood of producing that action according\r
to the reward signal that is subsequently received. The action component of a contingent\r
eligibility trace implements this action memory. Because of the complexity of the learning\r
task, however, contingent eligibility is merely a preliminary step in the credit assignment\r
process: the relationship between a single team member’s action and changes in the\r
team’s reward signal is a statistical correlation that has to be estimated over many trials.\r
Contingent eligibility is an essential but preliminary step in this process.\r
Learning with non-contingent eligibility traces does not work at all in the team setting\r
because it does not provide a way to correlate actions with consequent changes in the\r
reward signal. Non-contingent eligibility traces are adequate for learning to predict, as\r
the critic component of the actor–critic algorithm does, but they do not support learning\r
to control, as the actor component must do. The members of a population of critic-like\r
agents may still receive a common reinforcement signal, but they would all learn to\r
predict the same quantity (which in the case of an actor–critic method, would be the\r
expected return for the current policy). How successful each member of the population\r
would be in learning to predict the expected return would depend on the information it\r
receives, which could be very di↵erent for di↵erent members of the population. There\r
would be no need for the population to produce di↵erentiated patterns of activity. This\r
is not a team problem as defined here.\r
A second requirement for collective learning in a team problem is that there has to be\r
variability in the actions of the team members in order for the team to explore the space\r
of collective actions. The simplest way for a team of reinforcement learning agents to do\r
this is for each member to independently explore its own action space through persistent\r
variability in its output. This will cause the team as a whole to vary its collective actions.\r
For example, a team of the actor units described in Section 15.8 explores the space\r
of collective actions because the output of each unit, being a Bernoulli-logistic unit,\r
probabilistically depends on the weighted sum of its input vector’s components. The\r
weighted sum biases firing probability up or down, but there is always variability. Because\r
each unit uses a REINFORCE policy gradient algorithm (Chapter 13), each unit adjusts\r
its weights with the goal of maximizing the average reward rate it experiences while\r
stochastically exploring its own action space. One can show, as Williams (1992) did, that\r
a team of Bernoulli-logistic REINFORCE units implements a policy gradient algorithm\r
as a whole with respect to average rate of the team’s common reward signal, where the\r
actions are the collective actions of the team.\r
Further, Williams (1992) showed that a team of Bernoulli-logistic units using REIN\u0002FORCE ascends the average reward gradient when the units in the team are interconnected\r
to form a multilayer ANN. In this case, the reward signal is broadcast to all the units in

15.11. Model-based Methods in the Brain 407\r
the network, though reward may depend only on the collective actions of the network’s\r
output units. This means that a multilayer team of Bernoulli-logistic REINFORCE\r
units learns like a multilayer network trained by the widely-used error backpropagation\r
method, but in this case the backpropagation process is replaced by the broadcasted\r
reward signal. In practice, the error backpropagation method is considerably faster,\r
but the reinforcement learning team method is more plausible as a neural mechanism,\r
especially in light of what is being learned about reward-modulated STDP as discussed\r
in Section 15.8.\r
Exploration through independent exploration by team members is only the simplest\r
way for a team to explore; more sophisticated methods are possible if the team members\r
coordinate their actions to focus on particular parts of the collective action space, either\r
by communicating with one another or by responding to common inputs. There are also\r
mechanisms more sophisticated than contingent eligibility traces for addressing structural\r
credit assignment, which is easier in a team problem when the set of possible collective\r
actions is restricted in some way. An extreme case is a winner-take-all arrangement (for\r
example, the result of lateral inhibition in the brain) that restricts collective actions to\r
those to which only one, or a few, team members contribute. In this case the winners get\r
the credit or blame for resulting reward or punishment.\r
Details of learning in cooperative games (or team problems) and non-cooperative\r
game problems are beyond the scope of this book. The Bibliographical and Historical\r
Remarks section at the end of this chapter cites a selection of the relevant publications,\r
including extensive references to research on implications for neuroscience of collective\r
reinforcement learning.\r
15.11 Model-based Methods in the Brain\r
Reinforcement learning’s distinction between model-free and model-based algorithms is\r
proving to be useful for thinking about animal learning and decision processes. Section 14.6\r
discusses how this distinction aligns with that between habitual and goal-directed animal\r
behavior. The hypothesis discussed above about how the brain might implement an\r
actor–critic algorithm is relevant only to an animal’s habitual mode of behavior because\r
the basic actor–critic method is model-free. What neural mechanisms are responsible\r
for producing goal-directed behavior, and how do they interact with those underlying\r
habitual behavior?\r
One way to investigate questions about the brain structures involved in these modes\r
of behavior is to inactivate an area of a rat’s brain and then observe what the rat does in\r
an outcome-devaluation experiment (Section 14.6). Results from experiments like these\r
indicate that the actor–critic hypothesis described above is too simple in placing the\r
actor in the dorsal striatum. Inactivating one part of the dorsal striatum, the dorsolateral\r
striatum (DLS), impairs habit learning, causing the animal to rely more on goal-directed\r
processes. On the other hand, inactivating the dorsomedial striatum (DMS) impairs\r
goal-directed processes, requiring the animal to rely more on habit learning. Results\r
like these support the view that the DLS in rodents is more involved in model-free\r
processes, whereas their DMS is more involved in model-based processes. Results of

408 Chapter 15: Neuroscience\r
studies with human subjects in similar experiments using functional neuroimaging, and\r
with non-human primates, support the view that the analogous structures in the primate\r
brain are di↵erentially involved in habitual and goal-directed modes of behavior.\r
Other studies identify activity associated with model-based processes in the prefrontal\r
cortex of the human brain, the front-most part of the frontal cortex implicated in\r
executive function, including planning and decision making. Specifically implicated is the\r
orbitofrontal cortex (OFC), the part of the prefrontal cortex immediately above the eyes.\r
Functional neuroimaging in humans, and also recordings of the activities of single neurons\r
in monkeys, reveals strong activity in the OFC related to the subjective reward value\r
of biologically significant stimuli, as well as activity related to the reward expected as a\r
consequence of actions. Although not free of controversy, these results suggest significant\r
involvement of the OFC in goal-directed choice. It may be critical for the reward part of\r
an animal’s environment model.\r
Another structure involved in model-based behavior is the hippocampus, a structure\r
critical for memory and spatial navigation. A rat’s hippocampus plays a critical role\r
in the rat’s ability to navigate a maze in the goal-directed manner that led Tolman to\r
the idea that animals use models, or cognitive maps, in selecting actions (Section 14.5).\r
The hippocampus may also be a critical component of our human ability to imagine\r
new experiences (Hassabis and Maguire, 2007; Olafsd´ottir, Barry, Saleem, Hassabis, and ´\r
Spiers, 2015).\r
The findings that most directly implicate the hippocampus in planning—the process\r
needed to enlist an environment model in making decisions—come from experiments\r
that decode the activity of neurons in the hippocampus to determine what part of space\r
hippocampal activity is representing on a moment-to-moment basis. When a rat pauses\r
at a choice point in a maze, the representation of space in the hippocampus sweeps\r
forward (and not backwards) along the possible paths the animal can take from that\r
point (Johnson and Redish, 2007). Furthermore, the spatial trajectories represented by\r
these sweeps closely correspond to the rat’s subsequent navigational behavior (Pfei↵er\r
and Foster, 2013). These results suggest that the hippocampus is critical for the state\u0002transition part of an animal’s environment model, and that it is part of a system that\r
uses the model to simulate possible future state sequences to assess the consequences of\r
possible courses of action: a form of planning.\r
The results described above add to a voluminous literature on neural mechanisms\r
underlying goal-directed, or model-based, learning and decision making, but many\r
questions remain unanswered. For example, how can areas as structurally similar as the\r
DLS and DMS be essential components of modes of learning and behavior that are as\r
di↵erent as model-free and model-based algorithms? Are separate structures responsible\r
for (what we call) the transition and reward components of an environment model? Is all\r
planning conducted at decision time via simulations of possible future courses of action\r
as the forward sweeping activity in the hippocampus suggests? In other words, is all\r
planning something like a rollout algorithm (Section 8.10)? Or are models sometimes\r
engaged in the background to refine or recompute value information as illustrated by the\r
Dyna architecture (Section 8.2)? How does the brain arbitrate between the use of the\r
habit and goal-directed systems? Is there, in fact, a clear separation between the neural\r
substrates of these systems?

15.12. Addiction 409\r
The evidence is not pointing to a positive answer to this last question. Summarizing\r
the situation, Doll, Simon, and Daw (2012) wrote that “model-based influences appear\r
ubiquitous more or less wherever the brain processes reward information,” and this is\r
true even in the regions thought to be critical for model-free learning. This includes the\r
dopamine signals themselves, which can exhibit the influence of model-based information\r
in addition to the reward prediction errors thought to be the basis of model-free processes.\r
Continuing neuroscience research informed by reinforcement learning’s model-free and\r
model-based distinction has the potential to sharpen our understanding of habitual and\r
goal-directed processes in the brain. A better grasp of these neural mechanisms may lead\r
to algorithms combining model-free and model-based methods in ways that have not yet\r
been explored in computational reinforcement learning.\r
15.12 Addiction\r
Understanding the neural basis of drug abuse is a high-priority goal of neuroscience with\r
the potential to produce new treatments for this serious public health problem. One\r
view is that drug craving is the result of the same motivation and learning processes that\r
lead us to seek natural rewarding experiences that serve our biological needs. Addictive\r
substances, by being intensely reinforcing, e↵ectively co-opt our natural mechanisms\r
of learning and decision making. This is plausible given that many—though not all—\r
drugs of abuse increase levels of dopamine either directly or indirectly in regions around\r
terminals of dopamine neuron axons in the striatum, a brain structure firmly implicated in\r
normal reward-based learning (Section 15.7). But the self-destructive behavior associated\r
with drug addiction is not characteristic of normal learning. What is di↵erent about\r
dopamine-mediated learning when the reward is the result of an addictive drug? Is\r
addiction the result of normal learning in response to substances that were largely\r
unavailable throughout our evolutionary history, so that evolution could not select against\r
their damaging e↵ects? Or do addictive substances somehow interfere with normal\r
dopamine-mediated learning?\r
The reward prediction error hypothesis of dopamine neuron activity and its connection\r
to TD learning are the basis of a model due to Redish (2004) of some—but certainly not\r
all—features of addiction. The model is based on the observation that administration of\r
cocaine and some other addictive drugs produces a transient increase in dopamine. In the\r
model, this dopamine surge is assumed to increase the TD error, , in a way that cannot\r
be cancelled out by changes in the value function. In other words, whereas  is reduced\r
to the degree that a normal reward is predicted by antecedent events (Section 15.6), the\r
contribution to  due to an addictive stimulus does not decrease as the reward signal\r
becomes predicted: drug rewards cannot be “predicted away.” The model does this by\r
preventing  from ever becoming negative when the reward signal is due to an addictive\r
drug, thus eliminating the error-correcting feature of TD learning for states associated\r
with administration of the drug. The result is that the values of these states increase\r
without bound, making actions leading to these states preferred above all others.

410 Chapter 15: Neuroscience\r
Addictive behavior is much more complicated than this result from Redish’s model, but\r
the model’s main idea may be a piece of the puzzle. Or the model might be misleading.\r
Dopamine appears not to play a critical role in all forms of addiction, and not everyone\r
is equally susceptible to developing addictive behavior. Moreover, the model does not\r
include the changes in many circuits and brain regions that accompany chronic drug\r
taking, for example, changes that lead to a drug’s diminishing e↵ect with repeated use.\r
It is also likely that addiction involves model-based processes. Still, Redish’s model\r
illustrates how reinforcement learning theory can be enlisted in the e↵ort to understand\r
a major health problem. In a similar manner, reinforcement learning theory has been\r
influential in the development of the new field of computational psychiatry, which aims\r
to improve understanding of mental disorders through mathematical and computational\r
methods.\r
15.13 Summary\r
The neural pathways involved in the brain’s reward system are complex and incompletely\r
understood, but neuroscience research directed toward understanding these pathways\r
and their roles in behavior is progressing rapidly. This research is revealing striking\r
correspondences between the brain’s reward system and the theory of reinforcement\r
learning as presented in this book.\r
The reward prediction error hypothesis of dopamine neuron activity was proposed by\r
scientists who recognized striking parallels between the behavior of TD errors and the\r
activity of neurons that produce dopamine, a neurotransmitter essential in mammals\r
for reward-related learning and behavior. Experiments conducted in the late 1980s and\r
1990s in the laboratory of neuroscientist Wolfram Schultz showed that dopamine neurons\r
respond to rewarding events with substantial bursts of activity, called phasic responses,\r
only if the animal does not expect those events, suggesting that dopamine neurons are\r
signaling reward prediction errors instead of reward itself. Further, these experiments\r
showed that as an animal learns to predict a rewarding event on the basis of preceding\r
sensory cues, the phasic activity of dopamine neurons shifts to earlier predictive cues\r
while decreasing to later predictive cues. This parallels the backing-up e↵ect of the TD\r
error as a reinforcement learning agent learns to predict reward.\r
Other experimental results firmly establish that the phasic activity of dopamine neurons\r
is a reinforcement signal for learning that reaches multiple areas of the brain by means of\r
profusely branching axons of dopamine producing neurons. These results are consistent\r
with the distinction we make between a reward signal, Rt, and a reinforcement signal,\r
which is the TD error t in most of the algorithms we present. Phasic responses of\r
dopamine neurons are reinforcement signals, not reward signals.\r
A prominent hypothesis is that the brain implements something like an actor–critic\r
algorithm. Two structures in the brain (the dorsal and ventral subdivisions of the\r
striatum), both of which play critical roles in reward-based learning, may function\r
respectively like an actor and a critic. That the TD error is the reinforcement signal for\r
both the actor and the critic fits well with the facts that dopamine neuron axons target\r
both the dorsal and ventral subdivisions of the striatum; that dopamine appears to be

15.13. Summary 411\r
critical for modulating synaptic plasticity in both structures; and that the e↵ect on a\r
target structure of a neuromodulator such as dopamine depends on properties of the\r
target structure and not just on properties of the neuromodulator.\r
The actor and the critic can be implemented by ANNs consisting of neuron-like units\r
having learning rules based on the policy-gradient actor–critic method described in\r
Section 13.5. Each connection in these networks is like a synapse between neurons in\r
the brain, and the learning rules correspond to rules governing how synaptic ecacies\r
change as functions of the activities of the presynaptic and the postsynaptic neurons,\r
together with neuromodulatory input corresponding to input from dopamine neurons. In\r
this setting, each synapse has its own eligibility trace that records past activity involving\r
that synapse. The only di↵erence between the actor and critic learning rules is that they\r
use di↵erent kinds of eligibility traces: the critic unit’s traces are non-contingent because\r
they do not involve the critic unit’s output, whereas the actor unit’s traces are contingent\r
because in addition to the actor unit’s input, they depend on the actor unit’s output. In\r
the hypothetical implementation of an actor–critic system in the brain, these learning\r
rules respectively correspond to rules governing plasticity of corticostriatal synapses that\r
convey signals from the cortex to the principal neurons in the dorsal and ventral striatal\r
subdivisions, synapses that also receive inputs from dopamine neurons.\r
The learning rule of an actor unit in the actor–critic network closely corresponds to\r
reward-modulated spike-timing-dependent plasticity. In spike-timing-dependent plasticity\r
(STDP), the relative timing of pre- and postsynaptic activity determines the direction of\r
synaptic change. In reward-modulated STDP, changes in synapses in addition depend\r
on a neuromodulator, such as dopamine, arriving within a time window that can last\r
up to 10 seconds after the conditions for STDP are met. Evidence is accumulating that\r
reward-modulated STDP occurs at corticostriatal synapses, where the actor’s learning\r
takes place in the hypothetical neural implementation of an actor–critic system, adds to\r
the plausibility of the hypothesis that something like an actor–critic system exists in the\r
brains of some animals.\r
The idea of synaptic eligibility and basic features of the actor learning rule derive\r
from Klopf’s hypothesis of the “hedonistic neuron” (Klopf, 1972, 1981). He conjectured\r
that individual neurons seek to obtain reward and to avoid punishment by adjusting the\r
ecacies of their synapses on the basis of rewarding or punishing consequences of their\r
action potentials. A neuron’s activity can a↵ect its later input because the neuron is\r
embedded in many feedback loops, some within the animal’s nervous system and body\r
and others passing through the animal’s external environment. Klopf’s idea of eligibility\r
is that synapses are temporarily marked as eligible for modification if they participated\r
in the neuron’s firing (making this the contingent form of eligibility trace). A synapse’s\r
ecacy is modified if a reinforcing signal arrives while the synapse is eligible. We alluded\r
to the chemotactic behavior of a bacterium as an example of a single cell that directs its\r
movements in order to seek some molecules and to avoid others.\r
A conspicuous feature of the dopamine system is that fibers releasing dopamine project\r
widely to multiple parts of the brain. Although it is likely that only some populations\r
of dopamine neurons broadcast the same reinforcement signal, if this signal reaches\r
the synapses of many neurons involved in actor-type learning, then the situation can

412 Chapter 15: Neuroscience\r
be modeled as a team problem. In this type of problem, each agent in a collection of\r
reinforcement learning agents receives the same reinforcement signal, where that signal\r
depends on the activities of all members of the collection, or team. If each team member\r
uses a suciently capable learning algorithm, the team can learn collectively to improve\r
performance of the entire team as evaluated by the globally-broadcast reinforcement\r
signal, even if the team members do not directly communicate with one another. This\r
is consistent with the wide dispersion of dopamine signals in the brain and provides\r
a neurally plausible alternative to the widely-used error-backpropagation method for\r
training multilayer networks.\r
The distinction between model-free and model-based reinforcement learning is helping\r
neuroscientists investigate the neural bases of habitual and goal-directed learning and\r
decision making. Research so far points to their being some brain regions more involved\r
in one type of process than the other, but the picture remains unclear because model-free\r
and model-based processes do not appear to be neatly separated in the brain. Many\r
questions remain unanswered. Perhaps most intriguing is evidence that the hippocampus,\r
a structure traditionally associated with spatial navigation and memory, appears to be\r
involved in simulating possible future courses of action as part of an animal’s decision\u0002making process. This suggests that it is part of a system that uses an environment model\r
for planning.\r
Reinforcement learning theory is also influencing thinking about neural processes\r
underlying drug abuse. A model of some features of drug addiction is based on the reward\r
prediction error hypothesis. It proposes that an addicting stimulant, such as cocaine,\r
destabilizes TD learning to produce unbounded growth in the values of actions associated\r
with drug intake. This is far from a complete model of addiction, but it illustrates how\r
a computational perspective suggests theories that can be tested with further research.\r
The new field of computational psychiatry similarly focuses on the use of computational\r
models, some derived from reinforcement learning, to better understand mental disorders.\r
This chapter only touched the surface of how the neuroscience of reinforcement learning\r
and the development of reinforcement learning in computer science and engineering\r
have influenced one another. Most features of reinforcement learning algorithms owe\r
their design to purely computational considerations, but some have been influenced by\r
hypotheses about neural learning mechanisms. Remarkably, as experimental data has\r
accumulated about the brain’s reward processes, many of the purely computationally\u0002motivated features of reinforcement learning algorithms are turning out to be consistent\r
with neuroscience data. Other features of computational reinforcement learning, such as\r
eligibility traces and the ability of teams of reinforcement learning agents to learn to act\r
collectively under the influence of a globally-broadcast reinforcement signal, may also\r
turn out to parallel experimental data as neuroscientists continue to unravel the neural\r
basis of reward-based animal learning and behavior.

15.13. Summary 413\r
Bibliographical and Historical Remarks\r
The number of publications treating parallels between the neuroscience of learning and\r
decision making and the approach to reinforcement learning presented in this book is\r
enormous. We can cite only a small selection. Niv (2009), Dayan and Niv (2008), Gimcher\r
(2011), Ludvig, Bellemare, and Pearson (2011), and Shah (2012) are good places to start.\r
Together with economics, evolutionary biology, and mathematical psychology, reinforce\u0002ment learning theory is helping to formulate quantitative models of the neural mechanisms\r
of choice in humans and non-human primates. With its focus on learning, this chapter\r
only lightly touches upon the neuroscience of decision making. Glimcher (2003) introduced\r
the field of “neuroeconomics,” in which reinforcement learning contributes to the study\r
of the neural basis of decision making from an economics perspective. See also Glimcher\r
and Fehr (2013). The text on computational and mathematical modeling in neuroscience\r
by Dayan and Abbott (2001) includes reinforcement learning’s role in these approaches.\r
Sterling and Laughlin (2015) examined the neural basis of learning in terms of general\r
design principles that enable ecient adaptive behavior.\r
15.1 There are many good expositions of basic neuroscience. Kandel, Schwartz, Jessell,\r
Siegelbaum, and Hudspeth (2013) is an authoritative and very comprehensive\r
source.\r
15.2 Berridge and Kringelbach (2008) reviewed the neural basis of reward and pleasure,\r
pointing out that reward processing has many dimensions and involves many\r
neural systems. Space prevents discussion of the influential research of Berridge\r
and Robinson (1998), who distinguish between the hedonic impact of a stimulus,\r
which they call “liking,” and the motivational e↵ect, which they call “wanting.”\r
Hare, O’Doherty, Camerer, Schultz, and Rangel (2008) examined the neural basis\r
of value-related signals from an economic perspective, distinguishing between\r
goal values, decision values, and prediction errors. Decision value is goal value\r
minus action cost. See also Rangel, Camerer, and Montague (2008), Rangel and\r
Hare (2010), and Peters and B¨uchel (2010).\r
15.3 The reward prediction error hypothesis of dopamine neuron activity is most\r
prominently discussed by Schultz, Dayan, and Montague (1997). The hypothesis\r
was first explicitly put forward by Montague, Dayan, and Sejnowski (1996). As\r
they stated the hypothesis, it referred to reward prediction errors (RPEs) but\r
not specifically to TD errors; however, their development of the hypothesis made\r
it clear that they were referring to TD errors. The earliest recognition of the\r
TD-error/dopamine connection of which we are aware is that of Montague, Dayan,\r
Nowlan, Pouget, and Sejnowski (1993), who proposed a TD-error-modulated\r
Hebbian learning rule motivated by results on dopamine signaling from Schultz’s\r
group. The connection was also pointed out in an abstract by Quartz, Dayan,\r
Montague, and Sejnowski (1992). Montague and Sejnowski (1994) emphasized\r
the importance of prediction in the brain and outlined how predictive Hebbian\r
learning modulated by TD errors could be implemented via a di↵use neuromod\u0002ulatory system, such as the dopamine system. Friston, Tononi, Reeke, Sporns,

414 Chapter 15: Neuroscience\r
and Edelman (1994) presented a model of value-dependent learning in the brain\r
in which synaptic changes are mediated by a TD-like error provided by a global\r
neuromodulatory signal (although they did not single out dopamine). Mon\u0002tague, Dayan, Person, and Sejnowski (1995) presented a model of honeybee\r
foraging using the TD error. The model is based on research by Hammer, Men\u0002zel, and colleagues (Hammer and Menzel, 1995; Hammer, 1997) showing that\r
the neuromodulator octopamine acts as a reinforcement signal in the honeybee.\r
Montague et al. (1995) pointed out that dopamine likely plays a similar role\r
in the vertebrate brain. Barto (1995a) related the actor–critic architecture to\r
basal-ganglionic circuits and discussed the relationship between TD learning\r
and the main results from Schultz’s group. Houk, Adams, and Barto (1995)\r
suggested how TD learning and the actor–critic architecture might map onto the\r
anatomy, physiology, and molecular mechanism of the basal ganglia. Doya and\r
Sejnowski (1998) extended their earlier paper on a model of birdsong learning\r
(Doya and Sejnowski, 1995) by including a TD-like error identified with dopamine\r
to reinforce the selection of auditory input to be memorized. O’Reilly and Frank\r
(2006) and O’Reilly, Frank, Hazy, and Watz (2007) argued that phasic dopamine\r
signals are RPEs but not TD errors. In support of their theory they cited results\r
with variable interstimulus intervals that do not match predictions of a simple\r
TD model, as well as the observation that higher-order conditioning beyond\r
second-order conditioning is rarely observed, while TD learning is not so limited.\r
Dayan and Niv (2008) discussed “the good, the bad, and the ugly” of how\r
reinforcement learning theory and the reward prediction error hypothesis align\r
with experimental data. Glimcher (2011) reviewed the empirical findings that\r
support the reward prediction error hypothesis and emphasized the significance\r
of the hypothesis for contemporary neuroscience.\r
15.4 Graybiel (2000) is a brief primer on the basal ganglia. The experiments mentioned\r
that involve optogenetic activation of dopamine neurons were conducted by Tsai,\r
Zhang, Adamantidis, Stuber, Bonci, de Lecea, and Deisseroth (2009), Steinberg,\r
Keiflin, Boivin, Witten, Deisseroth, and Janak (2013), and Claridge-Chang,\r
Roorda, Vrontou, Sjulson, Li, Hirsh, and Miesenb¨ock (2009). Fiorillo, Yun, and\r
Song (2013), Lammel, Lim, and Malenka (2014), and Saddoris, Cacciapaglia,\r
Wightmman, and Carelli (2015) are among studies showing that the signaling\r
properties of dopamine neurons are specialized for di↵erent target regions. RPE\u0002signaling neurons may belong to one among multiple populations of dopamine\r
neurons having di↵erent targets and subserving di↵erent functions. Eshel, Tian,\r
Bukwich, and Uchida (2016) found homogeneity of reward prediction error\r
responses of dopamine neurons in the lateral VTA during classical conditioning\r
in mice, though their results do not rule out response diversity across wider areas.\r
Gershman, Pesaran, and Daw (2009) studied reinforcement learning tasks that\r
can be decomposed into independent components with separate reward signals,\r
finding evidence in human neuroimaging data suggesting that the brain exploits\r
this kind of structure.

15.13. Summary 415\r
15.5 Schultz’s 1998 survey article is a good entr´ee into the very extensive literature\r
on reward predicting signaling of dopamine neurons. Berns, McClure, Pagnoni,\r
and Montague (2001), Breiter, Aharon, Kahneman, Dale, and Shizgal (2001),\r
Pagnoni, Zink, Montague, and Berns (2002), and O’Doherty, Dayan, Friston,\r
Critchley, and Dolan (2003) described functional brain imaging studies supporting\r
the existence of signals like TD errors in the human brain.\r
15.6 This section roughly follows Barto (1995a) in explaining how TD errors mimic the\r
main results from Schultz’s group on the phasic responses of dopamine neurons.\r
15.7 This section is largely based on Takahashi, Schoenbaum, and Niv (2008) and Niv\r
(2009). To the best of our knowledge, Barto (1995a) and Houk, Adams, and Barto\r
(1995) first speculated about possible implementations of actor–critic algorithms\r
in the basal ganglia. On the basis of functional magnetic resonance imaging of\r
human subjects while engaged in instrumental conditioning, O’Doherty, Dayan,\r
Schultz, Deichmann, Friston, and Dolan (2004) suggested that the actor and\r
the critic are most likely located respectively in the dorsal and ventral striatum.\r
Gershman, Moustafa, and Ludvig (2014) focused on how time is represented in\r
reinforcement learning models of the basal ganglia, discussing evidence for, and\r
implications of, various computational approaches to time representation.\r
The hypothetical neural implementation of the actor–critic architecture described\r
in this section includes very little detail about known basal ganglia anatomy and\r
physiology. In addition to the more detailed hypothesis of Houk, Adams, and\r
Barto (1995), a number of other hypotheses include more specific connections to\r
anatomy and physiology and are claimed to explain additional data. These include\r
hypotheses proposed by Suri and Schultz (1998, 1999), Brown, Bullock, and\r
Grossberg (1999), Contreras-Vidal and Schultz (1999), Suri, Bargas, and Arbib\r
(2001), O’Reilly and Frank (2006), and O’Reilly, Frank, Hazy, and Watz (2007).\r
Joel, Niv, and Ruppin (2002) critically evaluated the anatomical plausibility of\r
several of these models and present an alternative intended to accommodate\r
some neglected features of basal ganglionic circuitry.\r
15.8 The actor learning rule discussed here is more complicated than the one in the\r
early actor–critic network of Barto et al. (1983). Actor-unit eligibility traces in\r
that network were traces of just At⇥x(St) instead of the full (At⇡(1|St, ✓))x(St).\r
That work did not benefit from the policy-gradient theory presented in Chapter 13\r
or the contributions of Williams (1986, 1992), who showed how an ANN of\r
Bernoulli-logistic units could implement a policy-gradient method.\r
Reynolds and Wickens (2002) proposed a three-factor rule for synaptic plasticity\r
in the corticostriatal pathway in which dopamine modulates changes in corti\u0002costriatal synaptic ecacy. They discussed the experimental support for this\r
kind of learning rule and its possible molecular basis. The definitive demon\u0002stration of spike-timing-dependent plasticity (STDP) is attributed to Markram,\r
L¨ubke, Frotscher, and Sakmann (1997), with evidence from earlier experiments

416 Chapter 15: Neuroscience\r
by Levy and Steward (1983) and others that the relative timing of pre- and\r
postsynaptic spikes is critical for inducing changes in synaptic ecacy. Rao and\r
Sejnowski (2001) suggested how STDP could be the result of a TD-like mechanism\r
at synapses with non-contingent eligibility traces lasting about 10 milliseconds.\r
Dayan (2002) commented that this would require an error as in Sutton and\r
Barto’s (1981a) early model of classical conditioning and not a true TD error.\r
Representative publications from the extensive literature on reward-modulated\r
STDP are Wickens (1990), Reynolds and Wickens (2002), and Calabresi, Picconi,\r
Tozzi and Di Filippo (2007). Pawlak and Kerr (2008) showed that dopamine is\r
necessary to induce STDP at the corticostriatal synapses of medium spiny neurons.\r
See also Pawlak, Wickens, Kirkwood, and Kerr (2010). Yagishita, Hayashi-Takagi,\r
Ellis-Davies, Urakubo, Ishii, and Kasai (2014) found that dopamine promotes\r
spine enlargement of the medium spiny neurons of mice only during a time window\r
of from 0.3 to 2 seconds after STDP stimulation. Izhikevich (2007) proposed\r
and explored the idea of using STDP timing conditions to trigger contingent\r
eligibility traces. Fr´emaux, Sprekeler, and Gerstner (2010) proposed theoretical\r
conditions for successful learning by rules based on reward-modulated STDP.\r
15.9 Klopf’s hedonistic neuron hypothesis (Klopf 1972, 1982) inspired our actor–critic\r
algorithm implemented as an ANN with a single neuron-like unit, called the\r
actor unit, implementing a Law-of-E↵ect-like learning rule (Barto, Sutton, and\r
Anderson, 1983). Ideas related to Klopf’s synaptically-local eligibility have been\r
proposed by others. Crow (1968) proposed that changes in the synapses of\r
cortical neurons are sensitive to the consequences of neural activity. Emphasizing\r
the need to address the time delay between neural activity and its consequences\r
in a reward-modulated form of synaptic plasticity, he proposed a contingent form\r
of eligibility, but associated with entire neurons instead of individual synapses.\r
According to his hypothesis, a wave of neuronal activity\r
leads to a short-term change in the cells involved in the wave such that they\r
are picked out from a background of cells not so activated. ... such cells are\r
rendered sensitive by the short-term change to a reward signal ... in such\r
a way that if such a signal occurs before the end of the decay time of the\r
change the synaptic connexions between the cells are made more e↵ective.\r
(Crow, 1968)\r
Crow argued against previous proposals that reverberating neural circuits play\r
this role by pointing out that the e↵ect of a reward signal on such a circuit would\r
“...establish the synaptic connexions leading to the reverberation (that is to say,\r
those involved in activity at the time of the reward signal) and not those on the\r
path which led to the adaptive motor output.” Crow further postulated that\r
reward signals are delivered via a “distinct neural fiber system,” presumably the\r
one into which Olds and Milner (1954) tapped, that would transform synaptic\r
connections “from a short into a long-term form.”

15.13. Summary 417\r
In another farsighted hypothesis, Miller proposed a Law-of-E↵ect-like learning\r
rule that includes synaptically-local contingent eligibility traces:\r
... it is envisaged that in a particular sensory situation neurone B, by chance,\r
fires a ‘meaningful burst’ of activity, which is then translated into motor acts,\r
which then change the situation. It must be supposed that the meaningful\r
burst has an influence, at the neuronal level, on all of its own synapses which\r
are active at the time ... thereby making a preliminary selection of the\r
synapses to be strengthened, though not yet actually strengthening them.\r
...The strengthening signal ... makes the final selection ... and accomplishes\r
the definitive change in the appropriate synapses. (Miller, 1981, p. 81)\r
Miller’s hypothesis also included a critic-like mechanism, which he called a “sen\u0002sory analyzer unit,” that worked according to classical conditioning principles to\r
provide reinforcement signals to neurons so that they would learn to move from\r
lower- to higher-valued states, thus anticipating the use of the TD error as a rein\u0002forcement signal in the actor–critic architecture. Miller’s idea not only parallels\r
Klopf’s (with the exception of its explicit invocation of a distinct “strengthening\r
signal”), it also anticipated the general features of reward-modulated STDP.\r
A related though di↵erent idea, which Seung (2003) called the “hedonistic\r
synapse,” is that synapses individually adjust the probability that they release\r
neurotransmitter in the manner of the Law of E↵ect: if reward follows release,\r
the release probability increases, and decreases if reward follows failure to release.\r
This is essentially the same as the learning scheme Minsky used in his 1954\r
Princeton PhD dissertation, where he called the synapse-like learning element\r
a SNARC (Stochastic Neural-Analog Reinforcement Calculator). Contingent\r
eligibility is involved in these ideas too, although it is contingent on the activity\r
of an individual synapse instead of the postsynaptic neuron. Also related is the\r
proposal of Unnikrishnan and Venugopal (1994) that uses the correlation-based\r
method of Harth and Tzanakou (1974) to adjust ANN weights.\r
Frey and Morris (1997) proposed the idea of a “synaptic tag” for the induction\r
of long-lasting strengthening of synaptic ecacy. Though not unlike Klopf’s\r
eligibility, their tag was hypothesized to consist of a temporary strengthening of a\r
synapse that could be transformed into a long-lasting strengthening by subsequent\r
neuron activation. The model of O’Reilly and Frank (2006) and O’Reilly, Frank,\r
Hazy, and Watz (2007) uses working memory to bridge temporal intervals instead\r
of eligibility traces. Wickens and Kotter (1995) discuss possible mechanisms\r
for synaptic eligibility. He, Huertas, Hong, Tie, Hell, Shouval, Kirkwood (2015)\r
provide evidence supporting the existence of contingent eligibility traces in\r
synapses of cortical neurons with time courses like those of the eligibility traces\r
Klopf postulated.\r
The metaphor of a neuron using a learning rule related to bacterial chemotaxis was\r
discussed by Barto (1989). Koshland’s extensive study of bacterial chemotaxis\r
was in part motivated by similarities between features of bacteria and features of\r
neurons (Koshland, 1980). See also Berg (1975). Shimansky (2009) proposed a

418 Chapter 15: Neuroscience\r
synaptic learning rule somewhat similar to Seung’s mentioned above in which\r
each synapse individually acts like a chemotactic bacterium. In this case a\r
collection of synapses “swims” toward attractants in the high-dimensional space\r
of synaptic weight values. Montague, Dayan, Person, and Sejnowski (1995)\r
proposed a chemotactic-like model of the bee’s foraging behavior involving the\r
neuromodulator octopamine.\r
15.10 Research on the behavior of reinforcement learning agents in team and game\r
problems has a long history roughly occurring in three phases. To the best\r
of our knowledge, the first phase began with investigations by the Russian\r
mathematician and physicist M. L. Tsetlin. A collection of his work was published\r
as Tsetlin (1973) after his death in 1966. Our Sections 1.7 and 4.8 refer to his\r
study of learning automata in connection to bandit problems. The Tsetlin\r
collection also includes studies of learning automata in team and game problems,\r
which led to later work in this area using stochastic learning automata as\r
described by Narendra and Thathachar (1974, 1989), Viswanathan and Narendra\r
(1974), Lakshmivarahan and Narendra (1982), Narendra and Wheeler (1983), and\r
Thathachar and Sastry (2002). Thathachar and Sastry (2011) is a more recent\r
comprehensive account. These studies were mostly restricted to non-associative\r
learning automata, meaning that they did not address associative, or contextual,\r
bandit problems (Section 2.9).\r
The second phase began with the extension of learning automata to the associative,\r
or contextual, case. Barto, Sutton, and Brouwer (1981) and Barto and Sutton\r
(1981b) experimented with associative stochastic learning automata in single\u0002layer ANNs to which a global reinforcement signal was broadcast. The learning\r
algorithm was an associative extension of the Alopex algorithm of Harth and\r
Tzanakou (1974). Barto et al. called neuron-like elements implementing this\r
kind of learning associative search elements (ASEs). Barto and Anandan (1985)\r
introduced an associative reinforcement learning algorithm called the associative\r
reward-penalty (ARP) algorithm. They proved a convergence result by combining\r
theory of stochastic learning automata with theory of pattern classification. Barto\r
(1985, 1986) and Barto and Jordan (1987) described results with teams of ARP\r
units connected into multi-layer ANNs, showing that they could learn nonlinear\r
functions, such as XOR and others, with a globally-broadcast reinforcement signal.\r
Barto (1985) extensively discussed this approach to ANNs and how this type of\r
learning rule is related to others in the literature at that time. Williams (1992)\r
mathematically analyzed and broadened this class of learning rules and related\r
their use to the error backpropagation method for training multilayer ANNs.\r
Williams (1988) described several ways that backpropagation and reinforcement\r
learning can be combined for training ANNs. Williams (1992) showed that a\r
special case of the ARP algorithm is a REINFORCE algorithm, although better\r
results were obtained with the general ARP algorithm (Barto, 1985).

15.13. Summary 419\r
The third phase of interest in teams of reinforcement learning agents was influ\u0002enced by increased understanding of the role of dopamine as a widely broadcast\r
neuromodulator and speculation about the existence of reward-modulated STDP.\r
Much more so than earlier research, this research considers details of synaptic\r
plasticity and other constraints from neuroscience. Publications include the\r
following (chronologically and alphabetically): Bartlett and Baxter (1999, 2000),\r
Xie and Seung (2004), Baras and Meir (2007), Farries and Fairhall (2007), Florian\r
(2007), Izhikevich (2007), Pecevski, Maass, and Legenstein (2008), Legenstein,\r
Pecevski, and Maass (2008), Kolodziejski, Porr, and W¨org¨otter (2009), Urbanczik\r
and Senn (2009), and Vasilaki, Fr´emaux, Urbanczik, Senn, and Gerstner (2009).\r
Now´e, Vrancx, and De Hauwere (2012) reviewed more recent developments in\r
the wider field of multi-agent reinforcement learning\r
15.11 Yin and Knowlton (2006) reviewed findings from outcome-devaluation experi\u0002ments with rodents supporting the view that habitual and goal-directed behavior\r
(as psychologists use the phrase) are respectively most associated with process\u0002ing in the dorsolateral striatum (DLS) and the dorsomedial striatum (DMS).\r
Results of functional imaging experiments with human subjects in the outcome\u0002devaluation setting by Valentin, Dickinson, and O’Doherty (2007) suggest that\r
the orbitofrontal cortex (OFC) is an important component of goal-directed choice.\r
Single unit recordings in monkeys by Padoa-Schioppa and Assad (2006) support\r
the role of the OFC in encoding values guiding choice behavior. Rangel, Camerer,\r
and Montague (2008) and Rangel and Hare (2010) reviewed findings from the\r
perspective of neuroeconomics about how the brain makes goal-directed decisions.\r
Pezzulo, van der Meer, Lansink, and Pennartz (2014) reviewed the neuroscience\r
of internally generated sequences and presented a model of how these mecha\u0002nisms might be components of model-based planning. Daw and Shohamy (2008)\r
proposed that while dopamine signaling connects well to habitual, or model-free,\r
behavior, other processes are involved in goal-directed, or model-based, behavior.\r
Data from experiments by Bromberg-Martin, Matsumoto, Hong, and Hikosaka\r
(2010) indicate that dopamine signals contain information pertinent to both\r
habitual and goal-directed behavior. Doll, Simon, and Daw (2012) argued that\r
there may not a clear separation in the brain between mechanisms that subserve\r
habitual and goal-directed learning and choice.\r
15.12 Keiflin and Janak (2015) reviewed connections between TD errors and addiction.\r
Nutt, Lingford-Hughes, Erritzoe, and Stokes (2015) critically evaluated the\r
hypothesis that addiction is due to a disorder of the dopamine system. Montague,\r
Dolan, Friston, and Dayan (2012) outlined the goals and early e↵orts in the field\r
of computational psychiatry, and Adams, Huys, and Roiser (2015) reviewed more\r
recent progress.

Chapter 16\r
Applications and Case Studies\r
In this chapter we present a few case studies of reinforcement learning. Several of these\r
are substantial applications of potential economic significance. One, Samuel’s checkers\r
player, is primarily of historical interest. Our presentations are intended to illustrate some\r
of the trade-o↵s and issues that arise in real applications. For example, we emphasize how\r
domain knowledge is incorporated into the formulation and solution of the problem. We\r
also highlight the representation issues that are so often critical to successful applications.\r
The algorithms used in some of these case studies are substantially more complex than\r
those we have presented in the rest of the book. Applications of reinforcement learning\r
are still far from routine and typically require as much art as science. Making applications\r
easier and more straightforward is one of the goals of current research in reinforcement\r
learning.\r
16.1 TD-Gammon\r
One of the most impressive applications of reinforcement learning to date is that by\r
Gerald Tesauro to the game of backgammon (Tesauro, 1992, 1994, 1995, 2002). Tesauro’s\r
program, TD-Gammon, required little backgammon knowledge, yet learned to play\r
extremely well, near the level of the world’s strongest grandmasters. The learning\r
algorithm in TD-Gammon was a straightforward combination of the TD() algorithm\r
and nonlinear function approximation using a multilayer artificial neural network (ANN)\r
trained by backpropagating TD errors.\r
Backgammon is a major game in the sense that it is played throughout the world,\r
with numerous tournaments and regular world championship matches. It is in part a\r
game of chance, and it is a popular vehicle for waging significant sums of money. There\r
are probably more professional backgammon players than there are professional chess\r
players. The game is played with 15 white and 15 black pieces on a board of 24 locations,\r
called points. To the right on the next page is shown a typical position early in the game,\r
seen from the perspective of the white player. White here has just rolled the dice and\r
obtained a 5 and a 2. This means that he can move one of his pieces 5 steps and one

422 Chapter 16: Applications and Case Studies\r
white pieces move \r
 counterclockwise\r
12 34 5 6 7 8 9 10 11 12\r
192021222324 18 17 16 15 14 13\r
 black pieces \r
move clockwise\r
A backgammon position\r
(possibly the same piece) 2 steps. For\r
example, he could move two pieces from\r
the 12 point, one to the 17 point, and\r
one to the 14 point. White’s objective is\r
to advance all of his pieces into the last\r
quadrant (points 19–24) and then o↵\r
the board. The first player to remove\r
all his pieces wins. One complication\r
is that the pieces interact as they pass\r
each other going in di↵erent directions.\r
For example, if it were black’s move, he\r
could use the dice roll of 2 to move a\r
piece from the 24 point to the 22 point,\r
“hitting” the white piece there. Pieces that have been hit are placed on the “bar” in the\r
middle of the board (where we already see one previously hit black piece), from whence\r
they reenter the race from the start. However, if there are two pieces on a point, then\r
the opponent cannot move to that point; the pieces are protected from being hit. Thus,\r
white cannot use his 5–2 dice roll to move either of his pieces on the 1 point, because\r
their possible resulting points are occupied by groups of black pieces. Forming contiguous\r
blocks of occupied points to block the opponent is one of the elementary strategies of the\r
game.\r
Backgammon involves several further complications, but the above description gives\r
the basic idea. With 30 pieces and 24 possible locations (26, counting the bar and\r
o↵-the-board) it should be clear that the number of possible backgammon positions is\r
enormous, far more than the number of memory elements one could have in any physically\r
realizable computer. The number of moves possible from each position is also large. For a\r
typical dice roll there might be 20 di↵erent ways of playing. In considering future moves,\r
such as the response of the opponent, one must consider the possible dice rolls as well.\r
The result is that the game tree has an e↵ective branching factor of about 400. This is\r
far too large to permit e↵ective use of the conventional heuristic search methods that\r
have proved so e↵ective in games like chess and checkers.\r
On the other hand, the game is a good match to the capabilities of TD learning\r
methods. Although the game is highly stochastic, a complete description of the game’s\r
state is available at all times. The game evolves over a sequence of moves and positions\r
until finally ending in a win for one player or the other, ending the game. The outcome\r
can be interpreted as a final reward to be predicted. On the other hand, the theoretical\r
results we have described so far cannot be usefully applied to this task. The number of\r
states is so large that a lookup table cannot be used, and the opponent is a source of\r
uncertainty and time variation.\r
TD-Gammon used a nonlinear form of TD(). The estimated value, vˆ(s,w), of any\r
state (board position) s was meant to estimate the probability of winning starting from\r
state s. To achieve this, rewards were defined as zero for all time steps except those\r
on which the game is won. To implement the value function, TD-Gammon used a\r
standard multilayer ANN, much like that shown to the right on the next page. (The real

16.1. TD-Gammon 423\r
Vt+1! Vt\r
hidden units (40-8\r
backgammon position (198 input units)\r
predicted probability\r
of winning, Vt\r
TD error,\r
. . . . . .\r
. . . . . .\r
. . . . . .\r
()\r
general update rule for this case is\r
wt+1 = wt + \r
h\r
Rt+1 + vˆ(St+1,wt)  vˆ(St,wt)\r
i\r
et, (15.1)\r
where wt is the vector of all modifiable parameters (in this case, the weights\r
of the network) and et is a vector of eligibility traces, one for each component\r
of wt, updated by\r
et = et1 + rwt vˆ(St,wt),\r
with e0 = 0. The gradient in this equation can be computed eciently by the\r
backpropagation procedure. For the backgammon application, in which  = 1\r
and the reward is always zero except upon winning, the TD error portion of the\r
learning rule is usually just ˆv(St+1,w)  vˆ(St,w), as suggested in Figure 15.2.\r
To apply the learning rule we need a source of backgammon games. Tesauro\r
obtained an unending sequence of games by playing his learning backgammon\r
player against itself. To choose its moves, TD-Gammon considered each of the\r
20 or so ways it could play its dice roll and the corresponding positions that\r
would result. The resulting positions are afterstates as discussed in Section 6.8.\r
The network was consulted to estimate each of their values. The move was\r
then selected that would lead to the position with the highest estimated value.\r
Continuing in this way, with TD-Gammon making the moves for both sides,\r
it was possible to easily generate large numbers of backgammon games. Each\r
game was treated as an episode, with the sequence of positions acting as\r
the states, S0, S1, S2,.... Tesauro applied the nonlinear TD rule (15.1) fully\r
incrementally, that is, after each individual move.\r
The weights of the network were set initially to small random values. The\r
initial evaluations were thus entirely arbitrary. Since the moves were selected\r
on the basis of these evaluations, the initial moves were inevitably poor, and\r
the initial games often lasted hundreds or thousands of moves before one side\r
or the other won, almost by accident. After a few dozen games however,\r
performance improved rapidly.\r
After playing about 300,000 games against itself, TD-Gammon 0.0 as de\u0002scribed above learned to play approximately as well as the best previous\r
backgammon computer programs. This was a striking result because all the\r
previous high-performance computer programs had used extensive backgam\u0002mon knowledge. For example, the reigning champion program at the time\r
was, arguably, Neurogammon, another program written by Tesauro that used\r
a neural network but not TD learning. Neurogammon’s network was trained\r
on a large training corpus of exemplary moves provided by backgammon ex\u0002perts, and, in addition, started with a set of features specially crafted for\r
TD error\r
+\r
h\r
+\r
(\r
+\r
) ()\r
i\r
(\r
where wt is the vector of all modifiable parameters (in this case, the weigh\r
of the network) and et is a vector of eligibility traces, one for each componen\r
of wt, updated by\r
et = et1 + rwt vˆ(St,wt),\r
with e0 = 0. The gradient in this equation can be computed eciently by th\r
backpropagation procedure. For the backgammon application, in which  = \r
and the reward is always zero except upon winning, the TD error portion of th\r
learning rule is usually just ˆv(St+1,w)  vˆ(St,w), as suggested in Figure 15.\r
To apply the learning rule we need a source of backgammon games. Tesaur\r
obtained an unending sequence of games by playing his learning backgammo\r
player against itself. To choose its moves, TD-Gammon considered each of th\r
20 or so ways it could play its dice roll and the corresponding positions tha\r
would result. The resulting positions are afterstates as discussed in Section 6.\r
The network was consulted to estimate each of their values. The move wa\r
then selected that would lead to the position with the highest estimated valu\r
Continuing in this way, with TD-Gammon making the moves for both side\r
it was possible to easily generate large numbers of backgammon games. Eac\r
game was treated as an episode, with the sequence of positions acting a\r
the states, S0, S1, S2,.... Tesauro applied the nonlinear TD rule (15.1) ful\r
incrementally, that is, after each individual move.\r
The weights of the network were set initially to small random values. Th\r
initial evaluations were thus entirely arbitrary. Since the moves were selecte\r
on the basis of these evaluations, the initial moves were inevitably poor, an\r
the initial games often lasted hundreds or thousands of moves before one sid\r
or the other won, almost by accident. After a few dozen games howeve\r
performance improved rapidly.\r
After playing about 300,000 games against itself, TD-Gammon 0.0 as d\r
scribed above learned to play approximately as well as the best previou\r
backgammon computer programs. This was a striking result because all th\r
previous high-performance computer programs had used extensive backgam\r
mon knowledge. For example, the reigning champion program at the tim\r
was, arguably, Neurogammon, another program written by Tesauro that use\r
a neural network but not TD learning. Neurogammon’s network was traine\r
on a large training corpus of exemplary moves provided by backgammon ex\r
perts, and, in addition, started with a set of features specially crafted fo\r
hidden units\r
(40-80)\r
Figure 16.1: The TD-Gammon ANN\r
network had two additional units in its\r
final layer to estimate the probability of\r
each player’s winning in a special way\r
called a “gammon” or “backgammon.”)\r
The network consisted of a layer of input\r
units, a layer of hidden units, and a final\r
output unit. The input to the network\r
was a representation of a backgammon\r
position, and the output was an estimate\r
of the value of that position.\r
In the first version of TD-Gammon,\r
TD-Gammon 0.0, backgammon posi\u0002tions were represented to the network\r
in a relatively direct way that involved\r
little backgammon knowledge. It did,\r
however, involve substantial knowledge of how ANNs work and how information is best\r
presented to them. It is instructive to note the exact representation Tesauro chose. There\r
were a total of 198 input units to the network. For each point on the backgammon board,\r
four units indicated the number of white pieces on the point. If there were no white\r
pieces, then all four units took on the value zero. If there was one piece, then the first\r
unit took on the value 1. This encoded the elementary concept of a “blot,” i.e., a piece\r
that can be hit by the opponent. If there were two or more pieces, then the second unit\r
was set to 1. This encoded the basic concept of a “made point” on which the opponent\r
cannot land. If there were exactly three pieces on the point, then the third unit was set\r
to 1. This encoded the basic concept of a “single spare,” i.e., an extra piece in addition\r
to the two pieces that made the point. Finally, if there were more than three pieces, the\r
fourth unit was set to a value proportionate to the number of additional pieces beyond\r
three. Letting n denote the total number of pieces on the point, if n > 3, then the fourth\r
unit took on the value (n3)/2. This encoded a linear representation of “multiple spares”\r
at the given point.\r
With four units for white and four for black at each of the 24 points, that made a\r
total of 192 units. Two additional units encoded the number of white and black pieces on\r
the bar (each took the value n/2, where n is the number of pieces on the bar), and two\r
more encoded the number of black and white pieces already successfully removed from\r
the board (these took the value n/15, where n is the number of pieces already borne\r
o↵). Finally, two units indicated in a binary fashion whether it was white’s or black’s\r
turn to move. The general logic behind these choices should be clear. Basically, Tesauro\r
tried to represent the position in a straightforward way, while keeping the number of\r
units relatively small. He provided one unit for each conceptually distinct possibility that\r
seemed likely to be relevant, and he scaled them to roughly the same range, in this case\r
between 0 and 1.\r
Given a representation of a backgammon position, the network computed its estimated\r
value in the standard way. Corresponding to each connection from an input unit to a\r
hidden unit was a real-valued weight. Signals from each input unit were multiplied by

424 Chapter 16: Applications and Case Studies\r
their corresponding weights and summed at the hidden unit. The output, h(j), of hidden\r
unit j was a nonlinear sigmoid function of the weighted sum:\r
h(j) = \r
 X\r
i\r
wijxi\r
!\r
= 1\r
1 + eP\r
i wijxi\r
,\r
where xi is the value of the ith input unit and wij is the weight of its connection to the\r
jth hidden unit (all the weights in the network together make up the parameter vector w).\r
The output of the sigmoid is always between 0 and 1, and has a natural interpretation as\r
a probability based on a summation of evidence. The computation from hidden units\r
to the output unit was entirely analogous. Each connection from a hidden unit to the\r
output unit had a separate weight. The output unit formed the weighted sum and then\r
passed it through the same sigmoid nonlinearity.\r
TD-Gammon used the semi-gradient form of the TD() algorithm described in Sec\u0002tion 12.2, with the gradients computed by the error backpropagation algorithm (Rumel\u0002hart, Hinton, and Williams, 1986). Recall that the general update rule for this case is\r
wt+1\r
.\r
= wt + ↵\r
h\r
Rt+1 + vˆ(St+1,wt)  vˆ(St,wt)\r
i\r
zt, (16.1)\r
where wt is the vector of all modifiable parameters (in this case, the weights of the\r
network) and zt is a vector of eligibility traces, one for each component of wt, updated by\r
zt\r
.\r
= zt1 + rvˆ(St,wt),\r
with z0\r
.\r
= 0. The gradient in this equation can be computed eciently by the backpropa\u0002gation procedure. For the backgammon application, in which  = 1 and the reward is\r
always zero except upon winning, the TD error portion of the learning rule is usually\r
just ˆv(St+1,w)  vˆ(St,w), as suggested in Figure 16.1.\r
To apply the learning rule we need a source of backgammon games. Tesauro obtained\r
an unending sequence of games by playing his learning backgammon player against itself.\r
To choose its moves, TD-Gammon considered each of the 20 or so ways it could play\r
its dice roll and the corresponding positions that would result. The resulting positions\r
are afterstates as discussed in Section 6.8. The network was consulted to estimate each\r
of their values. The move was then selected that would lead to the position with the\r
highest estimated value. Continuing in this way, with TD-Gammon making the moves\r
for both sides, it was possible to easily generate large numbers of backgammon games.\r
Each game was treated as an episode, with the sequence of positions acting as the states,\r
S0, S1, S2,.... Tesauro applied the nonlinear TD rule (16.1) fully incrementally, that is,\r
after each individual move.\r
The weights of the network were set initially to small random values. The initial\r
evaluations were thus entirely arbitrary. Because the moves were selected on the basis\r
of these evaluations, the initial moves were inevitably poor, and the initial games often\r
lasted hundreds or thousands of moves before one side or the other won, almost by\r
accident. After a few dozen games however, performance improved rapidly.\r
After playing about 300,000 games against itself, TD-Gammon 0.0 as described above\r
learned to play approximately as well as the best previous backgammon computer

16.1. TD-Gammon 425\r
programs. This was a striking result because all the previous high-performance computer\r
programs had used extensive backgammon knowledge. For example, the reigning champion\r
program at the time was, arguably, Neurogammon, another program written by Tesauro\r
that used an ANN but not TD learning. Neurogammon’s network was trained on a\r
large training corpus of exemplary moves provided by backgammon experts, and, in\r
addition, started with a set of features specially crafted for backgammon. Neurogammon\r
was a highly tuned, highly e↵ective backgammon program that decisively won the\r
World Backgammon Olympiad in 1989. TD-Gammon 0.0, on the other hand, was\r
constructed with essentially zero backgammon knowledge. That it was able to do as\r
well as Neurogammon and all other approaches is striking testimony to the potential of\r
self-play learning methods.\r
The tournament success of TD-Gammon 0.0 with zero expert backgammon knowledge\r
suggested an obvious modification: add the specialized backgammon features but keep\r
the self-play TD learning method. This produced TD-Gammon 1.0. TD-Gammon 1.0 was\r
clearly substantially better than all previous backgammon programs and found serious\r
competition only among human experts. Later versions of the program, TD-Gammon 2.0\r
(40 hidden units) and TD-Gammon 2.1 (80 hidden units), were augmented with a selective\r
two-ply search procedure. To select moves, these programs looked ahead not just to the\r
positions that would immediately result, but also to the opponent’s possible dice rolls\r
and moves. Assuming the opponent always took the move that appeared immediately\r
best for him, the expected value of each candidate move was computed and the best\r
was selected. To save computer time, the second ply of search was conducted only for\r
candidate moves that were ranked highly after the first ply, about four or five moves on\r
average. Two-ply search a↵ected only the moves selected; the learning process proceeded\r
exactly as before. The final versions of the program, TD-Gammon 3.0 and 3.1, used 160\r
hidden units and a selective three-ply search. TD-Gammon illustrates the combination\r
of learned value functions and decision-time search as in heuristic search and MCTS\r
methods. In follow-on work, Tesauro and Galperin (1997) explored trajectory sampling\r
methods as an alternative to full-width search, which reduced the error rate of live play\r
by large numerical factors (4x–6x) while keeping the think time reasonable at ⇠5–10\r
seconds per move.\r
During the 1990s, Tesauro was able to play his programs in a significant number of\r
games against world-class human players. A summary of the results is given in Table 16.1.\r
Program Hidden Training Opponents Results\r
Units Games\r
TD-Gammon 0.0 40 300,000 other programs tied for best\r
TD-Gammon 1.0 80 300,000 Robertie, Magriel, ... 13 pts / 51 games\r
TD-Gammon 2.0 40 800,000 various Grandmasters 7 pts / 38 games\r
TD-Gammon 2.1 80 1,500,000 Robertie 1 pt / 40 games\r
TD-Gammon 3.0 80 1,500,000 Kazaros +6 pts / 20 games\r
Table 16.1: Summary of TD-Gammon Results

426 Chapter 16: Applications and Case Studies\r
Based on these results and analyses by backgammon grandmasters (Robertie, 1992; see\r
Tesauro, 1995), TD-Gammon 3.0 appeared to play at close to, or possibly better than,\r
the playing strength of the best human players in the world. Tesauro reported in a\r
subsequent article (Tesauro, 2002) the results of an extensive rollout analysis of the move\r
decisions and doubling decisions of TD-Gammon relative to top human players. The\r
conclusion was that TD-Gammon 3.1 had a “lopsided advantage” in piece-movement\r
decisions, and a “slight edge” in doubling decisions, over top humans.\r
TD-Gammon had a significant impact on the way the best human players play the\r
game. For example, it learned to play certain opening positions di↵erently than was\r
the convention among the best human players. Based on TD-Gammon’s success and\r
further analysis, the best human players now play these positions as TD-Gammon does\r
(Tesauro, 1995). The impact on human play was greatly accelerated when several other\r
self-teaching ANN backgammon programs inspired by TD-Gammon, such as Jellyfish,\r
Snowie, and GNUBackgammon, became widely available. These programs enabled wide\r
dissemination of new knowledge generated by the ANNs, resulting in great improvements\r
in the overall caliber of human tournament play (Tesauro, 2002).\r
16.2 Samuel’s Checkers Player\r
An important precursor to Tesauro’s TD-Gammon was the seminal work of Arthur Samuel\r
(1959, 1967) in constructing programs for learning to play checkers. Samuel was one of\r
the first to make e↵ective use of heuristic search methods and of what we would now call\r
temporal-di↵erence learning. His checkers players are instructive case studies in addition\r
to being of historical interest. We emphasize the relationship of Samuel’s methods to\r
modern reinforcement learning methods and try to convey some of Samuel’s motivation\r
for using them.\r
Samuel first wrote a checkers-playing program for the IBM 701 in 1952. His first\r
learning program was completed in 1955 and was demonstrated on television in 1956.\r
Later versions of the program achieved good, though not expert, playing skill. Samuel\r
was attracted to game-playing as a domain for studying machine learning because games\r
are less complicated than problems “taken from life” while still allowing fruitful study of\r
how heuristic procedures and learning can be used together. He chose to study checkers\r
instead of chess because its relative simplicity made it possible to focus more strongly on\r
learning.\r
Samuel’s programs played by performing a lookahead search from each current position.\r
They used what we now call heuristic search methods to determine how to expand the\r
search tree and when to stop searching. The terminal board positions of each search were\r
evaluated, or “scored,” by a value function, or “scoring polynomial,” using linear function\r
approximation. In this and other respects Samuel’s work seems to have been inspired\r
by the suggestions of Shannon (1950). In particular, Samuel’s program was based on\r
Shannon’s minimax procedure to find the best move from the current position. Working\r
backward through the search tree from the scored terminal positions, each position was\r
given the score of the position that would result from the best move, assuming that the\r
machine would always try to maximize the score, while the opponent would always try to

16.2. Samuel’s Checkers Player 427\r
minimize it. Samuel called this the “backed-up score” of the position. When the minimax\r
procedure reached the search tree’s root—the current position—it yielded the best move\r
under the assumption that the opponent would be using the same evaluation criterion,\r
shifted to its point of view. Some versions of Samuel’s programs used sophisticated search\r
control methods analogous to what are known as “alpha-beta” cuto↵s (e.g., see Pearl,\r
1984).\r
Samuel used two main learning methods, the simplest of which he called rote learning.\r
It consisted simply of saving a description of each board position encountered during play\r
together with its backed-up value determined by the minimax procedure. The result was\r
that if a position that had already been encountered were to occur again as a terminal\r
position of a search tree, the depth of the search was e↵ectively amplified because this\r
position’s stored value cached the results of one or more searches conducted earlier. One\r
initial problem was that the program was not encouraged to move along the most direct\r
path to a win. Samuel gave it a “a sense of direction” by decreasing a position’s value\r
a small amount each time it was backed up a level (called a ply) during the minimax\r
analysis. “If the program is now faced with a choice of board positions whose scores\r
di↵er only by the ply number, it will automatically make the most advantageous choice,\r
choosing a low-ply alternative if winning and a high-ply alternative if losing” (Samuel,\r
1959, p. 80). Samuel found this discounting-like technique essential to successful learning.\r
Rote learning produced slow but continual improvement that was most e↵ective for\r
opening and endgame play. His program became a “better-than-average novice” after\r
learning from many games against itself, a variety of human opponents, and from book\r
games in a supervised learning mode.\r
Rote learning and other aspects of Samuel’s work strongly suggest the essential idea\r
of temporal-di↵erence learning—that the value of a state should equal the value of\r
likely following states. Samuel came closest to this idea in his second learning method,\r
his “learning by generalization” procedure for modifying the parameters of the value\r
function. Samuel’s method was the same in concept as that used much later by Tesauro\r
in TD-Gammon. He played his program many games against another version of itself and\r
performed an update after each move. The idea of Samuel’s update is suggested by the\r
backup diagram in Figure 16.2. Each open circle represents a position where the program\r
moves next, an on-move position, and each solid circle represents a position where the\r
opponent moves next. An update was made to the value of each on-move position after a\r
move by each side, resulting in a second on-move position. The update was toward the\r
minimax value of a search launched from the second on-move position. Thus, the overall\r
e↵ect was that of a backing-up over one full move of real events and then a search over\r
possible events, as suggested by Figure 16.2. Samuel’s actual algorithm was significantly\r
more complex than this for computational reasons, but this was the basic idea.\r
Samuel did not include explicit rewards. Instead, he fixed the weight of the most\r
important feature, the piece advantage feature, which measured the number of pieces\r
the program had relative to how many its opponent had, giving higher weight to kings,\r
and including refinements so that it was better to trade pieces when winning than when\r
losing. Thus, the goal of Samuel’s program was to improve its piece advantage, which in\r
checkers is highly correlated with winning.

428 Chapter 16: Applications and Case Studies\r
hypothetical events\r
actual events\r
backup\r
Figure 16.2: The backup diagram for Samuel’s checkers player.\r
However, Samuel’s learning method may have been missing an essential part of a sound\r
temporal-di↵erence algorithm. Temporal-di↵erence learning can be viewed as a way of\r
making a value function consistent with itself, and this we can clearly see in Samuel’s\r
method. But also needed is a way of tying the value function to the true value of the\r
states. We have enforced this via rewards and by discounting or giving a fixed value to\r
the terminal state. But Samuel’s method included no rewards and no special treatment of\r
the terminal positions of games. As Samuel himself pointed out, his value function could\r
have become consistent merely by giving a constant value to all positions. He hoped to\r
discourage such solutions by giving his piece-advantage term a large, nonmodifiable weight.\r
But although this may decrease the likelihood of finding useless evaluation functions,\r
it does not prohibit them. For example, a constant function could still be attained by\r
setting the modifiable weights so as to cancel the e↵ect of the nonmodifiable one.\r
Because Samuel’s learning procedure was not constrained to find useful evaluation\r
functions, it should have been possible for it to become worse with experience. In fact,\r
Samuel reported observing this during extensive self-play training sessions. To get the\r
program improving again, Samuel had to intervene and set the weight with the largest\r
absolute value back to zero. His interpretation was that this drastic intervention jarred\r
the program out of local optima, but another possibility is that it jarred the program out\r
of evaluation functions that were consistent but had little to do with winning or losing\r
the game.\r
Despite these potential problems, Samuel’s checkers player using the generalization\r
learning method approached “better-than-average” play. Fairly good amateur opponents\r
characterized it as “tricky but beatable” (Samuel, 1959). In contrast to the rote-learning\r
version, this version was able to develop a good middle game but remained weak in\r
opening and endgame play. This program also included an ability to search through sets of\r
features to find those that were most useful in forming the value function. A later version\r
(Samuel, 1967) included refinements in its search procedure, such as alpha-beta pruning,

16.3. Watson’s Daily-Double Wagering 429\r
extensive use of a supervised learning mode called “book learning,” and hierarchical\r
lookup tables called signature tables (Grith, 1966) to represent the value function\r
instead of linear function approximation. This version learned to play much better than\r
the 1959 program, though still not at a master level. Samuel’s checkers-playing program\r
was widely recognized as a significant achievement in artificial intelligence and machine\r
learning.\r
16.3 Watson’s Daily-Double Wagering\r
IBM Watson1 is the system developed by a team of IBM researchers to play the popular\r
TV quiz show Jeopardy!.\r
2 It gained fame in 2011 by winning first prize in an exhibition\r
match against human champions. Although the main technical achievement demonstrated\r
by Watson was its ability to quickly and accurately answer natural language questions\r
over broad areas of general knowledge, its winning Jeopardy! performance also relied on\r
sophisticated decision-making strategies for critical parts of the game. Tesauro, Gondek,\r
Lechner, Fan, and Prager (2012, 2013) adapted Tesauro’s TD-Gammon system described\r
above to create the strategy used by Watson in “Daily-Double” (DD) wagering in its\r
celebrated winning performance against human champions. These authors report that the\r
e↵ectiveness of this wagering strategy went well beyond what human players are able to\r
do in live game play, and that it, along with other advanced strategies, was an important\r
contributor to Watson’s impressive winning performance. Here we focus only on DD\r
wagering because it is the component of Watson that owes the most to reinforcement\r
learning.\r
Jeopardy! is played by three contestants who face a board showing 30 squares, each of\r
which hides a clue and has a dollar value. The squares are arranged in six columns, each\r
corresponding to a di↵erent category. A contestant selects a square, the host reads the\r
square’s clue, and each contestant may choose to respond to the clue by sounding a buzzer\r
(“buzzing in”). The first contestant to buzz in gets to try responding to the clue. If this\r
contestant’s response is correct, their score increases by the dollar value of the square; if\r
their response is not correct, or if they do not respond within five seconds, their score\r
decreases by that amount, and the other contestants get a chance to buzz in to respond\r
to the same clue. One or two squares (depending on the game’s current round) are\r
special DD squares. A contestant who selects one of these gets an exclusive opportunity\r
to respond to the square’s clue and has to decide—before the clue is revealed—on how\r
much to wager, or bet. The bet has to be greater than five dollars but not greater than\r
the contestant’s current score. If the contestant responds correctly to the DD clue, their\r
score increases by the bet amount; otherwise it decreases by the bet amount. At the end\r
of each game is a “Final Jeopardy” (FJ) round in which each contestant writes down\r
a sealed bet and then writes an answer after the clue is read. The contestant with the\r
highest score after three rounds of play (where a round consists of revealing all 30 clues)\r
is the winner. The game has many other details, but these are enough to appreciate\r
1Registered trademark of IBM Corp.\r
2Registered trademark of Jeopardy Productions Inc.

430 Chapter 16: Applications and Case Studies\r
the importance of DD wagering. Winning or losing often depends on a contestant’s DD\r
wagering strategy.\r
Whenever Watson selected a DD square, it chose its bet by comparing action values,\r
qˆ(s, bet), that estimated the probability of a win from the current game state, s, for\r
each round-dollar legal bet. Except for some risk-abatement measures described below,\r
Watson selected the bet with the maximum action value. Action values were computed\r
whenever a betting decision was needed by using two types of estimates that were learned\r
before any live game play took place. The first were estimated values of the afterstates\r
(Section 6.8) that would result from selecting each legal bet. These estimates were obtained\r
from a state-value function, vˆ(·,w), defined by parameters w, that gave estimates of the\r
probability of a win for Watson from any game state. The second estimates used to\r
compute action values gave the “in-category DD confidence,” pDD, which estimated the\r
likelihood that Watson would respond correctly to the as-yet unrevealed DD clue.\r
Tesauro et al. used the reinforcement learning approach of TD-Gammon described above\r
to learn vˆ(·,w): a straightforward combination of nonlinear TD() using a multilayer\r
ANN with weights w trained by backpropagating TD errors during many simulated\r
games. States were represented to the network by feature vectors specifically designed\r
for Jeopardy!. Features included the current scores of the three players, how many DDs\r
remained, the total dollar value of the remaining clues, and other information related to\r
the amount of play left in the game. Unlike TD-Gammon, which learned by self-play,\r
Watson’s vˆ was learned over millions of simulated games against carefully-crafted models\r
of human players. In-category confidence estimates were conditioned on the number of\r
right responses r and wrong responses w that Watson gave in previously-played clues in\r
the current category. The dependencies on (r, w) were estimated from Watson’s actual\r
accuracies over many thousands of historical categories.\r
With the previously learned value function vˆ and in-category DD confidence pDD,\r
Watson computed ˆq(s, bet) for each legal round-dollar bet as follows:\r
qˆ(s, bet) = pDD ⇥ vˆ(SW + bet,. . .) + (1  pDD) ⇥ vˆ(SW  bet,. . .), (16.2)\r
where SW is Watson’s current score, and vˆ gives the estimated value for the game state\r
after Watson’s response to the DD clue, which is either correct or incorrect. Computing\r
an action value this way corresponds to the insight from Exercise 3.19 that an action\r
value is the expected next state value given the action (except that here it is the expected\r
next afterstate value because the full next state of the entire game depends on the next\r
square selection).\r
Tesauro et al. found that selecting bets by maximizing action values incurred “a\r
frightening amount of risk,” meaning that if Watson’s response to the clue happened to\r
be wrong, the loss could be disastrous for its chances of winning. To decrease the downside\r
risk of a wrong answer, Tesauro et al. adjusted (16.2) by subtracting a small fraction of\r
the standard deviation over Watson’s correct/incorrect afterstate evaluations. They\r
further reduced risk by prohibiting bets that would cause the wrong-answer afterstate\r
value to decrease below a certain limit. These measures slightly reduced Watson’s\r
expectation of winning, but they significantly reduced downside risk, not only in terms of\r
average risk per DD bet, but even more so in extreme-risk scenarios where a risk-neutral\r
Watson would bet most or all of its bankroll.

16.3. Watson’s Daily-Double Wagering 431\r
Why was the TD-Gammon method of self-play not used to learn the critical value\r
function vˆ? Learning from self-play in Jeopardy! would not have worked very well\r
because Watson was so di↵erent from any human contestant. Self-play would have led to\r
exploration of state space regions that are not typical for play against human opponents,\r
particularly human champions. In addition, unlike backgammon, Jeopardy! is a game\r
of imperfect information because contestants do not have access to all the information\r
influencing their opponents’ play. In particular, Jeopardy! contestants do not know how\r
much confidence their opponents have for responding to clues in the various categories.\r
Self-play would have been something like playing poker with someone who is holding the\r
same cards that you hold.\r
As a result of these complications, much of the e↵ort in developing Watson’s DD\u0002wagering strategy was devoted to creating good models of human opponents. The models\r
did not address the natural language aspect of the game, but were instead stochastic\r
process models of events that can occur during play. Statistics were extracted from an\r
extensive fan-created archive of game information from the beginning of the show to the\r
present day. The archive includes information such as the ordering of the clues, right and\r
wrong contestant answers, DD locations, and DD and FJ bets for nearly 300,000 clues.\r
Three models were constructed: an Average Contestant model (based on all the data), a\r
Champion model (based on statistics from games with the 100 best players), and a Grand\r
Champion model (based on statistics from games with the 10 best players). In addition\r
to serving as opponents during learning, the models were used to assess the benefits\r
produced by the learned DD-wagering strategy. Watson’s win rate in simulation when it\r
used a baseline heuristic DD-wagering strategy was 61%; when it used the learned values\r
and a default confidence value, its win rate increased to 64%; and with live in-category\r
confidence, it was 67%. Tesauro et al. regarded this as a significant improvement, given\r
that the DD wagering was needed only about 1.5 to 2 times in each game.\r
Because Watson had only a few seconds to bet, as well as to select squares and decide\r
whether or not to buzz in, the computation time needed to make these decisions was\r
a critical factor. The ANN implementation of vˆ allowed DD bets to be made quickly\r
enough to meet the time constraints of live play. However, once games could be simulated\r
fast enough through improvements in the simulation software, near the end of a game\r
it was feasible to estimate the value of bets by averaging over many Monte-Carlo trials\r
in which the consequence of each bet was determined by simulating play to the game’s\r
end. Selecting endgame DD bets in live play based on Monte-Carlo trials instead of the\r
ANN significantly improved Watson’s performance because errors in value estimates\r
in endgames could seriously a↵ect its chances of winning. Making all the decisions via\r
Monte-Carlo trials might have led to better wagering decisions, but this was simply\r
impossible given the complexity of the game and the time constraints of live play.\r
Although its ability to quickly and accurately answer natural language questions\r
stands out as Watson’s major achievement, all of its sophisticated decision strategies\r
contributed to its impressive defeat of human champions. According to Tesauro et al.\r
(2012):\r
... it is plainly evident that our strategy algorithms achieve a level of quanti\u0002tative precision and real-time performance that exceeds human capabilities.

432 Chapter 16: Applications and Case Studies\r
This is particularly true in the cases of DD wagering and endgame buzzing,\r
where humans simply cannot come close to matching the precise equity and\r
confidence estimates and complex decision calculations performed by Watson.\r
16.4 Optimizing Memory Control\r
Most computers use dynamic random access memory (DRAM) as their main memory\r
because of its low cost and high capacity. The job of a DRAM memory controller is to\r
eciently use the interface between the processor chip and an o↵-chip DRAM system\r
to provide the high-bandwidth and low-latency data transfer necessary for high-speed\r
program execution. A memory controller needs to deal with dynamically changing\r
patterns of read/write requests while adhering to a large number of timing and resource\r
constraints required by the hardware. This is a formidable scheduling problem, especially\r
with modern processors with multiple cores sharing the same DRAM.\r
˙\r
Ipek, Mutlu, Mart´ınez, and Caruana (2008) (also Mart´ınez and ˙Ipek, 2009) designed\r
a reinforcement learning memory controller and demonstrated that it can significantly\r
improve the speed of program execution over what was possible with conventional\r
controllers at the time of their research. They were motivated by limitations of existing\r
state-of-the-art controllers that used policies that did not take advantage of past scheduling\r
experience and did not account for long-term consequences of scheduling decisions. ˙Ipek\r
et al.’s project was carried out by means of simulation, but they designed the controller\r
at the detailed level of the hardware needed to implement it—including the learning\r
algorithm—directly on a processor chip.\r
Accessing DRAM involves a number of steps that have to be done according to strict\r
time constraints. DRAM systems consist of multiple DRAM chips, each containing\r
multiple rectangular arrays of storage cells arranged in rows and columns. Each cell\r
stores a bit as the charge on a capacitor. Because the charge decreases over time, each\r
DRAM cell needs to be recharged—refreshed—every few milliseconds to prevent memory\r
content from being lost. This need to refresh the cells is why DRAM is called “dynamic.”\r
Each cell array has a row bu↵er that holds a row of bits that can be transferred into\r
or out of one of the array’s rows. An activate command “opens a row,” which means\r
moving the contents of the row whose address is indicated by the command into the\r
row bu↵er. With a row open, the controller can issue read and write commands to the\r
cell array. Each read command transfers a word (a short sequence of consecutive bits)\r
in the row bu↵er to the external data bus, and each write command transfers a word\r
in the external data bus to the row bu↵er. Before a di↵erent row can be opened, a\r
precharge command must be issued which transfers the (possibly updated) data in the\r
row bu↵er back into the addressed row of the cell array. After this, another activate\r
command can open a new row to be accessed. Read and write commands are column\r
commands because they sequentially transfer bits into or out of columns of the row bu↵er;\r
multiple bits can be transferred without re-opening the row. Read and write commands\r
to the currently-open row can be carried out more quickly than accessing a di↵erent row,\r
which would involve additional row commands: precharge and activate; this is sometimes

16.4. Optimizing Memory Control 433\r
referred to as “row locality.” A memory controller maintains a memory transaction queue\r
that stores memory-access requests from the processors sharing the memory system. The\r
controller has to process requests by issuing commands to the memory system while\r
adhering to a large number of timing constraints.\r
A controller’s policy for scheduling access requests can have a large e↵ect on the\r
performance of the memory system, such as the average latency with which requests\r
can be satisfied and the throughput the system is capable of achieving. The simplest\r
scheduling strategy handles access requests in the order in which they arrive by issuing\r
all the commands required by the request before beginning to service the next one. But if\r
the system is not ready for one of these commands, or executing a command would result\r
in resources being underutilized (e.g., due to timing constraints arising from servicing\r
that one command), it makes sense to begin servicing a newer request before finishing\r
the older one. Policies can gain eciency by reordering requests, for example, by giving\r
priority to read requests over write requests, or by giving priority to read/write commands\r
to already open rows. The policy called First-Ready, First-Come-First-Serve (FR-FCFS),\r
gives priority to column commands (read and write) over row commands (activate and\r
precharge), and in case of a tie gives priority to the oldest command. FR-FCFS was\r
shown to outperform other scheduling policies in terms of average memory-access latency\r
under conditions commonly encountered (Rixner, 2004).\r
Figure 16.3 is a high-level view of ˙Ipek et al.’s reinforcement learning memory controller.\r
They modeled the DRAM access process as an MDP whose states are the contents of the\r
transaction queue and whose actions are commands to the DRAM system: precharge,\r
activate, read, write, and NoOp. The reward signal is 1 whenever the action is read or\r
write, and otherwise it is 0. State transitions were considered to be stochastic because\r
the next state of the system not only depends on the scheduler’s command, but also on\r
aspects of the system’s behavior that the scheduler cannot control, such as the workloads\r
of the processor cores accessing the DRAM system.\r
Figure 16.3: High-level view of the reinforcement learning DRAM controller. The scheduler is\r
the reinforcement learning agent. Its environment is represented by features of the transaction\r
queue, and its actions are commands to the DRAM system. ©2009 IEEE. Reprinted, with\r
permission, from J. F. Mart´ınez and E. ˙Ipek, Dynamic multicore resource management: A\r
machine learning approach, Micro, IEEE, 29(5), p. 12.

434 Chapter 16: Applications and Case Studies\r
Critical to this MDP are constraints on the actions available in each state. Recall from\r
Chapter 3 that the set of available actions can depend on the state: At 2 A(St), where\r
At is the action at time step t and A(St) is the set of actions available in state St. In\r
this application, the integrity of the DRAM system was assured by not allowing actions\r
that would violate timing or resource constraints. Although ˙Ipek et al. did not make it\r
explicit, they e↵ectively accomplished this by pre-defining the sets A(St) for all possible\r
states St.\r
These constraints explain why the MDP has a NoOp action and why the reward signal\r
is 0 except when a read or write command is issued. NoOp is issued when it is the sole\r
legal action in a state. To maximize utilization of the memory system, the controller’s\r
task is to drive the system to states in which either a read or a write action can be\r
selected: only these actions result in sending data over the external data bus, so it is only\r
these that contribute to the throughput of the system. Although precharge and activate\r
produce no immediate reward, the agent needs to select these actions to make it possible\r
to later select the rewarded read and write actions.\r
The scheduling agent used Sarsa (Section 6.4) to learn an action-value function. States\r
were represented by six integer-valued features. To approximate the action-value function,\r
the algorithm used linear function approximation implemented by tile coding with hashing\r
(Section 9.5.4). The tile coding had 32 tilings, each storing 256 action values as 16-bit\r
fixed point numbers. Exploration was "-greedy with " = 0.05.\r
State features included the number of read requests in the transaction queue, the\r
number of write requests in the transaction queue, the number of write requests in the\r
transaction queue waiting for their row to be opened, and the number of read requests in\r
the transaction queue waiting for their row to be opened that are the oldest issued by\r
their requesting processors. (The other features depended on how the DRAM interacts\r
with cache memory, details we omit here.) The selection of the state features was based\r
on ˙Ipek et al.’s understanding of factors that impact DRAM performance. For example,\r
balancing the rate of servicing reads and writes based on how many of each are in the\r
transaction queue can help avoid stalling the DRAM system’s interaction with cache\r
memory. The authors in fact generated a relatively long list of potential features, and\r
then pared them down to a handful using simulations guided by stepwise feature selection.\r
An interesting aspect of this formulation of the scheduling problem as an MDP is\r
that the features input to the tile coding for defining the action-value function were\r
di↵erent from the features used to specify the action-constraint sets A(St). Whereas the\r
tile coding input was derived from the contents of the transaction queue, the constraint\r
sets depended on a host of other features related to timing and resource constraints that\r
had to be satisfied by the hardware implementation of the entire system. In this way, the\r
action constraints ensured that the learning algorithm’s exploration could not endanger\r
the integrity of the physical system, while learning was e↵ectively limited to a “safe”\r
region of the much larger state space of the hardware implementation.\r
Because an objective of this work was that the learning controller could be implemented\r
on a chip so that learning could occur online while a computer is running, hardware\r
implementation details were important considerations. The design included two five-stage\r
pipelines to calculate and compare two action values at every processor clock cycle, and

16.4. Optimizing Memory Control 435\r
to update the appropriate action value. This included accessing the tile coding which\r
was stored on-chip in static RAM. For the configuration ˙Ipek et al. simulated, which was\r
a 4GHz 4-core chip typical of high-end workstations at the time of their research, there\r
were 10 processor cycles for every DRAM cycle. Considering the cycles needed to fill\r
the pipes, up to 12 actions could be evaluated in each DRAM cycle. ˙Ipek et al. found\r
that the number of legal commands for any state was rarely greater than this, and that\r
performance loss was negligible if enough time was not always available to consider all\r
legal commands. These and other clever design details made it feasible to implement the\r
complete controller and learning algorithm on a multi-processor chip.\r
˙\r
Ipek et al. evaluated their learning controller in simulation by comparing it with three\r
other controllers: (1) the FR-FCFS controller mentioned above that produces the best\r
on-average performance, (2) a conventional controller that processes each request in\r
order, and (3) an unrealizable ideal controller, called the Optimistic controller, able\r
to sustain 100% DRAM throughput if given enough demand by ignoring all timing\r
and resource constraints, but otherwise modeling DRAM latency (as row bu↵er hits)\r
and bandwidth. They simulated nine memory-intensive parallel workloads consisting of\r
scientific and data-mining applications. Figure 16.4 shows the performance (the inverse\r
of execution time normalized to the performance of FR-FCFS) of each controller for\r
the nine applications, together with the geometric mean of their performances over the\r
applications. The learning controller, labeled RL in the figure, improved over that of\r
FR-FCFS by from 7% to 33% over the nine applications, with an average improvement of\r
19%. Of course, no realizable controller can match the performance of Optimistic, which\r
ignores all timing and resource constraints, but the learning controller’s performance\r
closed the gap with Optimistic’s upper bound by an impressive 27%.\r
Because the rationale for on-chip implementation of the learning algorithm was to\r
allow the scheduling policy to adapt online to changing workloads, ˙Ipek et al. analyzed\r
the impact of online learning compared to a previously-learned fixed policy. They trained\r
Figure 16.4: Performances of four controllers over a suite of 9 simulated benchmark applications.\r
The controllers are: the simplest ‘in-order’ controller, FR-FCFS, the learning controller RL,\r
and the unrealizable Optimistic controller which ignores all timing and resource constraints to\r
provide a performance upper bound. Performance, normalized to that of FR-FCFS, is the inverse\r
of execution time. At far right is the geometric mean of performances over the 9 benchmark\r
applications for each controller. Controller RL comes closest to the ideal performance. ©2009\r
IEEE. Reprinted, with permission, from J. F. Mart´ınez and E. ˙Ipek, Dynamic multicore resource\r
management: A machine learning approach, Micro, IEEE, 29(5), p. 13.

436 Chapter 16: Applications and Case Studies\r
their controller with data from all nine benchmark applications and then held the resulting\r
action values fixed throughout the simulated execution of the applications. They found\r
that the average performance of the controller that learned online was 8% better than\r
that of the controller using the fixed policy, leading them to conclude that online learning\r
is an important feature of their approach.\r
This learning memory controller was never committed to physical hardware because of\r
the large cost of fabrication. Nevertheless, ˙Ipek et al. could convincingly argue on the basis\r
of their simulation results that a memory controller that learns online via reinforcement\r
learning has the potential to improve performance to levels that would otherwise require\r
more complex and more expensive memory systems, while removing from human designers\r
some of the burden required to manually design ecient scheduling policies. Mukundan\r
and Mart´ınez (2012) took this project forward by investigating learning controllers with\r
additional actions, other performance criteria, and more complex reward functions derived\r
using genetic algorithms. They considered additional performance criteria related to\r
energy eciency. The results of these studies surpassed the earlier results described\r
above and significantly surpassed the 2012 state-of-the-art for all of the performance\r
criteria they considered. The approach is especially promising for developing sophisticated\r
power-aware DRAM interfaces.\r
16.5 Human-level Video Game Play\r
One of the greatest challenges in applying reinforcement learning to real-world problems\r
is deciding how to represent and store value functions and/or policies. Unless the state\r
set is finite and small enough to allow exhaustive representation by a lookup table—as in\r
many of our illustrative examples—one must use a parameterized function approximation\r
scheme. Whether linear or nonlinear, function approximation relies on features that\r
have to be readily accessible to the learning system and able to convey the information\r
necessary for skilled performance. Most successful applications of reinforcement learning\r
owe much to sets of features carefully handcrafted based on human knowledge and\r
intuition about the specific problem to be tackled.\r
A team of researchers at Google DeepMind developed an impressive demonstration that\r
a deep multi-layer ANN can automate the feature design process (Mnih et al., 2013, 2015).\r
Multi-layer ANNs have been used for function approximation in reinforcement learning\r
ever since the 1986 popularization of the backpropagation algorithm as a method for\r
learning internal representations (Rumelhart, Hinton, and Williams, 1986; see Section 9.7).\r
Striking results have been obtained by coupling reinforcement learning with backpropa\u0002gation. The results obtained by Tesauro and colleagues with TD-Gammon and Watson\r
discussed above are notable examples. These and other applications benefited from the\r
ability of multi-layer ANNs to learn task-relevant features. However, in all the examples\r
of which we are aware, the most impressive demonstrations required the network’s input\r
to be represented in terms of specialized features handcrafted for the given problem.\r
This is vividly apparent in the TD-Gammon results. TD-Gammon 0.0, whose network\r
input was essentially a “raw” representation of the backgammon board, meaning that it\r
involved very little knowledge of backgammon, learned to play approximately as well as

16.5. Human-level Video Game Play 437\r
the best previous backgammon computer programs. Adding specialized backgammon\r
features produced TD-Gammon 1.0 which was substantially better than all previous\r
backgammon programs and competed well against human experts.\r
Mnih et al. developed a reinforcement learning agent called deep Q-network (DQN)\r
that combined Q-learning with a deep convolutional ANN, a many-layered, or deep,\r
ANN specialized for processing spatial arrays of data such as images. We describe deep\r
convolutional ANNs in Section 9.7. By the time of Mnih et al.’s work with DQN, deep\r
ANNs, including deep convolutional ANNs, had produced impressive results in many\r
applications, but they had not been widely used in reinforcement learning.\r
Mnih et al. used DQN to show how a reinforcement learning agent can achieve a high\r
level of performance on any of a collection of di↵erent problems without having to use\r
di↵erent problem-specific feature sets. To demonstrate this, they let DQN learn to play\r
49 di↵erent Atari 2600 video games by interacting with a game emulator. DQN learned a\r
di↵erent policy for each of the 49 games (because the weights of its ANN were reset to\r
random values before learning on each game), but it used the same raw input, network\r
architecture, and parameter values (e.g., step size, discount rate, exploration parameters,\r
and many more specific to the implementation) for all the games. DQN achieved levels\r
of play at or beyond human level on a large fraction of these games. Although the games\r
were alike in being played by watching streams of video images, they varied widely in other\r
respects. Their actions had di↵erent e↵ects, they had di↵erent state-transition dynamics,\r
and they needed di↵erent policies for learning high scores. The deep convolutional ANN\r
learned to transform the raw input common to all the games into features specialized for\r
representing the action values required for playing at the high level DQN achieved for\r
most of the games.\r
The Atari 2600 is a home video game console that was sold in various versions by Atari\r
Inc. from 1977 to 1992. It introduced or popularized many arcade video games that are\r
now considered classics, such as Pong, Breakout, Space Invaders, and Asteroids. Although\r
much simpler than modern video games, Atari 2600 games are still entertaining and\r
challenging for human players, and they have been attractive as testbeds for developing\r
and evaluating reinforcement learning methods (Diuk, Cohen, Littman, 2008; Naddaf,\r
2010; Cobo, Zang, Isbell, and Thomaz, 2011; Bellemare, Veness, and Bowling, 2013).\r
Bellemare, Naddaf, Veness, and Bowling (2012) developed the publicly available Arcade\r
Learning Environment (ALE) to encourage and simplify using Atari 2600 games to study\r
learning and planning algorithms.\r
These previous studies and the availability of ALE made the Atari 2600 game collection\r
a good choice for Mnih et al.’s demonstration, which was also influenced by the impressive\r
human-level performance that TD-Gammon was able to achieve in backgammon. DQN\r
is similar to TD-Gammon in using a multi-layer ANN as the function approximation\r
method for a semi-gradient form of a TD algorithm, with the gradients computed by\r
the backpropagation algorithm. However, instead of using TD() as TD-Gammon did,\r
DQN used the semi-gradient form of Q-learning. TD-Gammon estimated the values of\r
afterstates, which were easily obtained from the rules for making backgammon moves.\r
To use the same algorithm for the Atari games would have required generating the next\r
states for each possible action (which would not have been afterstates in that case).\r
This could have been done by using the game emulator to run single-step simulations

438 Chapter 16: Applications and Case Studies\r
for all the possible actions (which ALE makes possible). Or a model of each game’s\r
state-transition function could have been learned and used to predict next states (Oh,\r
Guo, Lee, Lewis, and Singh, 2015). While these methods might have produced results\r
comparable to DQN’s, they would have been more complicated to implement and would\r
have significantly increased the time needed for learning. Another motivation for using\r
Q-learning was that DQN used the experience replay method, described below, which\r
requires an o↵-policy algorithm. Being model-free and o↵-policy made Q-learning a\r
natural choice.\r
Before describing the details of DQN and how the experiments were conducted, we look\r
at the skill levels DQN was able to achieve. Mnih et al. compared the scores of DQN with\r
the scores of the best performing learning system in the literature at the time, the scores\r
of a professional human games tester, and the scores of an agent that selected actions at\r
random. The best system from the literature used linear function approximation with\r
features designed using some knowledge about Atari 2600 games (Bellemare, Naddaf,\r
Veness, and Bowling, 2013). DQN learned on each game by interacting with the game\r
emulator for 50 million frames, which corresponds to about 38 days of experience with\r
the game. At the start of learning on each game, the weights of DQN’s network were reset\r
to random values. To evaluate DQN’s skill level after learning, its score was averaged\r
over 30 sessions on each game, each lasting up to 5 minutes and beginning with a random\r
initial game state. The professional human tester played using the same emulator (with\r
the sound turned o↵ to remove any possible advantage over DQN which did not process\r
audio). After 2 hours of practice, the human played about 20 episodes of each game for up\r
to 5 minutes each and was not allowed to take any break during this time. DQN learned\r
to play better than the best previous reinforcement learning systems on all but 6 of the\r
games, and played better than the human player on 22 of the games. By considering any\r
performance that scored at or above 75% of the human score to be comparable to, or\r
better than, human-level play, Mnih et al. concluded that the levels of play DQN learned\r
reached or exceeded human level on 29 of the 46 games. See Mnih et al. (2015) for a\r
more detailed account of these results.\r
For an artificial learning system to achieve these levels of play would be impressive\r
enough, but what makes these results remarkable—and what many at the time considered\r
to be breakthrough results for artificial intelligence—is that the very same learning system\r
achieved these levels of play on widely varying games without relying on any game-specific\r
modifications.\r
A human playing any of these 49 Atari games sees 210⇥160 pixel image frames with\r
128 colors at 60Hz. In principle, exactly these images could have formed the raw input to\r
DQN, but to reduce memory and processing requirements, Mnih et al. preprocessed each\r
frame to produce an 84⇥84 array of luminance values. Because the full states of many\r
of the Atari games are not completely observable from the image frames, Mnih et al.\r
“stacked” the four most recent frames so that the inputs to the network had dimension\r
84⇥84⇥4. This did not eliminate partial observability for all of the games, but it was\r
helpful in making many of them more Markovian.\r
An essential point here is that these preprocessing steps were exactly the same for all 46\r
games. No game-specific prior knowledge was involved beyond the general understanding\r
that it should still be possible to learn good policies with this reduced dimension and

16.5. Human-level Video Game Play 439\r
that stacking adjacent frames should help with the partial observability of some of the\r
games. Because no game-specific prior knowledge beyond this minimal amount was used\r
in preprocessing the image frames, we can think of the 84⇥84⇥4 input vectors as being\r
“raw” input to DQN.\r
The basic architecture of DQN is similar to the deep convolutional ANN illustrated in\r
Figure 9.15 (though unlike that network, subsampling in DQN is treated as part of each\r
convolutional layer, with feature maps consisting of units having only a selection of the\r
possible receptive fields). DQN has three hidden convolutional layers, followed by one\r
fully connected hidden layer, followed by the output layer. The three successive hidden\r
convolutional layers of DQN produce 32 20⇥20 feature maps, 64 9⇥9 feature maps,\r
and 64 7⇥7 feature maps. The activation function of the units of each feature map is a\r
rectifier nonlinearity (max(0, x)). The 3,136 (64⇥7⇥7) units in this third convolutional\r
layer all connect to each of 512 units in the fully connected hidden layer, which then each\r
connect to all 18 units in the output layer, one for each possible action in an Atari game.\r
The activation levels of DQN’s output units were the estimated optimal action values\r
of the corresponding state–action pairs, for the state represented by the network’s input.\r
The assignment of output units to a game’s actions varied from game to game, and\r
because the number of valid actions varied between 4 and 18 for the games, not all output\r
units had functional roles in all of the games. It helps to think of the network as if it\r
were 18 separate networks, one for estimating the optimal action value of each possible\r
action. In reality, these networks shared their initial layers, but the output units learned\r
to use the features extracted by these layers in di↵erent ways.\r
DQN’s reward signal indicated how a games’s score changed from one time step to\r
the next: +1 whenever it increased, 1 whenever it decreased, and 0 otherwise. This\r
standardized the reward signal across the games and made a single step-size parameter\r
work well for all the games despite their varying ranges of scores. DQN used an "-greedy\r
policy, with " decreasing linearly over the first million frames and remaining at a low\r
value for the rest of the learning session. The values of the various other parameters,\r
such as the learning step size, discount rate, and others specific to the implementation,\r
were selected by performing informal searches to see which values worked best for a small\r
selection of the games. These values were then held fixed for all of the games.\r
After DQN selected an action, the action was executed by the game emulator, which\r
returned a reward and the next video frame. The frame was preprocessed and added\r
to the four-frame stack that became the next input to the network. Skipping for the\r
moment the changes to the basic Q-learning procedure made by Mnih et al., DQN used\r
the following semi-gradient form of Q-learning to update the network’s weights:\r
wt+1 = wt + ↵\r
h\r
Rt+1 +  maxa qˆ(St+1, a, wt)  qˆ(St, At, wt)\r
i\r
rqˆ(St, At, wt), (16.3)\r
where wt is the vector of the network’s weights, At is the action selected at time step t,\r
and St and St+1 are respectively the preprocessed image stacks input to the network at\r
time steps t and t + 1.\r
The gradient in (16.3) was computed by backpropagation. Imagining again that there\r
was a separate network for each action, for the update at time step t, backpropagation\r
was applied only to the network corresponding to At. Mnih et al. took advantage of\r
techniques shown to improve the basic backpropagation algorithm when applied to large

440 Chapter 16: Applications and Case Studies\r
networks. They used a mini-batch method that updated weights only after accumulating\r
gradient information over a small batch of images (here after 32 images). This yielded\r
smoother sample gradients compared to the usual procedure that updates weights after\r
each action. They also used a gradient-ascent algorithm called RMSProp (Tieleman and\r
Hinton, 2012) that accelerates learning by adjusting the step-size parameter for each\r
weight based on a running average of the magnitudes of recent gradients for that weight.\r
Mnih et al. modified the basic Q-learning procedure in three ways. First, they used\r
a method called experience replay first studied by Lin (1992). This method stores the\r
agent’s experience at each time step in a replay memory that is accessed to perform the\r
weight updates. It worked like this in DQN. After the game emulator executed action\r
At in a state represented by the image stack St, and returned reward Rt+1 and image\r
stack St+1, it added the tuple (St, At, Rt+1, St+1) to the replay memory. This memory\r
accumulated experiences over many plays of the same game. At each time step multiple Q\u0002learning updates—a mini-batch—were performed based on experiences sampled uniformly\r
at random from the replay memory. Instead of St+1 becoming the new St for the next\r
update as it would in the usual form of Q-learning, a new unconnected experience was\r
drawn from the replay memory to supply data for the next update. Because Q-learning\r
is an o↵-policy algorithm, it does not need to be applied along connected trajectories.\r
Q-learning with experience replay provided several advantages over the usual form of\r
Q-learning. The ability to use each stored experience for many updates allowed DQN to\r
learn more eciently from its experiences. Experience replay reduced the variance of the\r
updates because successive updates were not correlated with one another as they would\r
be with standard Q-learning. And by removing the dependence of successive experiences\r
on the current weights, experience replay eliminated one source of instability.\r
Mnih et al. modified standard Q-learning in a second way to improve its stability. As\r
in other methods that bootstrap, the target for a Q-learning update depends on the\r
current action-value function estimate. When a parameterized function approximation\r
method is used to represent action values, the target is a function of the same parameters\r
that are being updated. For example, the target in the update given by (16.3) is\r
 maxa qˆ(St+1, a, wt). Its dependence on wt complicates the process compared to the\r
simpler supervised-learning situation in which the targets do not depend on the parameters\r
being updated. As discussed in Chapter 11 this can lead to oscillations and/or divergence.\r
To address this problem Mnih et al. used a technique that brought Q-learning closer\r
to the simpler supervised-learning case while still allowing it to bootstrap. Whenever\r
a certain number, C, of updates had been done to the weights w of the action-value\r
network, they inserted the network’s current weights into another network and held\r
these duplicate weights fixed for the next C updates of w. The outputs of this duplicate\r
network over the next C updates of w were used as the Q-learning targets. Letting q˜\r
denote the output of this duplicate network, then instead of (16.3) the update rule was:\r
wt+1 = wt + ↵\r
h\r
Rt+1 +  maxa q˜(St+1, a, wt)  qˆ(St, At, wt)\r
i\r
rqˆ(St, At, wt).\r
A final modification of standard Q-learning was also found to improve stability. They\r
clipped the error term Rt+1 +  maxa q˜(St+1, a, wt)  qˆ(St, At, wt) so that it remained in\r
the interval [1, 1].

16.6. Mastering the Game of Go 441\r
Mnih et al. conducted a large number of learning runs on 5 of the games to gain\r
insight into the e↵ect that various of DQN’s design features had on its performance.\r
They ran DQN with the four combinations of experience replay and the duplicate\r
target network being included or not included. Although the results varied from game\r
to game, each of these features alone significantly improved performance, and very\r
dramatically improved performance when used together. Mnih et al. also studied the role\r
played by the deep convolutional ANN in DQN’s learning ability by comparing the deep\r
convolutional version of DQN with a version having a network of just one linear layer, both\r
receiving the same stacked preprocessed video frames. Here, the improvement of the deep\r
convolutional version over the linear version was particularly striking across all 5 of the test\r
games.\r
Creating artificial agents that excel over a diverse collection of challenging tasks has\r
been an enduring goal of artificial intelligence. The promise of machine learning as\r
a means for achieving this has been frustrated by the need to craft problem-specific\r
representations. DeepMind’s DQN stands as a major step forward by demonstrating\r
that a single agent can learn problem-specific features enabling it to acquire human\u0002competitive skills over a range of tasks. This demonstration did not produce one agent\r
that simultaneously excelled at all the tasks (because learning occurred separately for\r
each task), but it showed that deep learning can reduce, and possibly eliminate, the need\r
for problem-specific design and tuning. As Mnih et al. point out, however, DQN is not\r
a complete solution to the problem of task-independent learning. Although the skills\r
needed to excel on the Atari games were markedly diverse, all the games were played by\r
observing video images, which made a deep convolutional ANN a natural choice for this\r
collection of tasks. In addition, DQN’s performance on some of the Atari 2600 games\r
fell considerably short of human skill levels on these games. The games most dicult\r
for DQN—especially Montezuma’s Revenge on which DQN learned to perform about as\r
well as the random player—require deep planning beyond what DQN was designed to\r
do. Further, learning control skills through extensive practice, like DQN learned how to\r
play the Atari games, is just one of the types of learning humans routinely accomplish.\r
Despite these limitations, DQN advanced the state-of-the-art in machine learning by\r
impressively demonstrating the promise of combining reinforcement learning with modern\r
methods of deep learning.\r
16.6 Mastering the Game of Go\r
The ancient Chinese game of Go has challenged artificial intelligence researchers for many\r
decades. Methods that achieve human-level skill, or even superhuman-level skill, in other\r
games have not been successful in producing strong Go programs. Thanks to a very\r
active community of Go programmers and international competitions, the level of Go\r
program play has improved significantly over the years. Until recently, however, no Go\r
program had been able to play anywhere near the level of a human Go master.\r
A team at DeepMind (Silver et al., 2016) developed the program AlphaGo that broke\r
this barrier by combining deep ANNs (Section 9.7), supervised learning, Monte Carlo

442 Chapter 16: Applications and Case Studies\r
tree search (MCTS, Section 8.11), and reinforcement learning. By the time of Silver et\r
al.’s 2016 publication, AlphaGo had been shown to be decisively stronger than other Go\r
programs, and it had defeated the European Go champion Fan Hui 5 games to 0. These\r
were the first victories of a Go program over a professional human Go player without\r
handicap in full Go games. Shortly thereafter, a similar version of AlphaGo won stunning\r
victories over the 18-time world champion Lee Sedol, winning 4 out of a 5 games in\r
a challenge match, making worldwide headline news. Artificial intelligence researchers\r
thought that it would be many more years, perhaps decades, before a program reached\r
this level of play.\r
Here we describe AlphaGo and a successor program called AlphaGo Zero (Silver et al.\r
2017a). Where in addition to reinforcement learning, AlphaGo relied on supervised learn\u0002ing from a large database of expert human moves, AlphaGo Zero used only reinforcement\r
learning and no human data or guidance beyond the basic rules of the game (hence the\r
Zero in its name). We first describe AlphaGo in some detail in order to highlight the\r
relative simplicity of AlphaGo Zero, which is both higher-performing and more of a pure\r
reinforcement learning program.\r
In many ways, both AlphaGo and AlphaGo Zero are descendants of Tesauro’s TD\u0002Gammon (Section 16.1), itself a descendant of Samuel’s checkers player (Section 16.2).\r
All these programs included reinforcement learning over simulated games of self-play.\r
AlphaGo and AlphaGo Zero also built upon the progress made by DeepMind on playing\r
Atari games with the program DQN (Section 16.5) that used deep convolutional ANNs\r
to approximate optimal value functions.\r
A Go board configuration\r
Go is a game between two players who alter\u0002nately place black and white ‘stones’ on unoccu\u0002pied intersections, or ‘points,’ on a board with\r
a grid of 19 horizontal and 19 vertical lines to\r
produce positions like that shown to the right.\r
The game’s goal is to capture an area of the\r
board larger than that captured by the oppo\u0002nent. Stones are captured according to simple\r
rules. A player’s stones are captured if they\r
are completely surrounded by the other player’s\r
stones, meaning that there is no horizontally\r
or vertically adjacent point that is unoccupied.\r
For example, the left panel of Figure 16.5 (on\r
the next page) shows three white stones with\r
an unoccupied adjacent point (labeled X). If\r
black were to place a stone on X, then the three\r
white stones would be captured and taken o↵\r
the board (middle panel). However, if white were to place a stone on point X first, then\r
the possibility of this capture would be blocked (right panel). Other rules are needed to\r
prevent infinite capturing/recapturing loops. The game ends when neither player wishes\r
to place another stone. These rules are simple, but they produce a very complex game\r
that has had wide appeal for thousands of years.

16.6. Mastering the Game of Go 443\r
X\r
Figure 16.5: Go capturing rule. Left: the three white stones are not surrounded because point\r
X is unoccupied. Middle: if black places a stone on X, the three white stones are captured and\r
removed from the board. Right: if white places a stone on point X first, the capture is blocked.\r
Methods that produce strong play for other games, such as chess, have not worked as\r
well for Go. The search space for Go is significantly larger than that of chess because\r
Go has a larger number of legal moves per position than chess (⇡ 250 versus ⇡ 35) and\r
Go games tend to involve more moves than chess games (⇡ 150 versus ⇡ 80). But the\r
size of the search space is not the major factor that makes Go so dicult. Exhaustive\r
search is infeasible for both chess and Go, and Go on smaller boards (e.g., 9 ⇥ 9) has\r
proven to be exceedingly dicult as well. Experts agree that the major stumbling block\r
to creating stronger-than-amateur Go programs is the diculty of defining an adequate\r
position evaluation function. A good evaluation function allows search to be truncated at\r
a feasible depth by providing relatively easy-to-compute predictions of what deeper search\r
would likely yield. According to M¨uller (2002): “No simple yet reasonable evaluation\r
function will ever be found for Go.” A major step forward was the introduction of MCTS\r
to Go programs. The strongest programs at the time of AlphaGo’s development all\r
included MCTS, but master-level skill remained elusive.\r
Recall from Section 8.11 that MCTS is a decision-time planning procedure that does\r
not attempt to learn and store a global evaluation function. Like a rollout algorithm\r
(Section 8.10), it runs many Monte Carlo simulations of entire episodes (here, entire\r
Go games) to select each action (here, each Go move: where to place a stone or to\r
resign). Unlike a simple rollout algorithm, however, MCTS is an iterative procedure that\r
incrementally extends a search tree whose root node represents the current environment\r
state. As illustrated in Figure 8.10, each iteration traverses the tree by simulating\r
actions guided by statistics associated with the tree’s edges. In its basic version, when\r
a simulation reaches a leaf node of the search tree, MCTS expands the tree by adding\r
some, or all, of the leaf node’s children to the tree. From the leaf node, or one of its\r
newly added child nodes, a rollout is executed: a simulation that typically proceeds all\r
the way to a terminal state, with actions selected by a rollout policy. When the rollout\r
completes, the statistics associated with the search tree’s edges that were traversed in\r
this iteration are updated by backing up the return produced by the rollout. MCTS\r
continues this process, starting each time at the search tree’s root at the current state, for\r
as many iterations as possible given the time constraints. Then, finally, an action from\r
the root node (which still represents the current environment state) is selected according\r
to statistics accumulated in the root node’s outgoing edges. This is the action the agent\r
takes. After the environment transitions to its next state, MCTS is executed again with\r
the root node set to represent the new current state. The search tree at the start of this\r
next execution might be just this new root node, or it might include descendants of this\r
node left over from MCTS’s previous execution. The remainder of the tree is discarded.

444 Chapter 16: Applications and Case Studies\r
16.6.1 AlphaGo\r
The main innovation that made AlphaGo such a strong player is that it selected moves by\r
a novel version of MCTS that was guided by both a policy and a value function learned\r
by reinforcement learning with function approximation provided by deep convolutional\r
ANNs. Another key feature is that instead of reinforcement learning starting from random\r
network weights, it started from weights that were the result of previous supervised\r
learning from a large collection of human expert moves.\r
The DeepMind team called AlphaGo’s modification of basic MCTS “asynchronous\r
policy and value MCTS,” or APV-MCTS. It selected actions via basic MCTS as described\r
above but with some twists in how it extended its search tree and how it evaluated action\r
edges. In contrast to basic MCTS, which expands its current search tree by using stored\r
action values to select an unexplored edge from a leaf node, APV-MCTS, as implemented\r
in AlphaGo, expanded its tree by choosing an edge according to probabilities supplied by\r
a 13-layer deep convolutional ANN, called the SL-policy network, trained previously by\r
supervised learning to predict moves contained in a database of nearly 30 million human\r
expert moves.\r
Then, also in contrast to basic MCTS, which evaluates the newly-added state node\r
solely by the return of a rollout initiated from it, APV-MCTS evaluated the node in two\r
ways: by this return of the rollout, but also by a value function, v✓, learned previously by\r
a reinforcement learning method. If s was the newly-added node, its value became\r
v(s) = (1  ⌘)v✓(s) + ⌘G, (16.4)\r
where G was the return of the rollout and ⌘ controlled the mixing of the values resulting\r
from these two evaluation methods. In AlphaGo, these values were supplied by the\r
value network, another 13-layer deep convolutional ANN that was trained as we describe\r
below to output estimated values of board positions. APV-MCTS’s rollouts in AlphaGo\r
were simulated games with both players using a fast rollout policy provided by a simple\r
linear network, also trained by supervised learning before play. Throughout its execution,\r
APV-MCTS kept track of how many simulations passed through each edge of the search\r
tree, and when its execution completed, the most-visited edge from the root node was\r
selected as the action to take, here the move AlphaGo actually made in a game.\r
The value network had the same structure as the deep convolutional SL policy network\r
except that it had a single output unit that gave estimated values of game positions\r
instead of the SL policy network’s probability distributions over legal actions. Ideally,\r
the value network would output optimal state values, and it might have been possible to\r
approximate the optimal value function along the lines of TD-Gammon described above:\r
self-play with nonlinear TD() coupled to a deep convolutional ANN. But the DeepMind\r
team took a di↵erent approach that held more promise for a game as complex as Go.\r
They divided the process of training the value network into two stages. In the first stage,\r
they created the best policy they could by using reinforcement learning to train an RL\r
policy network. This was a deep convolutional ANN with the same structure as the SL\r
policy network. It was initialized with the final weights of the SL policy network that\r
were learned via supervised learning, and then policy-gradient reinforcement learning was\r
used to improve upon the SL policy. In the second stage of training the value network,

16.6. Mastering the Game of Go 445\r
the team used Monte Carlo policy evaluation on data obtained from a large number of\r
simulated self-play games with moves selected by the RL policy network.\r
Figure 16.6 illustrates the networks used by AlphaGo and the steps taken to train them\r
in what the DeepMind team called the “AlphaGo pipeline.” All these networks were\r
trained before any live game play took place, and their weights remained fixed throughout\r
live play.\r
sampled state-action pairs (s, a), using stochastic gradient ascent to \r
maximize the likelihood of the human move a selected in state s\r
∆σ\r
σ\r
∝\r
∂ ( | )\r
∂\r
σ log p a s\r
We trained a 13-layer policy network, which we call the SL policy \r
network, from 30 million positions from the KGS Go Server. The net\u0002work predicted expert moves on a held out test set with an accuracy of \r
57.0% using all input features, and 55.7% using only raw board posi\u0002tion and move history as inputs, compared to the state-of-the-art from \r
other research groups of 44.4% at date of submission24 (full results in \r
Extended Data Table 3). Small improvements in accuracy led to large \r
improvements in playing strength (Fig. 2a); larger networks achieve \r
bttbt ltltdihWl\r
and its weights ρ are in\r
games between the curre\r
previous iteration of the \r
of opponents in this way \r
to the current policy. We \r
non-terminal time steps \r
nal reward at the end of \r
player at time step t: +1 \r
then updated at each tim\r
direction that maximizes \r
∆\r
Figure 1 | Neural network training pipeline and architecture. a, A fast \r
rollout policy pπ and supervised learning (SL) policy network pσ are \r
trained to predict human expert moves in a data set of positions. \r
A reinforcement learning (RL) policy network pρ is initialized to the SL \r
policy network, and is then improved by policy gradient learning to \r
maximize the outcome (that is, winning more games) against previous \r
versions of the policy network. A new data set is generated by playing \r
games of self-play with the RL policy network. Finally, a value network vθ\r
is trained by regression to predict the expected outcome (that is, whether \r
the current player wins) in \r
b, Schematic representatio\r
AlphaGo. The policy netw\r
s as its input, passes it thro\r
σ (SL policy network) or ρ \r
distribution ( | ) σp a s or ( \r
ρ\r
p a \r
probability map over the b\r
convolutional layers with p\r
that predicts the expected \r
Regression\r
Classi!cation\r
Classi!cation\r
Self Play\r
Policy gradient\r
a b\r
Human expert positions Self-play positions\r
Neural network Data\r
Rollout policy\r
pS pV \r
p \r
U QT\r
SL policy network RL policy network Value network Poli\r
Rollout Policy SL policy network RL policy Network Value Network\r
Policy gradient\r
Supervised Learning\r
MC Policy Evaluation\r
Self Play\r
Supervised Learning\r
]\r
]\r
Networks Data\r
Figure 16.6: AlphaGo pipeline. Adapted with permission from Macmillan Publishers Ltd:\r
Nature, vol. 529(7587), p. 485, ©2016.\r
Here is some more detail about AlphaGo’s ANNs and their training. The identically\u0002structured SL and RL policy networks were similar to DQN’s deep convolutional network\r
described in Section 16.5 for playing Atari games, except that they had 13 convolutional\r
layers with the final layer consisting of a soft-max unit for each point on the 19 ⇥ 19\r
Go board. The networks’ input was a 19 ⇥ 19 ⇥ 48 image stack in which each point\r
on the Go board was represented by the values of 48 binary or integer-valued features.\r
For example, for each point, one feature indicated if the point was occupied by one of\r
AlphaGo’s stones, one of its opponent’s stones, or was unoccupied, thus providing the\r
“raw” representation of the board configuration. Other features were based on the rules\r
of Go, such as the number of adjacent points that were empty, the number of opponent\r
stones that would be captured by placing a stone there, the number of turns since a stone\r
was placed there, and other features that the design team considered to be important.\r
Training the SL policy network took approximately 3 weeks using a distributed\r
implementation of stochastic gradient ascent on 50 processors. The network achieved 57%\r
accuracy, where the best accuracy achieved by other groups at the time of publication\r
was 44.4%. Training the RL policy network was done by policy gradient reinforcement\r
learning over simulated games between the RL policy network’s current policy and\r
opponents using policies randomly selected from policies produced by earlier iterations\r
of the learning algorithm. Playing against a randomly selected collection of opponents

446 Chapter 16: Applications and Case Studies\r
prevented overfitting to the current policy. The reward signal was +1 if the current\r
policy won, 1 if it lost, and zero otherwise. These games directly pitted the two policies\r
against one another without involving MCTS. By simulating many games in parallel on\r
50 processors, the DeepMind team trained the RL policy network on a million games\r
in a single day. In testing the final RL policy, they found that it won more than 80%\r
of games played against the SL policy, and it won 85% of games played against a Go\r
program using MCTS that simulated 100,000 games per move.\r
The value network, whose structure was similar to that of the SL and RL policy\r
networks except for its single output unit, received the same input as the SL and RL\r
policy networks with the exception that there was an additional binary feature giving\r
the current color to play. Monte Carlo policy evaluation was used to train the network\r
from data obtained from a large number of self-play games played using the RL policy.\r
To avoid overfitting and instability due to the strong correlations between positions\r
encountered in self-play, the DeepMind team constructed a data set of 30 million positions\r
each chosen randomly from a unique self-play game. Then training was done using 50\r
million mini-batches each of 32 positions drawn from this data set. Training took one\r
week on 50 GPUs.\r
The rollout policy was learned prior to play by a simple linear network trained by\r
supervised learning from a corpus of 8 million human moves. The rollout policy network\r
had to output actions quickly while still being reasonably accurate. In principle, the SL\r
or RL policy networks could have been used in the rollouts, but the forward propagation\r
through these deep networks took too much time for either of them to be used in rollout\r
simulations, a great many of which had to be carried out for each move decision during\r
live play. For this reason, the rollout policy network was less complex than the other\r
policy networks, and its input features could be computed more quickly than the features\r
used for the policy networks. The rollout policy network allowed approximately 1,000\r
complete game simulations per second to be run on each of the processing threads that\r
AlphaGo used.\r
One may wonder why the SL policy was used instead of the better RL policy to select\r
actions in the expansion phase of APV-MCTS. These policies took the same amount of\r
time to compute because they used the same network architecture. The team actually\r
found that AlphaGo played better against human opponents when APV-MCTS used as\r
the SL policy instead of the RL policy. They conjectured that the reason for this was\r
that the latter was tuned to respond to optimal moves rather than to the broader set\r
of moves characteristic of human play. Interestingly, the situation was reversed for the\r
value function used by APV-MCTS. They found that when APV-MCTS used the value\r
function derived from the RL policy, it performed better than if it used the value function\r
derived from the SL policy.\r
Several methods worked together to produce AlphaGo’s impressive playing skill. The\r
DeepMind team evaluated di↵erent versions of AlphaGo in order to assess the contributions\r
made by these various components. The parameter ⌘ in (16.4) controlled the mixing\r
of game state evaluations produced by the value network and by rollouts. With ⌘ = 0,\r
AlphaGo used just the value network without rollouts, and with ⌘ = 1, evaluation\r
relied just on rollouts. They found that AlphaGo using just the value network played

16.6. Mastering the Game of Go 447\r
better than the rollout-only AlphaGo, and in fact played better than the strongest of all\r
other Go programs existing at the time. The best play resulted from setting ⌘ = 0.5,\r
indicating that combining the value network with rollouts was particularly important\r
to AlphaGo’s success. These evaluation methods complemented one another: the value\r
network evaluated the high-performance RL policy that was too slow to be used in live\r
play, while rollouts using the weaker but much faster rollout policy were able to add\r
precision to the value network’s evaluations for specific states that occurred during games.\r
Overall, AlphaGo’s remarkable success fueled a new round of enthusiasm for the promise\r
of artificial intelligence, specifically for systems combining reinforcement learning with\r
deep ANNs, to address problems in other challenging domains.\r
16.6.2 AlphaGo Zero\r
Building upon the experience with AlphaGo, a DeepMind team developed AlphaGo Zero\r
(Silver et al. 2017a). In contrast to AlphaGo, this program used no human data or\r
guidance beyond the basic rules of the game (hence the Zero in its name). It learned\r
exclusively from self-play reinforcement learning, with input giving just “raw” descriptions\r
of the placements of stones on the Go board. AlphaGo Zero implemented a form of\r
policy iteration (Section 4.3), interleaving policy evaluation with policy improvement.\r
Figure 16.7 is an overview of AlphaGo Zero’s algorithm. A significant di↵erence between\r
AlphaGo Zero and AlphaGo is that AlphaGo Zero used MCTS to select moves throughout\r
self-play reinforcement learning, whereas AlphaGo used MCTS for live play after—but not\r
during—learning. Other di↵erences besides not using any human data or human-crafted\r
features are that AlphaGo Zero used only one deep convolutional ANN and used a simpler\r
version of MCTS.\r
AlphaGo Zero’s MCTS was simpler than the version used by AlphaGo in that it did\r
not include rollouts of complete games, and therefore did not need a rollout policy. Each\r
iteration of AlphaGo Zero’s MCTS ran a simulation that ended at a leaf node of the\r
current search tree instead of at the terminal position of a complete game simulation.\r
But as in AlphaGo, each iteration of MCTS in AlphaGo Zero was guided by the output of\r
a deep convolutional network, labeled f✓ in Figure 16.7, where ✓ is the network’s weight\r
vector. The input to the network, whose architecture we describe below, consisted of raw\r
representations of board positions, and its output had two parts: a scalar value, v, an\r
estimate of the probability that the current player will win from from the current board\r
position, and a vector, p, of move probabilities, one for each possible stone placement on\r
the current board, plus the pass, or resign, move.\r
Instead of selecting self-play actions according to the probabilities p, however, AlphaGo\r
Zero used these probabilities, together with the network’s value output, to direct each\r
execution of MCTS, which returned new move probabilities, shown in Figure 16.7 as the\r
policies ⇡i. These policies benefitted from the many simulations that MCTS conducted\r
each time it executed. The result was that the policy actually followed by AlphaGo\r
Zero was an improvement over the policy given by the network’s outputs p. Silver et al.\r
(2017a) wrote that “MCTS may therefore be viewed as a powerful policy improvement\r
operator.”

448 Chapter 16: Applications and Case Studies\r
Figure 1: Self-play reinforcement learning in AlphaGo Zero. a The program plays a game\r
s1, ..., sT against itself. In each position st, a Monte-Carlo tree search (MCTS) is executed (see\r
Figure 2) using the latest neural network f. Moves are selected according to the search probabil\u0002ities computed by the MCTS, at ⇠ t. The terminal position sT is scored to compute the game\r
winner z. b Neural network training in AlphaGo Zero. The neural network takes the raw board\r
position s as its input, passes it through many convolutional layers with parameters , and outputs\r
both a vector p, representing a probability distribution over moves, and a scalar value v, represent\u0002ing the probability of the current player winning in position s. The neural network is trained on\r
randomly sampled steps from recent games of self-play, (s,, z). The parameters  are updated so\r
as to maximise the similarity of the policy vector p to the search probabilities , and to minimise\r
the error between the predicted winner v and the game winner z (see Equation 1).\r
4\r
Figure 16.7: AlphaGo Zero self-play reinforcement learning. a) The program played many\r
games against itself, one shown here as a sequence of board positions si, i = 1, 2,...,T, with\r
moves ai, i = 1, 2,...,T, and winner z. Each move ai was determined by action probabilities ⇡i\r
returned by MCTS executed from root node si and guided by a deep convolutional network,\r
here labeled f✓, with latest weights ✓. Shown here for just one position s but repeated for all\r
si, the network’s inputs were raw representations of board positions si (together with several\r
past positions, though not shown here), and its outputs were vectors p of move probabilities\r
that guided MCTS’s forward searches, and scalar values v that estimated the probability of the\r
current player winning from each position si. b) Deep convolutional network training. Training\r
examples were randomly sampled steps from recent self-play games. Weights ✓ were updated\r
to move the policy vector p toward the probabilities ⇡ returned by MCTS, and to include the\r
winners z in the estimated win probability v. Reprinted from draft of Silver et al. (2017a) with\r
permission of the authors and DeepMind.\r
Here is more detail about AlphaGo Zero’s ANN and how it was trained. The network\r
took as input a 19 ⇥ 19 ⇥ 17 image stack consisting of 17 binary feature planes. The first\r
8 feature planes were raw representations of the positions of the current player’s stones in\r
the current and seven past board configurations: a feature value was 1 if a player’s stone\r
was on the corresponding point, and was 0 otherwise. The next 8 feature planes similarly\r
coded the positions of the opponent’s stones. A final input feature plane had a constant\r
value indicating the color of the current play: 1 for black; 0 for white. Because repetition\r
is not allowed in Go and one player is given some number of “compensation points” for\r
not getting the first move, the current board position is not a Markov state of Go. This\r
is why features describing past board positions and the color feature were needed.\r
The network was “two-headed,” meaning that after a number of initial layers, the\r
network split into two separate “heads” of additional layers that separately fed into two\r
sets of output units. In this case, one head fed 362 output units producing 192 + 1 move

16.6. Mastering the Game of Go 449\r
probabilities p, one for each possible stone placement plus pass; the other head fed just\r
one output unit producing the scalar v, an estimate of the probability that the current\r
player will win from the current board position. The network before the split consisted of\r
41 convolutional layers, each followed by batch normalization, and with skip connections\r
added to implement residual learning by pairs of layers (see Section 9.7). Overall, move\r
probabilities and values were computed by 43 and 44 layers respectively.\r
Starting with random weights, the network was trained by stochastic gradient descent\r
(with momentum, regularization, and step-size parameter decreasing as training continues)\r
using batches of examples sampled uniformly at random from all the steps of the most\r
recent 500,000 games of self-play with the current best policy. Extra noise was added\r
to the network’s output p to encourage exploration of all possible moves. At periodic\r
checkpoints during training, which Silver et al. (2017a) chose to be at every 1,000 training\r
steps, the policy output by the ANN with the latest weights was evaluated by simulating\r
400 games (using MCTS with 1,600 iterations to select each move) against the current\r
best policy. If the new policy won (by a margin set to reduce noise in the outcome), then\r
it became the best policy to be used in subsequent self-play. The network’s weights were\r
updated to make the network’s policy output p more closely match the policy returned\r
by MCTS, and to make its value output, v, more closely match the probability that the\r
current best policy wins from the board position represented by the network’s input.\r
The DeepMind team trained AlphaGo Zero over 4.9 million games of self-play, which\r
took about 3 days. Each move of each game was selected by running MCTS for 1,600\r
iterations, taking approximately 0.4 second per move. Network weights were updated over\r
700,000 batches each consisting of 2,048 board configurations. They then ran tournaments\r
with the trained AlphaGo Zero playing against the version of AlphaGo that defeated Fan\r
Hui by 5 games to 0, and against the version that defeated Lee Sedol by 4 games to 1.\r
They used the Elo rating system to evaluate the relative performances of the programs.\r
The di↵erence between two Elo ratings is meant to predict the outcome of games between\r
the players. The Elo ratings of AlphaGo Zero, the version of AlphaGo that played against\r
Fan Hui, and the version that played against Lee Sedol were respectively 4,308, 3,144,\r
and 3,739. The gaps in these Elo ratings translate into predictions that AlphaGo Zero\r
would defeat these other programs with probabilities very close to one. In a match of 100\r
games between AlphaGo Zero, trained as described, and the exact version of AlphaGo\r
that defeated Lee Sedol held under the same conditions that were used in that match,\r
AlphaGo Zero defeated AlphaGo in all 100 games.\r
The DeepMind team also compared AlphaGo Zero with a program using an ANN with\r
the same architecture but trained by supervised learning to predict human moves in a\r
data set containing nearly 30 million positions from 160,000 games. They found that the\r
supervised-learning player initially played better than AlphaGo Zero, and was better at\r
predicting human expert moves, but played less well after AlphaGo Zero was trained for\r
a day. This suggested that AlphaGo Zero had discovered a strategy for playing that was\r
di↵erent from how humans play. In fact, AlphaGo Zero discovered, and came to prefer,\r
some novel variations of classical move sequences.\r
Final tests of AlphaGo Zero’s algorithm were conducted with a version having a larger\r
ANN and trained over 29 million self-play games, which took about 40 days, again starting

450 Chapter 16: Applications and Case Studies\r
with random weights. This version achieved an Elo rating of 5,185. The team pitted\r
this version of AlphaGo Zero against a program called AlphaGo Master , the strongest\r
program at the time, that was identical to AlphaGo Zero but, like AlphaGo, used human\r
data and features. AlphaGo Master ’s Elo rating was 4,858, and it had defeated the\r
strongest human professional players 60 to 0 in online games. In a 100 game match,\r
AlphaGo Zero with the larger network and more extensive learning defeated AlphaGo\r
Master 89 games to 11, thus providing a convincing demonstration of the problem-solving\r
power of AlphaGo Zero’s algorithm.\r
AlphaGo Zero soundly demonstrated that superhuman performance can be achieved\r
by pure reinforcement learning, augmented by a simple version of MCTS, and deep ANNs\r
with very minimal knowledge of the domain and no reliance on human data or guidance.\r
We will surely see systems inspired by the DeepMind accomplishments of both AlphaGo\r
and AlphaGo Zero applied to challenging problems in other domains.\r
Recently, yet a better program, AlphaZero, was described by Silver et al. (2017b) that\r
does not even incorporate knowledge of Go. AlphaZero is a general reinforcement learning\r
algorithm that improves over the world’s hitherto best programs in the diverse games of\r
Go, chess, and shogi.\r
16.7 Personalized Web Services\r
Personalizing web services such as the delivery of news articles or advertisements is one\r
approach to increasing users’ satisfaction with a website or to increase the yield of a\r
marketing campaign. A policy can recommend content considered to be the best for each\r
particular user based on a profile of that user’s interests and preferences inferred from\r
their history of online activity. This is a natural domain for machine learning, and in\r
particular, for reinforcement learning. A reinforcement learning system can improve a\r
recommendation policy by making adjustments in response to user feedback. One way\r
to obtain user feedback is by means of website satisfaction surveys, but for acquiring\r
feedback in real time it is common to monitor user clicks as indicators of interest in a\r
link.\r
A method long used in marketing called A/B testing is a simple type of reinforcement\r
learning used to decide which of two versions, A or B, of a website users prefer. Because\r
it is non-associative, like a two-armed bandit problem, this approach does not personalize\r
content delivery. Adding context consisting of features describing individual users and\r
the content to be delivered allows personalizing service. This has been formalized as a\r
contextual bandit problem (or an associative reinforcement learning problem, Section 2.9)\r
with the objective of maximizing the total number of user clicks. Li, Chu, Langford, and\r
Schapire (2010) applied a contextual bandit algorithm to the problem of personalizing\r
the Yahoo! Front Page Today webpage (one of the most visited pages on the internet at\r
the time of their research) by selecting the news story to feature. Their objective was to\r
maximize the click-through rate (CTR), which is the ratio of the total number of clicks\r
all users make on a webpage to the total number of visits to the page. Their contextual\r
bandit algorithm improved over a standard non-associative bandit algorithm by 12.5%.\r
Theocharous, Thomas, and Ghavamzadeh (2015) argued that better results are possible

16.7. Personalized Web Services 451\r
by formulating personalized recommendation as a Markov decision problem (MDP) with\r
the objective of maximizing the total number of clicks users make over repeated visits to\r
a website. Policies derived from the contextual bandit formulation are greedy in the sense\r
that they do not take long-term e↵ects of actions into account. These policies e↵ectively\r
treat each visit to a website as if it were made by a new visitor uniformly sampled from\r
the population of the website’s visitors. By not using the fact that many users repeatedly\r
visit the same websites, greedy policies do not take advantage of possibilities provided by\r
long-term interactions with individual users.\r
As an example of how a marketing strategy might take advantage of long-term user\r
interaction, Theocharous et al. contrasted a greedy policy with a longer-term policy for\r
displaying ads for buying a product, say a car. The ad displayed by the greedy policy\r
might o↵er a discount if the user buys the car immediately. A user either takes the o↵er\r
or leaves the website, and if they ever return to the site, they would likely see the same\r
o↵er. A longer-term policy, on the other hand, can transition the user “down a sales\r
funnel” before presenting the final deal. It might start by describing the availability of\r
favorable financing terms, then praise an excellent service department, and then, on the\r
next visit, o↵er the final discount. This type of policy can result in more clicks by a user\r
over repeated visits to the site, and if the policy is suitably designed, more eventual sales.\r
Working at Adobe Systems Incorporated, Theocharous et al. conducted experiments\r
to see if policies designed to maximize clicks over the long term could in fact improve\r
over short-term greedy policies. The Adobe Marketing Cloud, a set of tools that many\r
companies use to run digital marketing campaigns, provides infrastructure for automating\r
user-targeted advertising and fund-raising campaigns. Actually deploying novel policies\r
using these tools entails significant risk because a new policy may end up performing\r
poorly. For this reason, the research team needed to assess what a policy’s performance\r
would be if it were to be actually deployed, but to do so on the basis of data collected\r
under the execution of other policies. A critical aspect of this research, then, was o↵-policy\r
evaluation. Further, the team wanted to do this with high confidence to reduce the\r
risk of deploying a new policy. Although high confidence o↵-policy evaluation was a\r
central component of this research (see also Thomas, 2015; Thomas, Theocharous, and\r
Ghavamzadeh, 2015), here we focus only on the algorithms and their results.\r
Theocharous et al. compared the results of two algorithms for learning ad recommen\u0002dation policies. The first algorithm, which they called greedy optimization, had the goal\r
of maximizing only the probability of immediate clicks. As in the standard contextual\r
bandit formulation, this algorithm did not take the long-term e↵ects of recommendations\r
into account. The other algorithm, a reinforcement learning algorithm based on an MDP\r
formulation, aimed at improving the number of clicks users made over multiple visits to\r
a website. They called this latter algorithm life-time value (LTV) optimization. Both\r
algorithms faced challenging problems because the reward signal in this domain is very\r
sparse because users usually do not click on ads, and user clicking is very random so that\r
returns have high variance.\r
Data sets from the banking industry were used for training and testing these algorithms.\r
The data sets consisted of many complete trajectories of customer interaction with a\r
bank’s website that showed each customer one out of a collection of possible o↵ers. If

452 Chapter 16: Applications and Case Studies\r
a customer clicked, the reward was 1, and otherwise it was 0. One data set contained\r
approximately 200,000 interactions from a month of a bank’s campaign that randomly\r
o↵ered one of 7 o↵ers. The other data set from another bank’s campaign contained\r
4,000,000 interactions involving 12 possible o↵ers. All interactions included customer\r
features such as the time since the customer’s last visit to the website, the number of their\r
visits so far, the last time the customer clicked, geographic location, one of a collection of\r
interests, and features giving demographic information.\r
Greedy optimization was based on a mapping estimating the probability of a click\r
as a function of user features. The mapping was learned via supervised learning from\r
one of the data sets by means of a random forest (RF) algorithm (Breiman, 2001). RF\r
algorithms have been widely used for large-scale applications in industry because they are\r
e↵ective predictive tools that tend not to overfit and are relatively insensitive to outliers\r
and noise. Theocharous et al. then used the mapping to define an "-greedy policy that\r
selected with probability 1-" the o↵er predicted by the RF algorithm to have the highest\r
probability of producing a click, and otherwise selected from the other o↵ers uniformly at\r
random.\r
LTV optimization used a batch-mode reinforcement learning algorithm called fitted\r
Q iteration (FQI). It is a variant of fitted value iteration (Gordon, 1999) adapted to\r
Q-learning. Batch mode means that the entire data set for learning is available from\r
the start, as opposed to the online mode of the algorithms we focus on in this book in\r
which data are acquired sequentially while the learning algorithm executes. Batch-mode\r
reinforcement learning algorithms are sometimes necessary when online learning is not\r
practical, and they can use any batch-mode supervised learning regression algorithm,\r
including algorithms known to scale well to high-dimensional spaces. The convergence of\r
FQI depends on properties of the function approximation algorithm (Gordon, 1999). For\r
their application to LTV optimization, Theocharous et al. used the same RF algorithm\r
they used for the greedy optimization approach. Because in this case FQI convergence\r
is not monotonic, Theocharous et al. kept track of the best FQI policy by o↵-policy\r
evaluation using a validation training set. The final policy for testing the LTV approach\r
was the "-greedy policy based on the best policy produced by FQI with the initial\r
action-value function set to the mapping produced by the RF for the greedy optimization\r
approach.\r
To measure the performance of the policies produced by the greedy and LTV approaches,\r
Theocharous et al. used the CTR metric and a metric they called the LTV metric. These\r
metrics are similar, except that the LTV metric critically distinguishes between individual\r
website visitors:\r
CTR = Total # of Clicks\r
Total # of Visits,\r
LTV = Total # of Clicks\r
Total # of Visitors.\r
Figure 16.8 illustrates how these metrics di↵er. Each circle represents a user visit to\r
the site; black circles are visits at which the user clicks. Each row represents visits by\r
a particular user. By not distinguishing between visitors, the CTR for these sequences\r
is 0.35, whereas the LTV is 1.5. Because LTV is larger than CTR to the extent that

16.8. Thermal Soaring 453\r
individual users revisit the site, it is an indicator of how successful a policy is in encouraging\r
users to engage in extended interactions with the site.\r
Figure 16.8: Click through rate (CTR) versus life-time value (LTV). Each circle represents\r
a user visit; black circles are visits at which the user clicks. Adapted from Theocharous et\r
al. (2015).\r
Testing the policies produced by the greedy and LTV approaches was done using\r
a high confidence o↵-policy evaluation method on a test data set consisting of real\u0002world interactions with a bank website served by a random policy. As expected, results\r
showed that greedy optimization performed best as measured by the CTR metric, while\r
LTV optimization performed best as measured by the LTV metric. Furthermore—\r
although we have omitted its details—the high confidence o↵-policy evaluation method\r
provided probabilistic guarantees that the LTV optimization method would, with high\r
probability, produce policies that improve upon policies currently deployed. Assured by\r
these probabilistic guarantees, Adobe announced in 2016 that the new LTV algorithm\r
would be a standard component of the Adobe Marketing Cloud so that a retailer could\r
issue a sequence of o↵ers following a policy likely to yield higher return than a policy\r
that is insensitive to long-term results.\r
16.8 Thermal Soaring\r
Birds and gliders take advantage of upward air currents—thermals—to gain altitude in\r
order to maintain flight while expending little, or no, energy. Thermal soaring, as this\r
behavior is called, is a complex skill requiring responding to subtle environmental cues\r
to increase altitude by exploiting a rising column of air for as long as possible. Reddy,\r
Celani, Sejnowski, and Vergassola (2016) used reinforcement learning to investigate\r
thermal soaring policies that are e↵ective in the strong atmospheric turbulence usually\r
accompanying rising air currents. Their primary goal was to provide insight into the\r
cues birds sense and how they use them to achieve their impressive thermal soaring\r
performance, but the results also contribute to technology relevant to autonomous gliders.\r
Reinforcement learning had previously been applied to the problem of navigating eciently\r
to the vicinity of a thermal updraft (Woodbury, Dunn, and Valasek, 2014) but not to the\r
more challenging problem of soaring within the turbulence of the updraft itself.\r
Reddy et al. modeled the soaring problem as a continuing MDP with discounting.\r
The agent interacted with a detailed model of a glider flying in turbulent air. They

454 Chapter 16: Applications and Case Studies\r
devoted significant e↵ort toward making the model generate realistic thermal soaring\r
conditions, including investigating several di↵erent approaches to atmospheric modeling.\r
For the learning experiments, air flow in a three-dimensional box with one kilometer\r
sides, one of which was at ground level, was modeled by a sophisticated physics-based\r
set of partial di↵erential equations involving air velocity, temperature, and pressure.\r
Introducing small random perturbations into the numerical simulation caused the model\r
to produce analogs of thermal updrafts and accompanying turbulence (Figure 16.9 Left)\r
Glider flight was modeled by aerodynamic equations involving velocity, lift, drag, and\r
other factors governing powerless flight of a fixed-wing aircraft. Maneuvering the glider\r
involved changing its angle of attack (the angle between the glider’s wing and the direction\r
of air flow) and its bank angle (Figure 16.9 Right).\r
A\r
C z\r
y\r
Lift L\r
z\r
x\r
Lift L\r
Drag D\r
velocity direction\r
wing direction\r
bank angle\r
glide angle\r
angle of attack\r
B\r
D\r
of the vertical velocity (A) and the temperature fields (B) in our numerical simulations of 3D Rayleigh–Bénard conv\r
ed and blue colors indicate regions of large upward and downward flow, respectively. For the temperature field, t\r
high and low temperature, respectively. Notice that the hot and cold regions drive the upward and downward bran\r
with the basic physics of convection. (C) The force-body diagram of flight with no thrust, that is, without any engine \r
ows the bank angle μ (blue), the angle of attack α (green), and the glide angle γ (red). (D) The range of horizontal s\r
olling the angle of attack. At small angles of attack, the glider moves fast but also sinks fast, whereas at larger angles\r
If the angle of attack is too highat about 16°the glider stallsleading to a sudden drop in liftThe vertical black \r
contribute significantly and more exploratory strategies are\r
preferred.\r
The SARSA algorithm finds the optimal policy by estimating\r
for every state–action pair its Q function defined as the expected\r
sum of future rewards given the current state s and the action a.\r
At each step, the Q function is updated as follows:\r
Qðs, aÞ → Qðs, aÞ+ηðr +βQðs′, a′Þ − Qðs, aÞÞ, [5]\r
where r is the received reward and η is the learning rate. The\r
update is made online and does not require any prior model of\r
the flow or the flight. This feature is particularly relevant in\r
modeling decision-making processes in animals. When the algo\u0002rithm is close to convergence, the Q function approaches the\r
solution to Bellman’s dynamic programming equations (12).\r
In the sequel, w\r
as optimal. It sho\r
algorithm (as othe\r
identifies an appr\r
is skipped only fo\r
Results\r
Sensorimotor Cues \r
aspects of the lea\r
motor cues that th\r
of the reward use\r
state and action sp\r
necessary to discre\r
lookup table rep\r
averaged over dif\r
performance crite\r
A\r
C z\r
y\r
Lift L\r
z\r
x\r
Lift L\r
Drag D\r
velocity direction\r
wing direction\r
bank angle\r
glide angle\r
angle of attack\r
B\r
D\r
Fig. 1. Snapshots of the vertical velocity (A) and the temperature fields (B) in our numerical simulati\r
velocity field, the red and blue colors indicate regions of large upward and downward flow, respectiv\r
indicate regions of high and low temperature, respectively. Notice that the hot and cold regions drive \r
cell, in agreement with the basic physics of convection. (C) The force-body diagram of flight with no t\r
The figure also shows the bank angle μ (blue), the angle of attack α (green), and the glide angle γ (re\r
accessible by controlling the angle of attack. At small angles of attack, the glider moves fast but also si\r
sinks more slowly. If the angle of attack is too high, at about 16°, the glider stalls, leading to a sudde\r
fixed angle of attack for most of the simulations (Results, Control over the Angle of Attack).\r
↵\r
Figure 16.9: Thermal soaring model: Left: snapshot of the vertical velocity field of the\r
simulated cube of air: in red (blue) is a region of large upward (downward) flow. Right: diagram\r
of powerless flight showing bank angle µ and angle of attack ↵. Adapted with permission From\r
PNAS vol. 113(22), p. E4879, 2016, Reddy, Celani, Sejnowski, and Vergassola, Learning to Soar\r
in Turbulent Environments.\r
The interface between the agent and the environment required defining the agent’s\r
actions, the state information the agent receives from the environment, and the reward\r
signal. By experimenting with various possibilities, Reddy et al. decided that three actions\r
each for the angle of attack and the bank angle were enough for their purposes: increment\r
or decrement the current bank angle and angle of attack by 5 and 2.5, respectively, or\r
leave them unchanged. This resulted in 32 possible actions. The bank angle was bounded\r
to remain between 15 and +15.\r
Because a goal of their study was to try to determine what minimal set of sensory\r
cues are necessary for e↵ective soaring, both to shed light on the cues birds might use for\r
soaring and to minimize the sensing complexity required for automated glider soaring,\r
the authors tried various sets of signals as input to the reinforcement learning agent.\r
They started by using state aggregation (Section 9.3) of a four-dimensional state space\r
with dimensions giving local vertical wind speed, local vertical wind acceleration, torque\r
depending on the di↵erence between the vertical wind velocities at the left and right wing\r
tips, and the local temperature. Each dimension was discretized into three bins: positive

16.8. Thermal Soaring 455\r
high, negative high, and small. Results, described below, showed that only two of these\r
dimensions were critical for e↵ective soaring behavior.\r
The overall objective of thermal soaring is to gain as much altitude as possible from\r
each rising column of air. Reddy et al. tried a straightforward reward signal that rewarded\r
the agent at the end of each episode based on the altitude gained over the episode, a\r
large negative reward signal if the glider touched the ground, and zero otherwise. They\r
found that learning was not successful with this reward signal for episodes of realistic\r
duration and that eligibility traces did not help. By experimenting with various reward\r
signals, they found that learning was best with a reward signal that at each time step\r
linearly combined the vertical wind velocity and vertical wind acceleration observed on\r
the previous time step.\r
Learning was by one-step Sarsa, with actions selected according to a soft-max dis\u0002tribution based on normalized action values. Specifically, the action probabilities were\r
computed according to (13.2) with action preferences:\r
h(s, a, ✓) = qˆ(s, a, ✓)  minb qˆ(s, b, ✓)\r
⌧\r
\r
maxb qˆ(s, b, ✓)  minb qˆ(s, b, ✓)\r
 ,\r
where ✓ is a parameter vector with one component for each action and aggregated group\r
of states, and qˆ(s, a, ✓) merely returned the component corresponding to s, a in the usual\r
way for state aggregation methods. The above equation forms the action preferences\r
by normalizing the approximate action values to the interval [0, 1] then dividing by ⌧ , a\r
positive “temperature parameter.”3 As ⌧ increases, the probability of selecting an action\r
becomes less dependent on its preference; as ⌧ decreases toward zero, the probability of\r
selecting the most highly-preferred action approaches one, making the policy approach\r
the greedy policy. The temperature parameter ⌧ was initialized to 2.0 and incrementally\r
decreased to 0.2 during learning. Action preferences were computed from the current\r
estimates of the action values: the action with the maximum estimated action value was\r
given preference 1/⌧ , the action with the minimum estimated action value was given\r
preference 0, and the preferences of the other actions were scaled between these extremes.\r
The step-size and discount-rate parameters were fixed at 0.1 and 0.98 respectively.\r
Each learning episode took place with the agent controlling simulated flight in an\r
independently generated period of simulated turbulent air currents. Each episode lasted\r
2.5 minutes simulated with a 1 second time step. Learning e↵ectively converged after a\r
few hundred episodes. The left panel of Figure 16.10 shows a sample trajectory before\r
learning where the agent selects actions randomly. Starting at the top of the volume\r
shown, the glider’s trajectory is in the direction indicated by the arrow and quickly loses\r
altitude. Figure 16.10’s right panel is a trajectory after learning. The glider starts at the\r
same place (here appearing at the bottom of the volume) and gains altitude by spiraling\r
within the rising column of air. Although Reddy et al. found that performance varied\r
widely over di↵erent simulated periods of air flow, the number of times the glider touched\r
the ground consistently decreased to nearly zero as learning progressed.\r
After experimenting with di↵erent sets of features available to the learning agent, it\r
turned out that the combination of just vertical wind acceleration and torques worked\r
3Reddy et al. described this slightly di↵erently, but our version is equivalent to theirs.

456 Chapter 16: Applications and Case Studies\r
(a) (b) Figure 16.10: Sample thermal soaring trajectories, with arrows showing the direction of\r
flight from the same starting point (note that the altitude scales are shifted). Left: before\r
learning: the agent selects actions randomly and the glider descends. Right: after learning:\r
the glider gains altitude by following a spiral trajectory. Adapted with permission from PNAS\r
vol. 113(22), p. E4879, 2016, Reddy, Celani, Sejnowski, and Vergassola, Learning to Soar in\r
Turbulent Environments.\r
best. The authors conjectured that because these features give information about the\r
gradient of vertical wind velocity in two di↵erent directions, they allow the controller to\r
select between turning by changing the bank angle or continuing along the same course\r
by leaving the bank angle alone. This allows the glider to stay within a rising column of\r
air. Vertical wind velocity is indicative of the strength of the thermal but does not help\r
in staying within the flow. They found that sensitivity to temperature was of little help.\r
They also found that controlling the angle of attack is not helpful in staying within a\r
particular thermal, being useful instead for traveling between thermals when covering\r
large distances, as in cross-country gliding and bird migration.\r
Due to the fact that soaring in di↵erent levels of turbulence requires di↵erent policies,\r
training was done in conditions ranging from weak to strong turbulence. In strong\r
turbulence the rapidly changing wind and glider velocities allowed less time for the\r
controller to react. This reduced the amount of control possible compared to what\r
was possible for maneuvering when fluctuations were weak. Reddy et al. examined the\r
policies Sarsa learned under these di↵erent conditions. Common to policies learned in all\r
regimes were these features: when sensing negative wind acceleration, bank sharply in the\r
direction of the wing with the higher lift; when sensing large positive wind acceleration\r
and no torque, do nothing. However, di↵erent levels of turbulence led to policy di↵erences.\r
Policies learned in strong turbulence were more conservative in that they preferred small\r
bank angles, whereas in weak turbulence, the best action was to turn as much as possible\r
by banking sharply. Systematic study of the bank angles preferred by the policies learned\r
under the di↵erent conditions led the authors to suggest that by detecting when vertical

16.8. Thermal Soaring 457\r
wind acceleration crosses a certain threshold the controller can adjust its policy to cope\r
with di↵erent turbulence regimes.\r
Reddy et al. also conducted experiments to investigate the e↵ect of the discount-rate\r
parameter  on the performance of the learned policies. They found that the altitude\r
gained in an episode increased as  increased, reaching a maximum for  = .99, suggesting\r
that e↵ective thermal soaring requires taking into account long-term e↵ects of control\r
decisions.\r
This computational study of thermal soaring illustrates how reinforcement learning\r
can further progress toward di↵erent kinds of objectives. Learning policies having\r
access to di↵erent sets of environmental cues and control actions contributes to both\r
the engineering objective of designing autonomous gliders and the scientific objective of\r
improving understanding of the soaring skills of birds. In both cases, hypotheses resulting\r
from the learning experiments can be tested in the field by instrumenting real gliders4\r
and by comparing predictions with observed bird soaring behavior.\r
4This work has recently been applied to real gliders. See Reddy, Wong-Ng, Celani, Sejnowski, and\r
Vergassola, “Glider soaring via reinforcement learning in the field.” Nature 562:236–239, 2018.

Chapter 17\r
Frontiers\r
In this final chapter we touch on some topics that are beyond the scope of this book but\r
that we see as particularly important for the future of reinforcement learning. Many of\r
these topics bring us beyond what is reliably known, and some bring us beyond the MDP\r
framework.\r
17.1 General Value Functions and Auxiliary Tasks\r
Over the course of this book, our notion of value function has become quite general.\r
With o↵-policy learning we allowed a value function to be conditional on an arbitrary\r
target policy. Then in Section 12.8 we generalized discounting to a termination function\r
 : S 7! [0, 1], so that a di↵erent discount rate could be applied at each time step in\r
determining the return (12.17). This allowed us to express predictions about how much\r
reward we will get over an arbitrary, state-dependent horizon. The next, and perhaps\r
final, step is to generalize beyond rewards to permit predictions about arbitrary signals.\r
Rather than predicting the sum of future rewards, we might predict the sum of the future\r
values of a sound or color sensation, or of an internal, highly processed signal such as\r
another prediction. Whatever signal is added up in this way in a value-function-like\r
prediction, we call it the cumulant of that prediction. We formalize it in a cumulant\r
signal Ct 2 R. Using this, a general value function, or GVF, is written\r
v⇡,,C (s) .= E\r
"\r
X1\r
k=t\r
 Y\r
k\r
i=t+1\r
(Si)\r
!\r
Ck+1\r
\r
\r
\r
\r
\r
St =s, At:1 ⇠⇡\r
#\r
. (17.1)\r
As with conventional value functions (such as v⇡ or q⇤) this is an ideal function that\r
we seek to approximate with a parameterized form, which we might continue to denote\r
vˆ(s,w), although of course there would have to be a di↵erent w for each prediction, that\r
is, for each choice of ⇡, , and C. Because a GVF has no necessary connection to reward,\r
it is perhaps a misnomer to call it a value function. You might simply call it a prediction\r
or, to make it more distinctive, a forecast (Ring, in preparation). Whatever it is called, it

460 Chapter 17: Frontiers\r
is in the form of a value function and thus can be learned in the usual ways using the\r
methods developed in this book for learning approximate value functions. Along with\r
the learned predictions, we might also learn policies to maximize the predictions in the\r
usual ways by Generalized Policy Iteration (Section 4.6) or by actor–critic methods. In\r
this way an agent could learn to predict and control great numbers of signals, not just\r
long-term reward.\r
Why might it be useful to predict and control signals other than long-term reward?\r
These are auxiliary tasks in that they are extra (in addition to) the main task of\r
maximizing reward. One answer is that the ability to predict and control a diverse\r
multitude of signals can constitute a powerful kind of environmental model. As we saw\r
in Chapter 8, a good model can enable the agent to get reward more eciently. It takes\r
a couple of further concepts to develop this answer clearly, so we postpone it to the next\r
section. First let’s consider two simpler ways in which a multitude of diverse predictions\r
can be helpful to a reinforcement learning agent.\r
One simple way in which auxiliary tasks can help on the main task is that they may\r
require some of the same representations as are needed on the main task. Some of the\r
auxiliary tasks may be easier, with less delay and a clearer connection between actions\r
and outcomes. If good features can be found early on easy auxiliary tasks, then those\r
features may significantly speed learning on the main task. There is no necessary reason\r
why this has to be true, but in many cases it seems plausible. For example, if you learn\r
to predict and control your sensors over short time scales, say seconds, then you might\r
plausibly come up with part of the idea of physical objects, which would then greatly\r
help with the prediction and control of long-term reward.\r
We might imagine an artificial neural network (ANN) in which the last layer is split\r
into multiple parts, or heads, each working on a di↵erent task. One head might produce\r
the approximate value function for the main task (with reward as its cumulant) whereas\r
the others would produce solutions to various auxiliary tasks. All heads could propagate\r
errors by stochastic gradient descent into the same body—the shared preceding part\r
of the network—which would then try to form representations, in its next-to-last layer,\r
to support all the heads. Researchers have experimented with auxiliary tasks such as\r
predicting change in pixels, predicting the next time step’s reward, and predicting the\r
distribution of the return. In many cases this approach has been shown to greatly\r
accelerate learning on the main task (Jaderberg et al., 2017). Multiple predictions\r
have similarly been repeatedly proposed as a way of directing the construction of state\r
estimates (see Section 17.3).\r
Another simple way in which the learning of auxiliary tasks can improve performance\r
is best explained by analogy to the psychological phenomena of classical conditioning\r
(Section 14.2). One way of understanding classical conditioning is that evolution has\r
built in a reflexive (non-learned) association to a particular action from the prediction\r
of a particular signal. For example, humans and many other animals appear to have a\r
built-in reflex to blink whenever their prediction of being poked in the eye exceeds some\r
threshold. The prediction is learned, but the association from prediction to eye closure\r
is built in, and thus the animal is saved many unprotected pokes in its eye. Similarly,\r
the association from fear to increased heart rate, or to freezing, may be built in. Agent

17.2. Temporal Abstraction via Options 461\r
designers can do something similar, connecting by design (without learning) predictions\r
of specific events to predetermined actions. For example, a self-driving car that learns to\r
predict whether going forward will produce a collision could be given a built-in reflex to\r
stop, or to turn away, whenever the prediction is above some threshold. Or consider a\r
vacuum-cleaning robot that learned to predict whether it might run out of battery power\r
before returning to the charger and that reflexively headed back to the charger whenever\r
the prediction became non-zero. The correct prediction would depend on the size of the\r
house, the room the robot was in, and the age of the battery, all of which would be hard\r
for the robot designer to know. It would be dicult for the designer to build in a reliable\r
algorithm for deciding whether to head back to the charger in sensory terms, but it might\r
be easy to do this in terms of the learned prediction. We foresee many possible ways\r
like this in which learned predictions might combine usefully with built-in algorithms for\r
controlling behavior.\r
Finally, perhaps the most important role for auxiliary tasks is in moving beyond the\r
assumption we have made throughout this book that the state representation is fixed\r
and given to the agent. To explain this role, we first have to take a few steps back to\r
appreciate the magnitude of this assumption and the implications of removing it. We do\r
that in Section 17.3.\r
17.2 Temporal Abstraction via Options\r
An appealing aspect of the MDP formalism is that it can be applied usefully to tasks\r
at many di↵erent time scales. It can be used to formalize the task of deciding which\r
muscles to twitch to grasp an object, which airplane flight to take to arrive conveniently\r
at a distant city, and which job to take to lead a satisfying life. These tasks di↵er greatly\r
in their time scales, yet each can be usefully formulated as an MDP that can be solved\r
by planning or learning processes as described in this book. All involve interaction with\r
the world, sequential decision making, and a goal usefully conceived of as accumulating\r
rewards over time, and so all can be formulated as MDPs.\r
Although all these tasks can be formulated as MDPs, you might think that they cannot\r
be formulated as a single MDP. They involve such di↵erent time scales, such di↵erent\r
notions of choice and action! It would be no good, for example, to plan a flight across a\r
continent at the level of muscle twitches. Yet for other tasks—such as grasping objects,\r
throwing darts, or hitting a baseball—low-level muscle twitches may be just the right\r
level. People do all these things seamlessly without appearing to switch between levels.\r
Can the MDP framework be stretched to cover all the levels simultaneously?\r
Perhaps it can. One popular idea is to formalize an MDP at a detailed level, with a\r
small time step, yet enable planning at higher levels using extended courses of action that\r
correspond to many base-level time steps. To do this we need a notion of course of action\r
that extends over many time steps and includes a notion of termination. A general way to\r
formulate these two ideas is as a policy, ⇡, and a state-dependent termination function, ,\r
as in GVFs. We define a pair of these as a generalized notion of action termed an option.\r
To execute an option ! = h⇡!, !i at time t is to obtain the action to take, At, from\r
⇡!(·|St), then terminate at time t + 1 with probability 1  !(St+1). If the option does

462 Chapter 17: Frontiers\r
not terminate at t+ 1, then At+1 is selected from ⇡!(·|St+1), and the option terminates at\r
t + 2 with probability 1  !(St+2), and so on until eventual termination. It is convenient\r
to consider low-level actions to be special cases of options—each action a corresponds\r
to an option h⇡!, !i whose policy picks the action (⇡!(s)=a for all s 2 S) and whose\r
termination function is zero (!(s) = 0 for all s 2 S+). Options e↵ectively extend the\r
action space. The agent can either select a low-level action/option, terminating after one\r
time step, or select an extended option that might execute for many time steps before\r
terminating.\r
Options are designed so that they are interchangable with low-level actions. For\r
example, the notion of an action-value function q⇡ naturally generalizes to an option\u0002value function that takes a state and option as input and returns the expected return\r
starting from that state, executing that option to termination, and thereafter following\r
the policy, ⇡. We can also generalize the notion of policy to a hierarchical policy that\r
selects from options rather than actions, where options, when selected, execute until\r
termination. With these ideas, many of the algorithms in this book can be generalized to\r
learn approximate option-value functions and hierarchical policies. In the simplest case,\r
the learning process ‘jumps’ from option initiation to option termination, with an update\r
only occurring when an option terminates. More subtly, updates can be made on each\r
time step, using “intra-option” learning algorithms, which in general require o↵-policy\r
learning.\r
Perhaps the most important generalization made possible by option ideas is that of the\r
environmental model as developed in Chapters 3, 4, and 8. The conventional model of an\r
action is the state-transition probabilities and the expected immediate reward for taking\r
the action in each state. How do conventional action models generalize to option models?\r
For options, the appropriate model is again of two parts, one corresponding to the state\r
transition resulting from executing the option and one corresponding to the expected\r
cumulative reward along the way. The reward part of an option model, analogous to the\r
expected reward for state–action pairs (3.5), is\r
r(s, !) .= E\r
⇥\r
R1 + R2 + 2R3 + ··· + ⌧1R⌧\r
\r
 S0 =s, A0:⌧1 ⇠⇡!, ⌧ ⇠!\r
⇤\r
, (17.2)\r
for all options ! and all states s 2 S, where ⌧ is the random time step at which the option\r
terminates according to !. Note the role of the overall discounting parameter  in this\r
equation—discounting is according to , but termination of the option is according to\r
!. The state-transition part of an option model is a little more subtle. This part of\r
the model characterizes the probability of each possible resulting state (as in (3.4)), but\r
now this state may result after various numbers of time steps, each of which must be\r
discounted di↵erently. The model for option ! specifies, for each state s that ! might\r
start executing in, and for each state s0 that ! might terminate in,\r
p(s0|s, !) .= X1\r
k=1\r
kPr{Sk =s0, ⌧ =k | S0 =s, A0:k1 ⇠⇡!, ⌧ ⇠!}. (17.3)\r
Note that, because of the factor of k, this p(s0|s, !) is no longer a transition probability\r
and no longer sums to one over all values of s0. (Nevertheless, we continue to use the ‘|’\r
notation in p.)

17.2. Temporal Abstraction via Options 463\r
The above definition of the transition part of an option model allows us to formulate\r
Bellman equations and dynamic programming algorithms that apply to all options,\r
including low-level actions as a special case. For example, the general Bellman equation\r
for the state values of a hierarchical policy ⇡ is\r
v⇡(s) = X\r
!2⌦(s)\r
⇡(!|s)\r
"\r
r(s, !) +X\r
s0\r
p(s0|s, !)v⇡(s0)\r
#\r
, (17.4)\r
where ⌦(s) denotes the set of options available in state s. If ⌦(s) includes only the\r
low-level actions, then this equation reduces to a version of the usual Bellman equation\r
(3.14), except of course  is included in the new p (17.3) and thus does not appear.\r
Similarly, the corresponding planning algorithms also have no . For example, the value\r
iteration algorithm with options, analogous to (4.10), is\r
vk+1(s) .= max !2⌦(s)\r
"\r
r(s, !) +X\r
s0\r
p(s0|s, !)vk(s0)\r
#\r
, for all s 2 S.\r
If ⌦(s) includes all the low-level actions available in each state s, then this algorithm\r
converges to the conventional v⇤, from which the optimal policy can be computed.\r
However, it is particularly useful to plan with options when only a subset of the possible\r
options are considered (in ⌦(s)) in each state. Value iteration will then converge to the\r
best hierarchical policy limited to the restricted set of options. Although this policy may\r
be sub-optimal, convergence can be much faster because fewer options are considered\r
and because each option can jump over many time steps.\r
To plan with options, the agent must either be given the option models, or learn them.\r
One natural way to learn an option model is to formulate it as a collection of GVFs (as\r
defined in the preceding section) and then learn the GVFs using the methods presented\r
in this book. It is not dicult to see how this could be done for the reward part of the\r
option model. You merely choose one GVF’s cumulant to be the reward (Ct = Rt), its\r
policy to be the option’s policy (⇡=⇡!), and its termination function to be the discount\r
rate times the option’s termination function ((s) =  · !(s)). The true GVF then\r
equals the reward part of the option model, v⇡,,C (s) = r(s, !), and the learning methods\r
described in this book can be used to approximate it. The state-transition part of the\r
option model is a little more complicated. You need to allocate one GVF for each state\r
that the option might terminate in. We don’t want these GVFs to accumulate anything\r
except when the option terminates, and then only when termination is in the appropriate\r
state. This can be achieved by choosing the cumulant of the GVF that predicts transition\r
to state s0 to be Ct = (1  !(St)) St=s0 . The GVF’s policy and termination functions\r
are chosen the same as for the reward part of the option model. The true GVF then\r
equals the s0 portion of the option’s state-transition model, v⇡,,C (s) = p(s0 |s, !), and\r
again this book’s methods could be employed to learn it. Although each of these steps\r
is seemingly natural, putting them all together (including function approximation and\r
other essential components) is quite challenging and beyond the current state of the art.

464 Chapter 17: Frontiers\r
Exercise 17.1 This section has presented options for the discounted case, but discounting\r
is arguably inappropriate for control when using function approximation (Section 10.4).\r
What is the natural Bellman equation for a hierarchical policy, analogous to (17.4), but\r
for the average reward setting (Section 10.3)? What are the two parts of the option\r
model, analogous to (17.2) and (17.3), for the average reward setting? ⇤\r
17.3 Observations and State\r
Throughout this book we have written the learned approximate value functions (and\r
the policies in Chapter 13) as functions of the environment’s state. This is a significant\r
limitation of the methods presented in Part I, in which the learned value function was\r
implemented as a table such that any value function could be exactly approximated;\r
that case is tantamount to assuming that the state of the environment is completely\r
observed by the agent. But in many cases of interest, and certainly in the lives of all\r
natural intelligences, the sensory input gives only partial information about the state of\r
the world. Some objects may be occluded by others, or behind the agent, or miles away.\r
In these cases, potentially important aspects of the environment’s state are not directly\r
observable, and it is a strong, unrealistic, and limiting assumption to assume that the\r
learned value function is implemented as a table over the environment’s state space.\r
The framework of parametric function approximation that we developed in Part II is far\r
less restrictive and, arguably, no limitation at all. In Part II we retained the assumption\r
that the learned value functions (and policies) are functions of the environment’s state,\r
but allowed these functions to be arbitrarily restricted by the parameterization. It is\r
somewhat surprising and not widely recognized that function approximation includes\r
important aspects of partial observability. For example, if there is a state variable that is\r
not observable, then the parameterization can be chosen such that the approximate value\r
does not depend on that state variable. The e↵ect is just as if the state variable were not\r
observable. Because of this, all the results obtained for the parameterized case apply to\r
partial observability without change. In this sense, the case of parameterized function\r
approximation includes the case of partial observability.\r
Nevertheless, there are many issues that cannot be investigated without a more explicit\r
treatment of partial observability. Although we cannot give them a full treatment here,\r
we can outline the changes that would be needed to do so. There are four steps.\r
First, we would change the problem. The environment would emit not its states, but\r
only observations—signals that depend on its state but, like a robot’s sensors, provide\r
only partial information about it. For convenience, without loss of generality, we assume\r
that the reward is a direct, known function of the observation (perhaps the observation is\r
a vector, and the reward is one of its components). The environmental interaction would\r
then have no explicit states or rewards, but could simply be an alternating sequence of\r
actions At 2 A and observations Ot 2 O:\r
A0, O1, A1, O2, A2, O3, A3, O4,...,\r
going on forever (cf. Equation 3.1) or forming episodes each ending with a special terminal\r
observation.

17.3. Observations and State 465\r
Second, we can recover the idea of state as used in this book from the sequence of\r
observations and actions. Let us use the word history, and the notation Ht, for an initial\r
portion of the trajectory up to an observation: Ht\r
.\r
= A0, O1,...,At1, Ot. The history\r
represents the most that we can know about the past without looking outside of the\r
data stream (because the history is the whole past data stream). Of course, the history\r
grows with t and can become large and unwieldy. The idea of state is that of a compact\r
summary of the history that is useful for predicting future sequences. To be a summary\r
of the history, a state must be a function of history, St = f(Ht). The summary would be\r
informationally perfect if it retained all information about the history (and thus could\r
be used to predict futures as accurately as could be done from the full history). In this\r
case, the state St and the function f are said to have the Markov property, and St is a\r
state as we have used the term in this book. Let us henceforth call it a Markov state\r
to distinguish it from states that are summaries of the history but are not sucient to\r
predict all futures. In practice, the states of real agents will not be Markov but may\r
approach it as an ideal.\r
To be more explicit about the Markov property it is useful to formalize possible futures.\r
Let a test be any specific sequence of alternating actions and observations that might\r
occur in the future. For example, a three-step test might be denoted ⌧ = a1 o1 a2 o2 a3 o3.\r
The probability of this test given a specific history h is defined as\r
p(⌧ |h) .= Pr{Ot+1 =o1, Ot+2 =o2, Ot+3 =o3 | Ht =h, At =a1, At+1 =a2, At+2 =a3}.(17.5)\r
Formally, f is Markov if and only if, for any test ⌧ , and for any histories h and h0 that\r
map to the same state under f, the test’s probabilities given the two histories are equal:\r
f(h) = f(h0) ) p(⌧ |h) = p(⌧ |h0), for all h, h0, ⌧ 2 {A ⇥ O}⇤. (17.6)\r
A Markov state summarizes all the information in the history necessary for determining\r
any test’s probability. In fact, it summarizes all that is necessary for making any prediction,\r
including any GVF. It also summarizes all that is necessary for optimal behavior: if f is\r
Markov, then there is always a deterministic function ⇡ such that choosing At\r
.\r
= ⇡(f(Ht))\r
is an optimal policy.\r
The third step in extending reinforcement learning to partial observability is to deal\r
with certain computational considerations. As mentioned earlier, we want the state to be\r
compact—relatively small compared to the history. (The identity function, for example,\r
is not a good f even though it is Markov, because the corresponding state St =Ht would\r
grow unboundedly with time.) In addition, we don’t really want a function f that takes\r
whole histories. Instead, we want an f that can be compactly implemented with an\r
incremental, recursive update that computes St+1 from St, incorporating only the next\r
increment of data, At and Ot+1:\r
St+1\r
.\r
= u(St, At, Ot+1), for all t  0, (17.7)\r
with the first state S0 given. The function u is called the state-update function. For\r
example, if f were the identity (St =Ht), then u would merely extend St by appending At\r
and Ot+1 to it. Given f, it is always possible to construct a corresponding u, but it may

466 Chapter 17: Frontiers\r
Policy, \r
Value fn.\r
World\r
Planner\r
Model\r
O A\r
R\r
A, R\r
S A u\r
Figure 17.1: A conceptual agent architecture including a model, a planner, and a state-update\r
function. The world in this case receives actions A and emits observations O. The observations\r
and a copy of the action are used by the state-update function u to produce the new state. The\r
new state is input to the policy and value function, producing the next action, and is also input\r
to the planner (and to u). The information flows most responsible for learning are shown by\r
dashed lines that pass diagonally across the boxes that they change. The reward R directly\r
changes the policy and value function. The action, reward, and state change the model, which\r
works closely with the planner to also change the policy and value function. Note that the\r
operation of the planner can be decoupled from the agent–environment interaction, whereas the\r
other processes should operate in lock step with this interaction to keep up with the arrival of\r
new data. Also note that the model and planner do not deal with observations directly, but only\r
with the states produced by u, which can act as targets for model learning.\r
not be computationally convenient and, as in the identity example, it may not produce\r
a compact state. The state-update function is a central part of any agent architecture\r
that handles partial observability. It must be eciently computable, as no actions or\r
predictions can be made until the state is available. An overall diagram of such an agent\r
architecture is given in Figure 17.1.\r
A common strategy for finding a Markov state is to look for something compact that\r
is recursively updatable and enables accurate short-term predictions. In fact, it is only\r
necessary to make accurate one-step predictions. An important fact is that, if an f\r
is incrementally updatable, then it is Markov if and only if all one-step tests can be\r
accurately predicted, that is, if and only if\r
f(h) = f(h0) ) Pr{Ot+1 =o|Ht =h, At =a} = Pr{Ot+1 =o|Ht =h0, At =a}, (17.8)\r
for all h, h0 2 {A ⇥ O}⇤, o 2 O and a 2 A. Accurate one-step predictions are informa\u0002tionally sucient, together with the state-update function, to accurately predict the\r
probability of any test of any length. This can be done by iteratively and alternately\r
making one-step predictions and applying the state-update function. From the whole\r
tree of possibilities the exact probability of any test or the expectation of any GVF\r
can be determined. These observations have led many researchers to focus on one-step\r
predictions rather than directly on multi-step predictions such as GVFs. However, note

17.3. Observations and State 467\r
that determining long-term predictions from single-step predictions is exponentially com\u0002plex in the length of the predictions. Moreover, one-step predictions can be iterated\r
to give accurate long-term predictions only if they are exact. If there is any error or\r
approximation in the one-step predictions, then it can compound to make the long-term\r
predictions wildly inaccurate. In practice this is often what happens.\r
An example of obtaining Markov states through a state-update function is provided\r
by the popular Bayesian approach known as Partially Observable MDPs, or POMDPs.\r
In this approach the environment is assumed to have a well defined latent state Xt that\r
underlies and produces the environment’s observations, but is never available to the agent\r
(and is not to be confused with the state St used by the agent to make predictions and\r
decisions). The natural Markov state, St, for a POMDP is the distribution over the latent\r
states given the history, called the belief state. For concreteness, assume the usual case in\r
which there are a finite number of hidden states, Xt 2 {1, 2,...,d}. Then the belief state\r
is the vector St\r
.\r
= st 2 [0, 1]d with components\r
st[i] .= Pr{Xt =i | Ht}, for all possible latent states i 2 {1, 2,...,d}. (17.9)\r
The belief state remains the same size (same number of components) even as t grows. It\r
can also be incrementally updated by Bayes’ rule, assuming complete knowledge of the\r
internal workings of the environment. Specifically, the ith component of the belief-state\r
update function is\r
u(s, a, o)[i] .=\r
Pd\r
x=1 s[x]p(i, o|x, a)\r
Pd\r
x=1\r
Pd\r
x0=1 s[x]p(x0\r
, o|x, a)\r
, for all a 2 A, o 2 O, (17.10)\r
and for all belief states s with components s[x], where the four-argument p function here\r
is not the usual one for MDPs (as in Chapter 3), but the analogous one for POMDPs,\r
in terms of the latent state: p(x0, o|x, a) .= Pr{Xt =x0, Ot =o |Xt1 =x, At1 =a}. This\r
approach is popular in theoretical work and has many significant applications, but its\r
assumptions and computational complexity scale poorly, and we do not recommend it as\r
an approach to artificial intelligence.\r
Another example of Markov states is provided by Predictive State Representations,\r
or PSRs. PSRs address the weakness of the POMDP approach that the semantics of\r
its agent state St are grounded in the environment state, Xt, which is never observed\r
and thus is dicult to learn about. In PSRs and related approaches, the semantics of\r
the agent state is instead grounded in predictions about future observations and actions,\r
which are readily observable. In PSRs, a Markov state is defined as a d-vector of the\r
probabilities of d specially chosen “core” tests as defined above (17.5). The vector is then\r
updated by a state-update function u that is analogous to Bayes rule, but with a semantics\r
grounded in observable data, which arguably makes it easier to learn. This approach has\r
been extended in many ways, including end-tests, compositional tests, powerful “spectral”\r
methods, and closed-loop and temporally abstract tests learned by TD methods. Some of\r
the best theoretical developments are for systems known as Observable Operator Models\r
(OOMs) and Sequential Systems (Thon, 2017).\r
The fourth and final step in our brief outline of how to handle partial observability in\r
reinforcement learning is to re-introduce approximation. As discussed in the introduction

468 Chapter 17: Frontiers\r
to Part II, to approach artificial intelligence ambitiously we must embrace approximation.\r
This is just as true for states as it is for value functions. We must accept and work with\r
an approximate notion of state. The approximate state will play the same role in our\r
algorithms as before, so we continue to use the notation St for the state used by the\r
agent, even though it may not be Markov.\r
Perhaps the simplest example of an approximate state is just the latest observation,\r
St\r
.\r
=Ot. Of course this approach cannot handle any hidden state information. It would\r
be better to use the last k observations and actions, St\r
.\r
= Ot, At1, Ot1,...,Atk, for\r
some k  1, which can be achieved by a state-update function that just shifts the new\r
data in and the oldest data out. This kth-order history approach is still very simple,\r
but can greatly increase the agent’s capabilities compared to trying to use the single\r
immediate observation directly as the state.\r
What happens when the Markov property (17.8) is only approximately satisfied?\r
Unfortunately, long-term prediction performance can degrade dramatically when one\u0002step predictions become even slightly inaccurate. Longer-term tests, GVFs, and state\u0002update functions may or may not approximate better. The short-term and long-term\r
approximation objectives are just di↵erent, and there are no useful theoretical guarantees\r
at present.\r
Nevertheless, there are still reasons to think that the general idea outlined in this\r
section applies to the approximate case. The general idea is that a state that is good for\r
some predictions is also good for others—in particular, that a Markov state, sucient for\r
one-step predictions, is also sucient for all others. If we step back from that specific\r
result for the Markov case, the general idea is similar to what we discussed in Section 17.1\r
with multi-headed learning and auxiliary tasks. We discussed how representations that\r
were good for the auxiliary tasks were often also good for the main task. Taken together,\r
these suggest an approach to both partial observability and representation learning in\r
which multiple predictions are pursued and used to direct the construction of state\r
features. The guarantee provided by the perfect-but-impractical Markov property is\r
replaced by the heuristic that what’s good for some predictions may be good for others.\r
This approach scales well with computational resources. With a powerful computer we\r
could experiment with large numbers of predictions, perhaps favoring those that are most\r
similar to the ones of ultimate interest, that are easiest to learn reliably, or that satisfy\r
other criteria. It is important here to move beyond selecting the predictions manually.\r
The agent should do it. This would require a general language for predictions, so that\r
the agent can systematically explore a large space of possible predictions, sifting through\r
them for the ones that are most useful.\r
In particular, both POMDP and PSR approaches can be applied with approximate\r
states. The semantics of the state is often useful in forming the state-update function, as\r
it is in these two approaches and in the kth-order history approach. However, there is not\r
a strong need for the state to be accurate with respect to its semantics in order to retain\r
useful information. Some approaches to state augmentation, such as Echo state networks\r
(Jaeger, 2002), keep almost arbitrary information about the history and can nevertheless\r
perform well. There are many possibilities, and we expect more work and ideas in this\r
area. Learning the state-update function for an approximate state is a major part of the\r
representation learning problem as it arises in reinforcement learning.

17.4. Designing Reward Signals 469\r
17.4 Designing Reward Signals\r
A major advantage of reinforcement learning over supervised learning is that reinforcement\r
learning does not rely on detailed instructional information: generating a reward signal\r
does not depend on knowledge of what the agent’s correct actions should be. But the\r
success of a reinforcement learning application strongly depends on how well the reward\r
signal frames the goal of the application’s designer and how well the signal assesses\r
progress in reaching that goal. For these reasons, designing a reward signal is a critical\r
part of any application of reinforcement learning.\r
By designing a reward signal we mean designing the part of an agent’s environment\r
that is responsible for computing each scalar reward Rt and sending it to the agent at\r
each time t. In our discussion of terminology at the end of Chapter 14, we said that Rt\r
is more like a signal generated inside an animal’s brain than it is like an object or event\r
in the animal’s external environment. The parts of our brains that generate these signals\r
for us evolved over millions of years to be well suited to the challenges our ancestors\r
had to face in their struggles to propagate their genes to future generations. We should\r
therefore not think that designing a good reward signal is always an easy thing to do!\r
One challenge is to design a reward signal so that as an agent learns, its behavior\r
approaches, and ideally eventually achieves, what the application’s designer actually\r
desires. This can be easy if the designer’s goal is simple and easy to identify, such as\r
finding the solution to a well-defined problem or earning a high score in a well-defined\r
game. In cases like these, it is usual to reward the agent according to its success in solving\r
the problem or its success in improving its score. But some problems involve goals that\r
are dicult to translate into reward signals. This is especially true when the problem\r
requires the agent to skillfully perform a complex task or set of tasks, such as would be\r
required of a useful household robotic assistant. Further, reinforcement learning agents\r
can discover unexpected ways to make their environments deliver reward, some of which\r
might be undesirable, or even dangerous. This is a longstanding and critical challenge for\r
any method, like reinforcement learning, that is based on optimization. We discuss this\r
issue more in Section 17.6, the final section of this book.\r
Even when there is a simple and easily identifiable goal, the problem of sparse reward\r
often arises. Delivering non-zero reward frequently enough to allow the agent to achieve\r
the goal once, let alone to learn to achieve it eciently from multiple initial conditions,\r
can be a daunting challenge. State–action pairs that clearly deserve to trigger reward may\r
be few and far between, and rewards that mark progress toward a goal can be infrequent\r
because progress is dicult or even impossible to detect. The agent may wander aimlessly\r
for long periods of time (what Minsky, 1961, called the “plateau problem”).\r
In practice, designing a reward signal is often left to an informal trial-and-error search\r
for a signal that produces acceptable results. If the agent fails to learn, learns too slowly,\r
or learns the wrong thing, then the designer tweaks the reward signal and tries again.\r
To do this, the designer judges the agent’s performance by criteria that he or she is\r
attempting to translate into a reward signal so that the agent’s goal matches his or her\r
own. And if learning is too slow, the designer may try to design a non-sparse reward signal\r
that e↵ectively guides learning throughout the agent’s interaction with its environment.

470 Chapter 17: Frontiers\r
It is tempting to address the sparse reward problem by rewarding the agent for achieving\r
subgoals that the designer thinks are important way stations to the overall goal. But\r
augmenting the reward signal with well-intentioned supplemental rewards may lead the\r
agent to behave di↵erently from what is intended; the agent may end up not achieving\r
the overall goal. A better way to provide such guidance is to leave the reward signal\r
alone and instead augment the value-function approximation with an initial guess of what\r
it should ultimately be, or augment it with initial guesses as to what certain parts of it\r
should be. For example, suppose we wants to o↵er v0 : S ! R as an initial guess at the\r
true optimal value function v⇤, and that we are using linear function approximation with\r
features x : S ! Rd. Then we would define the initial value function approximation as\r
vˆ(s,w) .= w>x(s) + v0(s), (17.11)\r
and update the weights w as usual. If the initial weight vector is 0, then the initial\r
value function will be v0, but the asymptotic solution quality will be determined by the\r
feature vectors as usual. This initialization can also be done for arbitrary nonlinear\r
approximators and arbitrary forms of v0, though it is not guaranteed to always accelerate\r
learning.\r
A particularly e↵ective approach to the sparse reward problem is the shaping tech\u0002nique introduced by the psychologist B. F. Skinner and described in Section 14.3. The\r
e↵ectiveness of this technique relies on the fact that sparse reward problems are not just\r
problems with the reward signal; they are also problems with an agent’s policy in that\r
it prevents the agent from frequently encountering rewarding states. Shaping involves\r
changing the reward signal as learning proceeds, starting from a reward signal that is\r
not sparse given the agent’s initial behavior, and gradually modifying it toward a reward\r
signal suited to the problem of original interest. Shaping might also involve modifying\r
the dynamics of the task as learning proceeds. Each modification is made so that the\r
agent is frequently rewarded given its current behavior. The agent faces a sequence of\r
increasingly-dicult reinforcement learning problems, where what is learned at each stage\r
makes the next-harder problem relatively easy because the agent now encounters reward\r
more frequently than it would if it did not have prior experience with easier problems.\r
This kind of shaping is an essential technique in training animals, and it is e↵ective in\r
computational reinforcement learning as well.\r
What if you have no idea what the rewards should be, but there is another agent,\r
perhaps a person, who is already expert at the task and whose behavior can be observed?\r
In this case you could use methods known variously as “imitation learning,” “learning\r
from demonstration,” and “apprenticeship learning.” The idea here is to benefit from\r
the expert agent but leave open the possibility of eventually performing better. Learning\r
from an expert’s behavior can be done either by learning directly by supervised learning\r
or by extracting a reward signal using what is known as “inverse reinforcement learning”\r
and then using a reinforcement learning algorithm with that reward signal to learn a\r
policy. The task of inverse reinforcement learning as explored by Ng and Russell (2000)\r
is to try to recover the expert’s reward signal from the expert’s behavior alone. This\r
cannot be done exactly because a policy can be optimal with respect to many di↵erent\r
reward signals (for example, all policies are optimal with respect to a constant reward\r
signal), but it is possible to find plausible reward signal candidates. Unfortunately, strong

17.4. Designing Reward Signals 471\r
assumptions are required, including knowledge of the environment’s dynamics and of the\r
feature vectors in which the reward signal is linear. The method also requires completely\r
solving the problem (e.g., by dynamic programming methods) multiple times. These\r
diculties notwithstanding, Abbeel and Ng (2004) argue that the inverse reinforcement\r
learning approach can sometimes be more e↵ective than supervised learning for benefiting\r
from the behavior of an expert.\r
Another approach to finding a good reward signal is to automate the trial-and-error\r
search for a good signal that we mentioned above. From an application perspective, the\r
reward signal is a parameter of the learning algorithm. As is true for other algorithm\r
parameters, the search for a good reward signal can be automated by defining a space of\r
feasible candidates and applying an optimization algorithm. The optimization algorithm\r
evaluates each candidate reward signal by running the reinforcement learning system with\r
that signal for some number of steps, and then scoring the overall result by a “high-level”\r
objective function intended to encode the designer’s true goal, ignoring the limitations\r
of the agent. Reward signals can even be improved via online gradient ascent, where\r
the gradient is that of the high-level objective function (Sorg, Lewis, and Singh, 2010).\r
Relating this approach to the natural world, the algorithm for optimizing the high-level\r
objective function is analogous to evolution, where the high-level objective function is an\r
animal’s evolutionary fitness determined by the number of its o↵spring that survive to\r
reproductive age.\r
Computational experiments with this bilevel optimization approach—one level analo\u0002gous to evolution, and the other due to reinforcement learning by individual agents—have\r
confirmed that intuition alone is not always adequate to devise a good reward signal\r
(Singh, Lewis, and Barto, 2009). The performance of a reinforcement learning agent as\r
evaluated by the high-level objective function can be very sensitive to details of the agent’s\r
reward signal in subtle ways determined by the agent’s limitations and the environment\r
in which it acts and learns. These experiments have also demonstrated that an agent’s\r
goal should not always be the same as the goal of the agent’s designer.\r
At first this seems counterintuitive, but it may be impossible for the agent to achieve the\r
designer’s goal no matter what its reward signal is. The agent has to learn under various\r
kinds of constraints, such as limited computational power, limited access to information\r
about its environment, or limited time to learn. When there are constraints like these,\r
learning to achieve a goal that is di↵erent from the designer’s goal can sometimes end up\r
getting closer to the designer’s goal than if that goal were pursued directly (Sorg, Singh,\r
and Lewis, 2010; Sorg, 2011). Examples of this in the natural world are easy to find.\r
Because we cannot directly assess the nutritional value of most foods, evolution—the\r
designer of our reward signal—gave us a reward signal that makes us seek certain tastes.\r
Though certainly not infallible (indeed, possibly detrimental in environments that di↵er in\r
certain ways from ancestral environments), this compensates for many of our limitations:\r
our limited sensory abilities, the limited time over which we can learn, and the risks\r
involved in finding a healthy diet through personal experimentation. Similarly, because\r
an animal cannot always observe its own evolutionary fitness, that objective function\r
does not work as a reward signal for learning. Evolution instead provides reward signals\r
that are sensitive to observable predictors of evolutionary fitness.\r
Finally, remember that a reinforcement learning agent is not necessarily like a complete

472 Chapter 17: Frontiers\r
organism or robot; it can be a component of a larger behaving system. This means\r
that reward signals may be influenced by things inside the larger behaving agent, such\r
as motivational states, memories, ideas, or even hallucinations. Reward signals may\r
also depend on properties of the learning process itself, such as measures of how much\r
progress learning is making. Making reward signals sensitive to information about internal\r
factors such as these makes it possible for an agent to learn how to control the “cognitive\r
architecture” of which it is a part, as well as to acquire knowledge and skills that would be\r
dicult to learn from a reward signal that depended only on external events. Possibilities\r
like these led to the idea of “intrinsically-motivated reinforcement learning” that we\r
briefly discuss further at the end of the following section.\r
17.5 Remaining Issues\r
In this book we have presented the foundations of a reinforcement learning approach to\r
artificial intelligence. Roughly speaking, that approach is based on model-free and model\u0002based methods working together, as in the Dyna architecture of Chapter 8, combined\r
with function approximation as developed in Part II. The focus has been on online and\r
incremental algorithms, which we see as fundamental even to model-based methods, and\r
on how these can be applied in o↵-policy training situations. The full rationale for the\r
latter has been presented only in this last chapter. That is, we have all along presented o↵-\r
policy learning as an appealing way to deal with the explore/exploit dilemma, but only in\r
this chapter have we discussed learning about many diverse auxiliary tasks simultaneously\r
with GVFs and learning about the world hierarchically in terms of temporally-abstract\r
option models, both of which involve o↵-policy learning. Much remains to be worked\r
out, as we have indicated throughout the book and as evidenced by the directions for\r
additional research discussed in this chapter. But suppose we are generous and grant the\r
broad outlines of everything that we have done in the book and everything that has been\r
outlined so far in this chapter. What would remain after that? Of course we can’t know\r
for sure what will be required, but we can make some guesses. In this section we highlight\r
six further issues which it seems to us will still need to be addressed by future research.\r
First, we still need powerful parametric function approximation methods that work well\r
in fully incremental and online settings. Methods based on deep learning and ANNs are\r
a major step in this direction but, still, only work well with batch training on large data\r
sets, with training from extensive o↵-line self play, or with learning from the interleaved\r
experience of multiple agents on the same task. These and other settings are ways of\r
working around a basic limitation of today’s deep learning methods, which struggle to\r
learn rapidly in the incremental, online settings that are most natural for the reinforcement\r
learning algorithms emphasized in this book. The problem is sometimes described as\r
one of “catastrophic interference” or “correlated data.” When something new is learned\r
it tends to replace what has previously been learned rather than adding to it, with the\r
result that the benefit of the older learning is lost. Techniques such as “replay bu↵ers”\r
are often used to retain and replay old data so that its benefits are not permanently lost.\r
An honest assessment has to be that current deep learning methods are not well suited to\r
online learning. We see no reason that this limitation is insurmountable, but algorithms

17.5. Remaining Issues 473\r
that address it, while at the same time retaining the advantages of deep learning, have\r
not yet been devised. Most current deep learning research is directed toward working\r
around this limitation rather than removing it.\r
Second (and perhaps closely related), we still need methods for learning features such\r
that subsequent learning generalizes well. This issue is an instance of a general problem\r
variously called “representation learning,” “constructive induction,” and “meta-learning”—\r
how can we use experience not just to learn a given desired function, but to learn inductive\r
biases such that future learning generalizes better and is thus faster? This is an old\r
problem, dating back to the origins of artificial intelligence and pattern recognition in\r
the 1950s and 1960s.1 Such age should give one pause. Perhaps there is no solution. But\r
it is equally likely that the time for finding a solution and demonstrating its e↵ectiveness\r
has not yet arrived. Today machine learning is conducted at a far larger scale than it has\r
been in the past, and the potential benefits of a good representation learning method have\r
become much more apparent. We note that a new annual conference—the International\r
Conference on Learning Representations—has been exploring this and related topics\r
every year since 2013. It is also less common to explore representation learning within\r
a reinforcement learning context. Reinforcement learning brings some new possibilities\r
to this old issue, such as the auxiliary tasks discussed in Section 17.1. In reinforcement\r
learning, the problem of representation learning can be identified with the problem of\r
learning the state-update function discussed in Section 17.3.\r
Third, we still need scalable methods for planning with learned environment models.\r
Planning methods have proven extremely e↵ective in applications such as AlphaGo\r
Zero and computer chess in which the model of the environment is known from the\r
rules of the game or can otherwise be supplied by human designers. But cases of full\r
model-based reinforcement learning, in which the environment model is learned from\r
data and then used for planning, are rare. The Dyna system described in Chapter 8 is\r
one example, but as described there and in most subsequent work it uses a tabular model\r
without function approximation, which greatly limits its applicability. Only a few studies\r
have included learned linear models, and even fewer have also explored the inclusion of\r
temporally-abstract models using options as discussed in Section 17.2.\r
More work is needed before planning with learned models can be e↵ective. For example,\r
the learning of the model needs to be selective because the scope of a model strongly\r
a↵ects planning eciency. If a model focuses on the key consequences of the most\r
important options, then planning can be ecient and rapid, but if a model includes\r
details of unimportant consequences of options that are unlikely to be selected, then\r
planning may be almost useless. Environment models should be constructed judiciously\r
with regard to both their states and dynamics with the goal of optimizing the planning\r
process. The various parts of the model should be continually monitored as to the degree\r
to which they contribute to, or detract from, planning eciency. The field has not\r
yet addressed this complex of issues or designed model-learning methods that take into\r
account their implications.\r
1Some would claim that deep learning solves this problem, for example, that DQN as described in\r
Section 16.5 illustrates a solution, but we are unconvinced. There is as yet little evidence that deep\r
learning alone solves the representation learning problem in a general and ecient way.

474 Chapter 17: Frontiers\r
A fourth issue that needs to be addressed in future research is that of automating the\r
choice of tasks on which an agent works and uses to structure its developing competence.\r
It is usual in machine learning for human designers to set the tasks that the learning\r
agent is expected to master. Because these tasks are known in advance and remain fixed,\r
they can be built into the learning algorithm code. However, looking ahead, we will want\r
the agent to make its own choices about what tasks it should try to master. These might\r
be subtasks of a specific overall task that is already known, or they might be intended to\r
create building blocks that permit more ecient learning of many di↵erent tasks that the\r
agent is likely to face in the future but which are currently unknown.\r
These tasks may be like the auxiliary tasks or the GVFs discussed in Section 17.1, or\r
tasks solved by options as discussed in Section 17.2. In forming a GVF, for example, what\r
should the cumulant, the policy, and the termination function be? The current state of\r
the art is to select these manually, but far greater power and generality would come from\r
making these task choices automatically, particularly when they derive from what the\r
agent has previously constructed as a result of representation learning or experience with\r
previous subproblems. If GVF design is automated, then the design choices themselves\r
will have to be explicitly represented. Rather than the task choices being in the mind\r
of the designer and built into the code, they will have to be in the machine itself in\r
such a way that they can be set and changed, monitored, filtered, and searched among\r
automatically. Tasks could then be built hierarchically upon others much like features are\r
in an ANN. The tasks are the questions, and the contents of the ANN are the answers to\r
those questions. We expect there will need to be a full hierarchy of questions to match\r
the hierarchy of answers provided by modern deep learning methods.\r
The fifth issue that we would like to highlight for future research is that of the\r
interaction between behavior and learning via some computational analog of curiosity.\r
In this chapter we have been imagining a setting in which many tasks are being learned\r
simultaneously, using o↵-policy methods, from the same stream of experience. The actions\r
taken will of course influence this stream of experience, which in turn will determine how\r
much learning occurs and which tasks are learned. When reward is not available, or not\r
strongly influenced by behavior, the agent is free to choose actions that maximize in\r
some sense the learning on the tasks, that is, to use some measure of learning progress\r
as an internal or “intrinsic” reward, implementing a computational form of curiosity. In\r
addition to measuring learning progress, intrinsic reward can, among other possibilities,\r
signal the receipt of unexpected, novel, or otherwise interesting input, or can assess the\r
agent’s ability to cause changes in its environment. Intrinsic reward signals generated in\r
these ways can be used by an agent to pose tasks for itself by defining auxiliary tasks,\r
GVFs, or options, as discussed above, so that skills learned in this way can contribute\r
to the agent’s ability to master future tasks. The result is a computational analog of\r
something like play. Many preliminary studies of such uses of intrinsic reward signals\r
have been conducted, and exciting topics for future research remain in this general area.\r
A final issue that demands attention in future research is that of developing methods to\r
make it acceptably safe to embed reinforcement learning agents into physical environments.\r
This is one of the most pressing areas for future research, and we discuss it further in the\r
following section.

17.6. Reinforcement Learning and the Future of Artificial Intelligence 475\r
17.6 Reinforcement Learning and the Future of\r
Artificial Intelligence\r
When we were writing the first edition of this book in the mid-1990s, artificial intelligence\r
was making significant progress and was having an impact on society, though it was\r
mostly still the promise of artificial intelligence that was inspiring developments. Machine\r
learning was part of that outlook, but it had not yet become indispensable to artificial\r
intelligence. By today that promise has transitioned to applications that are changing the\r
lives of millions of people, and machine learning has come into its own as a key technology.\r
As we write this second edition, some of the most remarkable developments in artificial\r
intelligence have involved reinforcement learning, most notably “deep reinforcement\r
learning”—reinforcement learning with function approximation by deep artificial neural\r
networks. We are at the beginning of a wave of real-world applications of artificial\r
intelligence, many of which will include reinforcement learning, deep and otherwise, that\r
will impact our lives in ways that are hard to predict.\r
But an abundance of successful real-world applications does not mean that true\r
artificial intelligence has arrived. Despite great progress in many areas, the gulf between\r
artificial intelligence and the intelligence of humans, and other animals, remains great.\r
Superhuman performance can be achieved in some domains, even formidable domains\r
like Go, but it remains a significant challenge to develop systems that are like us in\r
being complete, interactive agents having general adaptability and problem-solving skills,\r
emotional sophistication, creativity, and the ability to learn quickly from experience.\r
With its focus on learning by interacting with dynamic environments, reinforcement\r
learning, as it develops over the future, will be a critical component of agents with these\r
abilities.\r
Reinforcement learning’s connections to psychology and neuroscience (Chapters 14\r
and 15) underscore its relevance to another longstanding goal of artificial intelligence:\r
shedding light on fundamental questions about the mind and how it emerges from the\r
brain. Reinforcement learning theory is already contributing to our understanding of\r
the brain’s reward, motivation, and decision-making processes, and there is good reason\r
to believe that through its links to computational psychiatry, reinforcement learning\r
theory will contribute to methods for treating mental disorders, including drug abuse\r
and addiction.\r
Another contribution that reinforcement learning can make over the future is as an\r
aid to human decision making. Policies derived by reinforcement learning in simulated\r
environments can advise human decision makers in such areas as education, healthcare,\r
transportation, energy, and public-sector resource allocation. Particularly relevant is the\r
key feature of reinforcement learning that it takes long-term consequences of decisions\r
into account. This is very clear in games like backgammon and Go, where some of\r
the most impressive results of reinforcement learning have been demonstrated, but it\r
is also a property of many high-stakes decisions that a↵ect our lives and our planet.\r
Reinforcement learning follows related methods for advising human decision making that\r
have been developed in the past by decision analysts in many disciplines. With advanced\r
function approximation methods and massive computational power, reinforcement learning

476 Chapter 17: Frontiers\r
methods have the potential to overcome some of the diculties of scaling up traditional\r
decision-support methods to larger and more complex problems.\r
The rapid pace of advances in artificial intelligence has led to warnings that artificial\r
intelligence poses serious threats to our societies, even to humanity itself. The renowned\r
scientist and artificial intelligence pioneer Herbert Simon anticipated the warnings we are\r
hearing today in a presentation at the Earthware Symposium at CMU in 2000 (Simon,\r
2000). He spoke of the eternal conflict between the promise and perils of any new\r
knowledge, reminding us of the Greek myths of Prometheus, the idealized hero of modern\r
science, who stole fire from the gods for the benefit of mankind, and of Pandora, whose\r
mythical box could be opened by a small and innocent action to release untold perils on\r
the world. While accepting that this conflict is inevitable, Simon urged us to recognize\r
that as designers of our future and not mere spectators, the decisions we make can tilt\r
the scale in Prometheus’ favor. This is certainly true for reinforcement learning, which\r
can benefit society but can also produce undesirable outcomes if it is carelessly deployed.\r
Thus, the safety of artificial intelligence applications involving reinforcement learning is a\r
topic that deserves careful attention.\r
A reinforcement learning agent can learn by interacting with either the real world or\r
with a simulation of some piece of the real world, or by a mixture of these two sources of\r
experience. Simulators provide safe environments in which an agent can explore and learn\r
without risking real damage to itself or to its environment. In most current applications,\r
policies are learned from simulated experience instead of direct interaction with the\r
real world. In addition to avoiding undesirable real-world consequences, learning from\r
simulated experience can make virtually unlimited data available for learning, generally\r
at less cost than needed to obtain real experience, and because simulations typically run\r
much faster than real time, learning can often occur more quickly than if it relied on real\r
experience.\r
Nevertheless, the full potential of reinforcement learning requires reinforcement learning\r
agents to be embedded into the flow of real-world experience, where they act, explore,\r
and learn in our world, and not just in their worlds. After all, reinforcement learning\r
algorithms—at least those upon which we focus in this book—are designed to learn online,\r
and they emulate many aspects of how animals are able to survive in nonstationary and\r
hostile environments. Embedding reinforcement learning agents in the real world can be\r
transformative in realizing the promises of artificial intelligence to amplify and extend\r
human abilities.\r
A major reason for wanting a reinforcement learning agent to act and learn in the real\r
world is that it is often dicult, sometimes impossible, to simulate real-world experience\r
with enough fidelity to make the resulting policies, whether derived by reinforcement\r
learning or by other methods, work well—and safely—when directing real actions. This\r
is especially true for environments whose dynamics depend on the behavior of humans,\r
such as in education, healthcare, transportation, and public policy—domains that can\r
surely benefit from improved decision making. However, it is for real-world embedded\r
agents that warnings about potential dangers of artificial intelligence need to be heeded.\r
Some of these warnings are particularly relevant to reinforcement learning. Because\r
reinforcement learning is based on optimization, it inherits the plusses and minuses of all\r
optimization methods. On the minus side is the problem of devising objective functions,

17.6. Reinforcement Learning and the Future of Artificial Intelligence 477\r
or reward signals in the case of reinforcement learning, so that optimization produces\r
the desired results while avoiding undesirable results. We said in Section 17.4 that\r
reinforcement learning agents can discover unexpected ways to make their environments\r
deliver reward, some of which might be undesirable, or even dangerous. When we specify\r
what we want a system to learn only indirectly, as we do in designing a reinforcement\r
learning system’s reward signal, we will not know how closely the agent will fulfill our desire\r
until its learning is complete. This is hardly a new problem with reinforcement learning;\r
recognition of it has a long history in both literature and engineering. For example, in\r
Goethe’s poem “The Sorcerer’s Apprentice” (Goethe, 1878), the apprentice uses magic to\r
enchant a broom to do his job of fetching water, but the result is an unintended flood due\r
to the apprentice’s inadequate knowledge of magic. In the engineering context, Norbert\r
Wiener, the founder of cybernetics, warned of this problem more than half a century ago\r
by relating the supernatural story of “The Monkey’s Paw” (Wiener, 1964): “... it grants\r
what you ask for, not what you should have asked for or what you intend” (p. 59). The\r
problem has also been discussed at length in a modern context by Nick Bostrom (2014).\r
Anyone having experience with reinforcement learning has likely seen their systems\r
discover unexpected ways to obtain a lot of reward. Sometimes the unexpected behavior\r
is good: it solves a problem in a nice new way. In other instances, what the agent\r
learns violates considerations that the system designer may never have thought about.\r
Careful design of reward signals is essential if an agent is to act in the real world with no\r
opportunity for human vetting of its actions or means to easily interrupt its behavior.\r
Despite the possibility of unintended negative consequences, optimization has been used\r
for hundreds of years by engineers, architects, and others whose designs have positively\r
impacted the world. We owe much that is good in our environment to the application\r
of optimization methods. Many approaches have been developed to mitigate the risk\r
of optimization, such as adding hard and soft constraints, restricting optimization to\r
robust and risk-sensitive policies, and optimizing with multiple objective functions. Some\r
of these approaches have been adapted to reinforcement learning, and more research is\r
needed to address these concerns. The problem of ensuring that a reinforcement learning\r
agent’s goal is attuned to our own remains a challenge.\r
Another challenge if reinforcement learning agents are to act and learn in the real world\r
is not just about what they might learn eventually, but about how they will behave while\r
they are learning. How do you make sure that an agent gets enough experience to learn a\r
high-performing policy, all the while not harming its environment, other agents, or itself\r
(or more realistically, while keeping the probability of harm acceptably low)? This problem\r
is also not novel or unique to reinforcement learning. Risk management and mitigation\r
for embedded reinforcement learning is similar to what control engineers have had to\r
confront from the beginning of using automatic control in situations where a controller’s\r
behavior can have unacceptable, possibly catastrophic, consequences, as in the control of\r
an aircraft or a delicate chemical process. Control applications rely on careful system\r
modeling, model validation, and extensive testing, and there is a highly-developed body\r
of theory aimed at ensuring convergence and stability of adaptive controllers designed for\r
use when the dynamics of the system to be controlled are not fully known. Theoretical\r
guarantees are never iron-clad because they depend on the validity of the assumptions\r
underlying the mathematics, but without this theory, combined with risk-management

478 Chapter 17: Frontiers\r
and mitigation practices, automatic control—adaptive and otherwise—would not be as\r
beneficial as it is today in improving the quality, eciency, and cost-e↵ectiveness of\r
processes on which we have come to rely. One of the most pressing areas for future\r
reinforcement learning research is to adapt and extend methods developed in control\r
engineering with the goal of making it acceptably safe to fully embed reinforcement\r
learning agents into physical environments.\r
In closing, we return to Simon’s call for us to recognize that we are designers of our\r
future and not simply spectators. By decisions we make as individuals, and by the\r
influence we can exert on how our societies are governed, we can work toward ensuring\r
that the benefits made possible by a new technology outweigh the harm it can cause.\r
There is ample opportunity to do this in the case of reinforcement learning, which can\r
help improve the quality, fairness, and sustainability of life on our planet, but which\r
can also release new perils. A threat already here is the displacement of jobs caused\r
by applications of artificial intelligence. Still there are good reasons to believe that the\r
benefits of artificial intelligence can outweigh the disruption it causes. As to safety,\r
hazards possible with reinforcement learning are not completely di↵erent from those\r
that have been managed successfully for related applications of optimization and control\r
methods. As reinforcement learning moves out into the real world in future applications,\r
developers have an obligation to follow best practices that have evolved for similar\r
technologies, while at the same time extending them to make sure that Prometheus keeps\r
the upper hand.\r
Bibliographical and Historical Remarks\r
17.1 General value functions were first explicitly identified by Sutton and colleagues\r
(Sutton, 1995a; Sutton et al., 2011; Modayil, White, and Sutton, 2013). Ring (in\r
preparation) developed an extensive thought experiment with GVFs (“forecasts”)\r
that has been influential despite not yet having been published.\r
The first demonstrations of multi-headed learning in reinforcement learning\r
were by Jaderberg et al. (2017). Bellemare, Dabney, and Munos (2017) showed\r
that predicting more things about the distribution of reward could significantly\r
accelerate learning to optimize its expectation, an instance of auxiliary tasks.\r
Many others have since taken up this line of research.\r
The general theory of classical conditioning as learned predictions together with\r
built-in, reflexive reactions to the predictions has not to our knowledge been\r
clearly articulated in the psychological literature. Modayil and Sutton (2014)\r
describe it as an approach to the engineering of robots and other agents, calling\r
it “Pavlovian control” to allude to its roots in classical conditioning.\r
17.2 The formalization of temporally abstract courses of action as options was intro\u0002duced by Sutton, Precup, and Singh (1999), building on prior work by Parr (1998)\r
and Sutton (1995a), and on classical work on Semi-MDPs (e.g., see Puterman,\r
1994). Precup’s (2000) PhD thesis developed option ideas fully. An important\r
limitation of these early works is that they did not treat the o↵-policy case

17.6. Reinforcement Learning and the Future of Artificial Intelligence 479\r
with function approximation. Intra-option learning in general requires o↵-policy\r
learning, which could not be done reliably with function approximation at that\r
time. Although now we have a variety of stable o↵-policy learning methods using\r
function approximation, their combination with option ideas had not been signif\u0002icantly explored at the time of publication of this book. Barto and Mahadevan\r
(2003) and Hengst (2012) review the options formalism and other approaches to\r
temporal abstraction.\r
Using GVFs to implement option models has not previously been described. Our\r
presentation uses the trick introduced by Modayil, White, and Sutton (2014) for\r
predicting signals at the termination of policies.\r
Among the few works that have learned option models with function approxi\u0002mation are those by Sorg and Singh (2010), and by Bacon, Harb, and Precup\r
(2017).\r
The extension of options and option models to the average-reward setting has\r
not yet been developed in the literature.\r
17.3 A good presentation of the POMDP approach is given by Monahan (1982). PSRs\r
and tests were introduced by Littman, Sutton, and Singh (2002). OOMs were\r
introduced by Jaeger (1997, 1998, 2000). Sequential Systems, which unify PSRs,\r
OOMs, and many other works, were introduced in the PhD thesis of Michael Thon\r
(2017; Thon and Jaeger, 2015). Extensions to networks of temporal relationships\r
were developed by Tanner (2006; Sutton and Tanner, 2005) and then extended\r
to options (Sutton, Rafols, and Koop, 2006).\r
The theory of reinforcement learning with a non-Markov state representation was\r
developed explicitly by Singh, Jaakkola, and Jordan (1994; Jaakkola, Singh, and\r
Jordan, 1995). Early reinforcement learning approaches to partial observability\r
were developed by Chrisman (1992), McCallum (1993, 1995), Parr and Russell\r
(1995), Littman, Cassandra, and Kaelbling (1995), and by Lin and Mitchell\r
(1992).\r
17.4 Early e↵orts to include advice and teaching in reinforcement learning include\r
those by Lin (1992), Maclin and Shavlik (1994), Clouse (1996), and Clouse and\r
Utgo↵ (1992).\r
Skinner’s shaping should not be confused with the “potential-based shaping”\r
technique introduced by Ng, Harada, and Russell (1999). Their technique has\r
been shown by Wiewiora (2003) to be equivalent to the simpler idea of providing\r
an initial approximation to the value function, as in (17.11).\r
17.5 We recommend the book by Goodfellow, Bengio, and Courville (2016) for discus\u0002sion of today’s deep learning techniques. The problem of catastrophic interference\r
in ANNs was developed by McCloskey and Cohen (1989), Ratcli↵ (1990), and\r
French (1999). The idea of a replay bu↵er was introduced by Lin (1992) and used\r
prominently in deep learning in the Atari game playing system (Section 16.5,\r
Mnih et al., 2013, 2015).

480 Chapter 17: Frontiers\r
Minsky (1961) was one of the first to identify the problem of representation\r
learning.\r
Among the few works to consider planning with learned, approximate models\r
are those by Kuvayev and Sutton (1996), Sutton, Szepesvari, Geramifard, and\r
Bowling (2008), Nouri and Littman (2009), and Hester and Stone (2012).\r
The need to be selective in model construction to avoid slowing planning is well\r
known in artificial intelligence. Some of the classic work is by Minton (1990) and\r
Tambe, Newell, and Rosenbloom (1990). Hauskrecht, Meuleau, Kaelbling, Dean,\r
and Boutilier (1998) showed this e↵ect in MDPs with deterministic options.\r
Schmidhuber (1991a, b) proposed how something like curiosity would result if\r
reward signals were a function of how quickly an agent’s environment model\r
is improving. The empowerment function proposed by Klyubin, Polani, and\r
Nehaniv (2005) is an information-theoretic measure of an agent’s ability to control\r
its environment that can function as an intrinsic reward signal. Baldassarre and\r
Mirolli (2013) is a collection of contributions by researchers studying intrinsic\r
reward and motivation from both biological and computational perspectives,\r
including a perspective on “intrinsically-motivated reinforcement learning,” to\r
use the term introduced by Singh, Barto, and Chentenez (2004). See also Oudeyer\r
and Kaplan (2007), Oudeyer, Kaplan, and Hafner (2007), and Barto (2013).

References\r
Abbeel, P., Ng, A. Y. (2004). Apprenticeship learning via inverse reinforcement learning. In\r
Proceedings of the 21st International Conference on Machine Learning. ACM, New York.\r
Abramson, B. (1990). Expected-outcome: A general model of static evaluation. IEEE Transac\u0002tions on Pattern Analysis and Machine Intelligence, 12 (2):182–193.\r
Adams, C. D. (1982). Variations in the sensitivity of instrumental responding to reinforcer\r
devaluation. The Quarterly Journal of Experimental Psychology, 34(2):77–98.\r
Adams, C. D., Dickinson, A. (1981). Instrumental responding following reinforcer devaluation.\r
The Quarterly Journal of Experimental Psychology, 33(2):109–121.\r
Adams, R. A., Huys, Q. J. M., Roiser, J. P. (2015). Computational Psychiatry: towards a\r
mathematically informed understanding of mental illness. Journal of Neurology, Neurosurgery\r
& Psychiatry. doi:10.1136/jnnp-2015-310737\r
Agrawal, R. (1995). Sample mean based index policies with O(logn) regret for the multi-armed\r
bandit problem. Advances in Applied Probability, 27 (4):1054–1078.\r
Agre, P. E. (1988). The Dynamic Structure of Everyday Life. PhD thesis, Massachusetts\r
Institute of Technology, Cambridge MA. AI-TR 1085, MIT Artificial Intelligence Laboratory.\r
Agre, P. E., Chapman, D. (1990). What are plans for? Robotics and Autonomous Systems,\r
6(1-2):17–34.\r
Aizerman, M. A., Braverman, E. ´I., Rozonoer, L. I. (1964). Probability problem of pattern\r
recognition learning and potential functions method. Avtomat. i Telemekh, 25 (9):1307–1323.\r
Albus, J. S. (1971). A theory of cerebellar function. Mathematical Biosciences, 10(1-2):25–61.\r
Albus, J. S. (1981). Brain, Behavior, and Robotics. Byte Books, Peterborough, NH.\r
Aleksandrov, V. M., Sysoev, V. I., Shemeneva, V. V. (1968). Stochastic optimization of systems.\r
Izv. Akad. Nauk SSSR, Tekh. Kibernetika:14–19.\r
Amari, S. I. (1998). Natural gradient works eciently in learning. Neural Computation,\r
10 (2):251–276.\r
An, P. C. E. (1991). An Improved Multi-dimensional CMAC Neural network: Receptive Field\r
Function and Placement. PhD thesis, University of New Hampshire, Durham.\r
An, P. C. E., Miller, W. T., Parks, P. C. (1991). Design improvements in associative memories for\r
cerebellar model articulation controllers (CMAC). Artificial Neural Networks, pp. 1207–1210,\r
Elsevier North-Holland. http://www.incompleteideas.net/papers/AnMillerParks1991.pdf\r
Anderson, C. W. (1986). Learning and Problem Solving with Multilayer Connectionist Systems.\r
PhD thesis, University of Massachusetts, Amherst.\r
Anderson, C. W. (1987). Strategy learning with multilayer connectionist representations. In\r
Proceedings of the 4th International Workshop on Machine Learning, pp. 103–114. Morgan\r
Kaufmann.

482 References\r
Anderson, C. W. (1989). Learning to control an inverted pendulum using neural networks. IEEE\r
Control Systems Magazine, 9 (3):31–37.\r
Anderson, J. A., Silverstein, J. W., Ritz, S. A., Jones, R. S. (1977). Distinctive features,\r
categorical perception, and probability learning: Some applications of a neural model.\r
Psychological Review, 84(5):413–451.\r
Andreae, J. H. (1963). STELLA, A scheme for a learning machine. In Proceedings of the 2nd\r
IFAC Congress, Basle, pp. 497–502. Butterworths, London.\r
Andreae, J. H. (1969). Learning machines—a unified view. In A. R. Meetham and R. A. Hudson\r
(Eds.), Encyclopedia of Information, Linguistics, and Control, pp. 261–270. Pergamon,\r
Oxford.\r
Andreae, J. H. (1977). Thinking with the Teachable Machine. Academic Press, London.\r
Andreae, J. H. (2017a). A model of how the brain learns: A short introduction to multiple\r
context associative learning (MCAL) and the PP system. Unpublished report.\r
Andreae, J. H. (2017b). Working memory for the associative learning of language. Unpublished\r
report.\r
Andreae, J. H., Cashin, P. M. (1969). A learning machine with monologue. International\r
Journal of Man–Machine Studies, 1(1):1–20.\r
Arthur, W. B. (1991). Designing economic agents that act like human agents: A behavioral\r
approach to bounded rationality. The American Economic Review, 81 (2):353–359.\r
Asadi, K., Allen, C., Roderick, M., Mohamed, A. R., Konidaris, G., Littman, M. (2017). Mean\r
actor critic. ArXiv:1709.00503.\r
Atkeson, C. G. (1992). Memory-based approaches to approximating continuous functions. In\r
Sante Fe Institute Studies in the Sciences of Complexity, Proceedings Vol. 12, pp. 521–521.\r
Addison-Wesley.\r
Atkeson, C. G., Moore, A. W., Schaal, S. (1997). Locally weighted learning. Artificial Intelligence\r
Review, 11 :11–73.\r
Auer, P., Cesa-Bianchi, N., Fischer, P. (2002). Finite-time analysis of the multiarmed bandit\r
problem. Machine learning, 47(2-3):235–256.\r
Bacon, P. L., Harb, J., Precup, D. (2017). The option-critic architecture. In Proceedings of the\r
Association for the Advancement of Artificial Intelligence, pp. 1726–1734.\r
Baird, L. C. (1995). Residual algorithms: Reinforcement learning with function approximation.\r
In Proceedings of the 12th International Conference on Machine Learning, pp. 30–37. Morgan\r
Kaufmann.\r
Baird, L. C. (1999). Reinforcement Learning through Gradient Descent. PhD thesis, Carnegie\r
Mellon University, Pittsburgh PA.\r
Baird, L. C., Klopf, A. H. (1993). Reinforcement learning with high-dimensional, continuous\r
actions. Wright Laboratory, Wright-Patterson Air Force Base, Tech. Rep. WL-TR-93-1147.\r
Baird, L., Moore, A. W. (1999). Gradient descent for general reinforcement learning. In Advances\r
in Neural Information Processing Systems 11, pp. 968–974. MIT Press, Cambridge MA.\r
Baldassarre, G., Mirolli, M. (Eds.) (2013). Intrinsically Motivated Learning in Natural and\r
Artificial Systems. Springer-Verlag, Berlin Heidelberg.\r
Balke, A., Pearl, J. (1994). Counterfactual probabilities: Computational methods, bounds\r
and applications. In Proceedings of the Tenth International Conference on Uncertainty in\r
Artificial Intelligence, pp. 46–54. Morgan Kaufmann.\r
Baras, D., Meir, R. (2007). Reinforcement learning, spike-time-dependent plasticity, and the\r
BCM rule. Neural Computation, 19(8):2245–2279.

References 483\r
Barnard, E. (1993). Temporal-di↵erence methods and Markov models. IEEE Transactions on\r
Systems, Man, and Cybernetics, 23(2):357–365.\r
Barreto, A. S., Precup, D., Pineau, J. (2011). Reinforcement learning using kernel-based\r
stochastic factorization. In Advances in Neural Information Processing Systems 24, pp. 720–\r
728. Curran Associates, Inc.\r
Bartlett, P. L., Baxter, J. (1999). Hebbian synaptic modifications in spiking neurons that\r
learn. Technical report, Research School of Information Sciences and Engineering, Australian\r
National University.\r
Bartlett, P. L., Baxter, J. (2000). A biologically plausible and locally optimal learning algorithm\r
for spiking neurons. Rapport technique, Australian National University.\r
Barto, A. G. (1985). Learning by statistical cooperation of self-interested neuron-like computing\r
elements. Human Neurobiology, 4(4):229–256.\r
Barto, A. G. (1986). Game-theoretic cooperativity in networks of self-interested units. In\r
J. S. Denker (Ed.), Neural Networks for Computing, pp. 41–46. American Institute of Physics,\r
New York.\r
Barto, A. G. (1989). From chemotaxis to cooperativity: Abstract exercises in neuronal learning\r
strategies. In R. Durbin, R. Maill and G. Mitchison (Eds.), The Computing Neuron, pp. 73–98.\r
Addison-Wesley, Reading, MA.\r
Barto, A. G. (1990). Connectionist learning for control: An overview. In T. Miller, R. S. Sutton,\r
and P. J. Werbos (Eds.), Neural Networks for Control, pp. 5–58. MIT Press, Cambridge,\r
MA.\r
Barto, A. G. (1991). Some learning tasks from a control perspective. In L. Nadel and D. L. Stein\r
(Eds.), 1990 Lectures in Complex Systems, pp. 195–223. Addison-Wesley, Redwood City, CA.\r
Barto, A. G. (1992). Reinforcement learning and adaptive critic methods. In D. A. White and\r
D. A. Sofge (Eds.), Handbook of Intelligent Control: Neural, Fuzzy, and Adaptive Approaches,\r
pp. 469–491. Van Nostrand Reinhold, New York.\r
Barto, A. G. (1995a). Adaptive critics and the basal ganglia. In J. C. Houk, J. L. Davis, and\r
D. G. Beiser (Eds.), Models of Information Processing in the Basal Ganglia, pp. 215–232.\r
MIT Press, Cambridge, MA.\r
Barto, A. G. (1995b). Reinforcement learning. In M. A. Arbib (Ed.), Handbook of Brain Theory\r
and Neural Networks, pp. 804–809. MIT Press, Cambridge, MA.\r
Barto, A. G. (2011). Adaptive real-time dynamic programming. In C. Sammut and G. I Webb\r
(Eds.), Encyclopedia of Machine Learning, pp. 19–22. Springer Science and Business Media.\r
Barto, A. G. (2013). Intrinsic motivation and reinforcement learning. In G. Baldassarre and M.\r
Mirolli (Eds.), Intrinsically Motivated Learning in Natural and Artificial Systems, pp. 17–47.\r
Springer-Verlag, Berlin Heidelberg.\r
Barto, A. G., Anandan, P. (1985). Pattern recognizing stochastic learning automata. IEEE\r
Transactions on Systems, Man, and Cybernetics, 15(3):360–375.\r
Barto, A. G., Anderson, C. W. (1985). Structural learning in connectionist systems. In\r
Proceedings of the Seventh Annual Conference of the Cognitive Science Society, pp. 43–54.\r
Barto, A. G., Anderson, C. W., Sutton, R. S. (1982). Synthesis of nonlinear control surfaces by\r
a layered associative search network. Biological Cybernetics, 43(3):175–185.\r
Barto, A. G., Bradtke, S. J., Singh, S. P. (1991). Real-time learning and control using\r
asynchronous dynamic programming. Technical Report 91-57. Department of Computer\r
and Information Science, University of Massachusetts, Amherst.\r
Barto, A. G., Bradtke, S. J., Singh, S. P. (1995). Learning to act using real-time dynamic\r
programming. Artificial Intelligence, 72(1-2):81–138.

484 References\r
Barto, A. G., Du↵, M. (1994). Monte Carlo matrix inversion and reinforcement learning. In\r
Advances in Neural Information Processing Systems 6, pp. 687–694. Morgan Kaufmann, San\r
Francisco.\r
Barto, A. G., Jordan, M. I. (1987). Gradient following without back-propagation in layered\r
networks. In M. Caudill and C. Butler (Eds.), Proceedings of the IEEE First Annual\r
Conference on Neural Networks, pp. II629–II636. SOS Printing, San Diego.\r
Barto, A. G., Mahadevan, S. (2003). Recent advances in hierarchical reinforcement learning.\r
Discrete Event Dynamic Systems, 13 (4):341–379.\r
Barto, A. G., Singh, S. P. (1990). On the computational economics of reinforcement learning. In\r
Connectionist Models: Proceedings of the 1990 Summer School. Morgan Kaufmann.\r
Barto, A. G., Sutton, R. S. (1981a). Goal seeking components for adaptive intelligence: An\r
initial assessment. Technical Report AFWAL-TR-81-1070. Air Force Wright Aeronautical\r
Laboratories/Avionics Laboratory, Wright-Patterson AFB, OH.\r
Barto, A. G., Sutton, R. S. (1981b). Landmark learning: An illustration of associative search.\r
Biological Cybernetics, 42(1):1–8.\r
Barto, A. G., Sutton, R. S. (1982). Simulation of anticipatory responses in classical conditioning\r
by a neuron-like adaptive element. Behavioural Brain Research, 4(3):221–235.\r
Barto, A. G., Sutton, R. S., Anderson, C. W. (1983). Neuronlike elements that can solve\r
dicult learning control problems. IEEE Transactions on Systems, Man, and Cybernetics,\r
13(5):835–846. Reprinted in J. A. Anderson and E. Rosenfeld (Eds.), Neurocomputing:\r
Foundations of Research, pp. 535–549. MIT Press, Cambridge, MA, 1988.\r
Barto, A. G., Sutton, R. S., Brouwer, P. S. (1981). Associative search network: A reinforcement\r
learning associative memory. Biological Cybernetics, 40(3):201–211.\r
Barto, A. G., Sutton, R. S., Watkins, C. J. C. H. (1990). Learning and sequential decision\r
making. In M. Gabriel and J. Moore (Eds.), Learning and Computational Neuroscience:\r
Foundations of Adaptive Networks, pp. 539–602. MIT Press, Cambridge, MA.\r
Baxter, J., Bartlett, P. L. (2001). Infinite-horizon policy-gradient estimation. Journal of Artificial\r
Intelligence Research, 15 :319–350.\r
Baxter, J., Bartlett, P. L., Weaver, L. (2001). Experiments with infinite-horizon, policy-gradient\r
estimation. Journal of Artificial Intelligence Research, 15 :351–381.\r
Bellemare, M. G., Dabney, W., Munos, R. (2017). A distributional perspective on reinforcement\r
learning. ArXiv:1707.06887.\r
Bellemare, M. G., Naddaf, Y., Veness, J., Bowling, M. (2013). The arcade learning environment:\r
An evaluation platform for general agents. Journal of Artificial Intelligence Research,\r
47:253–279.\r
Bellemare, M. G., Veness, J., Bowling, M. (2012). Investigating contingency awareness using\r
Atari 2600 games. In Proceedings of the Twenty-Sixth AAAI Conference on Artificial\r
Intelligence, pp. 864–871. AAAI Press, Menlo Park, CA.\r
Bellman, R. E. (1956). A problem in the sequential design of experiments. Sankhya, 16:221–229.\r
Bellman, R. E. (1957a). Dynamic Programming. Princeton University Press, Princeton.\r
Bellman, R. E. (1957b). A Markov decision process. Journal of Mathematics and Mechanics,\r
6(5):679–684.\r
Bellman, R. E., Dreyfus, S. E. (1959). Functional approximations and dynamic programming.\r
Mathematical Tables and Other Aids to Computation, 13:247–251.\r
Bellman, R. E., Kalaba, R., Kotkin, B. (1963). Polynomial approximation—A new computational\r
technique in dynamic programming: Allocation processes. Mathematical Computation,\r
17:155–161.

References 485\r
Bengio, Y. (2009). Learning deep architectures for AI. Foundations and Trends in Machine\r
Learning, 2(1):1–27.\r
Bengio, Y., Courville, A. C., Vincent, P. (2012). Unsupervised feature learning and deep learning:\r
A review and new perspectives. CoRR 1, ArXiv:1206.5538.\r
Bentley, J. L. (1975). Multidimensional binary search trees used for associative searching.\r
Communications of the ACM, 18 (9):509–517.\r
Berg, H. C. (1975). Chemotaxis in bacteria. Annual review of biophysics and bioengineering,\r
4(1):119–136.\r
Berns, G. S., McClure, S. M., Pagnoni, G., Montague, P. R. (2001). Predictability modulates\r
human brain response to reward. The journal of neuroscience, 21(8):2793–2798.\r
Berridge, K. C., Kringelbach, M. L. (2008). A↵ective neuroscience of pleasure: reward in humans\r
and animals. Psychopharmacology, 199(3):457–480.\r
Berridge, K. C., Robinson, T. E. (1998). What is the role of dopamine in reward: hedonic\r
impact, reward learning, or incentive salience? Brain Research Reviews, 28(3):309–369.\r
Berry, D. A., Fristedt, B. (1985). Bandit Problems. Chapman and Hall, London.\r
Bertsekas, D. P. (1982). Distributed dynamic programming. IEEE Transactions on Automatic\r
Control, 27(3):610–616.\r
Bertsekas, D. P. (1983). Distributed asynchronous computation of fixed points. Mathematical\r
Programming, 27(1):107–120.\r
Bertsekas, D. P. (1987). Dynamic Programming: Deterministic and Stochastic Models. Prentice\u0002Hall, Englewood Cli↵s, NJ.\r
Bertsekas, D. P. (2005). Dynamic Programming and Optimal Control, Volume 1, third edition.\r
Athena Scientific, Belmont, MA.\r
Bertsekas, D. P. (2012). Dynamic Programming and Optimal Control, Volume 2: Approximate\r
Dynamic Programming, fourth edition. Athena Scientific, Belmont, MA.\r
Bertsekas, D. P. (2013). Rollout algorithms for discrete optimization: A survey. In Handbook of\r
Combinatorial Optimization, pp. 2989–3013. Springer, New York.\r
Bertsekas, D. P., Tsitsiklis, J. N. (1989). Parallel and Distributed Computation: Numerical\r
Methods. Prentice-Hall, Englewood Cli↵s, NJ.\r
Bertsekas, D. P., Tsitsiklis, J. N. (1996). Neuro-Dynamic Programming. Athena Scientific,\r
Belmont, MA.\r
Bertsekas, D. P., Tsitsiklis, J. N., Wu, C. (1997). Rollout algorithms for combinatorial optimiza\u0002tion. Journal of Heuristics, 3 (3):245–262.\r
Bertsekas, D. P., Yu, H. (2009). Projected equation methods for approximate solution of large\r
linear systems. Journal of Computational and Applied Mathematics, 227 (1):27–50.\r
Bhat, N., Farias, V., Moallemi, C. C. (2012). Non-parametric approximate dynamic programming\r
via the kernel method. In Advances in Neural Information Processing Systems 25, pp. 386–394.\r
Curran Associates, Inc.\r
Bhatnagar, S., Sutton, R., Ghavamzadeh, M., Lee, M. (2009). Natural actor–critic algorithms.\r
Automatica, 45 (11).\r
Biermann, A. W., Fairfield, J. R. C., Beres, T. R. (1982). Signature table systems and learning.\r
IEEE Transactions on Systems, Man, and Cybernetics, 12(5):635–648.\r
Bishop, C. M. (1995). Neural Networks for Pattern Recognition. Clarendon, Oxford.\r
Bishop, C. M. (2006). Pattern Recognition and Machine Learning. Springer Science + Business\r
Media New York LLC.\r
Blodgett, H. C. (1929). The e↵ect of the introduction of reward upon the maze performance of\r
rats. University of California Publications in Psychology, 4:113–134.

486 References\r
Boakes, R. A., Costa, D. S. J. (2014). Temporal contiguity in associative learning: Iinterference\r
and decay from an historical perspective. Journal of Experimental Psychology: Animal\r
Learning and Cognition, 40(4):381–400.\r
Booker, L. B. (1982). Intelligent Behavior as an Adaptation to the Task Environment. PhD thesis,\r
University of Michigan, Ann Arbor.\r
Bostrom, N. (2014). Superintelligence: Paths, Dangers, Strategies. Oxford University Press.\r
Bottou, L., Vapnik, V. (1992). Local learning algorithms. Neural Computation, 4 (6):888–900.\r
Boyan, J. A. (1999). Least-squares temporal di↵erence learning. In Proceedings of the 16th\r
International Conference on Machine Learning, pp. 49–56.\r
Boyan, J. A. (2002). Technical update: Least-squares temporal di↵erence learning. Machine\r
Learning, 49(2):233–246.\r
Boyan, J. A., Moore, A. W. (1995). Generalization in reinforcement learning: Safely approximat\u0002ing the value function. In Advances in Neural Information Processing Systems 7, pp. 369–376.\r
MIT Press, Cambridge, MA.\r
Bradtke, S. J. (1993). Reinforcement learning applied to linear quadratic regulation. In Advances\r
in Neural Information Processing Systems 5, pp. 295–302. Morgan Kaufmann.\r
Bradtke, S. J. (1994). Incremental Dynamic Programming for On-Line Adaptive Optimal Control.\r
PhD thesis, University of Massachusetts, Amherst. Appeared as CMPSCI Technical Report\r
94-62.\r
Bradtke, S. J., Barto, A. G. (1996). Linear least–squares algorithms for temporal di↵erence\r
learning. Machine Learning, 22:33–57.\r
Bradtke, S. J., Ydstie, B. E., Barto, A. G. (1994). Adaptive linear quadratic control using policy\r
iteration. In Proceedings of the American Control Conference, pp. 3475–3479. American\r
Automatic Control Council, Evanston, IL.\r
Brafman, R. I., Tennenholtz, M. (2003). R-max – a general polynomial time algorithm for\r
near-optimal reinforcement learning. Journal of Machine Learning Research, 3 :213–231.\r
Breiman, L. (2001). Random forests. Machine Learning, 45 (1):5–32.\r
Breiter, H. C., Aharon, I., Kahneman, D., Dale, A., Shizgal, P. (2001). Functional imaging\r
of neural responses to expectancy and experience of monetary gains and losses. Neuron,\r
30(2):619–639.\r
Breland, K., Breland, M. (1961). The misbehavior of organisms. American Psychologist,\r
16(11):681–684.\r
Bridle, J. S. (1990). Training stochastic model recognition algorithms as networks can lead to\r
maximum mutual information estimates of parameters. In Advances in Neural Information\r
Processing Systems 2, pp. 211–217. Morgan Kaufmann, San Mateo, CA.\r
Broomhead, D. S., Lowe, D. (1988). Multivariable functional interpolation and adaptive networks.\r
Complex Systems, 2:321–355.\r
Bromberg-Martin, E. S., Matsumoto, M., Hong, S., Hikosaka, O. (2010). A pallidus-habenula\u0002dopamine pathway signals inferred stimulus values. Journal of Neurophysiology, 104(2):1068–\r
1076.\r
Browne, C.B., Powley, E., Whitehouse, D., Lucas, S.M., Cowling, P.I., Rohlfshagen, P., Tavener,\r
S., Perez, D., Samothrakis, S., Colton, S. (2012). A survey of monte carlo tree search methods.\r
IEEE Transactions on Computational Intelligence and AI in Games, 4(1):1–43.\r
Brown, J., Bullock, D., Grossberg, S. (1999). How the basal ganglia use parallel excitatory\r
and inhibitory learning pathways to selectively respond to unexpected rewarding cues. The\r
Journal of Neuroscience, 19(23):10502–10511.

References 487\r
Bryson, A. E., Jr. (1996). Optimal control—1950 to 1985. IEEE Control Systems, 13(3):26–33.\r
Buchanan, B. G., Mitchell, T., Smith, R. G., Johnson, C. R., Jr. (1978). Models of learning\r
systems. Encyclopedia of Computer Science and technology, 11.\r
Buhusi, C. V., Schmajuk, N. A. (1999). Timing in simple conditioning and occasion setting: A\r
neural network approach. Behavioural Processes, 45(1):33–57.\r
Bu¸soniu, L., Lazaric, A., Ghavamzadeh, M., Munos, R., Babu˘ska, R., De Schutter, B. (2012).\r
Least-squares methods for policy iteration. In M. Wiering and M. van Otterlo (Eds.),\r
Reinforcement Learning: State-of-the-Art, pp. 75–109. Springer-Verlag Berlin Heidelberg.\r
Bush, R. R., Mosteller, F. (1955). Stochastic Models for Learning. Wiley, New York.\r
Byrne, J. H., Gingrich, K. J., Baxter, D. A. (1990). Computational capabilities of single\r
neurons: Relationship to simple forms of associative and nonassociative learning in aplysia.\r
In R. D. Hawkins and G. H. Bower (Eds.), Computational Models of Learning, pp. 31–63.\r
Academic Press, New York.\r
Calabresi, P., Picconi, B., Tozzi, A., Filippo, M. D. (2007). Dopamine-mediated regulation of\r
corticostriatal synaptic plasticity. Trends in Neuroscience, 30(5):211–219.\r
Camerer, C. (2011). Behavioral Game Theory: Experiments in Strategic Interaction. Princeton\r
University Press.\r
Campbell, D. T. (1960). Blind variation and selective survival as a general strategy in knowledge\u0002processes. In M. C. Yovits and S. Cameron (Eds.), Self-Organizing Systems, pp. 205–231.\r
Pergamon, New York.\r
Cao, X. R. (2009). Stochastic learning and optimization—A sensitivity-based approach. Annual\r
Reviews in Control, 33 (1):11–24.\r
Cao, X. R., Chen, H. F. (1997). Perturbation realization, potentials, and sensitivity analysis of\r
Markov processes. IEEE Transactions on Automatic Control, 42 (10):1382–1393.\r
Carlstr¨om, J., Nordstr¨om, E. (1997). Control of self-similar ATM call trac by reinforcement\r
learning. In Proceedings of the International Workshop on Applications of Neural Networks\r
to Telecommunications 3, pp. 54–62. Erlbaum, Hillsdale, NJ.\r
Chapman, D., Kaelbling, L. P. (1991). Input generalization in delayed reinforcement learning:\r
An algorithm and performance comparisons. In Proceedings of the Twelfth International\r
Conference on Artificial Intelligence, pp. 726–731. Morgan Kaufmann, San Mateo, CA.\r
Chaslot, G., Bakkes, S., Szita, I., Spronck, P. (2008). Monte-Carlo tree search: A new framework\r
for game AI. In Proceedings of the Fourth AAAI Conference on Artificial Intelligence and\r
Interactive Digital Entertainment (AIDE-08), pp. 216–217. AAAI Press, Menlo Park, CA.\r
Chow, C.-S., Tsitsiklis, J. N. (1991). An optimal one-way multigrid algorithm for discrete-time\r
stochastic control. IEEE Transactions on Automatic Control, 36(8):898–914.\r
Chrisman, L. (1992). Reinforcement learning with perceptual aliasing: The perceptual distinc\u0002tions approach. In Proceedings of the Tenth National Conference on Artificial Intelligence,\r
pp. 183–188. AAAI/MIT Press, Menlo Park, CA.\r
Christensen, J., Korf, R. E. (1986). A unified theory of heuristic evaluation functions and\r
its application to learning. In Proceedings of the Fifth National Conference on Artificial\r
Intelligence, pp. 148–152. Morgan Kaufmann.\r
Cichosz, P. (1995). Truncating temporal di↵erences: On the ecient implementation of TD()\r
for reinforcement learning. Journal of Artificial Intelligence Research, 2:287–318.\r
Ciosek, K., Whiteson, S. (2017). Expected policy gradients. ArXiv:1706.05374v1. A revised ver\u0002sion appeared in Proceedings of the Annual Conference of the Association for the Advancement\r
of Artificial Intelligence, pp. 2868–2875.\r
Ciosek, K., Whiteson, S. (2018). Expected policy gradients for reinforcement learning. ArXiv:\r
1801.03326.

488 References\r
Claridge-Chang, A., Roorda, R. D., Vrontou, E., Sjulson, L., Li, H., Hirsh, J., Miesenb¨ock, G.\r
(2009). Writing memories with light-addressable reinforcement circuitry. Cell, 139(2):405–\r
415.\r
Clark, R. E., Squire, L. R. (1998). Classical conditioning and brain systems: the role of awareness.\r
Science, 280(5360):77–81.\r
Clark, W. A., Farley, B. G. (1955). Generalization of pattern recognition in a self-organizing\r
system. In Proceedings of the 1955 Western Joint Computer Conference, pp. 86–91.\r
Clouse, J. (1996). On Integrating Apprentice Learning and Reinforcement Learning TITLE2.\r
PhD thesis, University of Massachusetts, Amherst. Appeared as CMPSCI Technical Report\r
96-026.\r
Clouse, J., Utgo↵, P. (1992). A teaching method for reinforcement learning systems. In\r
Proceedings of the 9th International Workshop on Machine Learning, pp. 92–101. Morgan\r
Kaufmann.\r
Cobo, L. C., Zang, P., Isbell, C. L., Thomaz, A. L. (2011). Automatic state abstraction from\r
demonstration. In Proceedings of the Twenty-Second International Joint Conference on\r
Artificial Intelligence, pp. 1243-1248. AAAI Press.\r
Connell, J. (1989). A colony architecture for an artificial creature. Technical Report AI-TR-1151.\r
MIT Artificial Intelligence Laboratory, Cambridge, MA.\r
Connell, M. E., Utgo↵, P. E. (1987). Learning to control a dynamic physical system. Computa\u0002tional intelligence, 3 (1):330–337.\r
Contreras-Vidal, J. L., Schultz, W. (1999). A predictive reinforcement model of dopamine neurons\r
for learning approach behavior. Journal of Computational Neuroscience, 6(3):191–214.\r
Coulom, R. (2006). Ecient selectivity and backup operators in Monte-Carlo tree search. In\r
Proceedings of the 5th International Conference on Computers and Games (CG’06), pp. 72–83.\r
Springer-Verlag Berlin, Heidelberg.\r
Courville, A. C., Daw, N. D., Touretzky, D. S. (2006). Bayesian theories of conditioning in a\r
changing world. Trends in Cognitive Science, 10(7):294–300.\r
Craik, K. J. W. (1943). The Nature of Explanation. Cambridge University Press, Cambridge.\r
Cross, J. G. (1973). A stochastic learning model of economic behavior. The Quarterly Journal\r
of Economics, 87 (2):239–266.\r
Crow, T. J. (1968). Cortical synapses and reinforcement: a hypothesis. Nature, 219(5155):736–\r
737.\r
Curtiss, J. H. (1954). A theoretical comparison of the eciencies of two classical methods and a\r
Monte Carlo method for computing one component of the solution of a set of linear algebraic\r
equations. In H. A. Meyer (Ed.), Symposium on Monte Carlo Methods, pp. 191–233. Wiley,\r
New York.\r
Cybenko, G. (1989). Approximation by superpositions of a sigmoidal function. Mathematics of\r
control, signals and systems, 2(4):303–314.\r
Cziko, G. (1995). Without Miracles: Universal Selection Theory and the Second Darvinian\r
Revolution. MIT Press, Cambridge, MA.\r
Dabney, W. (2014). Adaptive step-sizes for reinforcement learning. PhD thesis, University of\r
Massachusetts, Amherst.\r
Dabney, W., Barto, A. G. (2012). Adaptive step-size for online temporal di↵erence learning. In\r
Proceedings of the Annual Conference of the Association for the Advancement of Artificial\r
Intelligence.\r
Daniel, J. W. (1976). Splines and eciency in dynamic programming. Journal of Mathematical\r
Analysis and Applications, 54:402–407.\r
Dann, C., Neumann, G., Peters, J. (2014). Policy evaluation with temporal di↵erences: A survey\r
and comparison. Journal of Machine Learning Research, 15 :809–883.

References 489\r
Daw, N. D., Courville, A. C., Touretzky, D. S. (2003). Timing and partial observability in the\r
dopamine system. In Advances in Neural Information Processing Systems 15, pp. 99–106.\r
MIT Press, Cambridge, MA.\r
Daw, N. D., Courville, A. C., Touretzky, D. S. (2006). Representation and timing in theories of\r
the dopamine system. Neural Computation, 18(7):1637–1677.\r
Daw, N. D., Niv, Y., Dayan, P. (2005). Uncertainty based competition between prefrontal and\r
dorsolateral striatal systems for behavioral control. Nature Neuroscience, 8(12):1704–1711.\r
Daw, N. D., Shohamy, D. (2008). The cognitive neuroscience of motivation and learning. Social\r
Cognition, 26(5):593–620.\r
Dayan, P. (1991). Reinforcement comparison. In D. S. Touretzky, J. L. Elman, T. J. Sejnowski,\r
and G. E. Hinton (Eds.), Connectionist Models: Proceedings of the 1990 Summer School,\r
pp. 45–51. Morgan Kaufmann.\r
Dayan, P. (1992). The convergence of TD() for general . Machine Learning, 8(3):341–362.\r
Dayan, P. (2002). Matters temporal. Trends in Cognitive Sciences, 6 (3):105–106.\r
Dayan, P., Abbott, L. F. (2001). Theoretical Neuroscience: Computational and Mathematical\r
Modeling of Neural Systems. MIT Press, Cambridge, MA.\r
Dayan, P., Berridge, K. C. (2014). Model-based and model-free Pavlovian reward learning:\r
Revaluation, revision, and revaluation. Cognitive, A↵ective, & Behavioral Neuroscience,\r
14(2):473–492.\r
Dayan, P., Niv, Y. (2008). Reinforcement learning: the good, the bad and the ugly. Current\r
Opinion in Neurobiology, 18(2):185–196.\r
Dayan, P., Niv, Y., Seymour, B., Daw, N. D. (2006). The misbehavior of value and the discipline\r
of the will. Neural Networks, 19 (8):1153–1160.\r
Dayan, P., Sejnowski, T. (1994). TD() converges with probability 1. Machine Learning,\r
14(3):295–301.\r
De Asis, K., Hernandez-Garcia, J. F., Holland, G. Z., Sutton, R. S. (2017). Multi-step Rein\u0002forcement Learning: A Unifying Algorithm. ArXiv:1703.01327.\r
de Farias, D. P. (2002). The Linear Programming Approach to Approximate Dynamic Program\u0002ming: Theory and Application. Stanford University PhD thesis.\r
de Farias, D. P., Van Roy, B. (2003). The linear programming approach to approximate dynamic\r
programming. Operations Research 51 (6):850–865.\r
Dean, T., Lin, S.-H. (1995). Decomposition techniques for planning in stochastic domains.\r
In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence,\r
pp. 1121–1127. Morgan Kaufmann. See also Technical Report CS-95-10, Brown University,\r
Department of Computer Science, 1995.\r
Degris, T., Pilarski, P. M., Sutton, R. S. (2012). Model-free reinforcement learning with\r
continuous action in practice. In 2012 American Control Conference, pp. 2177–2182. IEEE.\r
Degris, T., White, M., Sutton, R. S. (2012). O↵-policy actor–critic. In Proceedings of the 29th\r
International Conference on Machine Learning. ArXiv:1205.4839, 2012.\r
Denardo, E. V. (1967). Contraction mappings in the theory underlying dynamic programming.\r
SIAM Review, 9(2):165–177.\r
Dennett, D. C. (1978). Why the Law of E↵ect Will Not Go Away. Brainstorms, pp. 71–89.\r
Bradford/MIT Press, Cambridge, MA.\r
Derthick, M. (1984). Variations on the Boltzmann machine learning algorithm. Carnegie-Mellon\r
University Department of Computer Science Technical Report No. CMU-CS-84-120.\r
Deutsch, J. A. (1953). A new type of behaviour theory. British Journal of Psychology. General\r
Section, 44(4):304–317.

490 References\r
Deutsch, J. A. (1954). A machine with insight. Quarterly Journal of Experimental Psychology,\r
6(1):6–11.\r
Dick, T. (2015). Policy Gradient Reinforcement Learning Without Regret. M.Sc. thesis,\r
University of Alberta.\r
Dickinson, A. (1980). Contemporary Animal Learning Theory. Cambridge University Press.\r
Dickinson, A. (1985). Actions and habits: the development of behavioral autonomy. Phil. Trans.\r
R. Soc. Lond. B, 308(1135):67–78.\r
Dickinson, A., Balleine, B. W. (2002). The role of learning in motivation. In C. R. Gallistel\r
(Ed.), Stevens’ Handbook of Experimental Psychology, volume 3, pp. 497–533. Wiley, NY.\r
Dietterich, T. G., Buchanan, B. G. (1984). The role of the critic in learning systems. In O. G.\r
Selfridge, E. L. Rissland, and M. A. Arbib (Eds.), Adaptive Control of Ill-Defined Systems,\r
pp. 127–147. Plenum Press, NY.\r
Dietterich, T. G., Flann, N. S. (1995). Explanation-based learning and reinforcement learning:\r
A unified view. In A. Prieditis and S. Russell (Eds.), Proceedings of the 12th International\r
Conference on Machine Learning, pp. 176–184. Morgan Kaufmann.\r
Dietterich, T. G., Wang, X. (2002). Batch value function approximation via support vectors.\r
In Advances in Neural Information Processing Systems 14, pp. 1491–1498. MIT Press,\r
Cambridge, MA.\r
Diuk, C., Cohen, A., Littman, M. L. (2008). An object-oriented representation for ecient\r
reinforcement learning. In Proceedings of the 25th International Conference on Machine\r
Learning, pp. 240–247. ACM, New York.\r
Dolan, R. J., Dayan, P. (2013). Goals and habits in the brain. Neuron, 80(2):312–325.\r
Doll, B. B., Simon, D. A., Daw, N. D. (2012). The ubiquity of model-based reinforcement\r
learning. Current Opinion in Neurobiology, 22(6):1–7.\r
Donahoe, J. W., Burgos, J. E. (2000). Behavior analysis and revaluation. Journal of the\r
Experimental Analysis of Behavior, 74(3):331–346.\r
Dorigo, M., Colombetti, M. (1994). Robot shaping: Developing autonomous agents through\r
learning. Artificial Intelligence, 71(2):321–370.\r
Doya, K. (1996). Temporal di↵erence learning in continuous time and space. In Advances in\r
Neural Information Processing Systems 8, pp. 1073–1079. MIT Press, Cambridge, MA.\r
Doya, K., Sejnowski, T. J. (1995). A novel reinforcement model of birdsong vocalization\r
learning. In Advances in Neural Information Processing Systems 7, pp. 101–108. MIT Press,\r
Cambridge, MA.\r
Doya, K., Sejnowski, T. J. (1998). A computational model of birdsong learning by auditory\r
experience and auditory feedback. In P. W. F. Poon and J. F. Brugge (Eds.), Central\r
Auditory Processing and Neural Modeling, pp. 77–88. Springer, Boston, MA.\r
Doyle, P. G., Snell, J. L. (1984). Random Walks and Electric Networks. The Mathematical\r
Association of America. Carus Mathematical Monograph 22.\r
Dreyfus, S. E., Law, A. M. (1977). The Art and Theory of Dynamic Programming. Academic\r
Press, New York.\r
Du, S. S., Chen, J., Li, L., Xiao, L., Zhou, D. (2017). Stochastic variance reduction methods for\r
policy evaluation. Proceedings of the 34th International Conference on Machine Learning,\r
pp. 1049–1058. ArXiv:1702.07944.\r
Duda, R. O., Hart, P. E. (1973). Pattern Classification and Scene Analysis. Wiley, New York.

References 491\r
Du↵, M. O. (1995). Q-learning for bandit problems. In Proceedings of the 12th International\r
Conference on Machine Learning, pp. 209–217. Morgan Kaufmann.\r
Egger, D. M., Miller, N. E. (1962). Secondary reinforcement in rats as a function of information\r
value and reliability of the stimulus. Journal of Experimental Psychology, 64:97–104.\r
Eshel, N., Tian, J., Bukwich, M., Uchida, N. (2016). Dopamine neurons share common response\r
function for reward prediction error. Nature Neuroscience, 19 (3):479–486.\r
Estes, W. K. (1943). Discriminative conditioning. I. A discriminative property of conditioned\r
anticipation. Journal of Experimental Psychology, 32 (2):150–155.\r
Estes, W. K. (1948). Discriminative conditioning. II. E↵ects of a Pavlovian conditioned stimulus\r
upon a subsequently established operant response. Journal of Experimental Psychology,\r
38 (2):173–177.\r
Estes, W. K. (1950). Toward a statistical theory of learning. Psychololgical Review, 57(2):94–107.\r
Farley, B. G., Clark, W. A. (1954). Simulation of self-organizing systems by digital computer.\r
IRE Transactions on Information Theory, 4(4):76–84.\r
Farries, M. A., Fairhall, A. L. (2007). Reinforcement learning with modulated spike timing\u0002dependent synaptic plasticity. Journal of Neurophysiology, 98(6):3648–3665.\r
Feldbaum, A. A. (1965). Optimal Control Systems. Academic Press, New York.\r
Finch, G., Culler, E. (1934). Higher order conditioning with constant motivation. The American\r
Journal of Psychology:596–602.\r
Finnsson, H., Bj¨ornsson, Y. (2008). Simulation-based approach to general game playing. In\r
Proceedings of the Association for the Advancement of Artificial Intelligence, pp. 259–264.\r
Fiorillo, C. D., Yun, S. R., Song, M. R. (2013). Diversity and homogeneity in responses of\r
midbrain dopamine neurons. The Journal of Neuroscience, 33(11):4693–4709.\r
Florian, R. V. (2007). Reinforcement learning through modulation of spike-timing-dependent\r
synaptic plasticity. Neural Computation, 19(6):1468–1502.\r
Fogel, L. J., Owens, A. J., Walsh, M. J. (1966). Artificial Intelligence through Simulated Evolution.\r
John Wiley and Sons.\r
French, R. M. (1999). Catastrophic forgetting in connectionist networks. Trends in cognitive\r
sciences, 3 (4):128–135.\r
Frey, U., Morris, R. G. M. (1997). Synaptic tagging and long-term potentiation. Nature,\r
385(6616):533–536.\r
Fr´emaux, N., Sprekeler, H., Gerstner, W. (2010). Functional requirements for reward-modulated\r
spike-timing-dependent plasticity. The Journal of Neuroscience, 30 (40): 13326–13337\r
Friedman, J. H., Bentley, J. L., Finkel, R. A. (1977). An algorithm for finding best matches in\r
logarithmic expected time. ACM Transactions on Mathematical Software, 3 (3):209–226.\r
Friston, K. J., Tononi, G., Reeke, G. N., Sporns, O., Edelman, G. M. (1994). Value-dependent\r
selection in the brain: Simulation in a synthetic neural model. Neuroscience, 59(2):229–243.\r
Fu, K. S. (1970). Learning control systems—Review and outlook. IEEE Transactions on\r
Automatic Control, 15(2):210–221.\r
Galanter, E., Gerstenhaber, M. (1956). On thought: The extrinsic theory. Psychological Review,\r
63(4):218–227.\r
Gallistel, C. R. (2005). Deconstructing the law of e↵ect. Games and Economic Behavior,\r
52 (2):410–423.\r
Gardner, M. (1973). Mathematical games. Scientific American, 228(1):108–115.\r
Geist, M., Scherrer, B. (2014). O↵-policy learning with eligibility traces: A survey. Journal of\r
Machine Learning Research, 15 (1):289–333.

492 References\r
Gelly, S., Silver, D. (2007). Combining online and o✏ine knowledge in UCT. Proceedings of the\r
24th International Conference on Machine Learning, pp. 273–280.\r
Gelperin, A., Hopfield, J. J., Tank, D. W. (1985). The logic of limax learning. In A. Selverston\r
(Ed.), Model Neural Networks and Behavior, pp. 247–261. Plenum Press, New York.\r
Genesereth, M., Thielscher, M. (2014). General game playing. Synthesis Lectures on Artificial\r
Intelligence and Machine Learning, 8 (2):1–229.\r
Gershman, S. J., Moustafa, A. A., Ludvig, E. A. (2014). Time representation in reinforcement\r
learning models of the basal ganglia. Frontiers in Computational Neuroscience, 7:194.\r
Gershman, S. J., Pesaran, B., Daw, N. D. (2009). Human reinforcement learning subdivides\r
structured action spaces by learning e↵ector-specific values. The Journal of Neuroscience,\r
29 (43):13524–13531.\r
Ghiassian, S., Rafiee, B., Sutton, R. S. (2016). A first empirical study of emphatic temporal\r
di↵erence learning. Workshop on Continual Learning and Deep Learning at the Conference\r
on Neural Information Processing Systems. ArXiv:1705.04185.\r
Ghiassian, S., Patterson, A., White, M., Sutton, R. S., White, A. (2018). Online o↵-policy\r
prediction. ArXiv:1811.02597.\r
Gibbs, C. M., Cool, V., Land, T., Kehoe, E. J., Gormezano, I. (1991). Second-order conditioning\r
of the rabbit’s nictitating membrane response. Integrative Physiological and Behavioral\r
Science, 26 (4):282–295.\r
Gittins, J. C., Jones, D. M. (1974). A dynamic allocation index for the sequential design of\r
experiments. In J. Gani, K. Sarkadi, and I. Vincze (Eds.), Progress in Statistics, pp. 241–266.\r
North-Holland, Amsterdam–London.\r
Glimcher, P. W. (2011). Understanding dopamine and reinforcement learning: The dopamine\r
reward prediction error hypothesis. Proceedings of the National Academy of Sciences,\r
108(Supplement 3):15647–15654.\r
Glimcher, P. W. (2003). Decisions, Uncertainty, and the Brain: The science of Neuroeconomics.\r
MIT Press, Cambridge, MA.\r
Glimcher, P. W., Fehr, E. (Eds.) (2013). Neuroeconomics: Decision Making and the Brain,\r
Second Edition. Academic Press.\r
Goethe, J. W. V. (1878). The Sorcerer’s Apprentice. In The Permanent Goethe, p. 349. The\r
Dial Press, Inc., New York.\r
Goldstein, H. (1957). Classical Mechanics. Addison-Wesley, Reading, MA.\r
Goodfellow, I., Bengio, Y., Courville, A. (2016). Deep Learning. MIT Press, Cambridge, MA.\r
Goodwin, G. C., Sin, K. S. (1984). Adaptive Filtering Prediction and Control. Prentice-Hall,\r
Englewood Cli↵s, NJ.\r
Gopnik, A., Glymour, C., Sobel, D., Schulz, L. E., Kushnir, T., Danks, D. (2004). A theory of\r
causal learning in children: Causal maps and Bayes nets. Psychological Review, 111(1):3–32.\r
Gordon, G. J. (1995). Stable function approximation in dynamic programming. In A. Prieditis\r
and S. Russell (Eds.), Proceedings of the 12th International Conference on Machine Learning,\r
pp. 261–268. Morgan Kaufmann. An expanded version was published as Technical Report\r
CMU-CS-95-103. Carnegie Mellon University, Pittsburgh, PA, 1995.\r
Gordon, G. J. (1996a). Chattering in SARSA(). CMU learning lab internal report.\r
Gordon, G. J. (1996b). Stable fitted reinforcement learning. In Advances in Neural Information\r
Processing Systems 8, pp. 1052–1058. MIT Press, Cambridge, MA.\r
Gordon, G. J. (1999). Approximate Solutions to Markov Decision Processes. PhD thesis,\r
Carnegie Mellon University, Pittsburgh PA. Pittsburgh, PA.\r
Gordon, G. J. (2001). Reinforcement learning with function approximation converges to a

References 493\r
region. In Advances in Neural Information Processing Systems 13, pp. 1040–1046. MIT\r
Press, Cambridge, MA.\r
Graybiel, A. M. (2000). The basal ganglia. Current Biology, 10(14):R509–R511.\r
Greensmith, E., Bartlett, P. L., Baxter, J. (2002). Variance reduction techniques for gradient\r
estimates in reinforcement learning. In Advances in Neural Information Processing Systems\r
14, pp. 1507–1514. MIT Press, Cambridge, MA.\r
Greensmith, E., Bartlett, P. L., Baxter, J. (2004). Variance reduction techniques for gradient\r
estimates in reinforcement learning. Journal of Machine Learning Research, 5 (Nov):1471–\r
1530.\r
Grith, A. K. (1966). A new machine learning technique applied to the game of checkers.\r
Technical Report Project MAC, Artificial Intelligence Memo 94. Massachusetts Institute of\r
Technology, Cambridge, MA.\r
Grith, A. K. (1974). A comparison and evaluation of three machine learning procedures as\r
applied to the game of checkers. Artificial Intelligence, 5(2):137–148.\r
Grondman, I., Busoniu, L., Lopes, G. A., Babuska, R. (2012). A survey of actor–critic reinforce\u0002ment learning: Standard and natural policy gradients. IEEE Transactions on Systems, Man,\r
and Cybernetics, Part C (Applications and Reviews), 42 (6):1291–1307.\r
Grossberg, S. (1975). A neural model of attention, reinforcement, and discrimination learning.\r
International Review of Neurobiology, 18:263–327.\r
Grossberg, S., Schmajuk, N. A. (1989). Neural dynamics of adaptive timing and temporal\r
discrimination during associative learning. Neural Networks, 2(2):79–102.\r
Gullapalli, V. (1990). A stochastic reinforcement algorithm for learning real-valued functions.\r
Neural Networks, 3(6): 671–692.\r
Gullapalli, V., Barto, A. G. (1992). Shaping as a method for accelerating reinforcement learning.\r
In Proceedings of the 1992 IEEE International Symposium on Intelligent Control, pp. 554–559.\r
IEEE.\r
Gurvits, L., Lin, L.-J., Hanson, S. J. (1994). Incremental learning of evaluation functions\r
for absorbing Markov chains: New methods and theorems. Siemans Corporate Research,\r
Princeton, NJ.\r
Hackman, L. (2012). Faster Gradient-TD Algorithms. M.Sc. thesis, University of Alberta,\r
Edmonton.\r
Hallak, A., Tamar, A., Mannor, S. (2015). Emphatic TD Bellman operator is a contraction.\r
ArXiv:1508.03411.\r
Hallak, A., Tamar, A., Munos, R., Mannor, S. (2016). Generalized emphatic temporal di↵erence\r
learning: Bias-variance analysis. In Proceedings of the Thirtieth AAAI Conference on\r
Artificial Intelligence, pp. 1631–1637. AAAI Press, Menlo Park, CA.\r
Hammer, M. (1997). The neural basis of associative reward learning in honeybees. Trends in\r
Neuroscience, 20(6):245–252.\r
Hammer, M., Menzel, R. (1995). Learning and memory in the honeybee. The Journal of\r
Neuroscience, 15(3):1617–1630.\r
Hampson, S. E. (1983). A Neural Model of Adaptive Behavior. PhD thesis, University of\r
California, Irvine.\r
Hampson, S. E. (1989). Connectionist Problem Solving: Computational Aspects of Biological\r
Learning. Birkhauser, Boston.\r
Hare, T. A., O’Doherty, J., Camerer, C. F., Schultz, W., Rangel, A. (2008). Dissociating the role\r
of the orbitofrontal cortex and the striatum in the computation of goal values and prediction\r
errors. The Journal of Neuroscience, 28(22):5623–5630.

494 References\r
Harth, E., Tzanakou, E. (1974). Alopex: A stochastic method for determining visual receptive\r
fields. Vision Research, 14 (12):1475–1482.\r
Hassabis, D., Maguire, E. A. (2007). Deconstructing episodic memory with construction. Trends\r
in Cognitive Sciences, 11(7):299–306.\r
Hauskrecht, M., Meuleau, N., Kaelbling, L. P., Dean, T., Boutilier, C. (1998). Hierarchical\r
solution of Markov decision processes using macro-actions. In Proceedings of the Fourteenth\r
Conference on Uncertainty in Artificial Intelligence, pp. 220–229. Morgan Kaufmann.\r
Hawkins, R. D., Kandel, E. R. (1984). Is there a cell-biological alphabet for simple forms of\r
learning? Psychological Review, 91(3):375–391.\r
Haykin, S. (1994). Neural networks: A Comprehensive Foundation, Macmillan, New York.\r
He, K., Huertas, M., Hong, S. Z., Tie, X., Hell, J. W., Shouval, H., Kirkwood, A. (2015). Distinct\r
eligibility traces for LTP and LTD in cortical synapses. Neuron, 88(3):528–538.\r
He, K., Zhang, X., Ren, S., Sun, J. (2016). Deep residual learning for image recognition. In\r
Proceedings of the 1992 IEEE Conference on Computer Vision and Pattern Recognition,\r
pp. 770–778.\r
Hebb, D. O. (1949). The Organization of Behavior: A Neuropsychological Theory. John Wiley\r
and Sons Inc., New York. Reissued by Lawrence Erlbaum Associates Inc., Mahwah NJ, 2002.\r
Hengst, B. (2012). Hierarchical approaches. In M. Wiering and M. van Otterlo (Eds.), Rein\u0002forcement Learning: State-of-the-Art, pp. 293–323. Springer-Verlag Berlin Heidelberg.\r
Herrnstein, R. J. (1970). On the Law of E↵ect. Journal of the Experimental Analysis of Behavior,\r
13 (2):243–266.\r
Hersh, R., Griego, R. J. (1969). Brownian motion and potential theory. Scientific American,\r
220(3):66–74.\r
Hester, T., Stone, P. (2012). Learning and using models. In M. Wiering and M. van Otterlo (Eds.),\r
Reinforcement Learning: State-of-the-Art, pp. 111–141. Springer-Verlag Berlin Heidelberg.\r
Hesterberg, T. C. (1988), Advances in Importance Sampling, PhD thesis, Statistics Department,\r
Stanford University.\r
Hilgard, E. R. (1956). Theories of Learning, Second Edition. Appleton-Century-Cofts, Inc.,\r
New York.\r
Hilgard, E. R., Bower, G. H. (1975). Theories of Learning. Prentice-Hall, Englewood Cli↵s, NJ.\r
Hinton, G. E. (1984). Distributed representations. Technical Report CMU-CS-84-157. Depart\u0002ment of Computer Science, Carnegie-Mellon University, Pittsburgh, PA.\r
Hinton, G. E., Osindero, S., Teh, Y. (2006). A fast learning algorithm for deep belief nets.\r
Neural Computation, 18(7):1527–1554.\r
Hochreiter, S., Schmidhuber, J. (1997). LTSM can solve hard time lag problems. In Advances\r
in Neural Information Processing Systems 9, pp. 473–479. MIT Press, Cambridge, MA.\r
Holland, J. H. (1975). Adaptation in Natural and Artificial Systems. University of Michigan\r
Press, Ann Arbor.\r
Holland, J. H. (1976). Adaptation. In R. Rosen and F. M. Snell (Eds.), Progress in Theoretical\r
Biology, vol. 4, pp. 263–293. Academic Press, New York.\r
Holland, J. H. (1986). Escaping brittleness: The possibility of general-purpose learning algorithms\r
applied to rule-based systems. In R. S. Michalski, J. G. Carbonell, and T. M. Mitchell\r
(Eds.), Machine Learning: An Artificial Intelligence Approach, vol. 2, pp. 593–623. Morgan\r
Kaufmann.\r
Hollerman, J. R., Schultz, W. (1998). Dopmine neurons report an error in the temporal\r
prediction of reward during learning. Nature Neuroscience, 1(4):304–309.

References 495\r
Houk, J. C., Adams, J. L., Barto, A. G. (1995). A model of how the basal ganglia generates and\r
uses neural signals that predict reinforcement. In J. C. Houk, J. L. Davis, and D. G. Beiser\r
(Eds.), Models of Information Processing in the Basal Ganglia, pp. 249–270. MIT Press,\r
Cambridge, MA.\r
Howard, R. (1960). Dynamic Programming and Markov Processes. MIT Press, Cambridge, MA.\r
Hull, C. L. (1932). The goal-gradient hypothesis and maze learning. Psychological Review,\r
39(1):25–43.\r
Hull, C. L. (1943). Principles of Behavior. Appleton-Century, New York.\r
Hull, C. L. (1952). A Behavior System. Wiley, New York.\r
Io↵e, S., Szegedy, C. (2015). Batch normalization: Accelerating deep network training by\r
reducing internal covariate shift. ArXiv:1502.03167.\r
˙\r
Ipek, E., Mutlu, O., Mart´ınez, J. F., Caruana, R. (2008). Self-optimizing memory controllers: A\r
reinforcement learning approach. In ISCA’08:Proceedings of the 35th Annual International\r
Symposium on Computer Architecture, pp. 39–50. IEEE Computer Society Washington, DC.\r
Izhikevich, E. M. (2007). Solving the distal reward problem through linkage of STDP and\r
dopamine signaling. Cerebral Cortex, 17(10):2443–2452.\r
Jaakkola, T., Jordan, M. I., Singh, S. P. (1994). On the convergence of stochastic iterative\r
dynamic programming algorithms. Neural Computation, 6:1185–1201.\r
Jaakkola, T., Singh, S. P., Jordan, M. I. (1995). Reinforcement learning algorithm for partially\r
observable Markov decision problems. In Advances in Neural Information Processing Systems\r
7, pp. 345–352. MIT Press, Cambridge, MA.\r
Jacobs, R. A. (1988). Increased rates of convergence through learning rate adaptation. Neural\r
Networks, 1 (4):295–307.\r
Jaderberg, M., Mnih, V., Czarnecki, W. M., Schaul, T., Leibo, J. Z., Silver, D., Kavukcuoglu, K.\r
(2016). Reinforcement learning with unsupervised auxiliary tasks. ArXiv:1611.05397.\r
Jaeger, H. (1997). Observable operator models and conditioned continuation representations. Ar\u0002beitspapiere der GMD 1043, GMD Forschungszentrum Informationstechnik, Sankt Augustin,\r
Germany.\r
Jaeger, H. (1998). Discrete Time, Discrete Valued Observable Operator Models: A Tutorial.\r
GMD-Forschungszentrum Informationstechnik.\r
Jaeger, H. (2000). Observable operator models for discrete stochastic time series. Neural\r
Computation, 12 (6):1371–1398.\r
Jaeger, H. (2002). Tutorial on training recurrent neural networks, covering BPPT, RTRL, EKF\r
and the ‘echo state network’ approach. German National Research Center for Information\r
Technology, Technical Report GMD report 159, 2002.\r
Joel, D., Niv, Y., Ruppin, E. (2002). Actor–critic models of the basal ganglia: New anatomical\r
and computational perspectives. Neural Networks, 15(4):535–547.\r
Johnson, A., Redish, A. D. (2007). Neural ensembles in CA3 transiently encode paths forward\r
of the animal at a decision point. The Journal of Neuroscience, 27(45):12176–12189.\r
Kaelbling, L. P. (1993a). Hierarchical learning in stochastic domains: Preliminary results. In\r
Proceedings of the 10th International Conference on Machine Learning, pp. 167–173. Morgan\r
Kaufmann.\r
Kaelbling, L. P. (1993b). Learning in Embedded Systems. MIT Press, Cambridge, MA.\r
Kaelbling, L. P. (Ed.) (1996). Special triple issue on reinforcement learning, Machine Learning,\r
22(1/2/3).\r
Kaelbling, L. P., Littman, M. L., Moore, A. W. (1996). Reinforcement learning: A survey.\r
Journal of Artificial Intelligence Research, 4:237–285.

496 References\r
Kakade, S. M. (2002). A natural policy gradient. In Advances in Neural Information Processing\r
Systems 14, pp. 1531–1538. MIT Press, Cambridge, MA.\r
Kakade, S. M. (2003). On the Sample Complexity of Reinforcement Learning. PhD thesis,\r
University of London.\r
Kakutani, S. (1945). Markov processes and the Dirichlet problem. Proceedings of the Japan\r
Academy, 21(3-10):227–233.\r
Kalos, M. H., Whitlock, P. A. (1986). Monte Carlo Methods. Wiley, New York.\r
Kamin, L. J. (1968). “Attention-like” processes in classical conditioning. In M. R. Jones (Ed.),\r
Miami Symposium on the Prediction of Behavior, 1967: Aversive Stimulation, pp. 9–31.\r
University of Miami Press, Coral Gables, Florida.\r
Kamin, L. J. (1969). Predictability, surprise, attention, and conditioning. In B. A. Campbell and\r
R. M. Church (Eds.), Punishment and Aversive Behavior, pp. 279–296. Appleton-Century\u0002Crofts, New York.\r
Kandel, E. R., Schwartz, J. H., Jessell, T. M., Siegelbaum, S. A., Hudspeth, A. J. (Eds.) (2013).\r
Principles of Neural Science, Fifth Edition. McGraw-Hill Companies, Inc.\r
Karampatziakis, N., Langford, J. (2010). Online importance weight aware updates. ArXiv:1011.1576.\r
Kashyap, R. L., Blaydon, C. C., Fu, K. S. (1970). Stochastic approximation. In J. M. Mendel\r
and K. S. Fu (Eds.), Adaptive, Learning, and Pattern Recognition Systems: Theory and\r
Applications, pp. 329–355. Academic Press, New York.\r
Kearney, A., Veeriah, V, Travnik, J, Sutton, R. S., Pilarski, P. M. (in preparation). TIDBD:\r
Adapting Temporal-di↵erence Step-sizes Through Stochastic Meta-descent.\r
Kearns, M., Singh, S. (2002). Near-optimal reinforcement learning in polynomial time. Machine\r
Learning, 49 (2-3):209–232.\r
Keerthi, S. S., Ravindran, B. (1997). Reinforcement learning. In E. Fieslerm and R. Beale\r
(Eds.), Handbook of Neural Computation, C3. Oxford University Press, New York.\r
Kehoe, E. J. (1982). Conditioning with serial compound stimuli: Theoretical and empirical\r
issues. Experimental Animal Behavior, 1:30–65.\r
Kehoe, E. J., Schreurs, B. G., Graham, P. (1987). Temporal primacy overrides prior training\r
in serial compound conditioning of the rabbit’s nictitating membrane response. Animal\r
Learning & Behavior, 15(4):455–464.\r
Keiflin, R., Janak, P. H. (2015). Dopamine prediction errors in reward learning and addiction:\r
From theory to neural circuitry. Neuron, 88(2):247– 263.\r
Kimble, G. A. (1961). Hilgard and Marquis’ Conditioning and Learning. Appleton-Century\u0002Crofts, New York.\r
Kimble, G. A. (1967). Foundations of Conditioning and Learning. Appleton-Century-Crofts,\r
New York.\r
Kingma, D., Ba, J. (2014). Adam: A method for stochastic optimization. ArXiv:1412.6980.\r
Klopf, A. H. (1972). Brain function and adaptive systems—A heterostatic theory. Technical\r
Report AFCRL-72-0164, Air Force Cambridge Research Laboratories, Bedford, MA. A\r
summary appears in Proceedings of the International Conference on Systems, Man, and\r
Cybernetics (1974). IEEE Systems, Man, and Cybernetics Society, Dallas, TX.\r
Klopf, A. H. (1975). A comparison of natural and artificial intelligence. SIGART Newsletter,\r
53:11–13.\r
Klopf, A. H. (1982). The Hedonistic Neuron: A Theory of Memory, Learning, and Intelligence.\r
Hemisphere, Washington, DC.\r
Klopf, A. H. (1988). A neuronal model of classical conditioning. Psychobiology, 16(2):85–125.

References 497\r
Klyubin, A. S., Polani, D., Nehaniv, C. L. (2005). Empowerment: A universal agent-centric\r
measure of control. In Proceedings of the 2005 IEEE Congress on Evolutionary Computation\r
(Vol. 1, pp. 128–135). IEEE.\r
Kober, J., Peters, J. (2012). Reinforcement learning in robotics: A survey. In M. Wiering, M.\r
van Otterlo (Eds.), Reinforcement Learning: State-of-the-Art, pp. 579–610. Springer-Verlag.\r
Kocsis, L., Szepesv´ari, Cs. (2006). Bandit based Monte-Carlo planning. In Proceedings of the\r
European Conference on Machine Learning, pp. 282–293. Springer-Verlag Berlin Heidelberg.\r
Kohonen, T. (1977). Associative Memory: A System Theoretic Approach. Springer-Verlag,\r
Berlin.\r
Koller, D., Friedman, N. (2009). Probabilistic Graphical Models: Principles and Techniques.\r
MIT Press.\r
Kolodziejski, C., Porr, B., W¨org¨otter, F. (2009). On the asymptotic equivalence between\r
di↵erential Hebbian and temporal di↵erence learning. Neural Computation, 21(4):1173–1202.\r
Kolter, J. Z. (2011). The fixed points of o↵-policy TD. In Advances in Neural Information\r
Processing Systems 24, pp. 2169–2177. Curran Associates, Inc.\r
Konda, V. R., Tsitsiklis, J. N. (2000). Actor-critic algorithms. In Advances in Neural Information\r
Processing Systems 12, pp. 1008–1014. MIT Press, Cambridge, MA.\r
Konda, V. R., Tsitsiklis, J. N. (2003). On actor-critic algorithms. SIAM Journal on Control\r
and Optimization, 42 (4):1143–1166.\r
Konidaris, G. D., Osentoski, S., Thomas, P. S. (2011). Value function approximation in\r
reinforcement learning using the Fourier basis . In Proceedings of the Twenty-Fifth Conference\r
of the Association for the Advancement of Artificial Intelligence, pp. 380–385.\r
Korf, R. E. (1988). Optimal path finding algorithms. In L. N. Kanal and V. Kumar (Eds.),\r
Search in Artificial Intelligence, pp. 223–267. Springer-Verlag, Berlin.\r
Korf, R. E. (1990). Real-time heuristic search. Artificial Intelligence, 42 (2–3), 189–211.\r
Koshland, D. E. (1980). Bacterial Chemotaxis as a Model Behavioral System. Raven Press, New\r
York.\r
Koza, J. R. (1992). Genetic Programming: On the Programming of Computers by Means of\r
Natural Selection (Vol. 1). MIT Press., Cambridge, MA.\r
Kraft, L. G., Campagna, D. P. (1990). A summary comparison of CMAC neural network and\r
traditional adaptive control systems. In T. Miller, R. S. Sutton, and P. J. Werbos (Eds.),\r
Neural Networks for Control, pp. 143–169. MIT Press, Cambridge, MA.\r
Kraft, L. G., Miller, W. T., Dietz, D. (1992). Development and application of CMAC neural\r
network-based control. In D. A. White and D. A. Sofge (Eds.), Handbook of Intelligent\r
Control: Neural, Fuzzy, and Adaptive Approaches, pp. 215–232. Van Nostrand Reinhold,\r
New York.\r
Kumar, P. R., Varaiya, P. (1986). Stochastic Systems: Estimation, Identification, and Adaptive\r
Control. Prentice-Hall, Englewood Cli↵s, NJ.\r
Kumar, P. R. (1985). A survey of some results in stochastic adaptive control. SIAM Journal of\r
Control and Optimization, 23(3):329–380.\r
Kumar, V., Kanal, L. N. (1988). The CDP, A unifying formulation for heuristic search, dynamic\r
programming, and branch-and-bound. In L. N. Kanal and V. Kumar (Eds.), Search in\r
Artificial Intelligence, pp. 1–37. Springer-Verlag, Berlin.\r
Kushner, H. J., Dupuis, P. (1992). Numerical Methods for Stochastic Control Problems in\r
Continuous Time. Springer-Verlag, New York.

498 References\r
Kuvayev, L., Sutton, R.S. (1996). Model-based reinforcement learning with an approximate,\r
learned model. Proceedings of the Ninth Yale Workshop on Adaptive and Learning Systems,\r
pp. 101–105, Yale University, New Haven, CT.\r
Lagoudakis, M., Parr, R. (2003). Least squares policy iteration. Journal of Machine Learning\r
Research, 4 (Dec):1107–1149.\r
Lai, T. L., Robbins, H. (1985). Asymptotically ecient adaptive allocation rules. Advances in\r
Applied Mathematics, 6 (1):4–22.\r
Lakshmivarahan, S., Narendra, K. S. (1982). Learning algorithms for two-person zero-sum\r
stochastic games with incomplete information: A unified approach. SIAM Journal of Control\r
and Optimization, 20(4):541–552.\r
Lammel, S., Lim, B. K., Malenka, R. C. (2014). Reward and aversion in a heterogeneous\r
midbrain dopamine system. Neuropharmacology, 76:353–359.\r
Lane, S. H., Handelman, D. A., Gelfand, J. J. (1992). Theory and development of higher-order\r
CMAC neural networks. IEEE Control Systems, 12 (2):23–30.\r
LeCun, Y. (1985). Une proc´edure d’apprentissage pour r´eseau a seuil asymmetrique (a learning\r
scheme for asymmetric threshold networks). In Proceedings of Cognitiva 85, Paris, France.\r
LeCun, Y., Bottou, L., Bengio, Y., Ha↵ner, P. (1998). Gradient-based learning applied to\r
document recognition. Proceedings of the IEEE, 86(11):2278–2324.\r
Legenstein, R. W., Maass, D. P. (2008). A learning theory for reward-modulated spike-timing\u0002dependent plasticity with application to biofeedback. PLoS Computational Biology, 4(10).\r
Levy, W. B., Steward, D. (1983). Temporal contiguity requirements for long-term associative\r
potentiation/depression in the hippocampus. Neuroscience, 8(4):791–797.\r
Lewis, F. L., Liu, D. (Eds.) (2012). Reinforcement Learning and Approximate Dynamic\r
Programming for Feedback Control. John Wiley and Sons.\r
Lewis, R. L., Howes, A., Singh, S. (2014). Computational rationality: Linking mechanism and\r
behavior through utility maximization. Topics in Cognitive Science, 6(2):279–311.\r
Li, L. (2012). Sample complexity bounds of exploration. In M. Wiering and M. van Otterlo (Eds.),\r
Reinforcement Learning: State-of-the-Art, pp. 175–204. Springer-Verlag Berlin Heidelberg.\r
Li, L., Chu, W., Langford, J., Schapire, R. E. (2010). A contextual-bandit approach to personal\u0002ized news article recommendation. In Proceedings of the 19th International Conference on\r
World Wide Web, pp. 661–670. ACM, New York.\r
Lin, C.-S., Kim, H. (1991). CMAC-based adaptive critic self-learning control. IEEE Transactions\r
on Neural Networks, 2(5):530–533.\r
Lin, L.-J. (1992). Self-improving reactive agents based on reinforcement learning, planning and\r
teaching. Machine Learning, 8(3-4):293–321.\r
Lin, L.-J., Mitchell, T. (1992). Reinforcement learning with hidden states. In Proceedings of\r
the Second International Conference on Simulation of Adaptive Behavior: From Animals to\r
Animats, pp. 271–280. MIT Press, Cambridge, MA.\r
Littman, M. L., Cassandra, A. R., Kaelbling, L. P. (1995). Learning policies for partially\r
observable environments: Scaling up. In Proceedings of the 12th International Conference on\r
Machine Learning, pp. 362–370. Morgan Kaufmann.\r
Littman, M. L., Dean, T. L., Kaelbling, L. P. (1995). On the complexity of solving Markov\r
decision problems. In Proceedings of the Eleventh Annual Conference on Uncertainty in\r
Artificial Intelligence, pp. 394–402.\r
Littman, M. L., Sutton, R. S., Singh, S. (2002). Predictive representations of state. In Advances\r
in Neural Information Processing Systems 14, pp. 1555–1561. MIT Press, Cambridge, MA.\r
Liu, J. S. (2001). Monte Carlo Strategies in Scientific Computing. Springer-Verlag, Berlin.

References 499\r
Ljung, L. (1998). System identification. In A. Proch´azka, J. Uhl´ıˆr, P. W. J. Rayner, and N. G.\r
Kingsbury (Eds.), Signal Analysis and Prediction, pp. 163–173. Springer Science + Business\r
Media New York, LLC.\r
Ljung, L., S¨oderstrom, T. (1983). Theory and Practice of Recursive Identification. MIT Press,\r
Cambridge, MA.\r
Ljungberg, T., Apicella, P., Schultz, W. (1992). Responses of monkey dopamine neurons during\r
learning of behavioral reactions. Journal of Neurophysiology, 67(1):145–163.\r
Lovejoy, W. S. (1991). A survey of algorithmic methods for partially observed Markov decision\r
processes. Annals of Operations Research, 28(1):47–66.\r
Luce, D. (1959). Individual Choice Behavior. Wiley, New York.\r
Ludvig, E. A., Bellemare, M. G., Pearson, K. G. (2011). A primer on reinforcement learning\r
in the brain: Psychological, computational, and neural perspectives. In E. Alonso and E.\r
Mondrag´on (Eds.), Computational Neuroscience for Advancing Artificial Intelligence: Models,\r
Methods and Applications, pp. 111–44. Medical Information Science Reference, Hershey PA.\r
Ludvig, E. A., Sutton, R. S., Kehoe, E. J. (2008). Stimulus representation and the timing of\r
reward-prediction errors in models of the dopamine system. Neural Computation, 20(12):3034–\r
3054.\r
Ludvig, E. A., Sutton, R. S., Kehoe, E. J. (2012). Evaluating the TD model of classical\r
conditioning. Learning & behavior, 40(3):305–319.\r
Machado, A. (1997). Learning the temporal dynamics of behavior. Psychological Review,\r
104(2):241–265.\r
Mackintosh, N. J. (1975). A theory of attention: Variations in the associability of stimuli with\r
reinforcement. Psychological Review, 82(4):276–298.\r
Mackintosh, N. J. (1983). Conditioning and Associative Learning. Clarendon Press, Oxford.\r
Maclin, R., Shavlik, J. W. (1994). Incorporating advice into agents that learn from reinforcements.\r
In Proceedings of the Twelfth National Conference on Artificial Intelligence, pp. 694–699.\r
AAAI Press, Menlo Park, CA.\r
Maei, H. R. (2011). Gradient Temporal-Di↵erence Learning Algorithms. PhD thesis, University\r
of Alberta, Edmonton.\r
Maei, H. R. (2018). Convergent actor-critic algorithms under o↵-policy training and function\r
approximation. ArXiv:1802.07842.\r
Maei, H. R., Sutton, R. S. (2010). GQ(): A general gradient algorithm for temporal-di↵erence\r
prediction learning with eligibility traces. In Proceedings of the Third Conference on Artificial\r
General Intelligence, pp. 91–96.\r
Maei, H. R., Szepesv´ari, Cs., Bhatnagar, S., Precup, D., Silver, D., Sutton, R. S. (2009).\r
Convergent temporal-di↵erence learning with arbitrary smooth function approximation. In\r
Advances in Neural Information Processing Systems 22, pp. 1204–1212. Curran Associates,\r
Inc.\r
Maei, H. R., Szepesv´ari, Cs., Bhatnagar, S., Sutton, R. S. (2010). Toward o↵-policy learning\r
control with function approximation. In Proceedings of the 27th International Conference on\r
Machine Learning, pp. 719–726).\r
Mahadevan, S. (1996). Average reward reinforcement learning: Foundations, algorithms, and\r
empirical results. Machine Learning, 22(1):159–196.\r
Mahadevan, S., Liu, B., Thomas, P., Dabney, W., Giguere, S., Jacek, N., Gemp, I., Liu, J. (2014).\r
Proximal reinforcement learning: A new theory of sequential decision making in primal-dual\r
spaces. ArXiv:1405.6757.\r
Mahadevan, S., Connell, J. (1992). Automatic programming of behavior-based robots using

500 References\r
reinforcement learning. Artificial Intelligence, 55(2-3):311–365.\r
Mahmood, A. R. (2017). Incremental O↵-Policy Reinforcement Learning Algorithms. PhD\r
thesis, University of Alberta, Edmonton.\r
Mahmood, A. R., Sutton, R. S. (2015). O↵-policy learning based on weighted importance sam\u0002pling with linear computational complexity. In Proceedings of the 31st Conference on Uncer\u0002tainty in Artificial Intelligence, pp. 552–561. AUAI Press Corvallis,\r
Oregon.\r
Mahmood, A. R., Sutton, R. S., Degris, T., Pilarski, P. M. (2012). Tuning-free step-size adapta\u0002tion. In 2012 IEEE International Conference on Acoustics, Speech and Signal Processing,\r
Proceedings, pp. 2121–2124. IEEE.\r
Mahmood, A. R., Yu, H, Sutton, R. S. (2017). Multi-step o↵-policy learning without importance\r
sampling ratios. ArXiv:1702.03006.\r
Mahmood, A. R., van Hasselt, H., Sutton, R. S. (2014). Weighted importance sampling for\r
o↵-policy learning with linear function approximation. Advances in Neural Information\r
Processing Systems 27, pp. 3014–3022. Curran Associates, Inc.\r
Marbach, P., Tsitsiklis, J. N. (1998). Simulation-based optimization of Markov reward processes.\r
MIT Technical Report LIDS-P-2411.\r
Marbach, P., Tsitsiklis, J. N. (2001). Simulation-based optimization of Markov reward processes.\r
IEEE Transactions on Automatic Control, 46 (2):191–209.\r
Markram, H., L¨ubke, J., Frotscher, M., Sakmann, B. (1997). Regulation of synaptic ecacy by\r
coincidence of postsynaptic APs and EPSPs. Science, 275(5297):213–215.\r
Mart´ınez, J. F., ˙Ipek, E. (2009). Dynamic multicore resource management: A machine learning\r
approach. Micro, IEEE, 29(5):8–17.\r
Mataric, M. J. (1994). Reward functions for accelerated learning. In Proceedings of the 11th\r
International Conference on Machine Learning, pp. 181–189. Morgan Kaufmann.\r
Matsuda, W., Furuta, T., Nakamura, K. C., Hioki, H., Fujiyama, F., Arai, R., Kaneko, T.\r
(2009). Single nigrostriatal dopaminergic neurons form widely spread and highly dense\r
axonal arborizations in the neostriatum. The Journal of Neuroscience, 29(2):444–453.\r
Mazur, J. E. (1994). Learning and Behavior, 3rd ed. Prentice-Hall, Englewood Cli↵s, NJ.\r
McCallum, A. K. (1993). Overcoming incomplete perception with utile distinction memory. In\r
Proceedings of the 10th International Conference on Machine Learning, pp. 190–196. Morgan\r
Kaufmann.\r
McCallum, A. K. (1995). Reinforcement Learning with Selective Perception and Hidden State.\r
PhD thesis, University of Rochester, Rochester NY.\r
McCloskey, M., Cohen, N. J. (1989). Catastrophic interference in connectionist networks: The\r
sequential learning problem. Psychology of Learning and Motivation, 24 :109–165.\r
McClure, S. M., Daw, N. D., Montague, P. R. (2003). A computational substrate for incentive\r
salience. Trends in Neurosciences, 26 (8):423–428.\r
McCulloch, W. S., Pitts, W. (1943). A logical calculus of the ideas immanent in nervous activity.\r
Bulletin of Mathematical Biophysics, 5 (4):115–133.\r
McMahan, H. B., Gordon, G. J. (2005). Fast Exact Planning in Markov Decision Processes.\r
In Proceedings of the International Conference on Automated Planning and Scheduling,\r
pp. 151-160.\r
Melo, F. S., Meyn, S. P., Ribeiro, M. I. (2008). An analysis of reinforcement learning with\r
function approximation. In Proceedings of the 25th International Conference on Machine\r
Learning, pp. 664–671.\r
Mendel, J. M. (1966). A survey of learning control systems. ISA Transactions, 5:297–303.

References 501\r
Mendel, J. M., McLaren, R. W. (1970). Reinforcement learning control and pattern recognition\r
systems. In J. M. Mendel and K. S. Fu (Eds.), Adaptive, Learning and Pattern Recognition\r
Systems: Theory and Applications, pp. 287–318. Academic Press, New York.\r
Michie, D. (1961). Trial and error. In S. A. Barnett and A. McLaren (Eds.), Science Survey,\r
Part 2, pp. 129–145. Penguin, Harmondsworth.\r
Michie, D. (1963). Experiments on the mechanisation of game learning. 1. characterization of\r
the model and its parameters. The Computer Journal, 6(3):232–263.\r
Michie, D. (1974). On Machine Intelligence. Edinburgh University Press, Edinburgh.\r
Michie, D., Chambers, R. A. (1968). BOXES, An experiment in adaptive control. In E. Dale\r
and D. Michie (Eds.), Machine Intelligence 2, pp. 137–152. Oliver and Boyd, Edinburgh.\r
Miller, R. (1981). Meaning and Purpose in the Intact Brain: A Philosophical, Psychological,\r
and Biological Account of Conscious Process. Clarendon Press, Oxford.\r
Miller, W. T., An, E., Glanz, F., Carter, M. (1990). The design of CMAC neural networks for\r
control. Adaptive and Learning Systems, 1 :140–145.\r
Miller, W. T., Glanz, F. H. (1996). UNH CMAC verison 2.1: The University of New Hampshire\r
Implementation of the Cerebellar Model Arithmetic Computer - CMAC. Robotics Laboratory\r
Technical Report, University of New Hampshire, Durham.\r
Miller, S., Williams, R. J. (1992). Learning to control a bioreactor using a neural net Dyna-Q\r
system. In Proceedings of the Seventh Yale Workshop on Adaptive and Learning Systems,\r
pp. 167–172. Center for Systems Science, Dunham Laboratory, Yale University, New Haven.\r
Miller, W. T., Scalera, S. M., Kim, A. (1994). Neural network control of dynamic balance for a\r
biped walking robot. In Proceedings of the Eighth Yale Workshop on Adaptive and Learning\r
Systems, pp. 156–161. Center for Systems Science, Dunham Laboratory, Yale University,\r
New Haven.\r
Minton, S. (1990). Quantitative results concerning the utility of explanation-based learning.\r
Artificial Intelligence, 42 (2-3):363–391.\r
Minsky, M. L. (1954). Theory of Neural-Analog Reinforcement Systems and Its Application to\r
the Brain-Model Problem. PhD thesis, Princeton University.\r
Minsky, M. L. (1961). Steps toward artificial intelligence. Proceedings of the Institute of Radio\r
Engineers, 49:8–30. Reprinted in E. A. Feigenbaum and J. Feldman (Eds.), Computers and\r
Thought, pp. 406–450. McGraw-Hill, New York, 1963.\r
Minsky, M. L. (1967). Computation: Finite and Infinite Machines. Prentice-Hall, Englewood\r
Cli↵s, NJ.\r
Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., Riedmiller, M.\r
(2013). Playing atari with deep reinforcement learning. ArXiv:1312.5602.\r
Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves,\r
A., Riedmiller, M., Fidjeland, A. K., Ostrovski, G., Petersen, S., Beattie, C., Sadik, A.,\r
Antonoglou, I., King, H., Kumaran, D., Wierstra, D., Legg, S., Hassabis, D. (2015). Human\u0002level control through deep reinforcement learning. Nature, 518(7540):529–533.\r
Modayil, J., Sutton, R. S. (2014). Prediction driven behavior: Learning predictions that drive\r
fixed responses. In AAAI-14 Workshop on Artificial Intelligence and Robotics, Quebec City,\r
Canada.\r
Modayil, J., White, A., Sutton, R. S. (2014). Multi-timescale nexting in a reinforcement learning\r
robot. Adaptive Behavior, 22(2):146–160.\r
Monahan, G. E. (1982). State of the art—a survey of partially observable Markov decision\r
processes: theory, models, and algorithms. Management Science, 28 (1):1–16.

502 References\r
Montague, P. R., Dayan, P., Nowlan, S. J., Pouget, A., Sejnowski, T. J. (1993). Using aperiodic\r
reinforcement for directed self-organization during development. In Advances in Neural\r
Information Processing Systems 5, pp. 969–976. Morgan Kaufmann.\r
Montague, P. R., Dayan, P., Person, C., Sejnowski, T. J. (1995). Bee foraging in uncertain\r
environments using predictive hebbian learning. Nature, 377(6551):725–728.\r
Montague, P. R., Dayan, P., Sejnowski, T. J. (1996). A framework for mesencephalic dopamine\r
systems based on predictive Hebbian learning. The Journal of Neuroscience, 16(5):1936–1947.\r
Montague, P. R., Dolan, R. J., Friston, K. J., Dayan, P. (2012). Computational psychiatry.\r
Trends in Cognitive Sciences, 16 (1):72–80.\r
Montague, P. R., Sejnowski, T. J. (1994). The predictive brain: Temporal coincidence and\r
temporal order in synaptic learningmechanisms. Learning & Memory, 1(1):1–33.\r
Moore, A. W. (1990). Ecient Memory-Based Learning for Robot Control. PhD thesis,\r
University of Cambridge.\r
Moore, A. W., Atkeson, C. G. (1993). Prioritized sweeping: Reinforcement learning with less\r
data and less real time. Machine Learning, 13(1):103–130.\r
Moore, A. W., Schneider, J., Deng, K. (1997). Ecient locally weighted polynomial regression\r
predictions. In Proceedings of the 14th International Conference on Machine Learning.\r
Morgan Kaufmann.\r
Moore, J. W., Blazis, D. E. J. (1989). Simulation of a classically conditioned response: A\r
cerebellar implementation of the sutton-barto-desmond model. In J. H. Byrne and W. O.\r
Berry (Eds.), Neural Models of Plasticity, pp. 187–207. Academic Press, San Diego, CA.\r
Moore, J. W., Choi, J.-S., Brunzell, D. H. (1998). Predictive timing under temporal uncertainty:\r
The time derivative model of the conditioned response. In D. A. Rosenbaum and C. E.\r
Collyer (Eds.), Timing of Behavior, pp. 3–34. MIT Press, Cambridge, MA.\r
Moore, J. W., Desmond, J. E., Berthier, N. E., Blazis, E. J., Sutton, R. S., Barto, A. G. (1986).\r
Simulation of the classically conditioned nictitating membrane response by a neuron-like\r
adaptive element: I. Response topography, neuronal firing, and interstimulus intervals.\r
Behavioural Brain Research, 21(2):143–154.\r
Moore, J. W., Marks, J. S., Castagna, V. E., Polewan, R. J. (2001). Parameter stability in the\r
TD model of complex CR topographies. In Society for Neuroscience Abstracts, 27:642.\r
Moore, J. W., Schmajuk, N. A. (2008). Kamin blocking. Scholarpedia, 3(5):3542.\r
Moore, J. W., Stickney, K. J. (1980). Formation of attentional-associative networks in real\r
time:Role of the hippocampus and implications for conditioning. Physiological Psychology,\r
8(2):207–217.\r
Mukundan, J., Mart´ınez, J. F. (2012). MORSE, Multi-objective reconfigurable self-optimizing\r
memory scheduler. In IEEE 18th International Symposium on High Performance Computer\r
Architecture, pp. 1–12.\r
M¨uller, M. (2002). Computer Go. Artificial Intelligence, 134(1):145–179.\r
Munos, R., Stepleton, T., Harutyunyan, A., Bellemare, M. (2016). Safe and ecient o↵-policy\r
reinforcement learning. In Advances in Neural Information Processing Systems 29, pp. 1046–\r
1054. Curran Associates, Inc.\r
Naddaf, Y. (2010). Game-Independent AI Agents for Playing Atari 2600 Console Games. PhD\r
thesis, University of Alberta, Edmonton.\r
Narendra, K. S., Thathachar, M. A. L. (1974). Learning automata—A survey. IEEE Transactions\r
on Systems, Man, and Cybernetics, 4:323–334.\r
Narendra, K. S., Thathachar, M. A. L. (1989). Learning Automata: An Introduction. Prentice\u0002Hall, Englewood Cli↵s, NJ.

References 503\r
Narendra, K. S., Wheeler, R. M. (1983). An N-player sequential stochastic game with identical\r
payo↵s. IEEE Transactions on Systems, Man, and Cybernetics, 6:1154–1158.\r
Narendra, K. S., Wheeler, R. M. (1986). Decentralized learning in finite Markov chains. IEEE\r
Transactions on Automatic Control, 31(6):519–526.\r
Nedi´c, A., Bertsekas, D. P. (2003). Least squares policy evaluation algorithms with linear\r
function approximation. Discrete Event Dynamic Systems, 13 (1-2):79–110.\r
Ng, A. Y. (2003). Shaping and Policy Search in Reinforcement Learning. PhD thesis, University\r
of California, Berkeley.\r
Ng, A. Y., Harada, D., Russell, S. (1999). Policy invariance under reward transformations:\r
Theory and application to reward shaping. In I. Bratko and S. Dzeroski (Eds.), Proceedings\r
of the 16th International Conference on Machine Learning, pp. 278–287.\r
Ng, A. Y., Russell, S. J. (2000). Algorithms for inverse reinforcement learning. In Proceedings of\r
the 17th International Conference on Machine Learning, pp. 663–670.\r
Niv, Y. (2009). Reinforcement learning in the brain. Journal of Mathematical Psychology,\r
53(3):139–154.\r
Niv, Y., Daw, N. D., Dayan, P. (2006). How fast to work: Response vigor, motivation and tonic\r
dopamine. In Advances in Neural Information Processing Systems 18, pp. 1019–1026. MIT\r
Press, Cambridge, MA.\r
Niv, Y., Daw, N. D., Joel, D., Dayan, P. (2007). Tonic dopamine: opportunity costs and the\r
control of response vigor. Psychopharmacology, 191(3):507–520.\r
Niv, Y., Joel, D., Dayan, P. (2006). A normative perspective on motivation. Trends in Cognitive\r
Sciences, 10(8):375–381.\r
Nouri, A., Littman, M. L. (2009). Multi-resolution exploration in continuous spaces. In Advances\r
in Neural Information Processing Systems 21, pp. 1209–1216. Curran Associates, Inc.\r
Now´e, A., Vrancx, P., Hauwere, Y.-M. D. (2012). Game theory and multi-agent reinforcement\r
learning. In M. Wiering and M. van Otterlo (Eds.), Reinforcement Learning: State-of-the-Art,\r
pp. 441–467. Springer-Verlag Berlin Heidelberg.\r
Nutt, D. J., Lingford-Hughes, A., Erritzoe, D., Stokes, P. R. A. (2015). The dopamine theory of\r
addiction: 40 years of highs and lows. Nature Reviews Neuroscience, 16(5):305–312.\r
O’Doherty, J. P., Dayan, P., Friston, K., Critchley, H., Dolan, R. J. (2003). Temporal di↵erence\r
models and reward-related learning in the human brain. Neuron, 38(2):329–337.\r
O’Doherty, J. P., Dayan, P., Schultz, J., Deichmann, R., Friston, K., Dolan, R. J. (2004).\r
Dissociable roles of ventral and dorsal striatum in instrumental conditioning. Science,\r
304(5669):452–454.\r
Olafsd´ottir, H. F., Barry, C., Saleem, A. B., Hassabis, D., Spiers, H. J. (2015). Hippocampal ´\r
place cells construct reward related sequences through unexplored space. Elife, 4:e06063.\r
Oh, J., Guo, X., Lee, H., Lewis, R. L., Singh, S. (2015). Action-conditional video prediction\r
using deep networks in Atari games. In Advances in Neural Information Processing Systems\r
28, pp. 2845–2853. Curran Associates, Inc.\r
Olds, J., Milner, P. (1954). Positive reinforcement produced by electrical stimulation of the septal\r
area and other regions of rat brain. Journal of Comparative and Physiological Psychology,\r
47(6):419–427.\r
O’Reilly, R. C., Frank, M. J. (2006). Making working memory work: A computational model of\r
learning in the prefrontal cortex and basal ganglia. Neural Computation, 18(2):283–328.\r
O’Reilly, R. C., Frank, M. J., Hazy, T. E., Watz, B. (2007). PVLV, the primary value and\r
learned value Pavlovian learning algorithm. Behavioral Neuroscience, 121(1):31–49.

504 References\r
Omohundro, S. M. (1987). Ecient algorithms with neural network behavior. Technical Report,\r
Department of Computer Science, University of Illinois at Urbana-Champaign.\r
Ormoneit, D., Sen, S. (2002). Kernel-based reinforcement learning. ´ Machine Learning, 49 (2-\r
3):161–178.\r
Oudeyer, P.-Y., Kaplan, F. (2007). What is intrinsic motivation? A typology of computational\r
approaches. Frontiers in Neurorobotics, 1:6.\r
Oudeyer, P.-Y., Kaplan, F., Hafner, V. V. (2007). Intrinsic motivation systems for autonomous\r
mental development. IEEE Transactions on Evolutionary Computation, 11(2):265–286.\r
Padoa-Schioppa, C., Assad, J. A. (2006). Neurons in the orbitofrontal cortex encode economic\r
value. Nature, 441 (7090):223–226.\r
Page, C. V. (1977). Heuristics for signature table analysis as a pattern recognition technique.\r
IEEE Transactions on Systems, Man, and Cybernetics, 7(2):77–86.\r
Pagnoni, G., Zink, C. F., Montague, P. R., Berns, G. S. (2002). Activity in human ventral\r
striatum locked to errors of reward prediction. Nature Neuroscience, 5(2):97–98.\r
Pan, W.-X., Schmidt, R., Wickens, J. R., Hyland, B. I. (2005). Dopamine cells respond to\r
predicted events during classical conditioning: Evidence for eligibility traces in the reward\u0002learning network. The Journal of Neuroscience, 25(26):6235–6242.\r
Park, J., Kim, J., Kang, D. (2005). An RLS-based natural actor–critic algorithm for locomotion\r
of a two-linked robot arm. Computational Intelligence and Security:65–72.\r
Parks, P. C., Militzer, J. (1991). Improved allocation of weights for associative memory storage\r
in learning control systems. In IFAC Design Methods of Control Systems, Zurich, Switzerland,\r
pp. 507–512.\r
Parr, R. (1988). Hierarchical Control and Learning for Markov Decision Processes. PhD thesis,\r
University of California, Berkeley.\r
Parr, R., Li, L., Taylor, G., Painter-Wakefield, C., Littman, M. L. (2008). An analysis of linear\r
models, linear value-function approximation, and feature selection for reinforcement learning.\r
In Proceedings of the 25th international conference on Machine learning, pp. 752–759).\r
Parr, R., Russell, S. (1995). Approximating optimal policies for partially observable stochastic\r
domains. In Proceedings of the Fourteenth International Joint Conference on Artificial\r
Intelligence, pp. 1088–1094. Morgan Kaufmann.\r
Pavlov, I. P. (1927). Conditioned Reflexes. Oxford University Press, London.\r
Pawlak, V., Kerr, J. N. D. (2008). Dopamine receptor activation is required for corticostriatal\r
spike-timing-dependent plasticity. The Journal of Neuroscience, 28(10):2435–2446.\r
Pawlak, V., Wickens, J. R., Kirkwood, A., Kerr, J. N. D. (2010). Timing is not every\u0002thing: neuromodulation opens the STDP gate. Frontiers in Synaptic Neuroscience, 2:146.\r
doi:10.3389/fnsyn.2010.00146.\r
Pearce, J. M., Hall, G. (1980). A model for Pavlovian learning: Variation in the e↵ectiveness of\r
conditioning but not unconditioned stimuli. Psychological Review, 87(6):532–552.\r
Pearl, J. (1984). Heuristics: Intelligent Search Strategies for Computer Problem Solving.\r
Addison-Wesley, Reading, MA.\r
Pearl, J. (1995). Causal diagrams for empirical research. Biometrika, 82(4):669-688.\r
Pecevski, D., Maass, W., Legenstein, R. A. (2008). Theoretical analysis of learning with\r
reward-modulated spike-timing-dependent plasticity. In Advances in Neural Information\r
Processing Systems 20, pp. 881–888. Curran Associates, Inc.\r
Peng, J. (1993). Ecient Dynamic Programming-Based Learning for Control. PhD thesis,\r
Northeastern University, Boston MA.

References 505\r
Peng, J. (1995). Ecient memory-based dynamic programming. In Proceedings of the 12th\r
International Conference on Machine Learning, pp. 438–446.\r
Peng, J., Williams, R. J. (1993). Ecient learning and planning within the Dyna framework.\r
Adaptive Behavior, 1(4):437–454.\r
Peng, J., Williams, R. J. (1994). Incremental multi-step Q-learning. In Proceedings of the\r
11th International Conference on Machine Learning, pp. 226–232. Morgan Kaufmann, San\r
Francisco.\r
Peng, J., Williams, R. J. (1996). Incremental multi-step Q-learning. Machine Learning,\r
22(1):283–290.\r
Perkins, T. J., Pendrith, M. D. (2002). On the existence of fixed points for Q-learning and\r
Sarsa in partially observable domains. In Proceedings of the 19th International Conference\r
on Machine Learning, pp. 490–497.\r
Perkins, T. J., Precup, D. (2003). A convergent form of approximate policy iteration. In Advances\r
in Neural Information Processing Systems 15, pp. 1627–1634. MIT Press, Cambridge, MA.\r
Peters, J., B¨uchel, C. (2010). Neural representations of subjective reward value. Behavioral\r
Brain Research, 213(2):135–141.\r
Peters, J., Schaal, S. (2008). Natural actor–critic. Neurocomputing, 71 (7):1180–1190.\r
Peters, J., Vijayakumar, S., Schaal, S. (2005). Natural actor–critic. In European Conference on\r
Machine Learning, pp. 280–291. Springer Berlin Heidelberg.\r
Pezzulo, G., van der Meer, M. A. A., Lansink, C. S., Pennartz, C. M. A. (2014). Internally\r
generated sequences in learning and executing goal-directed behavior. Trends in Cognitive\r
Science, 18(12):647–657.\r
Pfei↵er, B. E., Foster, D. J. (2013). Hippocampal place-cell sequences depict future paths to\r
remembered goals. Nature, 497(7447):74–79.\r
Phansalkar, V. V., Thathachar, M. A. L. (1995). Local and global optimization algorithms for\r
generalized learning automata. Neural Computation, 7(5):950–973.\r
Poggio, T., Girosi, F. (1989). A theory of networks for approximation and learning. A.I. Memo\r
1140. Artificial Intelligence Laboratory, Massachusetts Institute of Technology, Cambridge,\r
MA.\r
Poggio, T., Girosi, F. (1990). Regularization algorithms for learning that are equivalent to\r
multilayer networks. Science, 247(4945):978–982.\r
Polyak, B. T. (1990). New stochastic approximation type procedures. Automat. i Telemekh,\r
7 (98-107):2 (in Russian).\r
Polyak, B. T., Juditsky, A. B. (1992). Acceleration of stochastic approximation by averaging.\r
SIAM Journal on Control and Optimization, 30 (4):838–855.\r
Powell, M. J. D. (1987). Radial basis functions for multivariate interpolation: A review. In\r
J. C. Mason and M. G. Cox (Eds.), Algorithms for Approximation, pp. 143–167. Clarendon\r
Press, Oxford.\r
Powell, W. B. (2011). Approximate Dynamic Programming: Solving the Curses of Dimensionality,\r
Second edition. John Wiley and Sons.\r
Powers, W. T. (1973). Behavior: The Control of Perception. Aldine de Gruyter, Chicago. 2nd\r
expanded edition 2005.\r
Precup, D. (2000). Temporal Abstraction in Reinforcement Learning. PhD thesis, University of\r
Massachusetts, Amherst.\r
Precup, D., Sutton, R. S., Dasgupta, S. (2001). O↵-policy temporal-di↵erence learning with\r
function approximation. In Proceedings of the 18th International Conference on Machine\r
Learning, pp. 417–424.

506 References\r
Precup, D., Sutton, R. S., Paduraru, C., Koop, A., Singh, S. (2006). O↵-policy learning\r
with options and recognizers. In Advances in Neural Information Processing Systems 18,\r
pp. 1097–1104. MIT Press, Cambridge, MA.\r
Precup, D., Sutton, R. S., Singh, S. (2000). Eligibility traces for o↵-policy policy evaluation. In\r
Proceedings of the 17th International Conference on Machine Learning, pp. 759–766. Morgan\r
Kaufmann.\r
Puterman, M. L. (1994). Markov Decision Problems. Wiley, New York.\r
Puterman, M. L., Shin, M. C. (1978). Modified policy iteration algorithms for discounted\r
Markov decision problems. Management Science, 24(11):1127–1137.\r
Quartz, S., Dayan, P., Montague, P. R., Sejnowski, T. J. (1992). Expectation learning in the\r
brain using di↵use ascending connections. In Society for Neuroscience Abstracts, 18:1210.\r
Randløv, J., Alstrøm, P. (1998). Learning to drive a bicycle using reinforcement learning\r
and shaping. In Proceedings of the 15th International Conference on Machine Learning,\r
pp. 463–471.\r
Rangel, A., Camerer, C., Montague, P. R. (2008). A framework for studying the neurobiology of\r
value-based decision making. Nature Reviews Neuroscience, 9(7):545–556.\r
Rangel, A., Hare, T. (2010). Neural computations associated with goal-directed choice. Current\r
Opinion in Neurobiology, 20(2):262–270.\r
Rao, R. P., Sejnowski, T. J. (2001). Spike-timing-dependent Hebbian plasticity as temporal\r
di↵erence learning. Neural Computation, 13 (10):2221–2237.\r
Ratcli↵, R. (1990). Connectionist models of recognition memory: Constraints imposed by\r
learning and forgetting functions. Psychological Review, 97 (2):285–308.\r
Reddy, G., Celani, A., Sejnowski, T. J., Vergassola, M. (2016). Learning to soar in turbulent\r
environments. Proceedings of the National Academy of Sciences, 113(33):E4877–E4884.\r
Redish, D. A. (2004). Addiction as a computational process gone awry. Science, 306(5703):1944–\r
1947.\r
Reetz, D. (1977). Approximate solutions of a discounted Markovian decision process. Bonner\r
Mathematische Schriften, 98:77–92.\r
Rescorla, R. A., Wagner, A. R. (1972). A theory of Pavlovian conditioning: Variations in the\r
e↵ectiveness of reinforcement and nonreinforcement. In A. H. Black and W. F. Prokasy\r
(Eds.), Classical Conditioning II, pp. 64–99. Appleton-Century-Crofts, New York.\r
Revusky, S., Garcia, J. (1970). Learned associations over long delays. In G. Bower (Ed.), The\r
Psychology of Learning and Motivation, v. 4, pp. 1–84. Academic Press, Inc., New York.\r
Reynolds, J. N. J., Wickens, J. R. (2002). Dopamine-dependent plasticity of corticostriatal\r
synapses. Neural Networks, 15(4):507–521.\r
Ring, M. B. (in preparation). Representing knowledge as forecasts (and state as knowledge).\r
Ripley, B. D. (2007). Pattern Recognition and Neural Networks. Cambridge University Press.\r
Rixner, S. (2004). Memory controller optimizations for web servers. In Proceedings of the\r
37th annual IEEE/ACM International Symposium on Microarchitecture, p. 355–366. IEEE\r
Computer Society.\r
Robbins, H. (1952). Some aspects of the sequential design of experiments. Bulletin of the\r
American Mathematical Society, 58:527–535.\r
Robertie, B. (1992). Carbon versus silicon: Matching wits with TD-Gammon. Inside Backgam\u0002mon, 2(2):14–22.\r
Romo, R., Schultz, W. (1990). Dopamine neurons of the monkey midbrain: Contingencies of\r
responses to active touch during self-initiated arm movements. Journal of Neurophysiology,\r
63(3):592–624.

References 507\r
Rosenblatt, F. (1962). Principles of Neurodynamics: Perceptrons and the Theory of Brain\r
Mechanisms. Spartan Books, Washington, DC.\r
Ross, S. (1983). Introduction to Stochastic Dynamic Programming. Academic Press, New York.\r
Ross, T. (1933). Machines that think. Scientific American, 148(4):206–208.\r
Rubinstein, R. Y. (1981). Simulation and the Monte Carlo Method. Wiley, New York.\r
Rumelhart, D. E., Hinton, G. E., Williams, R. J. (1986). Learning internal representations by\r
error propagation. In D. E. Rumelhart and J. L. McClelland (Eds.), Parallel Distributed Pro\u0002cessing: Explorations in the Microstructure of Cognition, vol. I, Foundations. Bradford/MIT\r
Press, Cambridge, MA.\r
Rummery, G. A. (1995). Problem Solving with Reinforcement Learning. PhD thesis, University\r
of Cambridge.\r
Rummery, G. A., Niranjan, M. (1994). On-line Q-learning using connectionist systems. Technical\r
Report CUED/F-INFENG/TR 166. Engineering Department, Cambridge University.\r
Ruppert, D. (1988). Ecient estimations from a slowly convergent Robbins-Monro process.\r
Cornell University Operations Research and Industrial Engineering Technical Report No. 781.\r
Russell, S., Norvig, P. (2009). Artificial Intelligence: A Modern Approach, 3rd edition. Prentice\u0002Hall, Englewood Cli↵s, NJ.\r
Russo, D. J., Van Roy, B., Kazerouni, A., Osband, I., Wen, Z. (2018). A tutorial on Thompson\r
sampling, Foundations and Trends in Machine Learning. ArXiv:1707.02038.\r
Rust, J. (1996). Numerical dynamic programming in economics. In H. Amman, D. Kendrick, and\r
J. Rust (Eds.), Handbook of Computational Economics, pp. 614–722. Elsevier, Amsterdam.\r
Saddoris, M. P., Cacciapaglia, F., Wightmman, R. M., Carelli, R. M. (2015). Di↵erential\r
dopamine release dynamics in the nucleus accumbens core and shell reveal complemen\u0002tary signals for error prediction and incentive motivation. The Journal of Neuroscience,\r
35(33):11572–11582.\r
Saksida, L. M., Raymond, S. M., Touretzky, D. S. (1997). Shaping robot behavior using principles\r
from instrumental conditioning. Robotics and Autonomous Systems, 22(3):231–249.\r
Samuel, A. L. (1959). Some studies in machine learning using the game of checkers. IBM\r
Journal on Research and Development, 3(3), 210–229.\r
Samuel, A. L. (1967). Some studies in machine learning using the game of checkers. II—Recent\r
progress. IBM Journal on Research and Development, 11(6):601–617.\r
Schaal, S., Atkeson, C. G. (1994). Robot juggling: Implementation of memory-based learning.\r
IEEE Control Systems, 14 (1):57–71.\r
Schmajuk, N. A. (2008). Computational models of classical conditioning. Scholarpedia,\r
3(3):1664.\r
Schmidhuber, J. (1991a). Curious model-building control systems. In Proceedings of the IEEE\r
International Joint Conference on Neural Networks, pp. 1458–1463. IEEE.\r
Schmidhuber, J. (1991b). A possibility for implementing curiosity and boredom in model-building\r
neural controllers. In From Animals to Animats: Proceedings of the First International\r
Conference on Simulation of Adaptive Behavior, pp. 222–227. MIT Press, Cambridge, MA.\r
Schmidhuber, J. (2015). Deep learning in neural networks: An overview. Neural Networks,\r
6 :85–117.\r
Schmidhuber, J., Storck, J., Hochreiter, S. (1994). Reinforcement driven information acquisition\r
in nondeterministic environments. Technical report, Fakult¨at f¨ur Informatik, Technische\r
Universit¨at M¨unchen, M¨unchen, Germany.\r
Schraudolph, N. N. (1999). Local gain adaptation in stochastic gradient descent. In Proceedings\r
of the International Conference on Artificial Neural Networks, pp. 569–574. IEEE, London.

508 References\r
Schraudolph, N. N. (2002). Fast curvature matrix-vector products for second-order gradient\r
descent. Neural Computation, 14 (7):1723–1738.\r
Schraudolph, N. N., Yu, J., Aberdeen, D. (2006). Fast online policy gradient learning with SMD\r
gain vector adaptation. In Advances in Neural Information Processing Systems, pp. 1185–\r
1192.\r
Schulman, J., Chen, X., Abbeel, P. (2017). Equivalence between policy gradients and soft\r
Q-Learning. ArXiv:1704.06440.\r
Schultz, D. G., Melsa, J. L. (1967). State Functions and Linear Control Systems. McGraw-Hill,\r
New York.\r
Schultz, W. (1998). Predictive reward signal of dopamine neurons. Journal of Neurophysiology,\r
80(1):1–27.\r
Schultz, W., Apicella, P., Ljungberg, T. (1993). Responses of monkey dopamine neurons to\r
reward and conditioned stimuli during successive steps of learning a delayed response task.\r
The Journal of Neuroscience, 13 (3):900–913.\r
Schultz, W., Dayan, P., Montague, P. R. (1997). A neural substrate of prediction and reward.\r
Science, 275(5306):1593–1598.\r
Schultz, W., Romo, R. (1990). Dopamine neurons of the monkey midbrain: contingencies of\r
responses to stimuli eliciting immediate behavioral reactions. Journal of Neurophysiology,\r
63(3):607–624.\r
Schultz, W., Romo, R., Ljungberg, T., Mirenowicz, J., Hollerman, J. R., Dickinson, A. (1995).\r
Reward-related signals carried by dopamine neurons. In J. C. Houk, J. L. Davis, and D. G.\r
Beiser (Eds.), Models of Information Processing in the Basal Ganglia, pp. 233–248. MIT\r
Press, Cambridge, MA.\r
Schwartz, A. (1993). A reinforcement learning method for maximizing undiscounted rewards. In\r
Proceedings of the 10th International Conference on Machine Learning, pp. 298–305. Morgan\r
Kaufmann.\r
Schweitzer, P. J., Seidmann, A. (1985). Generalized polynomial approximations in Markovian\r
decision processes. Journal of Mathematical Analysis and Applications, 110(2):568–582.\r
Selfridge, O. G. (1978). Tracking and trailing: Adaptation in movement strategies. Technical\r
report, Bolt Beranek and Newman, Inc. Unpublished report.\r
Selfridge, O. G. (1984). Some themes and primitives in ill-defined systems. In O. G. Selfridge,\r
E. L. Rissland, and M. A. Arbib (Eds.), Adaptive Control of Ill-Defined Systems, pp. 21–26.\r
Plenum Press, NY. Proceedings of the NATO Advanced Research Institute on Adaptive\r
Control of Ill-defined Systems, NATO Conference Series II, Systems Science, Vol. 16.\r
Selfridge, O. J., Sutton, R. S., Barto, A. G. (1985). Training and tracking in robotics. In A. Joshi\r
(Ed.), Proceedings of the Ninth International Joint Conference on Artificial Intelligence,\r
pp. 670–672. Morgan Kaufmann.\r
Seo, H., Barraclough, D., Lee, D. (2007). Dynamic signals related to choices and outcomes in\r
the dorsolateral prefrontal cortex. Cerebral Cortex, 17(suppl 1):110–117.\r
Seung, H. S. (2003). Learning in spiking neural networks by reinforcement of stochastic synaptic\r
transmission. Neuron, 40(6):1063–1073.\r
Shah, A. (2012). Psychological and neuroscientific connections with reinforcement learning. In M.\r
Wiering and M. van Otterlo (Eds.), Reinforcement Learning: State-of-the-Art, pp. 507–537.\r
Springer-Verlag Berlin Heidelberg.\r
Shannon, C. E. (1950). Programming a computer for playing chess. Philosophical Magazine and\r
Journal of Science, 41(314):256–275.

References 509\r
Shannon, C. E. (1951). Presentation of a maze-solving machine. In H. V. Forester (Ed.), Cyber\u0002netics. Transactions of the Eighth Conference, pp. 173–180. Josiah Macy Jr. Foundation.\r
Shannon, C. E. (1952). “Theseus” maze-solving mouse. http://cyberneticzoo.com/mazesolvers/1952—\r
theseus-maze-solving-mouse—claude-shannon-american/.\r
Shelton, C. R. (2001). Importance Sampling for Reinforcement Learning with Multiple Objectives.\r
PhD thesis, Massachusetts Institute of Technology, Cambridge MA.\r
Shepard, D. (1968). A two-dimensional interpolation function for irregularly-spaced data. In\r
Proceedings of the 23rd ACM National Conference, pp. 517–524. ACM, New York.\r
Sherman, J., Morrison, W. J. (1949). Adjustment of an inverse matrix corresponding to changes\r
in the elements of a given column or a given row of the original matrix (abstract). Annals of\r
Mathematical Statistics, 20 (4):621.\r
Shewchuk, J., Dean, T. (1990). Towards learning time-varying functions with high input\r
dimensionality. In Proceedings of the Fifth IEEE International Symposium on Intelligent\r
Control, pp. 383–388. IEEE Computer Society Press, Los Alamitos, CA.\r
Shimansky, Y. P. (2009). Biologically plausible learning in neural networks: a lesson from\r
bacterial chemotaxis. Biological Cybernetics, 101(5-6):379–385.\r
Si, J., Barto, A., Powell, W., Wunsch, D. (Eds.) (2004). Handbook of Learning and Approximate\r
Dynamic Programming. John Wiley and Sons.\r
Silver, D. (2009). Reinforcement Learning and Simulation Based Search in the Game of Go.\r
PhD thesis, University of Alberta, Edmonton.\r
Silver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., van den Driessche, G., Schrittwieser,\r
J., Antonoglou, I., Panneershelvam, V., Lanctot, M., Dieleman, S., Grewe, D., Nham, J.,\r
Kalchbrenner, N., Sutskever, I., Lillicrap, T., Leach, M., Kavukcuoglu, K., Graepel, T.,\r
Hassabis, D. (2016). Mastering the game of Go with deep neural networks and tree search.\r
Nature, 529(7587):484–489.\r
Silver, D., Lever, G., Heess, N., Degris, T., Wierstra, D., Riedmiller, M. (2014). Deterministic\r
policy gradient algorithms. In Proceedings of the 31st International Conference on Machine\r
Learning, pp. 387–395.\r
Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez, A., Hubert, T.,\r
Baker, L., Lai, M., Bolton, A., Chen, Y., Lillicrap, L., Hui, F., Sifre, L., van den Driessche,\r
G., Graepel, T., Hassibis, D. (2017a). Mastering the game of Go without human knowledge.\r
Nature, 550 (7676):354–359.\r
Silver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai, M., Guez, A., Lanctot, M., Sifre, L.,\r
Kumaran, D., Graepel, T., Lillicrap, T., Simoyan, K., Hassibis, D. (2017b). Mastering chess\r
and shogi by self-play with a general reinforcement learning algorithm. ArXiv:1712.01815.\r
S¸im¸sek, O., Alg´orta, S., Kothiyal, A. (2016). Why most decisions are easy in tetris—And perhaps ¨\r
in other sequential decision problems, as well. In Proceedings of the 33rd International\r
Conference on Machine Learning, pp. 1757-1765.\r
Simon, H. (2000). Lecture at the Earthware Symposium, Carnegie Mellon University. https\r
://www.youtube.com/watch?v=EZhyi-8DBjc.\r
Singh, S. P. (1992a). Reinforcement learning with a hierarchy of abstract models. In Proceedings\r
of the Tenth National Conference on Artificial Intelligence, pp. 202–207. AAAI/MIT Press,\r
Menlo Park, CA.\r
Singh, S. P. (1992b). Scaling reinforcement learning algorithms by learning variable temporal\r
resolution models. In Proceedings of the 9th International Workshop on Machine Learning,\r
pp. 406–415. Morgan Kaufmann.\r
Singh, S. P. (1993). Learning to Solve Markovian Decision Processes. PhD thesis, University of\r
Massachusetts, Amherst.

510 References\r
Singh, S. P. (Ed.) (2002). Special double issue on reinforcement learning, Machine Learning,\r
49(2-3).\r
Singh, S., Barto, A. G., Chentanez, N. (2005). Intrinsically motivated reinforcement learning.\r
In Advances in Neural Information Processing Systems 17, pp. 1281–1288. MIT Press,\r
Cambridge, MA.\r
Singh, S. P., Bertsekas, D. (1997). Reinforcement learning for dynamic channel allocation\r
in cellular telephone systems. In Advances in Neural Information Processing Systems 9,\r
pp. 974–980. MIT Press, Cambridge, MA.\r
Singh, S. P., Jaakkola, T., Jordan, M. I. (1994). Learning without state-estimation in partially\r
observable Markovian decision problems. In Proceedings of the 11th International Conference\r
on Machine Learning, pp. 284–292. Morgan Kaufmann.\r
Singh, S., Jaakkola, T., Littman, M. L., Szepesv´ari, C. (2000). Convergence results for single-step\r
on-policy reinforcement-learning algorithms. Machine Learning, 38 (3):287–308.\r
Singh, S. P., Jaakkola, T., Jordan, M. I. (1995). Reinforcement learning with soft state\r
aggregation. In Advances in Neural Information Processing Systems 7, pp. 359–368. MIT\r
Press, Cambridge, MA.\r
Singh, S., Lewis, R. L., Barto, A. G. (2009). Where do rewards come from? In N. Taatgen\r
and H. van Rijn (Eds.), Proceedings of the 31st Annual Conference of the Cognitive Science\r
Society, pp. 2601–2606. Cognitive Science Society.\r
Singh, S., Lewis, R. L., Barto, A. G., Sorg, J. (2010). Intrinsically motivated reinforcement\r
learning: An evolutionary perspective. IEEE Transactions on Autonomous Mental Develop\u0002ment, 2(2):70–82. Special issue on Active Learning and Intrinsically Motivated Exploration\r
in Robots: Advances and Challenges.\r
Singh, S. P., Sutton, R. S. (1996). Reinforcement learning with replacing eligibility traces.\r
Machine Learning, 22(1-3):123–158.\r
Skinner, B. F. (1938). The Behavior of Organisms: An Experimental Analysis. Appleton-Century,\r
New York.\r
Skinner, B. F. (1958). Reinforcement today. American Psychologist, 13(3):94–99.\r
Skinner, B. F. (1963). Operant behavior. American Psychologist, 18 (8):503–515.\r
Sofge, D. A., White, D. A. (1992). Applied learning: Optimal control for manufacturing. In\r
D. A. White and D. A. Sofge (Eds.), Handbook of Intelligent Control: Neural, Fuzzy, and\r
Adaptive Approaches, pp. 259–281. Van Nostrand Reinhold, New York.\r
Sorg, J. D. (2011). The Optimal Reward Problem:Designing E↵ective Reward for Bounded\r
Agents. PhD thesis, University of Michigan, Ann Arbor.\r
Sorg, J., Lewis, R. L., Singh, S. P. (2010). Reward design via online gradient ascent. In Advances\r
in Neural Information Processing Systems 23, pp. 2190–2198. Curran Associates, Inc.\r
Sorg, J., Singh, S. (2010). Linear options. In Proceedings of the 9th International Conference on\r
Autonomous Agents and Multiagent Systems, pp. 31–38.\r
Sorg, J., Singh, S., Lewis, R. (2010). Internal rewards mitigate agent boundedness. In Proceedings\r
of the 27th International Conference on Machine Learning, pp. 1007–1014.\r
Spence, K. W. (1947). The role of secondary reinforcement in delayed reward learning. Psycho\u0002logical Review, 54(1):1–8.\r
Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., Salakhutdinov, R. (2014). Dropout:\r
A simple way to prevent neural networks from overfitting. Journal of Machine Learning\r
Research, 15(1):1929–1958.\r
Staddon, J. E. R. (1983). Adaptive Behavior and Learning. Cambridge University Press.

References 511\r
Stanfill, C., Waltz, D. (1986). Toward memory-based reasoning. Communications of the ACM,\r
29 (12):1213–1228.\r
Steinberg, E. E., Keiflin, R., Boivin, J. R., Witten, I. B., Deisseroth, K., Janak, P. H. (2013). A\r
causal link between prediction errors, dopamine neurons and learning. Nature Neuroscience,\r
16(7):966–973.\r
Sterling, P., Laughlin, S. (2015). Principles of Neural Design. MIT Press, Cambridge, MA.\r
Sternberg, S. (1963). Stochastic learning theory. In: Handbook of Mathematical Psychology,\r
Volume II, R. D. Luce, R. R. Bush, and E. Galanter (Eds.). John Wiley & Sons.\r
Sugiyama, M., Hachiya, H., Morimura, T. (2013). Statistical Reinforcement Learning: Modern\r
Machine Learning Approaches. Chapman & Hall/CRC.\r
Suri, R. E., Bargas, J., Arbib, M. A. (2001). Modeling functions of striatal dopamine modulation\r
in learning and planning. Neuroscience, 103(1):65–85.\r
Suri, R. E., Schultz, W. (1998). Learning of sequential movements by neural network model\r
with dopamine-like reinforcement signal. Experimental Brain Research, 121(3):350–354.\r
Suri, R. E., Schultz, W. (1999). A neural network model with dopamine-like reinforcement\r
signal that learns a spatial delayed response task. Neuroscience, 91(3):871–890.\r
Sutton, R. S. (1978a). Learning theory support for a single channel theory of the brain.\r
Unpublished report.\r
Sutton, R. S. (1978b). Single channel theory: A neuronal theory of learning. Brain Theory\r
Newsletter, 4:72–75. Center for Systems Neuroscience, University of Massachusetts, Amherst,\r
MA.\r
Sutton, R. S. (1978c). A unified theory of expectation in classical and instrumental conditioning.\r
Bachelors thesis, Stanford University.\r
Sutton, R. S. (1984). Temporal Credit Assignment in Reinforcement Learning. PhD thesis,\r
University of Massachusetts, Amherst.\r
Sutton, R. S. (1988). Learning to predict by the method of temporal di↵erences. Machine\r
Learning, 3(1):9–44 (important erratum p. 377).\r
Sutton, R. S. (1990). Integrated architectures for learning, planning, and reacting based on\r
approximating dynamic programming. In Proceedings of the 7th International Workshop on\r
Machine Learning, pp. 216–224. Morgan Kaufmann.\r
Sutton, R. S. (1991a). Dyna, an integrated architecture for learning, planning, and reacting.\r
SIGART Bulletin, 2(4):160–163. ACM, New York.\r
Sutton, R. S. (1991b). Planning by incremental dynamic programming. In Proceedings of the\r
8th International Workshop on Machine Learning, pp. 353–357. Morgan Kaufmann.\r
Sutton, R. S. (Ed.) (1992a). Reinforcement Learning. Kluwer Academic Press. Reprinting of a\r
special double issue on reinforcement learning, Machine Learning, 8(3-4).\r
Sutton, R. S. (1992b). Adapting bias by gradient descent: An incremental version of delta-bar\u0002delta. Proceedings of the Tenth National Conference on Artificial Intelligence, pp. 171–176,\r
MIT Press.\r
Sutton, R. S. (1992c). Gain adaptation beats least squares? Proceedings of the Seventh Yale\r
Workshop on Adaptive and Learning Systems, pp. 161–166, Yale University, New Haven, CT.\r
Sutton, R. S. (1995a). TD models: Modeling the world at a mixture of time scales. In\r
Proceedings of the 12th International Conference on Machine Learning, pp. 531–539. Morgan\r
Kaufmann.\r
Sutton, R. S. (1995b). On the virtues of linear learning and trajectory distributions. In\r
Proceedings of the Workshop on Value Function Approximation at The 12th International\r
Conference on Machine Learning.

512 References\r
Sutton, R. S. (1996). Generalization in reinforcement learning: Successful examples using sparse\r
coarse coding. In Advances in Neural Information Processing Systems 8, pp. 1038–1044.\r
MIT Press, Cambridge, MA.\r
Sutton, R. S. (2009). The grand challenge of predictive empirical abstract knowledge. Working\r
Notes of the IJCAI-09 Workshop on Grand Challenges for Reasoning from Experiences.\r
Sutton, R. S. (2015a) Introduction to reinforcement learning with function approximation.\r
Tutorial at the Conference on Neural Information Processing Systems, Montreal, December\r
7, 2015.\r
Sutton, R. S. (2015b) True online Emphatic TD(): Quick reference and implementation guide.\r
ArXiv:1507.07147. Code is available in Python and C++ by downloading the source files of\r
this arXiv paper as a zip archive.\r
Sutton, R. S., Barto, A. G. (1981a). Toward a modern theory of adaptive networks: Expectation\r
and prediction. Psychological Review, 88(2):135–170.\r
Sutton, R. S., Barto, A. G. (1981b). An adaptive network that constructs and uses an internal\r
model of its world. Cognition and Brain Theory, 3:217–246.\r
Sutton, R. S., Barto, A. G. (1987). A temporal-di↵erence model of classical conditioning. In\r
Proceedings of the Ninth Annual Conference of the Cognitive Science Society, pp. 355-378.\r
Erlbaum, Hillsdale, NJ.\r
Sutton, R. S., Barto, A. G. (1990). Time-derivative models of Pavlovian reinforcement. In\r
M. Gabriel and J. Moore (Eds.), Learning and Computational Neuroscience: Foundations of\r
Adaptive Networks, pp. 497–537. MIT Press, Cambridge, MA.\r
Sutton, R. S., Maei, H. R., Precup, D., Bhatnagar, S., Silver, D., Szepesv´ari, Cs., Wiewiora, E.\r
(2009a). Fast gradient-descent methods for temporal-di↵erence learning with linear function\r
approximation. In Proceedings of the 26th International Conference on Machine Learning,\r
pp. 993–1000. ACM, New York.\r
Sutton, R. S., Szepesv´ari, Cs., Maei, H. R. (2009b). A convergent O(d2) temporal-di↵erence\r
algorithm for o↵-policy learning with linear function approximation. In Advances in Neural\r
Information Processing Systems 21, pp. 1609–1616. Curran Associates, Inc.\r
Sutton, R. S., Mahmood, A. R., Precup, D., van Hasselt, H. (2014). A new Q() with interim\r
forward view and Monte Carlo equivalence. In Proceedings of the International Conference\r
on Machine Learning, 31. JMLR W&CP 32 (2).\r
Sutton, R. S., Mahmood, A. R., White, M. (2016). An emphatic approach to the problem of\r
o↵-policy temporal-di↵erence learning. Journal of Machine Learning Research, 17 (73):1–29.\r
Sutton, R. S., McAllester, D. A., Singh, S. P., Mansour, Y. (2000). Policy gradient methods\r
for reinforcement learning with function approximation. In Advances in Neural Information\r
Processing Systems 12, pp. 1057–1063. MIT Press, Cambridge, MA.\r
Sutton, R. S., Modayil, J., Delp, M., Degris, T., Pilarski, P. M., White, A., Precup, D. (2011).\r
Horde: A scalable real-time architecture for learning knowledge from unsupervised senso\u0002rimotor interaction. In Proceedings of the Tenth International Conference on Autonomous\r
Agents and Multiagent Systems, pp. 761–768, Taipei, Taiwan.\r
Sutton, R. S., Pinette, B. (1985). The learning of world models by connectionist networks. In\r
Proceedings of the Seventh Annual Conference of the Cognitive Science Society, pp. 54–64.\r
Sutton, R. S., Precup, D., Singh, S. (1999). Between MDPs and semi-MDPs: A framework for\r
temporal abstraction in reinforcement learning. Artificial Intelligence, 112 (1-2):181–211.\r
Sutton, R. S., Rafols, E., Koop, A. (2006). Temporal abstraction in temporal-di↵erence networks.\r
In Advances in neural information processing systems, pp. 1313–1320.\r
Sutton, R. S., Singh, S. P., McAllester, D. A. (2000). Comparing policy-gradient algorithms.\r
Unpublished manuscript.

References 513\r
Sutton, R. S., Szepesv´ari, Cs., Geramifard, A., Bowling, M., (2008). Dyna-style planning with\r
linear function approximation and prioritized sweeping. In Proceedings of the 24th Conference\r
on Uncertainty in Artificial Intelligence, pp. 528–536.\r
Sutton, R. S., Tanner, B. (2005). Temporal-di↵erence networks. In Advances in Neural\r
Information Processing Systems 17, p. 1377–1384.\r
Szepesv´ari, Cs. (2010). Algorithms for reinforcement learning. In Synthesis Lectures on Artificial\r
Intelligence and Machine Learning, 4(1):1–103. Morgan and Claypool.\r
Szita, I. (2012). Reinforcement learning in games. In M. Wiering and M. van Otterlo (Eds.),\r
Reinforcement Learning: State-of-the-Art, pp. 539–577. Springer-Verlag Berlin Heidelberg.\r
Tadepalli, P., Ok, D. (1994). H-learning: A reinforcement learning method to optimize\r
undiscounted average reward. Technical Report 94-30-01. Oregon State University, Computer\r
Science Department, Corvallis.\r
Tadepalli, P., Ok, D. (1996). Scaling up average reward reinforcement learning by approximating\r
the domain models and the value function. In Proceedings of the 13th International Conference\r
on Machine Learning, pp. 471–479.\r
Takahashi, Y., Schoenbaum, G., and Niv, Y. (2008). Silencing the critics: Understanding the\r
e↵ects of cocaine sensitization on dorsolateral and ventral striatum in the context of an\r
actor/critic model. Frontiers in Neuroscience, 2(1):86–99.\r
Tambe, M., Newell, A., Rosenbloom, P. S. (1990). The problem of expensive chunks and its\r
solution by restricting expressiveness. Machine Learning, 5 (3):299–348.\r
Tan, M. (1991). Learning a cost-sensitive internal representation for reinforcement learning. In\r
L. A. Birnbaum and G. C. Collins (Eds.), Proceedings of the 8th International Workshop on\r
Machine Learning, pp. 358–362. Morgan Kaufmann.\r
Tanner, B. (2006). Temporal-Di↵erence Networks. MSc thesis, University of Alberta.\r
Taylor, G., Parr, R. (2009). Kernelized value function approximation for reinforcement learning.\r
In Proceedings of the 26th International Conference on Machine Learning, pp. 1017–1024.\r
ACM, New York.\r
Taylor, M. E., Stone, P. (2009). Transfer learning for reinforcement learning domains: A survey.\r
Journal of Machine Learning Research, 10 :1633–1685.\r
Tesauro, G. (1986). Simple neural models of classical conditioning. Biological Cybernetics,\r
55(2-3):187–200.\r
Tesauro, G. (1992). Practical issues in temporal di↵erence learning. Machine Learning,\r
8(3-4):257–277.\r
Tesauro, G. (1994). TD-Gammon, a self-teaching backgammon program, achieves master-level\r
play. Neural Computation, 6(2):215–219.\r
Tesauro, G. (1995). Temporal di↵erence learning and TD-Gammon. Communications of the\r
ACM, 38(3):58–68.\r
Tesauro, G. (2002). Programming backgammon using self-teaching neural nets. Artificial\r
Intelligence, 134(1-2):181–199.\r
Tesauro, G., Galperin, G. R. (1997). On-line policy improvement using Monte-Carlo search. In\r
Advances in Neural Information Processing Systems 9, pp. 1068–1074. MIT Press, Cambridge,\r
MA.\r
Tesauro, G., Gondek, D. C., Lechner, J., Fan, J., Prager, J. M. (2012). Simulation, learning,\r
and optimization techniques in Watson’s game strategies. IBM Journal of Research and\r
Development, 56(3-4):16–1–16–11.\r
Tesauro, G., Gondek, D. C., Lenchner, J., Fan, J., Prager, J. M. (2013). Analysis of Watson’s\r
strategies for playing Jeopardy! Journal of Artificial Intelligence Research, 47:205–251.\r
Tham, C. K. (1994). Modular On-Line Function Approximation for Scaling up Reinforcement\r
Learning. PhD thesis, University of Cambridge.

514 References\r
Thathachar, M. A. L., Sastry, P. S. (1985). A new approach to the design of reinforcement\r
schemes for learning automata. IEEE Transactions on Systems, Man, and Cybernetics,\r
15(1):168–175.\r
Thathachar, M., Sastry, P. S. (2002). Varieties of learning automata: an overview. IEEE\r
Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics, 36(6):711–722.\r
Thathachar, M., Sastry, P. S. (2011). Networks of Learning Automata: Techniques for Online\r
Stochastic Optimization. Springer Science & Business Media.\r
Theocharous, G., Thomas, P. S., Ghavamzadeh, M. (2015). Personalized ad recommendation for\r
life-time value optimization guarantees. In Proceedings of the Twenty-Fourth International\r
Joint Conference on Artificial Intelligence. AAAI Press, Palo Alto, CA.\r
Thistlethwaite, D. (1951). A critical review of latent learning and related experiments. Psycho\u0002logical Bulletin, 48(2):97–129.\r
Thomas, P. S. (2014). Bias in natural actor–critic algorithms. In Proceedings of the 31st\r
International Conference on Machine Learning, JMLR W&CP 32 (1), pp. 441–448.\r
Thomas, P. S. (2015). Safe Reinforcement Learning. PhD thesis, University of Massachusetts,\r
Amherst.\r
Thomas, P. S., Brunskill, E. (2017). Policy gradient methods for reinforcement learning with\r
function approximation and action-dependent baselines. ArXiv:1706.06643.\r
Thomas, P. S., Theocharous, G., Ghavamzadeh, M. (2015). High-confidence o↵-policy evaluation.\r
In Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence, pp. 3000–3006.\r
AAAI Press, Menlo Park, CA.\r
Thompson, W. R. (1933). On the likelihood that one unknown probability exceeds another in\r
view of the evidence of two samples. Biometrika, 25(3-4):285–294.\r
Thompson, W. R. (1934). On the theory of apportionment. American Journal of Mathematics,\r
57: 450–457.\r
Thon, M. (2017). Spectral Learning of Sequential Systems. PhD thesis, Jacobs University\r
Bremen.\r
Thon, M., Jaeger, H. (2015). Links between multiplicity automata, observable operator models\r
and predictive state representations: a unified learning framework. The Journal of Machine\r
Learning Research, 16 (1):103–147.\r
Thorndike, E. L. (1898). Animal intelligence: An experimental study of the associative processes\r
in animals. The Psychological Review, Series of Monograph Supplements, II(4).\r
Thorndike, E. L. (1911). Animal Intelligence. Hafner, Darien, CT.\r
Thorp, E. O. (1966). Beat the Dealer: A Winning Strategy for the Game of Twenty-One.\r
Random House, New York.\r
Tian, T. (in preparation) An Empirical Study of Sliding-Step Methods in Temporal Di↵erence\r
Learning. M.Sc thesis, University of Alberta, Edmonton.\r
Tieleman, T., Hinton, G. (2012). Lecture 6.5–RMSProp. COURSERA: Neural networks for\r
machine learning 4.2:26–31.\r
Tolman, E. C. (1932). Purposive Behavior in Animals and Men. Century, New York.\r
Tolman, E. C. (1948). Cognitive maps in rats and men. Psychological Review, 55(4):189–208.\r
Tsai, H.-S., Zhang, F., Adamantidis, A., Stuber, G. D., Bonci, A., de Lecea, L., Deisseroth,\r
K. (2009). Phasic firing in dopaminergic neurons is sucient for behavioral conditioning.\r
Science, 324(5930):1080–1084.\r
Tsetlin, M. L. (1973). Automaton Theory and Modeling of Biological Systems. Academic Press,\r
New York.

References 515\r
Tsitsiklis, J. N. (1994). Asynchronous stochastic approximation and Q-learning. Machine\r
Learning, 16(3):185–202.\r
Tsitsiklis, J. N. (2002). On the convergence of optimistic policy iteration. Journal of Machine\r
Learning Research, 3:59–72.\r
Tsitsiklis, J. N., Van Roy, B. (1996). Feature-based methods for large scale dynamic programming.\r
Machine Learning, 22(1-3):59–94.\r
Tsitsiklis, J. N., Van Roy, B. (1997). An analysis of temporal-di↵erence learning with function\r
approximation. IEEE Transactions on Automatic Control, 42(5):674–690.\r
Tsitsiklis, J. N., Van Roy, B. (1999). Average cost temporal-di↵erence learning. Automatica,\r
35(11):1799–1808.\r
Turing, A. M. (1948). Intelligent machinery. In B. Jack Copeland (Ed.) (2004), The Essential\r
Turing, pp. 410–432. Oxford University Press, Oxford.\r
Ungar, L. H. (1990). A bioreactor benchmark for adaptive network-based process control. In\r
W. T. Miller, R. S. Sutton, and P. J. Werbos (Eds.), Neural Networks for Control, pp. 387–402.\r
MIT Press, Cambridge, MA.\r
Unnikrishnan, K. P., Venugopal, K. P. (1994). Alopex: A correlation-based learning algorithm\r
for feedforward and recurrent neural networks. N eural Computation, 6(3): 469–490.\r
Urbanczik, R., Senn, W. (2009). Reinforcement learning in populations of spiking neurons.\r
Nature neuroscience, 12(3):250–252.\r
Urbanowicz, R. J., Moore, J. H. (2009). Learning classifier systems: A complete introduction,\r
review, and roadmap. Journal of Artificial Evolution and Applications. 10.1155/2009/736398.\r
Valentin, V. V., Dickinson, A., O’Doherty, J. P. (2007). Determining the neural substrates of\r
goal-directed learning in the human brain. The Journal of Neuroscience, 27(15):4019–4026.\r
van Hasselt, H. (2010). Double Q-learning. In Advances in Neural Information Processing\r
Systems 23, pp. 2613–2621. Curran Associates, Inc.\r
van Hasselt, H. (2011). Insights in Reinforcement Learning: Formal Analysis and Empirical\r
Evaluation of Temporal-di↵erence Learning. SIKS dissertation series number 2011-04.\r
van Hasselt, H. (2012). Reinforcement learning in continuous state and action spaces. In M.\r
Wiering and M. van Otterlo (Eds.), Reinforcement Learning: State-of-the-Art, pp. 207–251.\r
Springer-Verlag Berlin Heidelberg.\r
van Hasselt, H., Sutton, R. S. (2015). Learning to predict independent of span. ArXiv:1508.04582.\r
Van Roy, B., Bertsekas, D. P., Lee, Y., Tsitsiklis, J. N. (1997). A neuro-dynamic programming\r
approach to retailer inventory management. In Proceedings of the 36th IEEE Conference on\r
Decision and Control, Vol. 4, pp. 4052–4057.\r
van Seijen, H. (2011). Reinforcement Learning under Space and Time Constraints. University of\r
Amsterdam PhD thesis. Hague: TNO.\r
van Seijen, H. (2016). E↵ective multi-step temporal-di↵erence learning for non-linear function\r
approximation. ArXiv:1608.05151.\r
van Seijen, H., Sutton, R. S. (2013). Ecient planning in MDPs by small backups. In: Proceedings\r
of the 30th International Conference on Machine Learning, pp. 361–369.\r
van Seijen, H., Sutton, R. S. (2014). True online TD(). In Proceedings of the 31st International\r
Conference on Machine Learning, pp. 692–700. JMLR W&CP 32(1),\r
van Seijen, H., Mahmood, A. R., Pilarski, P. M., Machado, M. C., Sutton, R. S. (2016). True\r
online temporal-di↵erence learning. Journal of Machine Learning Research, 17 (145):1–40.\r
van Seijen, H., Van Hasselt, H., Whiteson, S., Wiering, M. (2009). A theoretical and empirical\r
analysis of Expected Sarsa. In IEEE Symposium on Adaptive Dynamic Programming and\r
Reinforcement Learning, pp. 177–184.

516 References\r
van Seijen, H., Whiteson, S., van Hasselt, H., Wiering, M. (2011). Exploiting best-match\r
equations for ecient reinforcement learning. Journal of Machine Learning Research 12 :2045–\r
2094.\r
Varga, R. S. (1962). Matrix Iterative Analysis. Englewood Cli↵s, NJ: Prentice-Hall.\r
Vasilaki, E., Fr´emaux, N., Urbanczik, R., Senn, W., Gerstner, W. (2009). Spike-based rein\u0002forcement learning in continuous state and action space: when policy gradient methods fail.\r
PLoS Computational Biology, 5(12).\r
Viswanathan, R., Narendra, K. S. (1974). Games of stochastic automata. IEEE Transactions\r
on Systems, Man, and Cybernetics, 4(1):131–135.\r
Wagner, A. R. (2008). Evolution of an elemental theory of Pavlovian conditioning. Learning &\r
Behavior, 36 (3):253–265.\r
Walter, W. G. (1950). An imitation of life. Scientific American, 182(5):42–45.\r
Walter, W. G. (1951). A machine that learns. Scientific American, 185(2):60–63.\r
Waltz, M. D., Fu, K. S. (1965). A heuristic approach to reinforcement learning control systems.\r
IEEE Transactions on Automatic Control, 10(4):390–398.\r
Watkins, C. J. C. H. (1989). Learning from Delayed Rewards. PhD thesis, University of\r
Cambridge.\r
Watkins, C. J. C. H., Dayan, P. (1992). Q-learning. Machine Learning, 8(3-4):279–292.\r
Werbos, P. J. (1977). Advanced forecasting methods for global crisis warning and models of\r
intelligence. General Systems Yearbook, 22(12):25–38.\r
Werbos, P. J. (1982). Applications of advances in nonlinear sensitivity analysis. In R. F. Drenick\r
and F. Kozin (Eds.), System Modeling and Optimization, pp. 762–770. Springer-Verlag.\r
Werbos, P. J. (1987). Building and understanding adaptive systems: A statistical/numerical\r
approach to factory automation and brain research. IEEE Transactions on Systems, Man,\r
and Cybernetics, 17(1):7–20.\r
Werbos, P. J. (1988). Generalization of back propagation with applications to a recurrent gas\r
market model. Neural Networks, 1(4):339–356.\r
Werbos, P. J. (1989). Neural networks for control and system identification. In Proceedings of\r
the 28th Conference on Decision and Control, pp. 260–265. IEEE Control Systems Society.\r
Werbos, P. J. (1992). Approximate dynamic programming for real-time control and neural\r
modeling. In D. A. White and D. A. Sofge (Eds.), Handbook of Intelligent Control: Neural,\r
Fuzzy, and Adaptive Approaches, pp. 493–525. Van Nostrand Reinhold, New York.\r
Werbos, P. J. (1994). The Roots of Backpropagation: From Ordered Derivatives to Neural\r
Networks and Political Forecasting (Vol. 1). John Wiley and Sons.\r
Wiering, M., Van Otterlo, M. (2012). Reinforcement Learning: State-of-the-Art. Springer-Verlag\r
Berlin Heidelberg.\r
White, A. (2015). Developing a Predictive Approach to Knowledge. PhD thesis, University of\r
Alberta, Edmonton.\r
White, D. J. (1969). Dynamic Programming. Holden-Day, San Francisco.\r
White, D. J. (1985). Real applications of Markov decision processes. Interfaces, 15(6):73–83.\r
White, D. J. (1988). Further real applications of Markov decision processes. Interfaces,\r
18(5):55–61.\r
White, D. J. (1993). A survey of applications of Markov decision processes. Journal of the\r
Operational Research Society, 44(11):1073–1096.\r
White, A., White, M. (2016). Investigating practical linear temporal di↵erence learning. In\r
Proceedings of the 2016 International Conference on Autonomous Agents and Multiagent\r
Systems, pp. 494–502.

References 517\r
Whitehead, S. D., Ballard, D. H. (1991). Learning to perceive and act by trial and error.\r
Machine Learning, 7(1):45–83.\r
Whitt, W. (1978). Approximations of dynamic programs I. Mathematics of Operations Research,\r
3(3):231–243.\r
Whittle, P. (1982). Optimization over Time, vol. 1. Wiley, New York.\r
Whittle, P. (1983). Optimization over Time, vol. 2. Wiley, New York.\r
Wickens, J., K¨otter, R. (1995). Cellular models of reinforcement. In J. C. Houk, J. L. Davis and\r
D. G. Beiser (Eds.), Models of Information Processing in the Basal Ganglia, pp. 187–214.\r
MIT Press, Cambridge, MA.\r
Widrow, B., Gupta, N. K., Maitra, S. (1973). Punish/reward: Learning with a critic in adaptive\r
threshold systems. IEEE Transactions on Systems, Man, and Cybernetics, 3(5):455–465.\r
Widrow, B., Ho↵, M. E. (1960). Adaptive switching circuits. In 1960 WESCON Convention\r
Record Part IV, pp. 96–104. Institute of Radio Engineers, New York. Reprinted in J. A.\r
Anderson and E. Rosenfeld, Neurocomputing: Foundations of Research, pp. 126–134. MIT\r
Press, Cambridge, MA, 1988.\r
Widrow, B., Smith, F. W. (1964). Pattern-recognizing control systems. In J. T. Tou and\r
R. H. Wilcox (Eds.), Computer and Information Sciences, pp. 288–317. Spartan, Washington,\r
DC.\r
Widrow, B., Stearns, S. D. (1985). Adaptive Signal Processing. Prentice-Hall, Englewood Cli↵s,\r
NJ.\r
Wiener, N. (1964). God and Golem, Inc: A Comment on Certain Points where Cybernetics\r
Impinges on Religion. MIT Press, Cambridge, MA.\r
Wiewiora, E. (2003). Potential-based shaping and Q-value initialization are equivalent. Journal\r
of Artificial Intelligence Research, 19 :205–208.\r
Williams, R. J. (1986). Reinforcement learning in connectionist networks: A mathematical\r
analysis. Technical Report ICS 8605. Institute for Cognitive Science, University of California\r
at San Diego, La Jolla.\r
Williams, R. J. (1987). Reinforcement-learning connectionist systems. Technical Report\r
NU-CCS-87-3. College of Computer Science, Northeastern University, Boston.\r
Williams, R. J. (1988). On the use of backpropagation in associative reinforcement learning.\r
In Proceedings of the IEEE International Conference on Neural Networks, pp. I-263–I-270.\r
IEEE San Diego section and IEEE TAB Neural Network Committee.\r
Williams, R. J. (1992). Simple statistical gradient-following algorithms for connectionist\r
reinforcement learning. Machine Learning, 8(3-4):229–256.\r
Williams, R. J., Baird, L. C. (1990). A mathematical analysis of actor–critic architectures for\r
learning optimal controls through incremental dynamic programming. In Proceedings of the\r
Sixth Yale Workshop on Adaptive and Learning Systems, pp. 96–101. Center for Systems\r
Science, Dunham Laboratory, Yale University, New Haven.\r
Wilson, R. C., Takahashi, Y. K., Schoenbaum, G., Niv, Y. (2014). Orbitofrontal cortex as a\r
cognitive map of task space. Neuron, 81(2):267–279.\r
Wilson, S. W. (1994). ZCS, A zeroth order classifier system. Evolutionary Computation,\r
2(1):1–18.\r
Wise, R. A. (2004). Dopamine, learning, and motivation. Nature Reviews Neuroscience,\r
5(6):1–12.\r
Witten, I. H. (1976a). Learning to Control. University of Essex PhD thesis.\r
Witten, I. H. (1976b). The apparent conflict between estimation and control—A survey of the\r
two-armed problem. Journal of the Franklin Institute, 301(1-2):161–189.

518 References\r
Witten, I. H. (1977). An adaptive optimal controller for discrete-time Markov environments.\r
Information and Control, 34(4):286–295.\r
Witten, I. H., Corbin, M. J. (1973). Human operators and automatic adaptive controllers: A\r
comparative study on a particular control task. International Journal of Man–Machine\r
Studies, 5(1):75–104.\r
Woodbury, T., Dunn, C., and Valasek, J. (2014). Autonomous soaring using reinforcement\r
learning for trajectory generation. In 52nd Aerospace Sciences Meeting, p. 0990.\r
Woodworth, R. S. (1938). Experimental Psychology. New York: Henry Holt and Company.\r
Xie, X., Seung, H. S. (2004). Learning in neural networks by reinforcement of irregular spiking.\r
Physical Review E, 69(4):041909.\r
Xu, X., Xie, T., Hu, D., Lu, X. (2005). Kernel least-squares temporal di↵erence learning.\r
International Journal of Information Technology, 11 (9):54–63.\r
Yagishita, S., Hayashi-Takagi, A., Ellis-Davies, G. C. R., Urakubo, H., Ishii, S., Kasai, H. (2014).\r
A critical time window for dopamine actions on the structural plasticity of dendritic spines.\r
Science, 345(6204):1616–1619.\r
Yee, R. C., Saxena, S., Utgo↵, P. E., Barto, A. G. (1990). Explaining temporal di↵erences to\r
create useful concepts for evaluating states. In Proceedings of the Eighth National Conference\r
on Artificial Intelligence, pp. 882–888. AAAI Press, Menlo Park, CA.\r
Yin, H. H., Knowlton, B. J. (2006). The role of the basal ganglia in habit formation. Nature\r
Reviews Neuroscience, 7(6):464–476.\r
Young, P. (1984). Recursive Estimation and Time-Series Analysis. Springer-Verlag, Berlin.\r
Yu, H. (2010). Convergence of least squares temporal di↵erence methods under general conditions.\r
International Conference on Machine Learning 27, pp. 1207–1214.\r
Yu, H. (2012). Least squares temporal di↵erence methods: An analysis under general conditions.\r
SIAM Journal on Control and Optimization, 50 (6):3310–3343.\r
Yu, H. (2015). On convergence of emphatic temporal-di↵erence learning. In Proceedings of the\r
28th Annual Conference on Learning Theory, JMLR W&CP 40. Also ArXiv:1506.02582.\r
Yu, H. (2016). Weak convergence properties of constrained emphatic temporal-di↵erence learning\r
with constant and slowly diminishing stepsize. Journal of Machine Learning Research,\r
17 (220):1–58.\r
Yu, H. (2017). On convergence of some gradient-based temporal-di↵erences algorithms for\r
o↵-policy learning. ArXiv:1712.09652.\r
Yu, H., Mahmood, A. R., Sutton, R. S. (2017). On generalized bellman equations and temporal\u0002di↵erence learning. ArXiv:17041.04463. A summary appeared in Proceedings of the Canadian\r
Conference on Artificial Intelligence, pp. 3–14. Springer.

Index\r
Page numbers in italics are recommended to be consulted first. Page numbers in bold contain\r
boxed algorithms.\r
k-armed bandits, 25–45\r
absorbing state, 57\r
access-control queuing example, 256\r
action preferences, 322, 329, 336, 455\r
in bandit problems, 37, 42\r
action-value function, see value function, action\r
action-value methods, 321\r
for bandit problems, 27\r
actor–critic, 21, 239, 321, 331–332, 338, 406\r
advantage, A2C, 338\r
one-step (episodic), 332\r
with eligibility traces (episodic), 332\r
with eligibility traces (continuing), 333\r
neural, 395–415\r
addiction, 409–410\r
afterstates, 137, 140, 181, 182, 191, 424, 430\r
agent–environment interface, 47–58, 466\r
all-actions algorithm, 326\r
AlphaGo, AlphaGo Zero, AlphaZero, 441–450\r
Andreae, John, 17, 21, 69, 89\r
ANN, see artificial neural networks\r
applications and case studies, 421–457\r
approximate dynamic programming, 15\r
artificial intelligence, xvii, 1, 472, 478\r
artificial neural networks, 223–228, 238–240,\r
395–398, 423, 430, 436–450, 472\r
associative reinforcement learning, 45, 418\r
associative search, 41\r
asynchronous dynamic programming, 85, 88\r
Atari video game play, 436–441\r
auxiliary tasks, 460–461, 468, 474\r
average reward setting, 249–255, 258, 464\r
averagers, 264\r
backgammon, 11, 21, 182, 184, 421–426\r
backpropagation, 21, 225–227, 239, 407, 424,\r
436, 439\r
backup diagram, 60, 139\r
for dynamic programming, 59, 61, 64, 172\r
for Monte Carlo methods, 94\r
for Q-learning, 134\r
for TD(0), 121\r
for Sarsa, 129\r
for Expected Sarsa, 134\r
for Sarsa(), 304\r
for TD(), 289\r
for Q(), 313\r
for Tree Backup(), 314\r
for Truncated TD(), 296\r
for n-step Q(), 155\r
for n-step Expected Sarsa, 146\r
for n-step Sarsa, 146\r
for n-step TD, 142\r
for n-step Tree Backup, 152\r
for Samuel’s Checker Player, 428\r
compound, 288\r
half backups, 62\r
backward view of eligibility traces, 288, 293\r
Baird’s counterexample, 261–264, 280, 283, 285\r
bandit algorithm, simple, 32\r
bandit problems, 25–45\r
basal ganglia, 386\r
baseline, 37–40, 329, 330, 338\r
behavior policy, 103, 110, see o↵-policy learning\r
Bellman equation, 14\r
for v⇡, 59\r
for q⇡, 78\r
for optimal value functions: v⇤ and q⇤, 63\r
di↵erential, 250\r
for options, 463\r
Bellman error, 268, 270, 272, 273\r
learnability of, 274–278\r
vector, 267–269\r
Bellman operator, 267–269, 286\r
Bellman residual, 286, see Bellman error\r
Bellman, Richard, 14, 71, 89, 241\r
binary features, 215, 222, 245, 304, 305\r
bioreactor example, 51\r
blackjack example, 93–94, 99, 106\r
blocking maze example, 166\r
bootstrapping, 89, 189, 308\r
n-step, 141–158, 255\r
and dynamic programming, 89\r
and function approximation, 208, 264–274

520 Index\r
and Monte Carlo methods, 95\r
and stability, 263–265\r
and TD learning, 120\r
assessment of, 124–128, 248, 264, 291, 318\r
in psychology, 345, 349, 354, 355\r
parameter ( or n), 291, 307, 399\r
BOXES, 18, 71, 237\r
branching factor, 173–177, 422\r
breakfast example, 5, 22\r
bucket-brigade algorithm, 19, 21, 139\r
catastrophic interference, 472\r
certainty-equivalence estimate, 128\r
chess, 4, 20, 54, 182, 450\r
classical conditioning, 20, 343–357\r
blocking, 371\r
and higher-order conditioning, 345–355\r
delay and trace conditioning, 344\r
Rescorla-Wagner model, 346–349\r
TD model, 349–357\r
classifier systems, 19, 21\r
cli↵ walking example, 132, 133\r
CMAC, see tile coding\r
coarse coding, 215–220, 238\r
cognitive maps, 363–364\r
collective reinforcement learning, 404–407\r
complex backups, see compound update\r
compound stimulus, 345, 346–356, 371, 382\r
compound update/backup, 288, 319\r
conditioned/unconditioned stimulus, conditioned\r
response (CS/US, CR), 344\r
constant-↵ MC, 120\r
contextual bandits, 41\r
continuing tasks, 54, 57, 70, 124, 249, 294\r
continuous action, 73, 244, 335–336\r
continuous state, 73, 223, 238\r
continuous time, 11, 71\r
control and prediction, 342\r
control theory, 4, 71\r
control variates, 150–152, 155, 281\r
and eligibility traces, 309–312\r
credit assignment, 11, 17, 19, 47, 294, 401\r
in psychology, 346, 361\r
structural, 385, 405, 407\r
critic, 18, 239, 346, 417, see actor–critic\r
cumulant, 459\r
curiosity, 474\r
curse of dimensionality, 4, 14, 221, 231\r
cybernetics, xvii, 477\r
deadly triad, 264\r
deep learning, 12, 223, 441, 472–474, 479\r
deep reinforcement learning, 236\r
deep residual learning, 227\r
delayed reinforcement, 361–363\r
delayed reward, 1, 47, 249\r
dimensions of reinforcement learning methods,\r
189–191\r
direct and indirect RL, 162, 164, 192\r
discounting, 55, 199, 243, 249, 282, 324, 328,\r
427, 459\r
in pole balancing, 56\r
state dependent, 307\r
deprecated, 253, 256\r
distribution models, 159, 185\r
dopamine, 377, 381–387, 413–419\r
and addiction, 409–410\r
double learning, 134–136, 140\r
DP, see dynamic programming\r
driving-home example, 122–123\r
Dyna architecture, 164, 161–170\r
dynamic programming, 13–15, 73–90, 174, 262\r
and artificial intelligence, 89\r
and function approximation, 241\r
and options, 463\r
and the deadly triad, 264\r
computational eciency of, 87\r
eligibility traces, 287–320, 350, 362, 398–403\r
accumulating, 300, 306, 310\r
replacing, 301, 306\r
dutch, 300–303\r
contingent/non-contingent, 399–403, 411\r
o↵-policy, 309–316\r
with state-dependent  and , 309–316\r
Emphatic-TD methods, 234–235, 315\r
o↵-policy, 281–282\r
environment, 47–58\r
episodes, episodic tasks, 11, 54–58, 91\r
error reduction property, 144, 288\r
evaluative feedback, 17, 25, 47\r
evolution, 7, 359, 374, 471\r
evolutionary methods, 7, 8, 9, 11, 19\r
expected approximate value, 148, 155\r
Expected Sarsa, 133, see also Sarsa, Expected\r
expected update, 75, 172–181, 189\r
experience replay, 440–441\r
explore/exploit dilemma, 3, 103, 472\r
exploring starts, 96, 98–100, 178

Index 521\r
feature construction, 210–223\r
final time step (T), 54\r
Fourier basis, 211–215\r
function approximation, 195–200\r
gambler’s example, 84\r
game theory, 19\r
gazelle calf example, 5\r
general value functions (GVFs), 459–463, 474\r
generalized policy iteration (GPI), 86–87, 92,\r
97, 138, 189\r
genetic algorithms, 19\r
Gittins index, 43\r
gliding/soaring case study, 453–457\r
goal, see reward signal\r
golf example, 61, 63, 66\r
gradient, 201\r
gradient descent, see stochastic gradient de\u0002scent\r
Gradient-TD methods, 278–281, 314–315\r
greedy or "-greedy\r
as exploiting, 26–28\r
as shortsighted, 64\r
"-greedy policies, 100\r
gridworld examples, 60, 65, 76, 147\r
cli↵ walking, 132\r
Dyna blocking maze, 166\r
Dyna maze, 164\r
Dyna shortcut maze, 167\r
windy, 130, 131\r
habitual and goal-directed control, 364–368\r
hedonistic neurons, 402–404\r
heuristic search, 181–183, 190\r
as sequences of backups, 183\r
in Samuel’s checkers player, 426\r
in TD-Gammon, 425\r
history of reinforcement learning, 13–22\r
Holland, John, 19, 21, 44, 139, 241\r
Hull, Clark, 16, 359, 360, 362–363\r
importance sampling, 103–117, 151, 257\r
ratio, 104, 148, 258\r
weighted and ordinary, 105, 106\r
and eligibility traces, 309–312\r
and infinite variance, 106\r
discounting aware, 112–113\r
incremental implementation, 109\r
per-decision, 114–115\r
n-step, 148–156\r
incremental implementation\r
of averages, 30–33\r
of weighted averages, 109\r
instrumental conditioning, 357–361, see also\r
Law of E↵ect\r
and motivation, 360–361\r
Thorndike’s puzzle boxes, 358\r
interest and emphasis, 234–235, 282, 316\r
inverse reinforcement learning, 470\r
Jack’s car rental example, 81–82, 137, 210\r
kernel-based function approximation, 232–233\r
Klopf, A. Harry, xv, xvii, 19–21, 402–404, 411\r
latent learning, 192, 363, 366\r
Law of E↵ect, 15–16, 45, 343, 358–361, 417\r
learning automata, 18\r
Least Mean Square (LMS) algorithm, 279, 301\r
Least-Squares TD (LSTD), 228–229\r
linear function approx., 204–209, 266–269\r
linear programming, 87, 90\r
local and global optima, 200\r
Markov decision process (MDP), 2, 14, 47–71\r
Markov property, 49, 115, 465–468\r
Markov reward process (MRP), 125\r
maximization bias, 134–136\r
maximum-likelihood estimate, 128\r
MC, see Monte Carlo methods\r
Mean Square\r
Bellman Error, BE, 268\r
Projected Bellman Error, PBE, 269\r
Return Error, RE, 275\r
TD Error, TDE, 270\r
Value Error, VE, 199–200\r
memory-based function approx., 230–232\r
Michie, Donald, 17, 71, 117\r
Minsky, Marvin, 16, 17, 20, 89\r
model of the environment, 7, 159\r
model-based and model-free methods, 7, 159\r
in animal learning, 363–368\r
model-based reinforcement learning, 159–193\r
in neuroscience, 407–409\r
Monte Carlo methods, 91–117\r
first- and every-visit MC, 92\r
first-visit MC control, 101\r
first-visit MC prediction, 92

522 Index\r
gradient method for v⇡, 202\r
Monte Carlo ES (Exploring Starts), 99\r
o↵-policy control, 111, 110–112\r
o↵-policy prediction, 103–109, 110\r
Monte Carlo Tree Search (MCTS), 185–188\r
motivation, 360–361\r
mountain car example, 244–248, 305, 306\r
multi-armed bandits, 25–45\r
n-step methods, 141–158\r
Q(), 156\r
Sarsa, 147, 247\r
di↵erential, 255\r
o↵-policy, 149\r
TD, 144\r
Tree Backup, 154\r
truncated -return, 295\r
naughts and crosses, see tic-tac-toe\r
neural networks, see artificial neural networks\r
neurodynamic programming, 15\r
neuroeconomics, 413, 419\r
neuroscience, 4, 21, 377–419\r
nonstationarity, 30, 32–36, 44, 255\r
inherent, 91, 198\r
notation, xiii, xix\r
observations, 464\r
o↵-policy methods, 257–286\r
vs on-policy methods, 100, 103\r
Monte Carlo, 103–115\r
Q-learning, 131\r
Expected Sarsa, 133–134\r
n-step, 148–156\r
n-step Q(), 156\r
n-step Sarsa, 149\r
n-step Tree Backup, 154\r
and eligibility traces, 309–316\r
Emphatic-TD(), 315\r
GQ(), 315\r
GTD(), 314\r
HTD(), 315\r
Q(), 312–314\r
Tree Backup(), 312–314\r
reducing variance, 283–284\r
on-policy distribution, 175, 199, 208, 258, 262,\r
281, 282\r
vs uniform distribution, 176\r
on-policy methods, 100\r
actor–critic, 332, 333\r
approximate\r
control, 244, 247, 251, 255\r
prediction, 202, 203, 209\r
Monte Carlo, 101, 100–103, 328, 330\r
n-step, 144, 147\r
Sarsa, 130, 129–131\r
TD(0), 120, 119–128\r
with eligibility traces, 293, 300, 305, 307\r
operant conditioning, see instrumental learning\r
optimal control, 2, 14–15, 21\r
optimistic initial values, 34–35, 192\r
optimizing memory control, 432–436\r
options, 461–464\r
models of, 462\r
pain and pleasure, 6, 16, 413\r
Partially Observable MDPs (POMDPs), 467\r
Pavlov, Ivan, 16, 343–345, 362\r
Pavlovian\r
conditioning, see classical conditioning\r
control, 343, 371, 373, 478\r
personalizing web services, 450–453\r
planning, 3, 5, 7, 11, 138, 159–193\r
in psychology, 363, 364, 366\r
with learned models, 161–168, 473\r
with options, 461, 463\r
policy, 6, 41, 58\r
hierarchical, 462\r
soft and "-soft, 100–103, 110\r
policy approximation, 321–324\r
policy evaluation, 74–76, see also prediction\r
iterative, 75\r
policy gradient methods, 321–338\r
REINFORCE, 328, 330\r
actor–critic, 332, 333\r
policy gradient theorem, 324–326\r
proof, episodic case, 325\r
proof, continuing case, 334\r
policy improvement, 76–80\r
theorem, 78, 101\r
policy iteration, 14, 80, 80–82\r
polynomial basis, 210–211\r
prediction, 74–76, see also policy evaluation\r
and control, 342\r
Monte Carlo, 92–97\r
o↵-policy, 103–108\r
TD, 119–126\r
with approximation, 197–242\r
prior knowledge, 12, 34, 54, 137, 236, 324, 471

Index 523\r
prioritized sweeping, 170, 168–171\r
projected Bellman error, 285\r
vector, 267, 269\r
proximal TD methods, 286\r
pseudo termination, 282, 308\r
psychology, 4, 13, 19, 20, 341–376\r
Q(), Watkins’s, 312–314\r
Q-function, see action-value function\r
Q-learning, 21, 131, 131–135\r
double, 136\r
Q-planning, 161\r
Q(), 156, 154–156\r
queuing example, 252\r
R-learning, 256\r
racetrack exercise, 111\r
radial basis functions (RBFs), 221–222\r
random walk, 95\r
5-state, 125, 126, 127\r
19-state, 144, 291\r
TD() results on, 294, 295, 299\r
1000-state, 203–209, 217, 218\r
Fourier and polynomial bases, 214\r
real-time dynamic programming, 177–180\r
recycling robot example, 52\r
REINFORCE, 328, 326–331\r
with baseline, 330\r
reinforcement learning, 1–22\r
reinforcement signal, 380\r
representation learning, 473\r
residual-gradient algorithm, 272–274, 277\r
naive, 270, 271\r
return, 54–58\r
n-step, 143\r
for Q(), 155\r
for action values, 146\r
for Expected Sarsa, 148\r
for Tree Backup, 153\r
with control variates, 150, 151\r
with function approximation, 209\r
di↵erential, 250, 255, 334\r
flat partial, 113\r
with state-dependent termination, 308\r
-return, 288–291\r
truncated, 296\r
reward prediction error hypothesis, 381–383,\r
387–395\r
reward signal, 1, 6, 48, 53, 361, 380, 383, 397\r
and reinforcement, 373–375, 380–381\r
design of, 469–472, 477\r
intrinsic, 474\r
sparse, 469–470\r
rod maneuvering example, 171\r
rollout algorithms, 183–185\r
root mean square (RMS) error, 125\r
safety, 434, 478\r
sample and expected updates, 121, 170–174\r
sample or simulation model, 115\r
sample-average method, 27\r
Samuel’s checkers player, 20, 241, 426–429\r
Sarsa, 130, 129–131, 244\r
vs Q-learning, 132\r
di↵erential, one-step, 251\r
Expected, 133–134, 140\r
n-step, 148\r
n-step o↵-policy, 150\r
double, 136\r
n-step, 147, 145–148, 247\r
di↵erential, 255\r
o↵-policy, 149\r
Sarsa(), 305, 303–307\r
true online, 307\r
Schultz, Wolfram, 387–395, 410\r
search control, 163\r
secondary reinforcement, 20, 346, 354, 369\r
selective bootstrap adaptation, 239\r
semi-gradient methods, 202, 258–259\r
SGD, see stochastic gradient descent\r
Shannon, Claude, 16, 20, 71, 426\r
shaping, 360, 470\r
Skinner, B. F., 359–360, 375, 470, 479\r
soap bubble example, 95\r
soft and "-soft policies, 100–103, 110\r
soft-max, 322–323, 329, 336, 400, 445, 455\r
for bandits, 37, 45\r
spike-timing-dependent plasticity (STDP), 401\r
state, 7, 48, 49\r
kth-order history approach, 468\r
and observations, 464–468\r
Markov property, 465–468\r
belief, 467\r
latent, 467\r
observable operator models (OOMs), 467\r
partially observable MDPs, 14, 467\r
predictive state representations, 467\r
state-update function, 465

524 Index\r
state aggregation, 203–204\r
state-update function, 465\r
step-size parameter, 10, 31–33, 120, 125, 126\r
automatic adaptation, 238\r
in DQN, 439, 440\r
in psychological models, 347, 348\r
selecting manually, 222–223\r
with coarse coding, 216\r
with Fourier features, 213\r
with tile coding, 217, 223\r
stochastic approx. convergence conditions, 33\r
stochastic gradient descent (SGD), 200–204\r
in the Bellman error, 269–278\r
strong and weak methods, 4\r
supervised learning, xvii, 2, 17–19, 198\r
sweeps, 75, 160, see also prioritized sweeping\r
synaptic plasticity, 379\r
Hebbian, 400\r
two-factor and three factor, 400\r
system identification, 364\r
tabular solution methods, 23\r
target\r
policy, 103, 110\r
of update, 31, 143, 198\r
TD, see temporal-di↵erence learning\r
TD error, 121\r
n-step, 255\r
di↵erential, 250\r
with function approximation, 270\r
TD(), 293, 292–295\r
truncated, 295–297\r
true online, 300, 299–301\r
TD-Gammon, 21, 421–426\r
temporal abstraction, 461–464\r
temporal-di↵erence learning, 10, 119–140\r
history of, 20–21\r
advantages of, 124–126\r
optimality of, 126–128\r
TD(0), 120, 203\r
TD(1), 294\r
TD(), 293, 292–295\r
true online, 300, 299–301\r
-return methods\r
o↵-line, 290\r
online, 297–299\r
n-step, 144, 141–158, 209\r
termination function, 307, 459\r
Thompson sampling, 43, 45\r
Thorndike, Edward, see Law of E↵ect\r
tic-tac-toe, 8–13, 17, 137\r
tile coding, 217–221, 223, 238, 246, 434, 435\r
Tolman, Edward, 364, 408\r
trace-decay parameter (), 287, 289, 290, 292\r
state dependent, 307\r
trajectory sampling, 174–177\r
transition probabilities, 49\r
Tree Backup\r
n-step, 152–153, 154\r
Tree-Backup(), 312–314\r
trial-and-error, 1, 7, 15–21, 403, 404, see also\r
instrumental conditioning\r
true online TD(), 300, 299–301\r
Tsitsiklis and Van Roy’s Counterexample, 263\r
undiscounted continuing tasks, see average re\u0002ward setting\r
unsupervised learning, 2, 226\r
value, 6, 26, 47\r
value function, 6, 58–67\r
for a given policy: v⇡ and q⇡, 58\r
for an optimal policy: v⇤ and q⇤, 62\r
action, 58, 63, 65, 71, 129, 131\r
approximate action values: qˆ(s, a, w), 243\r
approximate state values: ˆv(s,w), 197\r
di↵erential, 243\r
vs evolutionary methods, 11\r
value iteration, 83, 82–84\r
value-function approximation, 198\r
Watkins, Chris, 15, 21, 89, 320\r
Watson (Jeopardy! player), 429–432\r
Werbos, Paul, 14, 21, 70, 89, 139, 239\r
Witten, Ian, 21, 70

525\r
Adaptive Computation and Machine Learning\r
Francis Bach, Editor\r
Bioinformatics: The Machine Learning Approach, Pierre Baldi and Søren Brunak\r
Reinforcement Learning: An Introduction, Richard S. Sutton and Andrew G. Barto\r
Graphical Models for Machine Learning and Digital Communication, Brendan J. Frey\r
Learning in Graphical Models, Michael I. Jordan\r
Causation, Prediction, and Search, second edition, Peter Spirtes, Clark Glymour, and\r
Richard Scheines\r
Principles of Data Mining, David Hand, Heikki Mannila, and Padhraic Smyth\r
Bioinformatics: The Machine Learning Approach, second edition, Pierre Baldi and Søren\r
Brunak\r
Learning Kernel Classifiers: Theory and Algorithms, Ralf Herbrich\r
Learning with Kernels: Support Vector Machines, Regularization, Optimization, and\r
Beyond, Bernhard Sch¨olkopf and Alexander J. Smola\r
Introduction to Machine Learning, Ethem Alpaydin\r
Gaussian Processes for Machine Learning, Carl Edward Rasmussen and Christopher K.I.\r
Williams\r
Semi-Supervised Learning, Olivier Chapelle, Bernhard Sch¨olkopf, and Alexander Zien,\r
Eds.\r
The Minimum Description Length Principle, Peter D. Gr¨unwald\r
Introduction to Statistical Relational Learning, Lise Getoor and Ben Taskar, Eds.

526\r
Probabilistic Graphical Models: Principles and Techniques, Daphne Koller and Nir Fried\u0002man\r
Introduction to Machine Learning, second edition, Ethem Alpaydin\r
Machine Learning in Non-Stationary Environments: Introduction to Covariate Shift\r
Adaptation, Masashi Sugiyama and Motoaki Kawanabe\r
Boosting: Foundations and Algorithms, Robert E. Schapire and Yoav Freund\r
Machine Learning: A Probabilistic Perspective, Kevin P. Murphy\r
Foundations of Machine Learning, Mehryar Mohri, Afshin Rostami, and Ameet Talwalker\r
Introduction to Machine Learning, third edition, Ethem Alpaydin\r
Deep Learning, Ian Goodfellow, Yoshua Bengio, and Aaron Courville\r
Elements of Causal Inference, Jonas Peters, Dominik Janzing, and Bernhard Sch¨olkopf\r
Machine Learning for Data Streams, with Practical Examples in MOA, Albert Bifet,\r
Ricard Gavald`a, Geo↵rey Holmes, Bernhard Pfahringer"""

[metadata]
title = "sutton barto 2020 rl intro"
authors = ["Unknown"]
year = 2020

[[sections]]
number = "0"
title = "Preamble"
text = """
ii

Adaptive Computation and Machine Learning\r
Francis Bach, series editor\r
A complete list of books published in the Adaptive Computation and Machine Learning\r
series appears at the back of this book.

Reinforcement Learning:\r
An Introduction\r
second edition\r
Richard S. Sutton and Andrew G. Barto\r
The MIT Press\r
Cambridge, Massachusetts\r
London, England

© 2018, 2020 Richard S. Sutton and Andrew G. Barto\r
All rights reserved. No part of this book may be reproduced in any form by any electronic\r
or mechanical means (including photocopying, recording, or information storage and retrieval)\r
without permission in writing from the copyright holder. This work is licensed under the\r
Creative Commons Attribution-NonCommercial-NoDerivs 2.0 Generic License. To view a copy\r
of this license, visit http://creativecommons.org/licenses/by-nc-nd/2.0/ or send a letter\r
to Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.\r
This book was set in 10/12, CMR by Westchester Publishing Services. Printed and bound in\r
the United States of America.\r
Library of Congress Cataloging-in-Publication Data\r
Names: Sutton, Richard S., author. | Barto, Andrew G., author.\r
Title: Reinforcement learning : an introduction / Richard S. Sutton and Andrew G. Barto.\r
Description: Second edition. | Cambridge, MA : The MIT Press, [2018] | Series: Adaptive\r
computation and machine learning series | Includes bibliographical references and index.\r
Identifiers: LCCN 2018023826 | ISBN 9780262039246 (hardcover : alk. paper)\r
Subjects: LCSH: Reinforcement learning.\r
Classification: LCC Q325.6 .R45 2018 | DDC 006.3/1--dc23 LC record available\r
at https://lccn.loc.gov/2018023826\r
10 9 8 7 6 5 4 3 2 1

In memory of A. Harry Klopf

Contents\r
Preface to the Second Edition xiii\r
Preface to the First Edition xvii\r
Summary of Notation xix"""

[[sections]]
number = "1"
title = "Introduction 1"
text = ""

[[sections]]
number = "1.1"
title = "Reinforcement Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1"
text = "1.2 Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4"

[[sections]]
number = "1.3"
title = "Elements of Reinforcement Learning . . . . . . . . . . . . . . . . . . . . . 6"
text = "1.4 Limitations and Scope . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7"

[[sections]]
number = "1.5"
title = "An Extended Example: Tic-Tac-Toe . . . . . . . . . . . . . . . . . . . . . 8"
text = "1.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13"

[[sections]]
number = "1.7"
title = "Early History of Reinforcement Learning . . . . . . . . . . . . . . . . . . . 13"
text = "I Tabular Solution Methods 23"

[[sections]]
number = "2"
title = "Multi-armed Bandits 25"
text = ""

[[sections]]
number = "2.1"
title = "A k-armed Bandit Problem . . . . . . . . . . . . . . . . . . . . . . . . . . 25"
text = """
2.2 Action-value Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\r
2.3 The 10-armed Testbed . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\r
2.4 Incremental Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . 30"""

[[sections]]
number = "2.5"
title = "Tracking a Nonstationary Problem . . . . . . . . . . . . . . . . . . . . . . 32"
text = "2.6 Optimistic Initial Values . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34"

[[sections]]
number = "2.7"
title = "Upper-Confidence-Bound Action Selection . . . . . . . . . . . . . . . . . . 35"
text = "2.8 Gradient Bandit Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . 37"

[[sections]]
number = "2.9"
title = "Associative Search (Contextual Bandits) . . . . . . . . . . . . . . . . . . . 41"
text = """
2.10 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42

viii Contents"""

[[sections]]
number = "3"
title = "Finite Markov Decision Processes 47"
text = ""

[[sections]]
number = "3.1"
title = "The Agent–Environment Interface . . . . . . . . . . . . . . . . . . . . . . 47"
text = """
3.2 Goals and Rewards . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53\r
3.3 Returns and Episodes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54"""

[[sections]]
number = "3.4"
title = "Unified Notation for Episodic and Continuing Tasks . . . . . . . . . . . . 57"
text = "3.5 Policies and Value Functions . . . . . . . . . . . . . . . . . . . . . . . . . 58"

[[sections]]
number = "3.6"
title = "Optimal Policies and Optimal Value Functions . . . . . . . . . . . . . . . 62"
text = ""

[[sections]]
number = "3.7"
title = "Optimality and Approximation . . . . . . . . . . . . . . . . . . . . . . . . 67"
text = "3.8 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68"

[[sections]]
number = "4"
title = "Dynamic Programming 73"
text = """
4.1 Policy Evaluation (Prediction) . . . . . . . . . . . . . . . . . . . . . . . . 74\r
4.2 Policy Improvement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76\r
4.3 Policy Iteration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80\r
4.4 Value Iteration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82"""

[[sections]]
number = "4.5"
title = "Asynchronous Dynamic Programming . . . . . . . . . . . . . . . . . . . . 85"
text = "4.6 Generalized Policy Iteration . . . . . . . . . . . . . . . . . . . . . . . . . . 86"

[[sections]]
number = "4.7"
title = "Eciency of Dynamic Programming . . . . . . . . . . . . . . . . . . . . . 87"
text = "4.8 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88"

[[sections]]
number = "5"
title = "Monte Carlo Methods 91"
text = "5.1 Monte Carlo Prediction . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92"

[[sections]]
number = "5.2"
title = "Monte Carlo Estimation of Action Values . . . . . . . . . . . . . . . . . . 96"
text = "5.3 Monte Carlo Control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97"

[[sections]]
number = "5.4"
title = "Monte Carlo Control without Exploring Starts . . . . . . . . . . . . . . . 100"
text = ""

[[sections]]
number = "5.5"
title = "O↵-policy Prediction via Importance Sampling . . . . . . . . . . . . . . . 103"
text = """
5.6 Incremental Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . 109\r
5.7 O↵-policy Monte Carlo Control . . . . . . . . . . . . . . . . . . . . . . . . 110\r
5.8 *Discounting-aware Importance Sampling . . . . . . . . . . . . . . . . . . 112\r
5.9 *Per-decision Importance Sampling . . . . . . . . . . . . . . . . . . . . . . 114\r
5.10 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115"""

[[sections]]
number = "6"
title = "Temporal-Di↵erence Learning 119"
text = "6.1 TD Prediction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 119"

[[sections]]
number = "6.2"
title = "Advantages of TD Prediction Methods . . . . . . . . . . . . . . . . . . . . 124"
text = """
6.3 Optimality of TD(0) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 126\r
6.4 Sarsa: On-policy TD Control . . . . . . . . . . . . . . . . . . . . . . . . . 129"""

[[sections]]
number = "6.5"
title = "Q-learning: O↵-policy TD Control . . . . . . . . . . . . . . . . . . . . . . 131"
text = "6.6 Expected Sarsa . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133"

[[sections]]
number = "6.7"
title = "Maximization Bias and Double Learning . . . . . . . . . . . . . . . . . . . 134"
text = ""

[[sections]]
number = "6.8"
title = "Games, Afterstates, and Other Special Cases . . . . . . . . . . . . . . . . 136"
text = """
6.9 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 138

Contents ix\r
7 n-step Bootstrapping 141\r
7.1 n-step TD Prediction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 142\r
7.2 n-step Sarsa . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 145\r
7.3 n-step O↵-policy Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . 148\r
7.4 *Per-decision Methods with Control Variates . . . . . . . . . . . . . . . . 150"""

[[sections]]
number = "7.5"
title = "O↵-policy Learning Without Importance Sampling:"
text = """
The n-step Tree Backup Algorithm . . . . . . . . . . . . . . . . . . . . . . 152\r
7.6 *A Unifying Algorithm: n-step Q()..................... 154\r
7.7 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 157"""

[[sections]]
number = "8"
title = "Planning and Learning with Tabular Methods 159"
text = "8.1 Models and Planning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 159"

[[sections]]
number = "8.2"
title = "Dyna: Integrated Planning, Acting, and Learning . . . . . . . . . . . . . . 161"
text = """
8.3 When the Model Is Wrong . . . . . . . . . . . . . . . . . . . . . . . . . . . 166\r
8.4 Prioritized Sweeping . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 168\r
8.5 Expected vs. Sample Updates . . . . . . . . . . . . . . . . . . . . . . . . . 172\r
8.6 Trajectory Sampling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 174"""

[[sections]]
number = "8.7"
title = "Real-time Dynamic Programming . . . . . . . . . . . . . . . . . . . . . . . 177"
text = """
8.8 Planning at Decision Time . . . . . . . . . . . . . . . . . . . . . . . . . . . 180\r
8.9 Heuristic Search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 181\r
8.10 Rollout Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 183\r
8.11 Monte Carlo Tree Search . . . . . . . . . . . . . . . . . . . . . . . . . . . . 185\r
8.12 Summary of the Chapter . . . . . . . . . . . . . . . . . . . . . . . . . . . . 188\r
8.13 Summary of Part I: Dimensions . . . . . . . . . . . . . . . . . . . . . . . . 189\r
II Approximate Solution Methods 195"""

[[sections]]
number = "9"
title = "On-policy Prediction with Approximation 197"
text = """
9.1 Value-function Approximation . . . . . . . . . . . . . . . . . . . . . . . . . 198\r
9.2 The Prediction Objective (VE) . . . . . . . . . . . . . . . . . . . . . . . . 199"""

[[sections]]
number = "9.3"
title = "Stochastic-gradient and Semi-gradient Methods . . . . . . . . . . . . . . . 200"
text = "9.4 Linear Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 204"

[[sections]]
number = "9.5"
title = "Feature Construction for Linear Methods . . . . . . . . . . . . . . . . . . 210"
text = ""

[[sections]]
number = "9.5.1"
title = "Polynomials . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 210"
text = ""

[[sections]]
number = "9.5.2"
title = "Fourier Basis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 211"
text = ""

[[sections]]
number = "9.5.3"
title = "Coarse Coding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 215"
text = ""

[[sections]]
number = "9.5.4"
title = "Tile Coding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 217"
text = ""

[[sections]]
number = "9.5.5"
title = "Radial Basis Functions . . . . . . . . . . . . . . . . . . . . . . . . . 221"
text = "9.6 Selecting Step-Size Parameters Manually . . . . . . . . . . . . . . . . . . . 222"

[[sections]]
number = "9.7"
title = "Nonlinear Function Approximation: Artificial Neural Networks . . . . . . 223"
text = """
9.8 Least-Squares TD . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 228

x Contents"""

[[sections]]
number = "9.9"
title = "Memory-based Function Approximation . . . . . . . . . . . . . . . . . . . 230"
text = ""

[[sections]]
number = "9.10"
title = "Kernel-based Function Approximation . . . . . . . . . . . . . . . . . . . . 232"
text = ""

[[sections]]
number = "9.11"
title = "Looking Deeper at On-policy Learning: Interest and Emphasis . . . . . . 234"
text = "9.12 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 236"

[[sections]]
number = "10"
title = "On-policy Control with Approximation 243"
text = """
10.1 Episodic Semi-gradient Control . . . . . . . . . . . . . . . . . . . . . . . . 243\r
10.2 Semi-gradient n-step Sarsa . . . . . . . . . . . . . . . . . . . . . . . . . . 247"""

[[sections]]
number = "10.3"
title = "Average Reward: A New Problem Setting for Continuing Tasks . . . . . . 249"
text = """
10.4 Deprecating the Discounted Setting . . . . . . . . . . . . . . . . . . . . . . 253\r
10.5 Di↵erential Semi-gradient n-step Sarsa . . . . . . . . . . . . . . . . . . . . 255\r
10.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 256\r
11 *O↵-policy Methods with Approximation 257\r
11.1 Semi-gradient Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 258"""

[[sections]]
number = "11.2"
title = "Examples of O↵-policy Divergence . . . . . . . . . . . . . . . . . . . . . . 260"
text = "11.3 The Deadly Triad . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 264"

[[sections]]
number = "11.4"
title = "Linear Value-function Geometry . . . . . . . . . . . . . . . . . . . . . . . 266"
text = "11.5 Gradient Descent in the Bellman Error . . . . . . . . . . . . . . . . . . . . 269"

[[sections]]
number = "11.6"
title = "The Bellman Error is Not Learnable . . . . . . . . . . . . . . . . . . . . . 274"
text = """
11.7 Gradient-TD Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 278\r
11.8 Emphatic-TD Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 281\r
11.9 Reducing Variance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 283\r
11.10 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 284"""

[[sections]]
number = "12"
title = "Eligibility Traces 287"
text = "12.1 The -return . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 288"

[[sections]]
number = "12.2"
title = "TD()...................................... 292"
text = "12.3 n-step Truncated -return Methods . . . . . . . . . . . . . . . . . . . . . 295"

[[sections]]
number = "12.4"
title = "Redoing Updates: Online -return Algorithm . . . . . . . . . . . . . . . . 297"
text = ""

[[sections]]
number = "12.5"
title = "True Online TD()............................... 299"
text = "12.6 *Dutch Traces in Monte Carlo Learning . . . . . . . . . . . . . . . . . . . 301"

[[sections]]
number = "12.7"
title = "Sarsa()..................................... 303"
text = ""

[[sections]]
number = "12.8"
title = "Variable  and  ................................ 307"
text = ""

[[sections]]
number = "12.9"
title = "O↵-policy Traces with Control Variates . . . . . . . . . . . . . . . . . . . 309"
text = ""

[[sections]]
number = "12.10"
title = "Watkins’s Q() to Tree-Backup()...................... 312"
text = ""

[[sections]]
number = "12.11"
title = "Stable O↵-policy Methods with Traces . . . . . . . . . . . . . . . . . . . 314"
text = """
12.12 Implementation Issues . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 316\r
12.13 Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 317

Contents xi"""

[[sections]]
number = "13"
title = "Policy Gradient Methods 321"
text = ""

[[sections]]
number = "13.1"
title = "Policy Approximation and its Advantages . . . . . . . . . . . . . . . . . . 322"
text = "13.2 The Policy Gradient Theorem . . . . . . . . . . . . . . . . . . . . . . . . . 324"

[[sections]]
number = "13.3"
title = "REINFORCE: Monte Carlo Policy Gradient . . . . . . . . . . . . . . . . . 326"
text = ""

[[sections]]
number = "13.4"
title = "REINFORCE with Baseline . . . . . . . . . . . . . . . . . . . . . . . . . . 329"
text = "13.5 Actor–Critic Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 331"

[[sections]]
number = "13.6"
title = "Policy Gradient for Continuing Problems . . . . . . . . . . . . . . . . . . 333"
text = ""

[[sections]]
number = "13.7"
title = "Policy Parameterization for Continuous Actions . . . . . . . . . . . . . . . 335"
text = """
13.8 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 337\r
III Looking Deeper 339"""

[[sections]]
number = "14"
title = "Psychology 341"
text = """
14.1 Prediction and Control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 342\r
14.2 Classical Conditioning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 343"""

[[sections]]
number = "14.2.1"
title = "Blocking and Higher-order Conditioning . . . . . . . . . . . . . . . 345"
text = ""

[[sections]]
number = "14.2.2"
title = "The Rescorla–Wagner Model . . . . . . . . . . . . . . . . . . . . . 346"
text = ""

[[sections]]
number = "14.2.3"
title = "The TD Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 349"
text = ""

[[sections]]
number = "14.2.4"
title = "TD Model Simulations . . . . . . . . . . . . . . . . . . . . . . . . . 350"
text = """
14.3 Instrumental Conditioning . . . . . . . . . . . . . . . . . . . . . . . . . . . 357\r
14.4 Delayed Reinforcement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 361\r
14.5 Cognitive Maps . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 363\r
14.6 Habitual and Goal-directed Behavior . . . . . . . . . . . . . . . . . . . . . 364\r
14.7 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 368"""

[[sections]]
number = "15"
title = "Neuroscience 377"
text = "15.1 Neuroscience Basics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 378"

[[sections]]
number = "15.2"
title = "Reward Signals, Reinforcement Signals, Values, and Prediction Errors . . 380"
text = ""

[[sections]]
number = "15.3"
title = "The Reward Prediction Error Hypothesis . . . . . . . . . . . . . . . . . . 381"
text = "15.4 Dopamine . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 383"

[[sections]]
number = "15.5"
title = "Experimental Support for the Reward Prediction Error Hypothesis . . . . 387"
text = ""

[[sections]]
number = "15.6"
title = "TD Error/Dopamine Correspondence . . . . . . . . . . . . . . . . . . . . . 390"
text = """
15.7 Neural Actor–Critic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 395\r
15.8 Actor and Critic Learning Rules . . . . . . . . . . . . . . . . . . . . . . . 398\r
15.9 Hedonistic Neurons . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 402\r
15.10 Collective Reinforcement Learning . . . . . . . . . . . . . . . . . . . . . . 404"""

[[sections]]
number = "15.11"
title = "Model-based Methods in the Brain . . . . . . . . . . . . . . . . . . . . . . 407"
text = """
15.12 Addiction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 409\r
15.13 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 410

xii Contents"""

[[sections]]
number = "16"
title = "Applications and Case Studies 421"
text = """
16.1 TD-Gammon . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 421\r
16.2 Samuel’s Checkers Player . . . . . . . . . . . . . . . . . . . . . . . . . . . 426"""

[[sections]]
number = "16.3"
title = "Watson’s Daily-Double Wagering . . . . . . . . . . . . . . . . . . . . . . . 429"
text = "16.4 Optimizing Memory Control . . . . . . . . . . . . . . . . . . . . . . . . . . 432"

[[sections]]
number = "16.5"
title = "Human-level Video Game Play . . . . . . . . . . . . . . . . . . . . . . . . 436"
text = "16.6 Mastering the Game of Go . . . . . . . . . . . . . . . . . . . . . . . . . . . 441"

[[sections]]
number = "16.6.1"
title = "AlphaGo . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 444"
text = ""

[[sections]]
number = "16.6.2"
title = "AlphaGo Zero . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 447"
text = """
16.7 Personalized Web Services . . . . . . . . . . . . . . . . . . . . . . . . . . . 450\r
16.8 Thermal Soaring . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 453"""

[[sections]]
number = "17"
title = "Frontiers 459"
text = ""

[[sections]]
number = "17.1"
title = "General Value Functions and Auxiliary Tasks . . . . . . . . . . . . . . . . 459"
text = ""

[[sections]]
number = "17.2"
title = "Temporal Abstraction via Options . . . . . . . . . . . . . . . . . . . . . . 461"
text = """
17.3 Observations and State . . . . . . . . . . . . . . . . . . . . . . . . . . . . 464\r
17.4 Designing Reward Signals . . . . . . . . . . . . . . . . . . . . . . . . . . . 469\r
17.5 Remaining Issues . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 472"""

[[sections]]
number = "17.6"
title = "Reinforcement Learning and the Future of Artificial Intelligence . . . . . . 475"
text = """
References 481\r
Index 519

Preface to the Second Edition\r
The twenty years since the publication of the first edition of this book have seen tremendous\r
progress in artificial intelligence, propelled in large part by advances in machine learning,\r
including advances in reinforcement learning. Although the impressive computational\r
power that became available is responsible for some of these advances, new developments\r
in theory and algorithms have been driving forces as well. In the face of this progress, a\r
second edition of our 1998 book was long overdue, and we finally began the project in\r
2012. Our goal for the second edition was the same as our goal for the first: to provide a\r
clear and simple account of the key ideas and algorithms of reinforcement learning that\r
is accessible to readers in all the related disciplines. The edition remains an introduction,\r
and we retain a focus on core, online learning algorithms. This edition includes some new\r
topics that rose to importance over the intervening years, and we expanded coverage of\r
topics that we now understand better. But we made no attempt to provide comprehensive\r
coverage of the field, which has exploded in many di↵erent directions. We apologize for\r
having to leave out all but a handful of these contributions.\r
As in the first edition, we chose not to produce a rigorous formal treatment of\r
reinforcement learning, or to formulate it in the most general terms. However, our deeper\r
understanding of some topics since the first edition required a bit more mathematics\r
to explain; we have set o↵ the more mathematical parts in shaded boxes that the non\u0002mathematically-inclined may choose to skip. We also use a slightly di↵erent notation\r
than was used in the first edition. In teaching, we have found that the new notation\r
helps to address some common points of confusion. It emphasizes the di↵erence between\r
random variables, denoted with capital letters, and their instantiations, denoted in lower\r
case. For example, the state, action, and reward at time step t are denoted St, At,\r
and Rt, while their possible values might be denoted s, a, and r. Along with this, it is\r
natural to use lower case for value functions (e.g., v⇡) and restrict capitals to their tabular\r
estimates (e.g., Qt(s, a)). Approximate value functions are deterministic functions of\r
random parameters and are thus also in lower case (e.g., vˆ(s,wt) ⇡ v⇡(s)). Vectors, such\r
as the weight vector wt (formerly ✓t) and the feature vector xt (formerly t), are bold\r
and written in lowercase even if they are random variables. Uppercase bold is reserved for\r
matrices. In the first edition we used special notations, Pa\r
ss0 and Ra\r
ss0 , for the transition\r
probabilities and expected rewards. One weakness of that notation is that it still did not\r
fully characterize the dynamics of the rewards, giving only their expectations, which is\r
sucient for dynamic programming but not for reinforcement learning. Another weakness

xiv Preface to the Second Edition\r
is the excess of subscripts and superscripts. In this edition we use the explicit notation of\r
p(s0, r|s, a) for the joint probability for the next state and reward given the current state\r
and action. All the changes in notation are summarized in a table on page xix.\r
The second edition is significantly expanded, and its top-level organization has been\r
changed. After the introductory first chapter, the second edition is divided into three new\r
parts. The first part (Chapters 2–8) treats as much of reinforcement learning as possible\r
without going beyond the tabular case for which exact solutions can be found. We cover\r
both learning and planning methods for the tabular case, as well as their unification\r
in n-step methods and in Dyna. Many algorithms presented in this part are new to\r
the second edition, including UCB, Expected Sarsa, Double learning, tree-backup, Q(),\r
RTDP, and MCTS. Doing the tabular case first, and thoroughly, enables core ideas to be\r
developed in the simplest possible setting. The second part of the book (Chapters 9–13)\r
is then devoted to extending the ideas to function approximation. It has new sections on\r
artificial neural networks, the fourier basis, LSTD, kernel-based methods, Gradient-TD\r
and Emphatic-TD methods, average-reward methods, true online TD(), and policy\u0002gradient methods. The second edition significantly expands the treatment of o↵-policy\r
learning, first for the tabular case in Chapters 5–7, then with function approximation in\r
Chapters 11 and 12. Another change is that the second edition separates the forward-view\r
idea of n-step bootstrapping (now treated more fully in Chapter 7) from the backward\u0002view idea of eligibility traces (now treated independently in Chapter 12). The third part\r
of the book has large new chapters on reinforcement learning’s relationships to psychology\r
(Chapter 14) and neuroscience (Chapter 15), as well as an updated case-studies chapter\r
including Atari game playing, Watson’s wagering strategy, and the Go playing programs\r
AlphaGo and AlphaGo Zero (Chapter 16). Still, out of necessity we have included only a\r
small subset of all that has been done in the field. Our choices reflect our long-standing\r
interests in inexpensive model-free methods that should scale well to large applications.\r
The final chapter now includes a discussion of the future societal impacts of reinforcement\r
learning. For better or worse, the second edition is about twice as large as the first.\r
This book is designed to be used as the primary text for a one- or two-semester\r
course on reinforcement learning. For a one-semester course, the first ten chapters should\r
be covered in order and form a good core, to which can be added material from the\r
other chapters, from other books such as Bertsekas and Tsitsiklis (1996), Wiering and\r
van Otterlo (2012), and Szepesv´ari (2010), or from the literature, according to taste.\r
Depending of the students’ background, some additional material on online supervised\r
learning may be helpful. The ideas of options and option models are a natural addition\r
(Sutton, Precup and Singh, 1999). A two-semester course can cover all the chapters as\r
well as supplementary material. The book can also be used as part of broader courses\r
on machine learning, artificial intelligence, or neural networks. In this case, it may be\r
desirable to cover only a subset of the material. We recommend covering Chapter 1 for a\r
brief overview, Chapter 2 through Section 2.4, Chapter 3, and then selecting sections\r
from the remaining chapters according to time and interests. Chapter 6 is the most\r
important for the subject and for the rest of the book. A course focusing on machine\r
learning or neural networks should cover Chapters 9 and 10, and a course focusing on\r
artificial intelligence or planning should cover Chapter 8. Throughout the book, sections\r
and chapters that are more dicult and not essential to the rest of the book are marked

Preface to the Second Edition xv\r
with a ⇤. These can be omitted on first reading without creating problems later on. Some\r
exercises are also marked with a ⇤ to indicate that they are more advanced and not\r
essential to understanding the basic material of the chapter.\r
Most chapters end with a section entitled “Bibliographical and Historical Remarks,”\r
wherein we credit the sources of the ideas presented in that chapter, provide pointers to\r
further reading and ongoing research, and describe relevant historical background. Despite\r
our attempts to make these sections authoritative and complete, we have undoubtedly left\r
out some important prior work. For that we again apologize, and we welcome corrections\r
and extensions for incorporation into the electronic version of the book.\r
Like the first edition, this edition of the book is dedicated to the memory of A. Harry\r
Klopf. It was Harry who introduced us to each other, and it was his ideas about the brain\r
and artificial intelligence that launched our long excursion into reinforcement learning.\r
Trained in neurophysiology and long interested in machine intelligence, Harry was a\r
senior scientist aliated with the Avionics Directorate of the Air Force Oce of Scientific\r
Research (AFOSR) at Wright-Patterson Air Force Base, Ohio. He was dissatisfied with\r
the great importance attributed to equilibrium-seeking processes, including homeostasis\r
and error-correcting pattern classification methods, in explaining natural intelligence\r
and in providing a basis for machine intelligence. He noted that systems that try to\r
maximize something (whatever that might be) are qualitatively di↵erent from equilibrium\u0002seeking systems, and he argued that maximizing systems hold the key to understanding\r
important aspects of natural intelligence and for building artificial intelligences. Harry was\r
instrumental in obtaining funding from AFOSR for a project to assess the scientific merit\r
of these and related ideas. This project was conducted in the late 1970s at the University\r
of Massachusetts Amherst (UMass Amherst), initially under the direction of Michael\r
Arbib, William Kilmer, and Nico Spinelli, professors in the Department of Computer\r
and Information Science at UMass Amherst, and founding members of the Cybernetics\r
Center for Systems Neuroscience at the University, a farsighted group focusing on the\r
intersection of neuroscience and artificial intelligence. Barto, a recent PhD from the\r
University of Michigan, was hired as post doctoral researcher on the project. Meanwhile,\r
Sutton, an undergraduate studying computer science and psychology at Stanford, had\r
been corresponding with Harry regarding their mutual interest in the role of stimulus\r
timing in classical conditioning. Harry suggested to the UMass group that Sutton would\r
be a great addition to the project. Thus, Sutton became a UMass graduate student,\r
whose PhD was directed by Barto, who had become an Associate Professor. The study\r
of reinforcement learning as presented in this book is rightfully an outcome of that\r
project instigated by Harry and inspired by his ideas. Further, Harry was responsible\r
for bringing us, the authors, together in what has been a long and enjoyable interaction.\r
By dedicating this book to Harry we honor his essential contributions, not only to the\r
field of reinforcement learning, but also to our collaboration. We also thank Professors\r
Arbib, Kilmer, and Spinelli for the opportunity they provided to us to begin exploring\r
these ideas. Finally, we thank AFOSR for generous support over the early years of our\r
research, and the NSF for its generous support over many of the following years.\r
We have very many people to thank for their inspiration and help with this second\r
edition. Everyone we acknowledged for their inspiration and help with the first edition

xvi Preface to the Second Edition\r
deserve our deepest gratitude for this edition as well, which would not exist were it not\r
for their contributions to edition number one. To that long list we must add many others\r
who contributed specifically to the second edition. Our students over the many years that\r
we have taught this material contributed in countless ways: exposing errors, o↵ering fixes,\r
and—not the least—being confused in places where we could have explained things better.\r
We especially thank Martha Steenstrup for reading and providing detailed comments\r
throughout. The chapters on psychology and neuroscience could not have been written\r
without the help of many experts in those fields. We thank John Moore for his patient\r
tutoring over many many years on animal learning experiments, theory, and neuroscience,\r
and for his careful reading of multiple drafts of Chapters 14 and 15. We also thank Matt\r
Botvinick, Nathaniel Daw, Peter Dayan, and Yael Niv for their penetrating comments on\r
drafts of these chapters, their essential guidance through the massive literature, and their\r
interception of many of our errors in early drafts. Of course, the remaining errors in these\r
chapters—and there must still be some—are totally our own. We thank Phil Thomas for\r
helping us make these chapters accessible to non-psychologists and non-neuroscientists,\r
and we thank Peter Sterling for helping us improve the exposition. We are grateful to Jim\r
Houk for introducing us to the subject of information processing in the basal ganglia and\r
for alerting us to other relevant aspects of neuroscience. Jos´e Mart´ınez, Terry Sejnowski,\r
David Silver, Gerry Tesauro, Georgios Theocharous, and Phil Thomas generously helped\r
us understand details of their reinforcement learning applications for inclusion in the\r
case-studies chapter, and they provided helpful comments on drafts of these sections.\r
Special thanks are owed to David Silver for helping us better understand Monte Carlo\r
Tree Search and the DeepMind Go-playing programs. We thank George Konidaris for his\r
help with the section on the Fourier basis. Emilio Cartoni, Thomas Cederborg, Stefan\r
Dernbach, Clemens Rosenbaum, Patrick Taylor, Thomas Colin, and Pierre-Luc Bacon\r
helped us in a number important ways for which we are most grateful.\r
Sutton would also like to thank the members of the Reinforcement Learning and\r
Artificial Intelligence laboratory at the University of Alberta for contributions to the\r
second edition. He owes a particular debt to Rupam Mahmood for essential contributions\r
to the treatment of o↵-policy Monte Carlo methods in Chapter 5, to Hamid Maei for\r
helping develop the perspective on o↵-policy learning presented in Chapter 11, to Eric\r
Graves for conducting the experiments in Chapter 13, to Shangtong Zhang for replicating\r
and thus verifying almost all the experimental results, to Kris De Asis for improving\r
the new technical content of Chapters 7 and 12, and to Harm van Seijen for insights\r
that led to the separation of n-step methods from eligibility traces and (along with Hado\r
van Hasselt) for the ideas involving exact equivalence of forward and backward views of\r
eligibility traces presented in Chapter 12. Sutton also gratefully acknowledges the support\r
and freedom he was granted by the Government of Alberta and the National Science and\r
Engineering Research Council of Canada throughout the period during which the second\r
edition was conceived and written. In particular, he would like to thank Randy Goebel\r
for creating a supportive and far-sighted environment for research in Alberta. He would\r
also like to thank DeepMind their support in the last six months of writing the book.\r
Finally, we owe thanks to the many careful readers of drafts of the second edition that\r
we posted on the internet. They found many errors that we had missed and alerted us to\r
potential points of confusion.

Preface to the First Edition\r
We first came to focus on what is now known as reinforcement learning in late 1979. We\r
were both at the University of Massachusetts, working on one of the earliest projects to\r
revive the idea that networks of neuronlike adaptive elements might prove to be a promising\r
approach to artificial adaptive intelligence. The project explored the “heterostatic theory\r
of adaptive systems” developed by A. Harry Klopf. Harry’s work was a rich source of\r
ideas, and we were permitted to explore them critically and compare them with the long\r
history of prior work in adaptive systems. Our task became one of teasing the ideas apart\r
and understanding their relationships and relative importance. This continues today,\r
but in 1979 we came to realize that perhaps the simplest of the ideas, which had long\r
been taken for granted, had received surprisingly little attention from a computational\r
perspective. This was simply the idea of a learning system that wants something, that\r
adapts its behavior in order to maximize a special signal from its environment. This\r
was the idea of a “hedonistic” learning system, or, as we would say now, the idea of\r
reinforcement learning.\r
Like others, we had a sense that reinforcement learning had been thoroughly explored\r
in the early days of cybernetics and artificial intelligence. On closer inspection, though,\r
we found that it had been explored only slightly. While reinforcement learning had clearly\r
motivated some of the earliest computational studies of learning, most of these researchers\r
had gone on to other things, such as pattern classification, supervised learning, and\r
adaptive control, or they had abandoned the study of learning altogether. As a result, the\r
special issues involved in learning how to get something from the environment received\r
relatively little attention. In retrospect, focusing on this idea was the critical step that\r
set this branch of research in motion. Little progress could be made in the computational\r
study of reinforcement learning until it was recognized that such a fundamental idea had\r
not yet been thoroughly explored.\r
The field has come a long way since then, evolving and maturing in several directions.\r
Reinforcement learning has gradually become one of the most active research areas in ma\u0002chine learning, artificial intelligence, and neural network research. The field has developed\r
strong mathematical foundations and impressive applications. The computational study\r
of reinforcement learning is now a large field, with hundreds of active researchers around\r
the world in diverse disciplines such as psychology, control theory, artificial intelligence,\r
and neuroscience. Particularly important have been the contributions establishing and\r
developing the relationships to the theory of optimal control and dynamic programming.

xviii Preface to the First Edition\r
The overall problem of learning from interaction to achieve goals is still far from being\r
solved, but our understanding of it has improved significantly. We can now place compo\u0002nent ideas, such as temporal-di↵erence learning, dynamic programming, and function\r
approximation, within a coherent perspective with respect to the overall problem.\r
Our goal in writing this book was to provide a clear and simple account of the key\r
ideas and algorithms of reinforcement learning. We wanted our treatment to be accessible\r
to readers in all of the related disciplines, but we could not cover all of these perspectives\r
in detail. For the most part, our treatment takes the point of view of artificial intelligence\r
and engineering. Coverage of connections to other fields we leave to others or to another\r
time. We also chose not to produce a rigorous formal treatment of reinforcement learning.\r
We did not reach for the highest possible level of mathematical abstraction and did not\r
rely on a theorem–proof format. We tried to choose a level of mathematical detail that\r
points the mathematically inclined in the right directions without distracting from the\r
simplicity and potential generality of the underlying ideas.\r
In some sense we have been working toward this book for thirty years, and we have lots\r
of people to thank. First, we thank those who have personally helped us develop the overall\r
view presented in this book: Harry Klopf, for helping us recognize that reinforcement\r
learning needed to be revived; Chris Watkins, Dimitri Bertsekas, John Tsitsiklis, and\r
Paul Werbos, for helping us see the value of the relationships to dynamic programming;\r
John Moore and Jim Kehoe, for insights and inspirations from animal learning theory;\r
Oliver Selfridge, for emphasizing the breadth and importance of adaptation; and, more\r
generally, our colleagues and students who have contributed in countless ways: Ron\r
Williams, Charles Anderson, Satinder Singh, Sridhar Mahadevan, Steve Bradtke, Bob\r
Crites, Peter Dayan, and Leemon Baird. Our view of reinforcement learning has been\r
significantly enriched by discussions with Paul Cohen, Paul Utgo↵, Martha Steenstrup,\r
Gerry Tesauro, Mike Jordan, Leslie Kaelbling, Andrew Moore, Chris Atkeson, Tom\r
Mitchell, Nils Nilsson, Stuart Russell, Tom Dietterich, Tom Dean, and Bob Narendra.\r
We thank Michael Littman, Gerry Tesauro, Bob Crites, Satinder Singh, and Wei Zhang\r
for providing specifics of Sections 4.7, 15.1, 15.4, 15.5, and 15.6 respectively. We thank\r
the Air Force Oce of Scientific Research, the National Science Foundation, and GTE\r
Laboratories for their long and farsighted support.\r
We also wish to thank the many people who have read drafts of this book and\r
provided valuable comments, including Tom Kalt, John Tsitsiklis, Pawel Cichosz, Olle\r
G¨allmo, Chuck Anderson, Stuart Russell, Ben Van Roy, Paul Steenstrup, Paul Cohen,\r
Sridhar Mahadevan, Jette Randlov, Brian Sheppard, Thomas O’Connell, Richard Coggins,\r
Cristina Versino, John H. Hiett, Andreas Badelt, Jay Ponte, Joe Beck, Justus Piater,\r
Martha Steenstrup, Satinder Singh, Tommi Jaakkola, Dimitri Bertsekas, Torbj¨orn Ekman,\r
Christina Bj¨orkman, Jakob Carlstr¨om, and Olle Palmgren. Finally, we thank Gwyn\r
Mitchell for helping in many ways, and Harry Stanton and Bob Prior for being our\r
champions at MIT Press.

Summary of Notation\r
Capital letters are used for random variables, whereas lower case letters are used for\r
the values of random variables and for scalar functions. Quantities that are required to\r
be real-valued vectors are written in bold and in lower case (even if random variables).\r
Matrices are bold capitals.\r
.\r
= equality relationship that is true by definition\r
⇡ approximately equal\r
/ proportional to\r
Pr{X =x} probability that a random variable X takes on the value x\r
X ⇠ p random variable X selected from distribution p(x) .= Pr{X =x}\r
E[X] expectation of a random variable X, i.e., E[X] .= P\r
x p(x)x\r
argmaxa f(a) a value of a at which f(a) takes its maximal value\r
ln x natural logarithm of x\r
ex, exp(x) the base of the natural logarithm, e ⇡ 2.71828, carried to power x; eln x = x\r
R set of real numbers\r
f : X ! Y function f from elements of set X to elements of set Y\r
 assignment\r
(a, b] the real interval between a and b including b but not including a\r
" probability of taking a random action in an "-greedy policy\r
↵,  step-size parameters\r
 discount-rate parameter\r
 decay-rate parameter for eligibility traces\r
predicate indicator function ( predicate\r
.\r
= 1 if the predicate is true, else 0)\r
In a multi-arm bandit problem:\r
k number of actions (arms)\r
t discrete time step or play number\r
q⇤(a) true value (expected reward) of action a\r
Qt(a) estimate at time t of q⇤(a)\r
Nt(a) number of times action a has been selected up prior to time t\r
Ht(a) learned preference for selecting action a at time t\r
⇡t(a) probability of selecting action a at time t\r
R¯t estimate at time t of the expected reward given ⇡t

xx Summary of Notation\r
In a Markov Decision Process:\r
s, s0 states\r
a an action\r
r a reward\r
S set of all nonterminal states\r
S+ set of all states, including the terminal state\r
A(s) set of all actions available in state s\r
R set of all possible rewards, a finite subset of R\r
⇢ subset of (e.g., R ⇢ R)\r
2 is an element of; e.g. (s 2 S, r 2 R)\r
|S| number of elements in set S\r
t discrete time step\r
T,T(t) final time step of an episode, or of the episode including time step t\r
At action at time t\r
St state at time t, typically due, stochastically, to St1 and At1\r
Rt reward at time t, typically due, stochastically, to St1 and At1\r
⇡ policy (decision-making rule)\r
⇡(s) action taken in state s under deterministic policy ⇡\r
⇡(a|s) probability of taking action a in state s under stochastic policy ⇡\r
Gt return following time t\r
h horizon, the time step one looks up to in a forward view\r
Gt:t+n, Gt:h n-step return from t + 1 to t + n, or to h (discounted and corrected)\r
G¯t:h flat return (undiscounted and uncorrected) from t + 1 to h (Section 5.8)\r
G\r
t -return (Section 12.1)\r
G\r
t:h truncated, corrected -return (Section 12.3)\r
Gs\r
t , Gat -return, corrected by estimated state, or action, values (Section 12.8)\r
p(s0, r|s, a) probability of transition to state s0 with reward r, from state s and action a\r
p(s0 |s, a) probability of transition to state s0, from state s taking action a\r
r(s, a) expected immediate reward from state s after action a\r
r(s, a, s0) expected immediate reward on transition from s to s0 under action a\r
v⇡(s) value of state s under policy ⇡ (expected return)\r
v⇤(s) value of state s under the optimal policy\r
q⇡(s, a) value of taking action a in state s under policy ⇡\r
q⇤(s, a) value of taking action a in state s under the optimal policy\r
V,Vt array estimates of state-value function v⇡ or v⇤\r
Q, Qt array estimates of action-value function q⇡ or q⇤\r
V¯t(s) expected approximate action value; for example, V¯t(s) .= P\r
a ⇡(a|s)Qt(s, a)\r
Ut target for estimate at time t

Summary of Notation xxi\r
t temporal-di↵erence (TD) error at t (a random variable) (Section 6.1)\r
s\r
t , at state- and action-specific forms of the TD error (Section 12.9)\r
n in n-step methods, n is the number of steps of bootstrapping\r
d dimensionality—the number of components of w\r
d0 alternate dimensionality—the number of components of ✓\r
w, wt d-vector of weights underlying an approximate value function\r
wi, wt,i ith component of learnable weight vector\r
vˆ(s,w) approximate value of state s given weight vector w\r
vw(s) alternate notation for ˆv(s,w)\r
qˆ(s, a, w) approximate value of state–action pair s, a given weight vector w\r
rvˆ(s,w) column vector of partial derivatives of ˆv(s,w) with respect to w\r
rqˆ(s, a, w) column vector of partial derivatives of ˆq(s, a, w) with respect to w\r
x(s) vector of features visible when in state s\r
x(s, a) vector of features visible when in state s taking action a\r
xi(s), xi(s, a) ith component of vector x(s) or x(s, a)\r
xt shorthand for x(St) or x(St, At)\r
w>x inner product of vectors, w>x .= P\r
i wixi; for example, ˆv(s,w) .\r
= w>x(s)\r
v, vt secondary d-vector of weights, used to learn w (Chapter 11)\r
zt d-vector of eligibility traces at time t (Chapter 12)\r
✓, ✓t parameter vector of target policy (Chapter 13)\r
⇡(a|s, ✓) probability of taking action a in state s given parameter vector ✓\r
⇡✓ policy corresponding to parameter ✓\r
r⇡(a|s, ✓) column vector of partial derivatives of ⇡(a|s, ✓) with respect to ✓\r
J(✓) performance measure for the policy ⇡✓\r
rJ(✓) column vector of partial derivatives of J(✓) with respect to ✓\r
h(s, a, ✓) preference for selecting action a in state s based on ✓\r
b(a|s) behavior policy used to select actions while learning about target policy ⇡\r
b(s) a baseline function b : S 7! R for policy-gradient methods\r
b branching factor for an MDP or search tree\r
⇢t:h importance sampling ratio for time t through time h (Section 5.5)\r
⇢t importance sampling ratio for time t alone, ⇢t\r
.\r
= ⇢t:t\r
r(⇡) average reward (reward rate) for policy ⇡ (Section 10.3)\r
R¯t estimate of r(⇡) at time t\r
µ(s) on-policy distribution over states (Section 9.2)\r
µ |S|-vector of the µ(s) for all s 2 S\r
kvk\r
2\r
µ µ-weighted squared norm of value function v, i.e., kvk\r
2\r
µ\r
.\r
= P\r
s2S µ(s)v(s)2\r
⌘(s) expected number of visits to state s per episode (page 199)\r
⇧ projection operator for value functions (page 268)\r
B⇡ Bellman operator for value functions (Section 11.4)

xxii Summary of Notation\r
A d ⇥ d matrix A .= E\r
h\r
xt\r
\r
xt  xt+1>i\r
b d-dimensional vector b .= E[Rt+1xt]\r
wTD TD fixed point wTD\r
.\r
= A1b (a d-vector, Section 9.4)\r
I identity matrix\r
P |S| ⇥ |S| matrix of state-transition probabilities under ⇡\r
D |S| ⇥ |S| diagonal matrix with µ on its diagonal\r
X |S| ⇥ d matrix with the x(s) as its rows\r
¯w(s) Bellman error (expected TD error) for vw at state s (Section 11.4) ¯w, BE Bellman error vector, with components ¯w(s)\r
VE(w) mean square value error VE(w) .= kvw  v⇡k\r
2\r
µ (Section 9.2)\r
BE(w) mean square Bellman error BE(w) .= \r
¯w\r
\r
\r
2\r
µ\r
PBE(w) mean square projected Bellman error PBE(w) .= \r
⇧¯w\r
\r
\r
2\r
µ\r
TDE(w) mean square temporal-di↵erence error TDE(w) .= Eb\r
⇥\r
⇢t2\r
t\r
⇤\r
(Section 11.5)\r
RE(w) mean square return error (Section 11.6)

Chapter 1\r
Introduction\r
The idea that we learn by interacting with our environment is probably the first to occur\r
to us when we think about the nature of learning. When an infant plays, waves its arms,\r
or looks about, it has no explicit teacher, but it does have a direct sensorimotor connection\r
to its environment. Exercising this connection produces a wealth of information about\r
cause and e↵ect, about the consequences of actions, and about what to do in order to\r
achieve goals. Throughout our lives, such interactions are undoubtedly a major source\r
of knowledge about our environment and ourselves. Whether we are learning to drive\r
a car or to hold a conversation, we are acutely aware of how our environment responds\r
to what we do, and we seek to influence what happens through our behavior. Learning\r
from interaction is a foundational idea underlying nearly all theories of learning and\r
intelligence.\r
In this book we explore a computational approach to learning from interaction. Rather\r
than directly theorizing about how people or animals learn, we primarily explore idealized\r
learning situations and evaluate the e↵ectiveness of various learning methods. That\r
is, we adopt the perspective of an artificial intelligence researcher or engineer. We\r
explore designs for machines that are e↵ective in solving learning problems of scientific or\r
economic interest, evaluating the designs through mathematical analysis or computational\r
experiments. The approach we explore, called reinforcement learning, is much more\r
focused on goal-directed learning from interaction than are other approaches to machine\r
learning."""

[[sections]]
number = "1.1"
title = "Reinforcement Learning"
text = """
Reinforcement learning is learning what to do—how to map situations to actions—so\r
as to maximize a numerical reward signal. The learner is not told which actions to\r
take, but instead must discover which actions yield the most reward by trying them. In\r
the most interesting and challenging cases, actions may a↵ect not only the immediate\r
reward but also the next situation and, through that, all subsequent rewards. These two\r
characteristics—trial-and-error search and delayed reward—are the two most important\r
distinguishing features of reinforcement learning."""

[[sections]]
number = "2"
title = "Chapter 1: Introduction"
text = """
Reinforcement learning, like many topics whose names end with “ing,” such as machine\r
learning and mountaineering, is simultaneously a problem, a class of solution methods\r
that work well on the problem, and the field that studies this problem and its solution\r
methods. It is convenient to use a single name for all three things, but at the same time\r
essential to keep the three conceptually separate. In particular, the distinction between\r
problems and solution methods is very important in reinforcement learning; failing to\r
make this distinction is the source of many confusions.\r
We formalize the problem of reinforcement learning using ideas from dynamical sys\u0002tems theory, specifically, as the optimal control of incompletely-known Markov decision\r
processes. The details of this formalization must wait until Chapter 3, but the basic idea\r
is simply to capture the most important aspects of the real problem facing a learning\r
agent interacting over time with its environment to achieve a goal. A learning agent\r
must be able to sense the state of its environment to some extent and must be able to\r
take actions that a↵ect the state. The agent also must have a goal or goals relating to\r
the state of the environment. Markov decision processes are intended to include just\r
these three aspects—sensation, action, and goal—in their simplest possible forms without\r
trivializing any of them. Any method that is well suited to solving such problems we\r
consider to be a reinforcement learning method.\r
Reinforcement learning is di↵erent from supervised learning, the kind of learning studied\r
in most current research in the field of machine learning. Supervised learning is learning\r
from a training set of labeled examples provided by a knowledgable external supervisor.\r
Each example is a description of a situation together with a specification—the label—of\r
the correct action the system should take in that situation, which is often to identify a\r
category to which the situation belongs. The object of this kind of learning is for the\r
system to extrapolate, or generalize, its responses so that it acts correctly in situations\r
not present in the training set. This is an important kind of learning, but alone it is not\r
adequate for learning from interaction. In interactive problems it is often impractical to\r
obtain examples of desired behavior that are both correct and representative of all the\r
situations in which the agent has to act. In uncharted territory—where one would expect\r
learning to be most beneficial—an agent must be able to learn from its own experience.\r
Reinforcement learning is also di↵erent from what machine learning researchers call\r
unsupervised learning, which is typically about finding structure hidden in collections of\r
unlabeled data. The terms supervised learning and unsupervised learning would seem\r
to exhaustively classify machine learning paradigms, but they do not. Although one\r
might be tempted to think of reinforcement learning as a kind of unsupervised learning\r
because it does not rely on examples of correct behavior, reinforcement learning is trying\r
to maximize a reward signal instead of trying to find hidden structure. Uncovering\r
structure in an agent’s experience can certainly be useful in reinforcement learning, but by\r
itself does not address the reinforcement learning problem of maximizing a reward signal.\r
We therefore consider reinforcement learning to be a third machine learning paradigm,\r
alongside supervised learning and unsupervised learning and perhaps other paradigms."""

[[sections]]
number = "1.1"
title = "Reinforcement Learning 3"
text = """
One of the challenges that arise in reinforcement learning, and not in other kinds\r
of learning, is the trade-o↵ between exploration and exploitation. To obtain a lot of\r
reward, a reinforcement learning agent must prefer actions that it has tried in the past\r
and found to be e↵ective in producing reward. But to discover such actions, it has to\r
try actions that it has not selected before. The agent has to exploit what it has already\r
experienced in order to obtain reward, but it also has to explore in order to make better\r
action selections in the future. The dilemma is that neither exploration nor exploitation\r
can be pursued exclusively without failing at the task. The agent must try a variety of\r
actions and progressively favor those that appear to be best. On a stochastic task, each\r
action must be tried many times to gain a reliable estimate of its expected reward. The\r
exploration–exploitation dilemma has been intensively studied by mathematicians for\r
many decades, yet remains unresolved. For now, we simply note that the entire issue of\r
balancing exploration and exploitation does not even arise in supervised and unsupervised\r
learning, at least in the purest forms of these paradigms.\r
Another key feature of reinforcement learning is that it explicitly considers the whole\r
problem of a goal-directed agent interacting with an uncertain environment. This is in\r
contrast to many approaches that consider subproblems without addressing how they\r
might fit into a larger picture. For example, we have mentioned that many machine\r
learning researchers have studied supervised learning without specifying how such an\r
ability would ultimately be useful. Other researchers have developed theories of planning\r
with general goals, but without considering planning’s role in real-time decision making,\r
or the question of where the predictive models necessary for planning would come from.\r
Although these approaches have yielded many useful results, their focus on isolated\r
subproblems is a significant limitation.\r
Reinforcement learning takes the opposite tack, starting with a complete, interactive,\r
goal-seeking agent. All reinforcement learning agents have explicit goals, can sense\r
aspects of their environments, and can choose actions to influence their environments.\r
Moreover, it is usually assumed from the beginning that the agent has to operate despite\r
significant uncertainty about the environment it faces. When reinforcement learning\r
involves planning, it has to address the interplay between planning and real-time action\r
selection, as well as the question of how environment models are acquired and improved.\r
When reinforcement learning involves supervised learning, it does so for specific reasons\r
that determine which capabilities are critical and which are not. For learning research to\r
make progress, important subproblems have to be isolated and studied, but they should\r
be subproblems that play clear roles in complete, interactive, goal-seeking agents, even if\r
all the details of the complete agent cannot yet be filled in.\r
By a complete, interactive, goal-seeking agent we do not always mean something like\r
a complete organism or robot. These are clearly examples, but a complete, interactive,\r
goal-seeking agent can also be a component of a larger behaving system. In this case, the\r
agent directly interacts with the rest of the larger system and indirectly interacts with\r
the larger system’s environment. A simple example is an agent that monitors the charge\r
level of robot’s battery and sends commands to the robot’s control architecture. This\r
agent’s environment is the rest of the robot together with the robot’s environment. It is"""

[[sections]]
number = "4"
title = "Chapter 1: Introduction"
text = """
important to look beyond the most obvious examples of agents and their environments\r
to appreciate the generality of the reinforcement learning framework.\r
One of the most exciting aspects of modern reinforcement learning is its substantive\r
and fruitful interactions with other engineering and scientific disciplines. Reinforcement\r
learning is part of a decades-long trend within artificial intelligence and machine learning\r
toward greater integration with statistics, optimization, and other mathematical subjects.\r
For example, the ability of some reinforcement learning methods to learn with parameter\u0002ized approximators addresses the classical “curse of dimensionality” in operations research\r
and control theory. More distinctively, reinforcement learning has also interacted strongly\r
with psychology and neuroscience, with substantial benefits going both ways. Of all the\r
forms of machine learning, reinforcement learning is the closest to the kind of learning\r
that humans and other animals do, and many of the core algorithms of reinforcement\r
learning were originally inspired by biological learning systems. Reinforcement learning\r
has also given back, both through a psychological model of animal learning that better\r
matches some of the empirical data, and through an influential model of parts of the\r
brain’s reward system. The body of this book develops the ideas of reinforcement learning\r
that pertain to engineering and artificial intelligence, with connections to psychology and\r
neuroscience summarized in Chapters 14 and 15.\r
Finally, reinforcement learning is also part of a larger trend in artificial intelligence\r
back toward simple general principles. Since the late 1960s, many artificial intelligence re\u0002searchers presumed that there are no general principles to be discovered, that intelligence is\r
instead due to the possession of a vast number of special purpose tricks, procedures, and\r
heuristics. It was sometimes said that if we could just get enough relevant facts into a\r
machine, say one million, or one billion, then it would become intelligent. Methods based\r
on general principles, such as search or learning, were characterized as “weak methods,”\r
whereas those based on specific knowledge were called “strong methods.” This view is\r
uncommon today. From our point of view, it was premature: too little e↵ort had been\r
put into the search for general principles to conclude that there were none. Modern\r
artificial intelligence now includes much research looking for general principles of learning,\r
search, and decision making. It is not clear how far back the pendulum will swing, but\r
reinforcement learning research is certainly part of the swing back toward simpler and\r
fewer general principles of artificial intelligence."""

[[sections]]
number = "1.2"
title = "Examples"
text = """
A good way to understand reinforcement learning is to consider some of the examples\r
and possible applications that have guided its development.\r
• A master chess player makes a move. The choice is informed both by planning—\r
anticipating possible replies and counterreplies—and by immediate, intuitive judg\u0002ments of the desirability of particular positions and moves.\r
• An adaptive controller adjusts parameters of a petroleum refinery’s operation in\r
real time. The controller optimizes the yield/cost/quality trade-o↵ on the basis"""

[[sections]]
number = "1.2"
title = "Examples 5"
text = """
of specified marginal costs without sticking strictly to the set points originally\r
suggested by engineers.\r
• A gazelle calf struggles to its feet minutes after being born. Half an hour later it is\r
running at 20 miles per hour.\r
• A mobile robot decides whether it should enter a new room in search of more trash\r
to collect or start trying to find its way back to its battery recharging station. It\r
makes its decision based on the current charge level of its battery and how quickly\r
and easily it has been able to find the recharger in the past.\r
• Phil prepares his breakfast. Closely examined, even this apparently mundane\r
activity reveals a complex web of conditional behavior and interlocking goal–subgoal\r
relationships: walking to the cupboard, opening it, selecting a cereal box, then\r
reaching for, grasping, and retrieving the box. Other complex, tuned, interactive\r
sequences of behavior are required to obtain a bowl, spoon, and milk carton. Each\r
step involves a series of eye movements to obtain information and to guide reaching\r
and locomotion. Rapid judgments are continually made about how to carry the\r
objects or whether it is better to ferry some of them to the dining table before\r
obtaining others. Each step is guided by goals, such as grasping a spoon or getting\r
to the refrigerator, and is in service of other goals, such as having the spoon to eat\r
with once the cereal is prepared and ultimately obtaining nourishment. Whether\r
he is aware of it or not, Phil is accessing information about the state of his body\r
that determines his nutritional needs, level of hunger, and food preferences.\r
These examples share features that are so basic that they are easy to overlook. All\r
involve interaction between an active decision-making agent and its environment, within\r
which the agent seeks to achieve a goal despite uncertainty about its environment. The\r
agent’s actions are permitted to a↵ect the future state of the environment (e.g., the\r
next chess position, the level of reservoirs of the refinery, the robot’s next location and\r
the future charge level of its battery), thereby a↵ecting the actions and opportunities\r
available to the agent at later times. Correct choice requires taking into account indirect,\r
delayed consequences of actions, and thus may require foresight or planning.\r
At the same time, in all of these examples the e↵ects of actions cannot be fully predicted;\r
thus the agent must monitor its environment frequently and react appropriately. For\r
example, Phil must watch the milk he pours into his cereal bowl to keep it from overflowing.\r
All these examples involve goals that are explicit in the sense that the agent can judge\r
progress toward its goal based on what it can sense directly. The chess player knows\r
whether or not he wins, the refinery controller knows how much petroleum is being\r
produced, the gazelle calf knows when it falls, the mobile robot knows when its batteries\r
run down, and Phil knows whether or not he is enjoying his breakfast.\r
In all of these examples the agent can use its experience to improve its performance\r
over time. The chess player refines the intuition he uses to evaluate positions, thereby\r
improving his play; the gazelle calf improves the eciency with which it can run; Phil\r
learns to streamline making his breakfast. The knowledge the agent brings to the task at\r
the start—either from previous experience with related tasks or built into it by design or"""

[[sections]]
number = "6"
title = "Chapter 1: Introduction"
text = """
evolution—influences what is useful or easy to learn, but interaction with the environment\r
is essential for adjusting behavior to exploit specific features of the task."""

[[sections]]
number = "1.3"
title = "Elements of Reinforcement Learning"
text = """
Beyond the agent and the environment, one can identify four main subelements of a\r
reinforcement learning system: a policy, a reward signal, a value function, and, optionally,\r
a model of the environment.\r
A policy defines the learning agent’s way of behaving at a given time. Roughly speaking,\r
a policy is a mapping from perceived states of the environment to actions to be taken\r
when in those states. It corresponds to what in psychology would be called a set of\r
stimulus–response rules or associations. In some cases the policy may be a simple function\r
or lookup table, whereas in others it may involve extensive computation such as a search\r
process. The policy is the core of a reinforcement learning agent in the sense that it alone\r
is sucient to determine behavior. In general, policies may be stochastic, specifying\r
probabilities for each action.\r
A reward signal defines the goal of a reinforcement learning problem. On each time\r
step, the environment sends to the reinforcement learning agent a single number called\r
the reward. The agent’s sole objective is to maximize the total reward it receives over\r
the long run. The reward signal thus defines what are the good and bad events for the\r
agent. In a biological system, we might think of rewards as analogous to the experiences\r
of pleasure or pain. They are the immediate and defining features of the problem faced\r
by the agent. The reward signal is the primary basis for altering the policy; if an action\r
selected by the policy is followed by low reward, then the policy may be changed to\r
select some other action in that situation in the future. In general, reward signals may\r
be stochastic functions of the state of the environment and the actions taken.\r
Whereas the reward signal indicates what is good in an immediate sense, a value\r
function specifies what is good in the long run. Roughly speaking, the value of a state is\r
the total amount of reward an agent can expect to accumulate over the future, starting\r
from that state. Whereas rewards determine the immediate, intrinsic desirability of\r
environmental states, values indicate the long-term desirability of states after taking into\r
account the states that are likely to follow and the rewards available in those states. For\r
example, a state might always yield a low immediate reward but still have a high value\r
because it is regularly followed by other states that yield high rewards. Or the reverse\r
could be true. To make a human analogy, rewards are somewhat like pleasure (if high)\r
and pain (if low), whereas values correspond to a more refined and farsighted judgment\r
of how pleased or displeased we are that our environment is in a particular state.\r
Rewards are in a sense primary, whereas values, as predictions of rewards, are secondary.\r
Without rewards there could be no values, and the only purpose of estimating values is to\r
achieve more reward. Nevertheless, it is values with which we are most concerned when\r
making and evaluating decisions. Action choices are made based on value judgments. We\r
seek actions that bring about states of highest value, not highest reward, because these\r
actions obtain the greatest amount of reward for us over the long run. Unfortunately, it\r
is much harder to determine values than it is to determine rewards. Rewards are basically\r
given directly by the environment, but values must be estimated and re-estimated from"""

[[sections]]
number = "1.4"
title = "Limitations and Scope 7"
text = """
the sequences of observations an agent makes over its entire lifetime. In fact, the most\r
important component of almost all reinforcement learning algorithms we consider is a\r
method for eciently estimating values. The central role of value estimation is arguably\r
the most important thing that has been learned about reinforcement learning over the\r
last six decades.\r
The fourth and final element of some reinforcement learning systems is a model of\r
the environment. This is something that mimics the behavior of the environment, or\r
more generally, that allows inferences to be made about how the environment will behave.\r
For example, given a state and action, the model might predict the resultant next state\r
and next reward. Models are used for planning, by which we mean any way of deciding\r
on a course of action by considering possible future situations before they are actually\r
experienced. Methods for solving reinforcement learning problems that use models and\r
planning are called model-based methods, as opposed to simpler model-free methods that\r
are explicitly trial-and-error learners—viewed as almost the opposite of planning. In\r
Chapter 8 we explore reinforcement learning systems that simultaneously learn by trial\r
and error, learn a model of the environment, and use the model for planning. Modern\r
reinforcement learning spans the spectrum from low-level, trial-and-error learning to\r
high-level, deliberative planning."""

[[sections]]
number = "1.4"
title = "Limitations and Scope"
text = """
Reinforcement learning relies heavily on the concept of state—as input to the policy and\r
value function, and as both input to and output from the model. Informally, we can\r
think of the state as a signal conveying to the agent some sense of “how the environment\r
is” at a particular time. The formal definition of state as we use it here is given by\r
the framework of Markov decision processes presented in Chapter 3. More generally,\r
however, we encourage the reader to follow the informal meaning and think of the state\r
as whatever information is available to the agent about its environment. In e↵ect, we\r
assume that the state signal is produced by some preprocessing system that is nominally\r
part of the agent’s environment. We do not address the issues of constructing, changing,\r
or learning the state signal in this book (other than briefly in Section 17.3). We take this\r
approach not because we consider state representation to be unimportant, but in order\r
to focus fully on the decision-making issues. In other words, our concern in this book is\r
not with designing the state signal, but with deciding what action to take as a function\r
of whatever state signal is available.\r
Most of the reinforcement learning methods we consider in this book are structured\r
around estimating value functions, but it is not strictly necessary to do this to solve\r
reinforcement learning problems. For example, solution methods such as genetic algo\u0002rithms, genetic programming, simulated annealing, and other optimization methods never\r
estimate value functions. These methods apply multiple static policies each interacting\r
over an extended period of time with a separate instance of the environment. The policies\r
that obtain the most reward, and random variations of them, are carried over to the\r
next generation of policies, and the process repeats. We call these evolutionary methods\r
because their operation is analogous to the way biological evolution produces organisms"""

[[sections]]
number = "8"
title = "Chapter 1: Introduction"
text = """
with skilled behavior even if they do not learn during their individual lifetimes. If the\r
space of policies is suciently small, or can be structured so that good policies are\r
common or easy to find—or if a lot of time is available for the search—then evolutionary\r
methods can be e↵ective. In addition, evolutionary methods have advantages on problems\r
in which the learning agent cannot sense the complete state of its environment.\r
Our focus is on reinforcement learning methods that learn while interacting with the\r
environment, which evolutionary methods do not do. Methods able to take advantage\r
of the details of individual behavioral interactions can be much more ecient than\r
evolutionary methods in many cases. Evolutionary methods ignore much of the useful\r
structure of the reinforcement learning problem: they do not use the fact that the policy\r
they are searching for is a function from states to actions; they do not notice which states\r
an individual passes through during its lifetime, or which actions it selects. In some cases\r
such information can be misleading (e.g., when states are misperceived), but more often it\r
should enable more ecient search. Although evolution and learning share many features\r
and naturally work together, we do not consider evolutionary methods by themselves to\r
be especially well suited to reinforcement learning problems and, accordingly, we do not\r
cover them in this book."""

[[sections]]
number = "1.5"
title = "An Extended Example: Tic-Tac-Toe"
text = """
To illustrate the general idea of reinforcement learning and contrast it with other ap\u0002proaches, we next consider a single example in more detail.\r
X\r
X\r
X\r
O O\r
O X\r
Consider the familiar child’s game of tic-tac-toe. Two players\r
take turns playing on a three-by-three board. One player plays\r
Xs and the other Os until one player wins by placing three marks\r
in a row, horizontally, vertically, or diagonally, as the X player\r
has in the game shown to the right. If the board fills up with\r
neither player getting three in a row, then the game is a draw.\r
Because a skilled player can play so as never to lose, let us assume\r
that we are playing against an imperfect player, one whose play\r
is sometimes incorrect and allows us to win. For the moment, in\r
fact, let us consider draws and losses to be equally bad for us. How might we construct a\r
player that will find the imperfections in its opponent’s play and learn to maximize its\r
chances of winning?\r
Although this is a simple problem, it cannot readily be solved in a satisfactory way\r
through classical techniques. For example, the classical “minimax” solution from game\r
theory is not correct here because it assumes a particular way of playing by the opponent.\r
For example, a minimax player would never reach a game state from which it could\r
lose, even if in fact it always won from that state because of incorrect play by the\r
opponent. Classical optimization methods for sequential decision problems, such as\r
dynamic programming, can compute an optimal solution for any opponent, but require\r
as input a complete specification of that opponent, including the probabilities with which\r
the opponent makes each move in each board state. Let us assume that this information\r
is not available a priori for this problem, as it is not for the vast majority of problems of"""

[[sections]]
number = "1.5"
title = "An Extended Example: Tic-Tac-Toe 9"
text = """
practical interest. On the other hand, such information can be estimated from experience,\r
in this case by playing many games against the opponent. About the best one can do\r
on this problem is first to learn a model of the opponent’s behavior, up to some level of\r
confidence, and then apply dynamic programming to compute an optimal solution given\r
the approximate opponent model. In the end, this is not that di↵erent from some of the\r
reinforcement learning methods we examine later in this book.\r
An evolutionary method applied to this problem would directly search the space\r
of possible policies for one with a high probability of winning against the opponent.\r
Here, a policy is a rule that tells the player what move to make for every state of the\r
game—every possible configuration of Xs and Os on the three-by-three board. For each\r
policy considered, an estimate of its winning probability would be obtained by playing\r
some number of games against the opponent. This evaluation would then direct which\r
policy or policies were considered next. A typical evolutionary method would hill-climb\r
in policy space, successively generating and evaluating policies in an attempt to obtain\r
incremental improvements. Or, perhaps, a genetic-style algorithm could be used that\r
would maintain and evaluate a population of policies. Literally hundreds of di↵erent\r
optimization methods could be applied.\r
Here is how the tic-tac-toe problem would be approached with a method making use\r
of a value function. First we would set up a table of numbers, one for each possible state\r
of the game. Each number will be the latest estimate of the probability of our winning\r
from that state. We treat this estimate as the state’s value, and the whole table is the\r
learned value function. State A has higher value than state B, or is considered “better”\r
than state B, if the current estimate of the probability of our winning from A is higher\r
than it is from B. Assuming we always play Xs, then for all states with three Xs in a row\r
the probability of winning is 1, because we have already won. Similarly, for all states\r
with three Os in a row, or that are filled up, the correct probability is 0, as we cannot\r
win from them. We set the initial values of all the other states to 0.5, representing a\r
guess that we have a 50% chance of winning.\r
We then play many games against the opponent. To select our moves we examine the\r
states that would result from each of our possible moves (one for each blank space on the\r
board) and look up their current values in the table. Most of the time we move greedily,\r
selecting the move that leads to the state with greatest value, that is, with the highest\r
estimated probability of winning. Occasionally, however, we select randomly from among\r
the other moves instead. These are called exploratory moves because they cause us to\r
experience states that we might otherwise never see. A sequence of moves made and\r
considered during a game can be diagrammed as in Figure 1.1.\r
While we are playing, we change the values of the states in which we find ourselves\r
during the game. We attempt to make them more accurate estimates of the probabilities\r
of winning. To do this, we “back up” the value of the state after each greedy move to\r
the state before the move, as suggested by the arrows in Figure 1.1. More precisely, the\r
current value of the earlier state is updated to be closer to the value of the later state.\r
This can be done by moving the earlier state’s value a fraction of the way toward the\r
value of the later state. If we let St denote the state before the greedy move, and St+1\r
the state after that move, then the update to the estimated value of St, denoted V (St),"""

[[sections]]
number = "10"
title = "Chapter 1: Introduction"
text = """
.\r
.\r
•\r
our move {\r
opponent's move { our move {\r
starting position\r
•\r
•\r
•\r
a\r
b\r
c*\r
d\r
e e*\r
opponent's move {\r
c\r
•f\r
•\r
g* g\r
opponent's move { our move { .\r
•\r
a\r
g*\r
c\r
starting position\r
b\r
c*\r
d\r
e* e\r
f\r
g\r
…\r
Figure 1.1: A sequence of tic-tac-toe moves. The solid black lines represent the moves taken\r
during a game; the dashed lines represent moves that we (our reinforcement learning player)\r
considered but did not make. The * indicates the move currently estimated to be the best. Our\r
second move was an exploratory move, meaning that it was taken even though another sibling\r
move, the one leading to e⇤, was ranked higher. Exploratory moves do not result in any learning,\r
but each of our other moves does, causing updates as suggested by the red arrows in which\r
estimated values are moved up the tree from later nodes to earlier nodes as detailed in the text.\r
can be written as\r
V (St) V (St) + ↵\r
h\r
V (St+1)  V (St)\r
i\r
,\r
where ↵ is a small positive fraction called the step-size parameter, which influences\r
the rate of learning. This update rule is an example of a temporal-di↵erence learning\r
method, so called because its changes are based on a di↵erence, V (St+1)V (St), between\r
estimates at two successive times.\r
The method described above performs quite well on this task. For example, if the\r
step-size parameter is reduced properly over time, then this method converges, for any\r
fixed opponent, to the true probabilities of winning from each state given optimal play\r
by our player. Furthermore, the moves then taken (except on exploratory moves) are in\r
fact the optimal moves against this (imperfect) opponent. In other words, the method\r
converges to an optimal policy for playing the game against this opponent. If the step-size\r
parameter is not reduced all the way to zero over time, then this player also plays well\r
against opponents that slowly change their way of playing."""

[[sections]]
number = "1.5"
title = "An Extended Example: Tic-Tac-Toe 11"
text = """
This example illustrates the di↵erences between evolutionary methods and methods\r
that learn value functions. To evaluate a policy, an evolutionary method holds the policy\r
fixed and plays many games against the opponent or simulates many games using a model\r
of the opponent. The frequency of wins gives an unbiased estimate of the probability\r
of winning with that policy, and can be used to direct the next policy selection. But\r
each policy change is made only after many games, and only the final outcome of each\r
game is used: what happens during the games is ignored. For example, if the player wins,\r
then all of its behavior in the game is given credit, independently of how specific moves\r
might have been critical to the win. Credit is even given to moves that never occurred!\r
Value function methods, in contrast, allow individual states to be evaluated. In the end,\r
evolutionary and value function methods both search the space of policies, but learning a\r
value function takes advantage of information available during the course of play.\r
This simple example illustrates some of the key features of reinforcement learning\r
methods. First, there is the emphasis on learning while interacting with an environment,\r
in this case with an opponent player. Second, there is a clear goal, and correct behavior\r
requires planning or foresight that takes into account delayed e↵ects of one’s choices. For\r
example, the simple reinforcement learning player would learn to set up multi-move traps\r
for a shortsighted opponent. It is a striking feature of the reinforcement learning solution\r
that it can achieve the e↵ects of planning and lookahead without using a model of the\r
opponent and without conducting an explicit search over possible sequences of future\r
states and actions.\r
While this example illustrates some of the key features of reinforcement learning, it is\r
so simple that it might give the impression that reinforcement learning is more limited\r
than it really is. Although tic-tac-toe is a two-person game, reinforcement learning\r
also applies in the case in which there is no external adversary, that is, in the case of\r
a “game against nature.” Reinforcement learning also is not restricted to problems in\r
which behavior breaks down into separate episodes, like the separate games of tic-tac-toe,\r
with reward only at the end of each episode. It is just as applicable when behavior\r
continues indefinitely and when rewards of various magnitudes can be received at any\r
time. Reinforcement learning is also applicable to problems that do not even break down\r
into discrete time steps like the plays of tic-tac-toe. The general principles apply to\r
continuous-time problems as well, although the theory gets more complicated and we\r
omit it from this introductory treatment.\r
Tic-tac-toe has a relatively small, finite state set, whereas reinforcement learning can\r
be used when the state set is very large, or even infinite. For example, Gerry Tesauro\r
(1992, 1995) combined the algorithm described above with an artificial neural network to\r
learn to play backgammon, which has approximately 1020 states. With this many states\r
it is impossible ever to experience more than a small fraction of them. Tesauro’s program\r
learned to play far better than any previous program and eventually better than the\r
world’s best human players (see Section 16.1). The artificial neural network provides the\r
program with the ability to generalize from its experience, so that in new states it selects\r
moves based on information saved from similar states faced in the past, as determined\r
by the network. How well a reinforcement learning system can work in problems with\r
such large state sets is intimately tied to how appropriately it can generalize from past"""

[[sections]]
number = "12"
title = "Chapter 1: Introduction"
text = """
experience. It is in this role that we have the greatest need for supervised learning\r
methods within reinforcement learning. Artificial neural networks and deep learning\r
(Section 9.7) are not the only, or necessarily the best, way to do this.\r
In this tic-tac-toe example, learning started with no prior knowledge beyond the\r
rules of the game, but reinforcement learning by no means entails a tabula rasa view of\r
learning and intelligence. On the contrary, prior information can be incorporated into\r
reinforcement learning in a variety of ways that can be critical for ecient learning (e.g.,\r
see Sections 9.5, 17.4, and 13.1). We also have access to the true state in the tic-tac-toe\r
example, whereas reinforcement learning can also be applied when part of the state is\r
hidden, or when di↵erent states appear to the learner to be the same.\r
Finally, the tic-tac-toe player was able to look ahead and know the states that would\r
result from each of its possible moves. To do this, it had to have a model of the game\r
that allowed it to foresee how its environment would change in response to moves that it\r
might never make. Many problems are like this, but in others even a short-term model\r
of the e↵ects of actions is lacking. Reinforcement learning can be applied in either case.\r
A model is not required, but models can easily be used if they are available or can be\r
learned (Chapter 8).\r
On the other hand, there are reinforcement learning methods that do not need any\r
kind of environment model at all. Model-free systems cannot even think about how\r
their environments will change in response to a single action. The tic-tac-toe player is\r
model-free in this sense with respect to its opponent: it has no model of its opponent\r
of any kind. Because models have to be reasonably accurate to be useful, model-free\r
methods can have advantages over more complex methods when the real bottleneck in\r
solving a problem is the diculty of constructing a suciently accurate environment\r
model. Model-free methods are also important building blocks for model-based methods.\r
In this book we devote several chapters to model-free methods before we discuss how\r
they can be used as components of more complex model-based methods.\r
Reinforcement learning can be used at both high and low levels in a system. Although\r
the tic-tac-toe player learned only about the basic moves of the game, nothing prevents\r
reinforcement learning from working at higher levels where each of the “actions” may\r
itself be the application of a possibly elaborate problem-solving method. In hierarchical\r
learning systems, reinforcement learning can work simultaneously on several levels.\r
Exercise 1.1: Self-Play Suppose, instead of playing against a random opponent, the\r
reinforcement learning algorithm described above played against itself, with both sides\r
learning. What do you think would happen in this case? Would it learn a di↵erent policy\r
for selecting moves? ⇤\r
Exercise 1.2: Symmetries Many tic-tac-toe positions appear di↵erent but are really\r
the same because of symmetries. How might we amend the learning process described\r
above to take advantage of this? In what ways would this change improve the learning\r
process? Now think again. Suppose the opponent did not take advantage of symmetries.\r
In that case, should we? Is it true, then, that symmetrically equivalent positions should\r
necessarily have the same value? ⇤\r
Exercise 1.3: Greedy Play Suppose the reinforcement learning player was greedy, that is,\r
it always played the move that brought it to the position that it rated the best. Might it"""

[[sections]]
number = "1.7"
title = "Early History of Reinforcement Learning 13"
text = """
learn to play better, or worse, than a nongreedy player? What problems might occur? ⇤\r
Exercise 1.4: Learning from Exploration Suppose learning updates occurred after all\r
moves, including exploratory moves. If the step-size parameter is appropriately reduced\r
over time (but not the tendency to explore), then the state values would converge to\r
a di↵erent set of probabilities. What (conceptually) are the two sets of probabilities\r
computed when we do, and when we do not, learn from exploratory moves? Assuming\r
that we do continue to make exploratory moves, which set of probabilities might be better\r
to learn? Which would result in more wins? ⇤\r
Exercise 1.5: Other Improvements Can you think of other ways to improve the reinforce\u0002ment learning player? Can you think of any better way to solve the tic-tac-toe problem\r
as posed? ⇤"""

[[sections]]
number = "1.6"
title = "Summary"
text = """
Reinforcement learning is a computational approach to understanding and automating\r
goal-directed learning and decision making. It is distinguished from other computational\r
approaches by its emphasis on learning by an agent from direct interaction with its\r
environment, without requiring exemplary supervision or complete models of the envi\u0002ronment. In our opinion, reinforcement learning is the first field to seriously address the\r
computational issues that arise when learning from interaction with an environment in\r
order to achieve long-term goals.\r
Reinforcement learning uses the formal framework of Markov decision processes to\r
define the interaction between a learning agent and its environment in terms of states,\r
actions, and rewards. This framework is intended to be a simple way of representing\r
essential features of the artificial intelligence problem. These features include a sense of\r
cause and e↵ect, a sense of uncertainty and nondeterminism, and the existence of explicit\r
goals.\r
The concepts of value and value function are key to most of the reinforcement learning\r
methods that we consider in this book. We take the position that value functions\r
are important for ecient search in the space of policies. The use of value functions\r
distinguishes reinforcement learning methods from evolutionary methods that search\r
directly in policy space guided by evaluations of entire policies."""

[[sections]]
number = "1.7"
title = "Early History of Reinforcement Learning"
text = """
The early history of reinforcement learning has two main threads, both long and rich, that\r
were pursued independently before intertwining in modern reinforcement learning. One\r
thread concerns learning by trial and error, and originated in the psychology of animal\r
learning. This thread runs through some of the earliest work in artificial intelligence\r
and led to the revival of reinforcement learning in the early 1980s. The second thread\r
concerns the problem of optimal control and its solution using value functions and\r
dynamic programming. For the most part, this thread did not involve learning. The\r
two threads were mostly independent, but became interrelated to some extent around a"""

[[sections]]
number = "14"
title = "Chapter 1: Introduction"
text = """
third, less distinct thread concerning temporal-di↵erence methods such as that used in\r
the tic-tac-toe example in this chapter. All three threads came together in the late 1980s\r
to produce the modern field of reinforcement learning as we present it in this book.\r
The thread focusing on trial-and-error learning is the one with which we are most\r
familiar and about which we have the most to say in this brief history. Before doing that,\r
however, we briefly discuss the optimal control thread.\r
The term “optimal control” came into use in the late 1950s to describe the problem of\r
designing a controller to minimize or maximize a measure of a dynamical system’s behavior\r
over time. One of the approaches to this problem was developed in the mid-1950s by\r
Richard Bellman and others through extending a nineteenth century theory of Hamilton\r
and Jacobi. This approach uses the concepts of a dynamical system’s state and of a\r
value function, or “optimal return function,” to define a functional equation, now often\r
called the Bellman equation. The class of methods for solving optimal control problems\r
by solving this equation came to be known as dynamic programming (Bellman, 1957a).\r
Bellman (1957b) also introduced the discrete stochastic version of the optimal control\r
problem known as Markov decision processes (MDPs). Ronald Howard (1960) devised\r
the policy iteration method for MDPs. All of these are essential elements underlying the\r
theory and algorithms of modern reinforcement learning.\r
Dynamic programming is widely considered the only feasible way of solving general\r
stochastic optimal control problems. It su↵ers from what Bellman called “the curse of\r
dimensionality,” meaning that its computational requirements grow exponentially with\r
the number of state variables, but it is still far more ecient and more widely applicable\r
than any other general method. Dynamic programming has been extensively developed\r
since the late 1950s, including extensions to partially observable MDPs (surveyed by\r
Lovejoy, 1991), many applications (surveyed by White, 1985, 1988, 1993), approximation\r
methods (surveyed by Rust, 1996), and asynchronous methods (Bertsekas, 1982, 1983).\r
Many excellent modern treatments of dynamic programming are available (e.g., Bertsekas,\r
2005, 2012; Puterman, 1994; Ross, 1983; Whittle, 1982, 1983). Bryson (1996) provides\r
an authoritative history of optimal control.\r
Connections between optimal control and dynamic programming, on the one hand,\r
and learning, on the other, were slow to be recognized. We cannot be sure about what\r
accounted for this separation, but its main cause was likely the separation between\r
the disciplines involved and their di↵erent goals. Also contributing may have been the\r
prevalent view of dynamic programming as an o↵-line computation depending essentially\r
on accurate system models and analytic solutions to the Bellman equation. Further,\r
the simplest form of dynamic programming is a computation that proceeds backwards\r
in time, making it dicult to see how it could be involved in a learning process that\r
must proceed in a forward direction. Some of the earliest work in dynamic programming,\r
such as that by Bellman and Dreyfus (1959), might now be classified as following\r
a learning approach. Witten’s (1977) work (discussed below) certainly qualifies as a\r
combination of learning and dynamic-programming ideas. Werbos (1987) argued explicitly\r
for greater interrelation of dynamic programming and learning methods and for dynamic\r
programming’s relevance to understanding neural and cognitive mechanisms. For us the\r
full integration of dynamic programming methods with online learning did not occur"""

[[sections]]
number = "1.7"
title = "Early History of Reinforcement Learning 15"
text = """
until the work of Chris Watkins in 1989, whose treatment of reinforcement learning using\r
the MDP formalism has been widely adopted. Since then these relationships have been\r
extensively developed by many researchers, most particularly by Dimitri Bertsekas and\r
John Tsitsiklis (1996), who coined the term “neurodynamic programming” to refer to\r
the combination of dynamic programming and artificial neural networks. Another term\r
currently in use is “approximate dynamic programming.” These various approaches\r
emphasize di↵erent aspects of the subject, but they all share with reinforcement learning\r
an interest in circumventing the classical shortcomings of dynamic programming.\r
We consider all of the work in optimal control also to be, in a sense, work in reinforce\u0002ment learning. We define a reinforcement learning method as any e↵ective way of solving\r
reinforcement learning problems, and it is now clear that these problems are closely\r
related to optimal control problems, particularly stochastic optimal control problems\r
such as those formulated as MDPs. Accordingly, we must consider the solution methods\r
of optimal control, such as dynamic programming, also to be reinforcement learning\r
methods. Because almost all of the conventional methods require complete knowledge\r
of the system to be controlled, it feels a little unnatural to say that they are part of\r
reinforcement learning. On the other hand, many dynamic programming algorithms are\r
incremental and iterative. Like learning methods, they gradually reach the correct answer\r
through successive approximations. As we show in the rest of this book, these similarities\r
are far more than superficial. The theories and solution methods for the cases of complete\r
and incomplete knowledge are so closely related that we feel they must be considered\r
together as part of the same subject matter.\r
Let us return now to the other major thread leading to the modern field of reinforcement\r
learning, the thread centered on the idea of trial-and-error learning. We only touch on\r
the major points of contact here, taking up this topic in more detail in Section 14.3.\r
According to American psychologist R. S. Woodworth (1938) the idea of trial-and-error\r
learning goes as far back as the 1850s to Alexander Bain’s discussion of learning by\r
“groping and experiment” and more explicitly to the British ethologist and psychologist\r
Conway Lloyd Morgan’s 1894 use of the term to describe his observations of animal\r
behavior. Perhaps the first to succinctly express the essence of trial-and-error learning as\r
a principle of learning was Edward Thorndike:\r
Of several responses made to the same situation, those which are accompanied\r
or closely followed by satisfaction to the animal will, other things being\r
equal, be more firmly connected with the situation, so that, when it recurs,\r
they will be more likely to recur; those which are accompanied or closely\r
followed by discomfort to the animal will, other things being equal, have their\r
connections with that situation weakened, so that, when it recurs, they will\r
be less likely to occur. The greater the satisfaction or discomfort, the greater\r
the strengthening or weakening of the bond. (Thorndike, 1911, p. 244)\r
Thorndike called this the “Law of E↵ect” because it describes the e↵ect of reinforcing\r
events on the tendency to select actions. Thorndike later modified the law to better\r
account for subsequent data on animal learning (such as di↵erences between the e↵ects\r
of reward and punishment), and the law in its various forms has generated considerable\r
controversy among learning theorists (e.g., see Gallistel, 2005; Herrnstein, 1970; Kimble,"""

[[sections]]
number = "16"
title = "Chapter 1: Introduction"
text = """
1961, 1967; Mazur, 1994). Despite this, the Law of E↵ect—in one form or another—is\r
widely regarded as a basic principle underlying much behavior (e.g., Hilgard and Bower,\r
1975; Dennett, 1978; Campbell, 1960; Cziko, 1995). It is the basis of the influential\r
learning theories of Clark Hull (1943, 1952) and the influential experimental methods of\r
B. F. Skinner (1938).\r
The term “reinforcement” in the context of animal learning came into use well after\r
Thorndike’s expression of the Law of E↵ect, first appearing in this context (to the best of\r
our knowledge) in the 1927 English translation of Pavlov’s monograph on conditioned\r
reflexes. Pavlov described reinforcement as the strengthening of a pattern of behavior due\r
to an animal receiving a stimulus—a reinforcer—in an appropriate temporal relationship\r
with another stimulus or with a response. Some psychologists extended the idea of\r
reinforcement to include weakening as well as strengthening of behavior, and extended\r
the idea of a reinforcer to include possibly the omission or termination of stimulus. To be\r
considered a reinforcer, the strengthening or weakening must persist after the reinforcer\r
is withdrawn; a stimulus that merely attracts an animal’s attention or that energizes its\r
behavior without producing lasting changes would not be considered a reinforcer.\r
The idea of implementing trial-and-error learning in a computer appeared among the\r
earliest thoughts about the possibility of artificial intelligence. In a 1948 report, Alan\r
Turing described a design for a “pleasure-pain system” that worked along the lines of the\r
Law of E↵ect:\r
When a configuration is reached for which the action is undetermined, a\r
random choice for the missing data is made and the appropriate entry is made\r
in the description, tentatively, and is applied. When a pain stimulus occurs\r
all tentative entries are cancelled, and when a pleasure stimulus occurs they\r
are all made permanent. (Turing, 1948)\r
Many ingenious electro-mechanical machines were constructed that demonstrated trial\u0002and-error learning. The earliest may have been a machine built by Thomas Ross (1933)\r
that was able to find its way through a simple maze and remember the path through\r
the settings of switches. In 1951 W. Grey Walter built a version of his “mechanical\r
tortoise” (Walter, 1950) capable of a simple form of learning. In 1952 Claude Shannon\r
demonstrated a maze-running mouse named Theseus that used trial and error to find\r
its way through a maze, with the maze itself remembering the successful directions\r
via magnets and relays under its floor (see also Shannon, 1951). J. A. Deutsch (1954)\r
described a maze-solving machine based on his behavior theory (Deutsch, 1953) that\r
has some properties in common with model-based reinforcement learning (Chapter 8).\r
In his PhD dissertation, Marvin Minsky (1954) discussed computational models of\r
reinforcement learning and described his construction of an analog machine composed of\r
components he called SNARCs (Stochastic Neural-Analog Reinforcement Calculators)\r
meant to resemble modifiable synaptic connections in the brain (Chapter 15). The\r
web site cyberneticzoo.com contains a wealth of information on these and many other\r
electro-mechanical learning machines.\r
Building electro-mechanical learning machines gave way to programming digital com\u0002puters to perform various types of learning, some of which implemented trial-and-error\r
learning. Farley and Clark (1954) described a digital simulation of a neural-network"""

[[sections]]
number = "1.7"
title = "Early History of Reinforcement Learning 17"
text = """
learning machine that learned by trial and error. But their interests soon shifted from\r
trial-and-error learning to generalization and pattern recognition, that is, from reinforce\u0002ment learning to supervised learning (Clark and Farley, 1955). This began a pattern\r
of confusion about the relationship between these types of learning. Many researchers\r
seemed to believe that they were studying reinforcement learning when they were actually\r
studying supervised learning. For example, artificial neural network pioneers such as\r
Rosenblatt (1962) and Widrow and Ho↵ (1960) were clearly motivated by reinforcement\r
learning—they used the language of rewards and punishments—but the systems they\r
studied were supervised learning systems suitable for pattern recognition and perceptual\r
learning. Even today, some researchers and textbooks minimize or blur the distinction\r
between these types of learning. For example, some textbooks have used the term “trial\u0002and-error” to describe artificial neural networks that learn from training examples. This\r
is an understandable confusion because these networks use error information to update\r
connection weights, but this misses the essential character of trial-and-error learning as\r
selecting actions on the basis of evaluative feedback that does not rely on knowledge of\r
what the correct action should be.\r
Partly as a result of these confusions, research into genuine trial-and-error learning\r
became rare in the 1960s and 1970s, although there were notable exceptions. In the 1960s\r
the terms “reinforcement” and “reinforcement learning” were used in the engineering\r
literature for the first time to describe engineering uses of trial-and-error learning (e.g.,\r
Waltz and Fu, 1965; Mendel, 1966; Fu, 1970; Mendel and McClaren, 1970). Particularly\r
influential was Minsky’s paper “Steps Toward Artificial Intelligence” (Minsky, 1961),\r
which discussed several issues relevant to trial-and-error learning, including prediction,\r
expectation, and what he called the basic credit-assignment problem for complex rein\u0002forcement learning systems: How do you distribute credit for success among the many\r
decisions that may have been involved in producing it? All of the methods we discuss in\r
this book are, in a sense, directed toward solving this problem. Minsky’s paper is well\r
worth reading today.\r
In the next few paragraphs we discuss some of the other exceptions and partial\r
exceptions to the relative neglect of computational and theoretical study of genuine\r
trial-and-error learning in the 1960s and 1970s.\r
One exception was the work of the New Zealand researcher John Andreae, who\r
developed a system called STeLLA that learned by trial and error in interaction with\r
its environment. This system included an internal model of the world and, later, an\r
“internal monologue” to deal with problems of hidden state (Andreae, 1963, 1969; Andreae\r
and Cashin, 1969). Andreae’s later work (1977) placed more emphasis on learning\r
from a teacher, but still included learning by trial and error, with the generation of\r
novel events being one of the system’s goals. A feature of this work was a “leakback\r
process,” elaborated more fully in Andreae (1998), that implemented a credit-assignment\r
mechanism similar to the backing-up update operations that we describe. Unfortunately,\r
his pioneering research was not well known and did not greatly impact subsequent\r
reinforcement learning research. Recent summaries are available (Andreae, 2017a,b).\r
More influential was the work of Donald Michie. In 1961 and 1963 he described a\r
simple trial-and-error learning system for learning how to play tic-tac-toe (or naughts"""

[[sections]]
number = "18"
title = "Chapter 1: Introduction"
text = """
and crosses) called MENACE (for Matchbox Educable Naughts and Crosses Engine). It\r
consisted of a matchbox for each possible game position, each matchbox containing a\r
number of colored beads, a di↵erent color for each possible move from that position. By\r
drawing a bead at random from the matchbox corresponding to the current game position,\r
one could determine MENACE’s move. When a game was over, beads were added to\r
or removed from the boxes used during play to reward or punish MENACE’s decisions.\r
Michie and Chambers (1968) described another tic-tac-toe reinforcement learner called\r
GLEE (Game Learning Expectimaxing Engine) and a reinforcement learning controller\r
called BOXES. They applied BOXES to the task of learning to balance a pole hinged to\r
a movable cart on the basis of a failure signal occurring only when the pole fell or the\r
cart reached the end of a track. This task was adapted from the earlier work of Widrow\r
and Smith (1964), who used supervised learning methods, assuming instruction from a\r
teacher already able to balance the pole. Michie and Chambers’s version of pole-balancing\r
is one of the best early examples of a reinforcement learning task under conditions of\r
incomplete knowledge. It influenced much later work in reinforcement learning, beginning\r
with some of our own studies (Barto, Sutton, and Anderson, 1983; Sutton, 1984). Michie\r
(1974) consistently emphasized trial and error and learning as essential aspects of artificial\r
intelligence.\r
Widrow, Gupta, and Maitra (1973) modified the Least-Mean-Square (LMS) algorithm\r
of Widrow and Ho↵ (1960) to produce a reinforcement learning rule that could learn\r
from success and failure signals instead of from training examples. They called this form\r
of learning “selective bootstrap adaptation” and described it as “learning with a critic”\r
instead of “learning with a teacher.” They analyzed this rule and showed how it could\r
learn to play blackjack. This was an isolated foray into reinforcement learning by Widrow,\r
whose contributions to supervised learning were much more influential. Our use of the\r
term “critic” is derived from Widrow, Gupta, and Maitra’s paper. Buchanan, Mitchell,\r
Smith, and Johnson (1978) independently used the term critic in the context of machine\r
learning (see also Dietterich and Buchanan, 1984), but for them a critic was an expert\r
system able to do more than evaluate performance.\r
Research on learning automata had a more direct influence on the trial-and-error\r
thread leading to modern reinforcement learning research. These are methods for solving\r
a nonassociative, purely selectional learning problem known as the k-armed bandit by\r
analogy to a slot machine, or “one-armed bandit,” except with k levers (see Chapter 2).\r
Learning automata are simple, low-memory machines for improving the probability\r
of reward in these problems. Learning automata originated with work in the 1960s\r
of the Russian mathematician and physicist M. L. Tsetlin and colleagues (published\r
posthumously in Tsetlin, 1973) and has been extensively developed since then within\r
engineering (see Narendra and Thathachar, 1974, 1989). These developments included the\r
study of stochastic learning automata, which are methods for updating action probabilities\r
on the basis of reward signals. Although not developed in the tradition of stochastic\r
learning automata, Harth and Tzanakou’s (1974) Alopex algorithm (for Algorithm of\r
pattern extraction) is a stochastic method for detecting correlations between actions and\r
reinforcement that influenced some of our early research (Barto, Sutton, and Brouwer,\r
1981). Stochastic learning automata were foreshadowed by earlier work in psychology,\r
beginning with William Estes’ (1950) e↵ort toward a statistical theory of learning and\r
further developed by others (e.g., Bush and Mosteller, 1955; Sternberg, 1963)."""

[[sections]]
number = "1.7"
title = "Early History of Reinforcement Learning 19"
text = """
The statistical learning theories developed in psychology were adopted by researchers in\r
economics, leading to a thread of research in that field devoted to reinforcement learning.\r
This work began in 1973 with the application of Bush and Mosteller’s learning theory to\r
a collection of classical economic models (Cross, 1973). One goal of this research was to\r
study artificial agents that act more like real people than do traditional idealized economic\r
agents (Arthur, 1991). This approach expanded to the study of reinforcement learning\r
in the context of game theory. Reinforcement learning in economics developed largely\r
independently of the early work in reinforcement learning in artificial intelligence, though\r
game theory remains a topic of interest in both fields (beyond the scope of this book).\r
Camerer (2011) discusses the reinforcement learning tradition in economics, and Now´e,\r
Vrancx, and De Hauwere (2012) provide an overview of the subject from the point of view\r
of multi-agent extensions to the approach that we introduce in this book. Reinforcement\r
learning in the context of game theory is a much di↵erent subject than reinforcement\r
learning used in programs to play tic-tac-toe, checkers, and other recreational games. See,\r
for example, Szita (2012) for an overview of this aspect of reinforcement learning and\r
games.\r
John Holland (1975) outlined a general theory of adaptive systems based on selectional\r
principles. His early work concerned trial and error primarily in its nonassociative\r
form, as in evolutionary methods and the k-armed bandit. In 1976 and more fully in\r
1986, he introduced classifier systems, true reinforcement learning systems including\r
association and value functions. A key component of Holland’s classifier systems was\r
the “bucket-brigade algorithm” for credit assignment, which is closely related to the\r
temporal di↵erence algorithm used in our tic-tac-toe example and discussed in Chapter 6.\r
Another key component was a genetic algorithm, an evolutionary method whose role was\r
to evolve useful representations. Classifier systems have been extensively developed by\r
many researchers to form a major branch of reinforcement learning research (reviewed by\r
Urbanowicz and Moore, 2009), but genetic algorithms—which we do not consider to be\r
reinforcement learning systems by themselves—have received much more attention, as\r
have other approaches to evolutionary computation (e.g., Fogel, Owens and Walsh, 1966;\r
Koza, 1992).\r
The individual most responsible for reviving the trial-and-error thread of reinforcement\r
learning within artificial intelligence was Harry Klopf (1972, 1975, 1982). Klopf recognized\r
that essential aspects of adaptive behavior were being lost as learning researchers came\r
to focus almost exclusively on supervised learning. What was missing, according to\r
Klopf, were the hedonic aspects of behavior: the drive to achieve some result from the\r
environment, to control the environment toward desired ends and away from undesired\r
ends (see Section 15.9). This is the essential idea of trial-and-error learning. Klopf’s\r
ideas were especially influential on the authors because our assessment of them (Barto\r
and Sutton, 1981a) led to our appreciation of the distinction between supervised and\r
reinforcement learning, and to our eventual focus on reinforcement learning. Much of\r
the early work that we and colleagues accomplished was directed toward showing that\r
reinforcement learning and supervised learning were indeed di↵erent (Barto, Sutton, and\r
Brouwer, 1981; Barto and Sutton, 1981b; Barto and Anandan, 1985). Other studies\r
showed how reinforcement learning could address important problems in artificial neural"""

[[sections]]
number = "20"
title = "Chapter 1: Introduction"
text = """
network learning, in particular, how it could produce learning algorithms for multilayer\r
networks (Barto, Anderson, and Sutton, 1982; Barto and Anderson, 1985; Barto, 1985,\r
1986; Barto and Jordan, 1987; see Section 15.10).\r
We turn now to the third thread to the history of reinforcement learning, that concerning\r
temporal-di↵erence learning. Temporal-di↵erence learning methods are distinctive in\r
being driven by the di↵erence between temporally successive estimates of the same\r
quantity—for example, of the probability of winning in the tic-tac-toe example. This\r
thread is smaller and less distinct than the other two, but it has played a particularly\r
important role in the field, in part because temporal-di↵erence methods seem to be new\r
and unique to reinforcement learning.\r
The origins of temporal-di↵erence learning are in part in animal learning psychology,\r
in particular, in the notion of secondary reinforcers. A secondary reinforcer is a stimulus\r
that has been paired with a primary reinforcer such as food or pain and, as a result, has\r
come to take on similar reinforcing properties. Minsky (1954) may have been the first to\r
realize that this psychological principle could be important for artificial learning systems.\r
Arthur Samuel (1959) was the first to propose and implement a learning method that\r
included temporal-di↵erence ideas, as part of his celebrated checkers-playing program\r
(Section 16.2).\r
Samuel made no reference to Minsky’s work or to possible connections to animal\r
learning. His inspiration apparently came from Claude Shannon’s (1950) suggestion that\r
a computer could be programmed to use an evaluation function to play chess, and that it\r
might be able to improve its play by modifying this function online. (It is possible that\r
these ideas of Shannon’s also influenced Bellman, but we know of no evidence for this.)\r
Minsky (1961) extensively discussed Samuel’s work in his “Steps” paper, suggesting the\r
connection to secondary reinforcement theories, both natural and artificial.\r
As we have discussed, in the decade following the work of Minsky and Samuel, little\r
computational work was done on trial-and-error learning, and apparently no computational\r
work at all was done on temporal-di↵erence learning. In 1972, Klopf brought trial-and\u0002error learning together with an important component of temporal-di↵erence learning.\r
Klopf was interested in principles that would scale to learning in large systems, and thus\r
was intrigued by notions of local reinforcement, whereby subcomponents of an overall\r
learning system could reinforce one another. He developed the idea of “generalized\r
reinforcement,” whereby every component (nominally, every neuron) views all of its\r
inputs in reinforcement terms: excitatory inputs as rewards and inhibitory inputs as\r
punishments. This is not the same idea as what we now know as temporal-di↵erence\r
learning, and in retrospect it is farther from it than was Samuel’s work. On the other\r
hand, Klopf linked the idea with trial-and-error learning and related it to the massive\r
empirical database of animal learning psychology.\r
Sutton (1978a,b,c) developed Klopf’s ideas further, particularly the links to animal\r
learning theories, describing learning rules driven by changes in temporally successive\r
predictions. He and Barto refined these ideas and developed a psychological model of\r
classical conditioning based on temporal-di↵erence learning (Sutton and Barto, 1981a;\r
Barto and Sutton, 1982). There followed several other influential psychological models of\r
classical conditioning based on temporal-di↵erence learning (e.g., Klopf, 1988; Moore et al.,"""

[[sections]]
number = "1.7"
title = "Early History of Reinforcement Learning 21"
text = """
1986; Sutton and Barto, 1987, 1990). Some neuroscience models developed at this time\r
are well interpreted in terms of temporal-di↵erence learning (Hawkins and Kandel, 1984;\r
Byrne, Gingrich, and Baxter, 1990; Gelperin, Hopfield, and Tank, 1985; Tesauro, 1986;\r
Friston et al., 1994), although in most cases there was no historical connection.\r
Our early work on temporal-di↵erence learning was strongly influenced by animal\r
learning theories and by Klopf’s work. Relationships to Minsky’s “Steps” paper and to\r
Samuel’s checkers players were recognized only afterward. By 1981, however, we were\r
fully aware of all the prior work mentioned above as part of the temporal-di↵erence and\r
trial-and-error threads. At this time we developed a method for using temporal-di↵erence\r
learning combined with trial-and-error learning, known as the actor–critic architecture,\r
and applied this method to Michie and Chambers’s pole-balancing problem (Barto,\r
Sutton, and Anderson, 1983). This method was extensively studied in Sutton’s (1984)\r
PhD dissertation and extended to use backpropagation neural networks in Anderson’s\r
(1986) PhD dissertation. Around this time, Holland (1986) incorporated temporal\u0002di↵erence ideas explicitly into his classifier systems in the form of his bucket-brigade\r
algorithm. A key step was taken by Sutton (1988) by separating temporal-di↵erence\r
learning from control, treating it as a general prediction method. That paper also\r
introduced the TD() algorithm and proved some of its convergence properties.\r
As we were finalizing our work on the actor–critic architecture in 1981, we discovered\r
a paper by Ian Witten (1977, 1976a) which appears to be the earliest publication of a\r
temporal-di↵erence learning rule. He proposed the method that we now call tabular TD(0)\r
for use as part of an adaptive controller for solving MDPs. This work was first submitted\r
for journal publication in 1974 and also appeared in Witten’s 1976 PhD dissertation.\r
Witten’s work was a descendant of Andreae’s early experiments with STeLLA and other\r
trial-and-error learning systems. Thus, Witten’s 1977 paper spanned both major threads\r
of reinforcement learning research—trial-and-error learning and optimal control—while\r
making a distinct early contribution to temporal-di↵erence learning.\r
The temporal-di↵erence and optimal control threads were fully brought together\r
in 1989 with Chris Watkins’s development of Q-learning. This work extended and\r
integrated prior work in all three threads of reinforcement learning research. Paul Werbos\r
(1987) contributed to this integration by arguing for the convergence of trial-and-error\r
learning and dynamic programming since 1977. By the time of Watkins’s work there had\r
been tremendous growth in reinforcement learning research, primarily in the machine\r
learning subfield of artificial intelligence, but also in artificial neural networks and artificial\r
intelligence more broadly. In 1992, the remarkable success of Gerry Tesauro’s backgammon\r
playing program, TD-Gammon, brought additional attention to the field.\r
In the time since publication of the first edition of this book, a flourishing subfield of\r
neuroscience developed that focuses on the relationship between reinforcement learning\r
algorithms and reinforcement learning in the nervous system. Most responsible for this is\r
an uncanny similarity between the behavior of temporal-di↵erence algorithms and the\r
activity of dopamine producing neurons in the brain, as pointed out by a number of\r
researchers (Friston et al., 1994; Barto, 1995a; Houk, Adams, and Barto, 1995; Montague,\r
Dayan, and Sejnowski, 1996; and Schultz, Dayan, and Montague, 1997). Chapter 15\r
provides an introduction to this exciting aspect of reinforcement learning. Other important"""

[[sections]]
number = "22"
title = "Chapter 1: Introduction"
text = """
contributions made in the recent history of reinforcement learning are too numerous to\r
mention in this brief account; we cite many of these at the end of the individual chapters\r
in which they arise.\r
Bibliographical Remarks\r
For additional general coverage of reinforcement learning, we refer the reader to the\r
books by Szepesv´ari (2010), Bertsekas and Tsitsiklis (1996), Kaelbling (1993a), and\r
Sugiyama, Hachiya, and Morimura (2013). Books that take a control or operations research\r
perspective include those of Si, Barto, Powell, and Wunsch (2004), Powell (2011), Lewis\r
and Liu (2012), and Bertsekas (2012). Cao’s (2009) review places reinforcement learning\r
in the context of other approaches to learning and optimization of stochastic dynamic\r
systems. Three special issues of the journal Machine Learning focus on reinforcement\r
learning: Sutton (1992a), Kaelbling (1996), and Singh (2002). Useful surveys are provided\r
by Barto (1995b); Kaelbling, Littman, and Moore (1996); and Keerthi and Ravindran\r
(1997). The volume edited by Weiring and van Otterlo (2012) provides an excellent\r
overview of recent developments."""

[[sections]]
number = "1.2"
title = "The example of Phil’s breakfast in this chapter was inspired by Agre (1988)."
text = ""

[[sections]]
number = "1.5"
title = "The temporal-di↵erence method used in the tic-tac-toe example is developed in"
text = """
Chapter 6.

Part I: Tabular Solution Methods\r
In this part of the book we describe almost all the core ideas of reinforcement learning\r
algorithms in their simplest forms: that in which the state and action spaces are small\r
enough for the approximate value functions to be represented as arrays, or tables. In\r
this case, the methods can often find exact solutions, that is, they can often find exactly\r
the optimal value function and the optimal policy. This contrasts with the approximate\r
methods described in the next part of the book, which only find approximate solutions,\r
but which in return can be applied e↵ectively to much larger problems.\r
The first chapter of this part of the book describes solution methods for the special\r
case of the reinforcement learning problem in which there is only a single state, called\r
bandit problems. The second chapter describes the general problem formulation that we\r
treat throughout the rest of the book—finite Markov decision processes—and its main\r
ideas including Bellman equations and value functions.\r
The next three chapters describe three fundamental classes of methods for solving finite\r
Markov decision problems: dynamic programming, Monte Carlo methods, and temporal\u0002di↵erence learning. Each class of methods has its strengths and weaknesses. Dynamic\r
programming methods are well developed mathematically, but require a complete and\r
accurate model of the environment. Monte Carlo methods don’t require a model and are\r
conceptually simple, but are not well suited for step-by-step incremental computation.\r
Finally, temporal-di↵erence methods require no model and are fully incremental, but are\r
more complex to analyze. The methods also di↵er in several ways with respect to their\r
eciency and speed of convergence.\r
The remaining two chapters describe how these three classes of methods can be\r
combined to obtain the best features of each of them. In one chapter we describe how\r
the strengths of Monte Carlo methods can be combined with the strengths of temporal\u0002di↵erence methods via multi-step bootstrapping methods. In the final chapter of this part\r
of the book we show how temporal-di↵erence learning methods can be combined with\r
model learning and planning methods (such as dynamic programming) for a complete\r
and unified solution to the tabular reinforcement learning problem.

Chapter 2\r
Multi-armed Bandits\r
The most important feature distinguishing reinforcement learning from other types of\r
learning is that it uses training information that evaluates the actions taken rather\r
than instructs by giving correct actions. This is what creates the need for active\r
exploration, for an explicit search for good behavior. Purely evaluative feedback indicates\r
how good the action taken was, but not whether it was the best or the worst action\r
possible. Purely instructive feedback, on the other hand, indicates the correct action to\r
take, independently of the action actually taken. This kind of feedback is the basis of\r
supervised learning, which includes large parts of pattern classification, artificial neural\r
networks, and system identification. In their pure forms, these two kinds of feedback\r
are quite distinct: evaluative feedback depends entirely on the action taken, whereas\r
instructive feedback is independent of the action taken.\r
In this chapter we study the evaluative aspect of reinforcement learning in a simplified\r
setting, one that does not involve learning to act in more than one situation. This\r
nonassociative setting is the one in which most prior work involving evaluative feedback\r
has been done, and it avoids much of the complexity of the full reinforcement learning\r
problem. Studying this case enables us to see most clearly how evaluative feedback di↵ers\r
from, and yet can be combined with, instructive feedback.\r
The particular nonassociative, evaluative feedback problem that we explore is a simple\r
version of the k-armed bandit problem. We use this problem to introduce a number\r
of basic learning methods which we extend in later chapters to apply to the full rein\u0002forcement learning problem. At the end of this chapter, we take a step closer to the full\r
reinforcement learning problem by discussing what happens when the bandit problem\r
becomes associative, that is, when the best action depends on the situation."""

[[sections]]
number = "2.1"
title = "A k-armed Bandit Problem"
text = """
Consider the following learning problem. You are faced repeatedly with a choice among\r
k di↵erent options, or actions. After each choice you receive a numerical reward chosen\r
from a stationary probability distribution that depends on the action you selected. Your"""

[[sections]]
number = "26"
title = "Chapter 2: Multi-armed Bandits"
text = """
objective is to maximize the expected total reward over some time period, for example,\r
over 1000 action selections, or time steps.\r
This is the original form of the k-armed bandit problem, so named by analogy to a slot\r
machine, or “one-armed bandit,” except that it has k levers instead of one. Each action\r
selection is like a play of one of the slot machine’s levers, and the rewards are the payo↵s\r
for hitting the jackpot. Through repeated action selections you are to maximize your\r
winnings by concentrating your actions on the best levers. Another analogy is that of\r
a doctor choosing between experimental treatments for a series of seriously ill patients.\r
Each action is the selection of a treatment, and each reward is the survival or well-being\r
of the patient. Today the term “bandit problem” is sometimes used for a generalization\r
of the problem described above, but in this book we use it to refer just to this simple\r
case.\r
In our k-armed bandit problem, each of the k actions has an expected or mean reward\r
given that that action is selected; let us call this the value of that action. We denote the\r
action selected on time step t as At, and the corresponding reward as Rt. The value then\r
of an arbitrary action a, denoted q⇤(a), is the expected reward given that a is selected:\r
q⇤(a) .= E[Rt | At =a] .\r
If you knew the value of each action, then it would be trivial to solve the k-armed bandit\r
problem: you would always select the action with highest value. We assume that you do\r
not know the action values with certainty, although you may have estimates. We denote\r
the estimated value of action a at time step t as Qt(a). We would like Qt(a) to be close\r
to q⇤(a).\r
If you maintain estimates of the action values, then at any time step there is at least\r
one action whose estimated value is greatest. We call these the greedy actions. When you\r
select one of these actions, we say that you are exploiting your current knowledge of the\r
values of the actions. If instead you select one of the nongreedy actions, then we say you\r
are exploring, because this enables you to improve your estimate of the nongreedy action’s\r
value. Exploitation is the right thing to do to maximize the expected reward on the one\r
step, but exploration may produce the greater total reward in the long run. For example,\r
suppose a greedy action’s value is known with certainty, while several other actions are\r
estimated to be nearly as good but with substantial uncertainty. The uncertainty is\r
such that at least one of these other actions probably is actually better than the greedy\r
action, but you don’t know which one. If you have many time steps ahead on which\r
to make action selections, then it may be better to explore the nongreedy actions and\r
discover which of them are better than the greedy action. Reward is lower in the short\r
run, during exploration, but higher in the long run because after you have discovered\r
the better actions, you can exploit them many times. Because it is not possible both to\r
explore and to exploit with any single action selection, one often refers to the “conflict”\r
between exploration and exploitation.\r
In any specific case, whether it is better to explore or exploit depends in a complex\r
way on the precise values of the estimates, uncertainties, and the number of remaining\r
steps. There are many sophisticated methods for balancing exploration and exploitation\r
for particular mathematical formulations of the k-armed bandit and related problems."""

[[sections]]
number = "2.2"
title = "Action-value Methods 27"
text = """
However, most of these methods make strong assumptions about stationarity and prior\r
knowledge that are either violated or impossible to verify in most applications and in\r
the full reinforcement learning problem that we consider in subsequent chapters. The\r
guarantees of optimality or bounded loss for these methods are of little comfort when the\r
assumptions of their theory do not apply.\r
In this book we do not worry about balancing exploration and exploitation in a\r
sophisticated way; we worry only about balancing them at all. In this chapter we present\r
several simple balancing methods for the k-armed bandit problem and show that they\r
work much better than methods that always exploit. The need to balance exploration\r
and exploitation is a distinctive challenge that arises in reinforcement learning; the\r
simplicity of our version of the k-armed bandit problem enables us to show this in a\r
particularly clear form."""

[[sections]]
number = "2.2"
title = "Action-value Methods"
text = """
We begin by looking more closely at methods for estimating the values of actions and\r
for using the estimates to make action selection decisions, which we collectively call\r
action-value methods. Recall that the true value of an action is the mean reward when\r
that action is selected. One natural way to estimate this is by averaging the rewards\r
actually received:\r
Qt(a) .= sum of rewards when a taken prior to t\r
number of times a taken prior to t =\r
Pt1\r
i=1 Ri · Ai=a Pt1\r
i=1 Ai=a\r
, (2.1)\r
where predicate denotes the random variable that is 1 if predicate is true and 0 if it is not.\r
If the denominator is zero, then we instead define Qt(a) as some default value, such as\r
0. As the denominator goes to infinity, by the law of large numbers, Qt(a) converges to\r
q⇤(a). We call this the sample-average method for estimating action values because each\r
estimate is an average of the sample of relevant rewards. Of course this is just one way\r
to estimate action values, and not necessarily the best one. Nevertheless, for now let us\r
stay with this simple estimation method and turn to the question of how the estimates\r
might be used to select actions.\r
The simplest action selection rule is to select one of the actions with the highest\r
estimated value, that is, one of the greedy actions as defined in the previous section.\r
If there is more than one greedy action, then a selection is made among them in some\r
arbitrary way, perhaps randomly. We write this greedy action selection method as\r
At\r
.\r
= argmax aQt(a), (2.2)\r
where argmaxa denotes the action a for which the expression that follows is maximized\r
(with ties broken arbitrarily). Greedy action selection always exploits current knowledge to\r
maximize immediate reward; it spends no time at all sampling apparently inferior actions\r
to see if they might really be better. A simple alternative is to behave greedily most of\r
the time, but every once in a while, say with small probability ", instead select randomly"""

[[sections]]
number = "28"
title = "Chapter 2: Multi-armed Bandits"
text = """
from among all the actions with equal probability, independently of the action-value\r
estimates. We call methods using this near-greedy action selection rule "-greedy methods.\r
An advantage of these methods is that, in the limit as the number of steps increases,\r
every action will be sampled an infinite number of times, thus ensuring that all the Qt(a)\r
converge to their respective q⇤(a). This of course implies that the probability of selecting\r
the optimal action converges to greater than 1  ", that is, to near certainty. These are\r
just asymptotic guarantees, however, and say little about the practical e↵ectiveness of\r
the methods.\r
Exercise 2.1 In "-greedy action selection, for the case of two actions and " = 0.5, what is\r
the probability that the greedy action is selected? ⇤"""

[[sections]]
number = "2.3"
title = "The 10-armed Testbed"
text = """
To roughly assess the relative e↵ectiveness of the greedy and "-greedy action-value\r
methods, we compared them numerically on a suite of test problems. This was a set\r
of 2000 randomly generated k-armed bandit problems with k = 10. For each bandit\r
problem, such as the one shown in Figure 2.1, the action values, q⇤(a), a = 1,..., 10,\r
0\r
1\r
2\r
3\r
-3\r
-2\r
-1\r
q⇤(1)\r
q⇤(2)\r
q⇤(3)\r
q⇤(4)\r
q⇤(5)\r
q⇤(6)\r
q⇤(7)\r
q⇤(8)\r
q⇤(9)\r
q⇤(10)\r
Reward\r
distribution\r
1 2 3 4 5 6 7 8 9 10\r
Action\r
Figure 2.1: An example bandit problem from the 10-armed testbed. The true value q⇤(a) of\r
each of the ten actions was selected according to a normal distribution with mean zero and unit\r
variance, and then the actual rewards were selected according to a mean q⇤(a), unit-variance\r
normal distribution, as suggested by these gray distributions."""

[[sections]]
number = "2.3"
title = "The 10-armed Testbed 29"
text = """
were selected according to a normal (Gaussian) distribution with mean 0 and variance 1.\r
Then, when a learning method applied to that problem selected action At at time step t,\r
the actual reward, Rt, was selected from a normal distribution with mean q⇤(At) and\r
variance 1. These distributions are shown in gray in Figure 2.1. We call this suite of test\r
tasks the 10-armed testbed. For any learning method, we can measure its performance\r
and behavior as it improves with experience over 1000 time steps when applied to one of\r
the bandit problems. This makes up one run. Repeating this for 2000 independent runs,\r
each with a di↵erent bandit problem, we obtained measures of the learning algorithm’s\r
average behavior.\r
Figure 2.2 compares a greedy method with two "-greedy methods ("= 0.01 and "= 0.1),\r
as described above, on the 10-armed testbed. All the methods formed their action-value\r
estimates using the sample-average technique (with an initial estimate of 0). The upper\r
graph shows the increase in expected reward with experience. The greedy method\r
improved slightly faster than the other methods at the very beginning, but then leveled\r
o↵ at a lower level. It achieved a reward-per-step of only about 1, compared with the best\r
possible of about 1.54 on this testbed. The greedy method performed significantly worse\r
in the long run because it often got stuck performing suboptimal actions. The lower graph\r
 (greedy)\r
0\r
0.5\r
1"""

[[sections]]
number = "1.5"
title = "Average"
text = """
reward\r
0 250 500 750 1000\r
Steps\r
0%\r
20%\r
40%\r
60%\r
80%\r
100%\r
%\r
Optimal\r
action\r
0 250 500 750 1000\r
Steps\r
1\r
1\r
"= 0.1\r
"= 0.01\r
"= 0.1\r
"= 0.01\r
"= 0\r
"= 0 (greedy)\r
Figure 2.2: Average performance of "-greedy action-value methods on the 10-armed testbed.\r
These data are averages over 2000 runs with di↵erent bandit problems. All methods used sample\r
averages as their action-value estimates."""

[[sections]]
number = "30"
title = "Chapter 2: Multi-armed Bandits"
text = """
shows that the greedy method found the optimal action in only approximately one-third\r
of the tasks. In the other two-thirds, its initial samples of the optimal action were\r
disappointing, and it never returned to it. The "-greedy methods eventually performed\r
better because they continued to explore and to improve their chances of recognizing\r
the optimal action. The " = 0.1 method explored more, and usually found the optimal\r
action earlier, but it never selected that action more than 91% of the time. The " = 0.01\r
method improved more slowly, but eventually would perform better than the " = 0.1\r
method on both performance measures shown in the figure. It is also possible to reduce "\r
over time to try to get the best of both high and low values.\r
The advantage of "-greedy over greedy methods depends on the task. For example,\r
suppose the reward variance had been larger, say 10 instead of 1. With noisier rewards\r
it takes more exploration to find the optimal action, and "-greedy methods should fare\r
even better relative to the greedy method. On the other hand, if the reward variances\r
were zero, then the greedy method would know the true value of each action after trying\r
it once. In this case the greedy method might actually perform best because it would\r
soon find the optimal action and then never explore. But even in the deterministic case\r
there is a large advantage to exploring if we weaken some of the other assumptions. For\r
example, suppose the bandit task were nonstationary, that is, the true values of the\r
actions changed over time. In this case exploration is needed even in the deterministic\r
case to make sure one of the nongreedy actions has not changed to become better than\r
the greedy one. As we shall see in the next few chapters, nonstationarity is the case\r
most commonly encountered in reinforcement learning. Even if the underlying task is\r
stationary and deterministic, the learner faces a set of banditlike decision tasks each of\r
which changes over time as learning proceeds and the agent’s decision-making policy\r
changes. Reinforcement learning requires a balance between exploration and exploitation.\r
Exercise 2.2: Bandit example Consider a k-armed bandit problem with k = 4 actions,\r
denoted 1, 2, 3, and 4. Consider applying to this problem a bandit algorithm using\r
"-greedy action selection, sample-average action-value estimates, and initial estimates\r
of Q1(a) = 0, for all a. Suppose the initial sequence of actions and rewards is A1 = 1,\r
R1 = 1, A2 = 2, R2 = 1, A3 = 2, R3 = 2, A4 = 2, R4 = 2, A5 = 3, R5 = 0. On some\r
of these time steps the " case may have occurred, causing an action to be selected at\r
random. On which time steps did this definitely occur? On which time steps could this\r
possibly have occurred? ⇤\r
Exercise 2.3 In the comparison shown in Figure 2.2, which method will perform best in\r
the long run in terms of cumulative reward and probability of selecting the best action?\r
How much better will it be? Express your answer quantitatively. ⇤"""

[[sections]]
number = "2.4"
title = "Incremental Implementation"
text = """
The action-value methods we have discussed so far all estimate action values as sample\r
averages of observed rewards. We now turn to the question of how these averages can be\r
computed in a computationally ecient manner, in particular, with constant memory\r
and constant per-time-step computation."""

[[sections]]
number = "2.4"
title = "Incremental Implementation 31"
text = """
To simplify notation we concentrate on a single action. Let Ri now denote the reward\r
received after the ith selection of this action, and let Qn denote the estimate of its action\r
value after it has been selected n  1 times, which we can now write simply as\r
Qn\r
.\r
= R1 + R2 + ··· + Rn1\r
n  1 .\r
The obvious implementation would be to maintain a record of all the rewards and then\r
perform this computation whenever the estimated value was needed. However, if this is\r
done, then the memory and computational requirements would grow over time as more\r
rewards are seen. Each additional reward would require additional memory to store it\r
and additional computation to compute the sum in the numerator.\r
As you might suspect, this is not really necessary. It is easy to devise incremental\r
formulas for updating averages with small, constant computation required to process\r
each new reward. Given Qn and the nth reward, Rn, the new average of all n rewards\r
can be computed by\r
Qn+1 = 1\r
n\r
Xn\r
i=1\r
Ri\r
= 1\r
n\r
 \r
Rn +\r
n\r
X1\r
i=1\r
Ri\r
!\r
= 1\r
n\r
 \r
Rn + (n  1) 1\r
n  1\r
n\r
X1\r
i=1\r
Ri\r
!\r
= 1\r
n\r
⇣\r
Rn + (n  1)Qn\r
⌘\r
= 1\r
n\r
⇣\r
Rn + nQn  Qn\r
⌘\r
= Qn +\r
1\r
n\r
h\r
Rn  Qn\r
i\r
, (2.3)\r
which holds even for n = 1, obtaining Q2 = R1 for arbitrary Q1. This implementation\r
requires memory only for Qn and n, and only the small computation (2.3) for each new\r
reward.\r
This update rule (2.3) is of a form that occurs frequently throughout this book. The\r
general form is\r
NewEstimate OldEstimate + StepSize hTarget  OldEstimatei. (2.4)\r
The expression ⇥TargetOldEstimate⇤is an error in the estimate. It is reduced by taking\r
a step toward the “Target.” The target is presumed to indicate a desirable direction in\r
which to move, though it may be noisy. In the case above, for example, the target is the\r
nth reward.\r
Note that the step-size parameter (StepSize) used in the incremental method (2.3)\r
changes from time step to time step. In processing the nth reward for action a, the"""

[[sections]]
number = "32"
title = "Chapter 2: Multi-armed Bandits"
text = """
method uses the step-size parameter 1\r
n . In this book we denote the step-size parameter\r
by ↵ or, more generally, by ↵t(a).\r
Pseudocode for a complete bandit algorithm using incrementally computed sample\r
averages and "-greedy action selection is shown in the box below. The function bandit(a)\r
is assumed to take an action and return a corresponding reward.\r
A simple bandit algorithm\r
Initialize, for a = 1 to k:\r
Q(a) 0\r
N(a) 0\r
Loop forever:\r
A \r
⇢ argmaxa Q(a) with probability 1  " (breaking ties randomly)\r
a random action with probability "\r
R bandit(A)\r
N(A) N(A)+1\r
Q(A) Q(A) + 1\r
N(A)\r
⇥\r
R  Q(A)\r
⇤"""

[[sections]]
number = "2.5"
title = "Tracking a Nonstationary Problem"
text = """
The averaging methods discussed so far are appropriate for stationary bandit problems,\r
that is, for bandit problems in which the reward probabilities do not change over time.\r
As noted earlier, we often encounter reinforcement learning problems that are e↵ectively\r
nonstationary. In such cases it makes sense to give more weight to recent rewards than\r
to long-past rewards. One of the most popular ways of doing this is to use a constant\r
step-size parameter. For example, the incremental update rule (2.3) for updating an\r
average Qn of the n  1 past rewards is modified to be\r
Qn+1\r
.\r
= Qn + ↵\r
h\r
Rn  Qn\r
i\r
, (2.5)\r
where the step-size parameter ↵ 2 (0, 1] is constant. This results in Qn+1 being a weighted\r
average of past rewards and the initial estimate Q1:\r
Qn+1 = Qn + ↵\r
h\r
Rn  Qn\r
i\r
= ↵Rn + (1  ↵)Qn\r
= ↵Rn + (1  ↵) [↵Rn1 + (1  ↵)Qn1]\r
= ↵Rn + (1  ↵)↵Rn1 + (1  ↵)\r
2Qn1\r
= ↵Rn + (1  ↵)↵Rn1 + (1  ↵)\r
2↵Rn2 +\r
··· + (1  ↵)\r
n1↵R1 + (1  ↵)nQ1\r
= (1  ↵)\r
nQ1 +Xn\r
i=1\r
↵(1  ↵)\r
ni\r
Ri. (2.6)"""

[[sections]]
number = "2.5"
title = "Tracking a Nonstationary Problem 33"
text = """
We call this a weighted average because the sum of the weights is (1  ↵)n + Pn\r
i=1 ↵(1 \r
↵)ni = 1, as you can check for yourself. Note that the weight, ↵(1  ↵)ni, given to the\r
reward Ri depends on how many rewards ago, n  i, it was observed. The quantity 1  ↵\r
is less than 1, and thus the weight given to Ri decreases as the number of intervening\r
rewards increases. In fact, the weight decays exponentially according to the exponent\r
on 1  ↵. (If 1  ↵ = 0, then all the weight goes on the very last reward, Rn, because\r
of the convention that 00 = 1.) Accordingly, this is sometimes called an exponential\r
recency-weighted average.\r
Sometimes it is convenient to vary the step-size parameter from step to step. Let ↵n(a)\r
denote the step-size parameter used to process the reward received after the nth selection\r
of action a. As we have noted, the choice ↵n(a) = 1\r
n results in the sample-average method,\r
which is guaranteed to converge to the true action values by the law of large numbers.\r
But of course convergence is not guaranteed for all choices of the sequence {↵n(a)}. A\r
well-known result in stochastic approximation theory gives us the conditions required to\r
assure convergence with probability 1:\r
X1\r
n=1\r
↵n(a) = 1 and X1\r
n=1\r
↵2\r
n(a) < 1. (2.7)\r
The first condition is required to guarantee that the steps are large enough to eventually\r
overcome any initial conditions or random fluctuations. The second condition guarantees\r
that eventually the steps become small enough to assure convergence.\r
Note that both convergence conditions are met for the sample-average case, ↵n(a) = 1\r
n ,\r
but not for the case of constant step-size parameter, ↵n(a) = ↵. In the latter case, the\r
second condition is not met, indicating that the estimates never completely converge but\r
continue to vary in response to the most recently received rewards. As we mentioned\r
above, this is actually desirable in a nonstationary environment, and problems that are\r
e↵ectively nonstationary are the most common in reinforcement learning. In addition,\r
sequences of step-size parameters that meet the conditions (2.7) often converge very slowly\r
or need considerable tuning in order to obtain a satisfactory convergence rate. Although\r
sequences of step-size parameters that meet these convergence conditions are often used\r
in theoretical work, they are seldom used in applications and empirical research.\r
Exercise 2.4 If the step-size parameters, ↵n, are not constant, then the estimate Qn is\r
a weighted average of previously received rewards with a weighting di↵erent from that\r
given by (2.6). What is the weighting on each prior reward for the general case, analogous\r
to (2.6), in terms of the sequence of step-size parameters? ⇤\r
Exercise 2.5 (programming) Design and conduct an experiment to demonstrate the\r
diculties that sample-average methods have for nonstationary problems. Use a modified\r
version of the 10-armed testbed in which all the q⇤(a) start out equal and then take\r
independent random walks (say by adding a normally distributed increment with mean 0\r
and standard deviation 0.01 to all the q⇤(a) on each step). Prepare plots like Figure 2.2\r
for an action-value method using sample averages, incrementally computed, and another\r
action-value method using a constant step-size parameter, ↵ = 0.1. Use " = 0.1 and\r
longer runs, say of 10,000 steps. ⇤"""

[[sections]]
number = "34"
title = "Chapter 2: Multi-armed Bandits"
text = ""

[[sections]]
number = "2.6"
title = "Optimistic Initial Values"
text = """
All the methods we have discussed so far are dependent to some extent on the initial\r
action-value estimates, Q1(a). In the language of statistics, these methods are biased\r
by their initial estimates. For the sample-average methods, the bias disappears once all\r
actions have been selected at least once, but for methods with constant ↵, the bias is\r
permanent, though decreasing over time as given by (2.6). In practice, this kind of bias\r
is usually not a problem and can sometimes be very helpful. The downside is that the\r
initial estimates become, in e↵ect, a set of parameters that must be picked by the user, if\r
only to set them all to zero. The upside is that they provide an easy way to supply some\r
prior knowledge about what level of rewards can be expected.\r
Initial action values can also be used as a simple way to encourage exploration. Suppose\r
that instead of setting the initial action values to zero, as we did in the 10-armed testbed,\r
we set them all to +5. Recall that the q⇤(a) in this problem are selected from a normal\r
distribution with mean 0 and variance 1. An initial estimate of +5 is thus wildly optimistic.\r
But this optimism encourages action-value methods to explore. Whichever actions are\r
initially selected, the reward is less than the starting estimates; the learner switches to\r
other actions, being “disappointed” with the rewards it is receiving. The result is that all\r
actions are tried several times before the value estimates converge. The system does a\r
fair amount of exploration even if greedy actions are selected all the time.\r
Figure 2.3 shows the performance on the 10-armed bandit testbed of a greedy method\r
using Q1(a) = +5, for all a. For comparison, also shown is an "-greedy method with\r
Q1(a) = 0. Initially, the optimistic method performs worse because it explores more,\r
but eventually it performs better because its exploration decreases with time. We call\r
this technique for encouraging exploration optimistic initial values. We regard it as\r
a simple trick that can be quite e↵ective on stationary problems, but it is far from\r
being a generally useful approach to encouraging exploration. For example, it is not\r
well suited to nonstationary problems because its drive for exploration is inherently\r
0%\r
20%\r
40%\r
60%\r
80%\r
100%\r
%\r
Optimal\r
action\r
0 200 400 600 800 1000\r
Plays\r
optimistic, greedy\r
Q0 = 5, \u0004\u0004= 0\r
realistic, \u0004-greedy\r
Q0 = 0, \u0004\u0004= 0.1 1"""

[[sections]]
number = "1"
title = "Steps"
text = ""

[[sections]]
number = "1"
title = "Optimistic, greedy"
text = """
Q1 = 5, "= 0\r
Realistic, -greedy "\r
Q1 = 0, "= 0.1\r
Figure 2.3: The e↵ect of optimistic initial action-value estimates on the 10-armed testbed.\r
Both methods used a constant step-size parameter, ↵ = 0.1."""

[[sections]]
number = "2.7"
title = "Upper-Confidence-Bound Action Selection 35"
text = """
temporary. If the task changes, creating a renewed need for exploration, this method\r
cannot help. Indeed, any method that focuses on the initial conditions in any special way\r
is unlikely to help with the general nonstationary case. The beginning of time occurs\r
only once, and thus we should not focus on it too much. This criticism applies as well to\r
the sample-average methods, which also treat the beginning of time as a special event,\r
averaging all subsequent rewards with equal weights. Nevertheless, all of these methods\r
are very simple, and one of them—or some simple combination of them—is often adequate\r
in practice. In the rest of this book we make frequent use of several of these simple\r
exploration techniques.\r
Exercise 2.6: Mysterious Spikes The results shown in Figure 2.3 should be quite reliable\r
because they are averages over 2000 individual, randomly chosen 10-armed bandit tasks.\r
Why, then, are there oscillations and spikes in the early part of the curve for the optimistic\r
method? In other words, what might make this method perform particularly better or\r
worse, on average, on particular early steps? ⇤\r
Exercise 2.7: Unbiased Constant-Step-Size Trick In most of this chapter we have used\r
sample averages to estimate action values because sample averages do not produce the\r
initial bias that constant step sizes do (see the analysis leading to (2.6)). However, sample\r
averages are not a completely satisfactory solution because they may perform poorly\r
on nonstationary problems. Is it possible to avoid the bias of constant step sizes while\r
retaining their advantages on nonstationary problems? One way is to use a step size of\r
n\r
.\r
= ↵/o¯n, (2.8)\r
to process the nth reward for a particular action, where ↵ > 0 is a conventional constant\r
step size, and ¯on is a trace of one that starts at 0:\r
o¯n\r
.\r
= ¯on1 + ↵(1  o¯n1), for n > 0, with ¯o0\r
.\r
= 0. (2.9)\r
Carry out an analysis like that in (2.6) to show that Qn is an exponential recency-weighted\r
average without initial bias. ⇤"""

[[sections]]
number = "2.7"
title = "Upper-Confidence-Bound Action Selection"
text = """
Exploration is needed because there is always uncertainty about the accuracy of the\r
action-value estimates. The greedy actions are those that look best at present, but some of\r
the other actions may actually be better. "-greedy action selection forces the non-greedy\r
actions to be tried, but indiscriminately, with no preference for those that are nearly\r
greedy or particularly uncertain. It would be better to select among the non-greedy\r
actions according to their potential for actually being optimal, taking into account both\r
how close their estimates are to being maximal and the uncertainties in those estimates.\r
One e↵ective way of doing this is to select actions according to\r
At\r
.\r
= argmax a\r
"\r
Qt(a) + c\r
s\r
ln t\r
Nt(a)\r
#\r
, (2.10)\r
where ln t denotes the natural logarithm of t (the number that e ⇡ 2.71828 would have\r
to be raised to in order to equal t), Nt(a) denotes the number of times that action a has"""

[[sections]]
number = "36"
title = "Chapter 2: Multi-armed Bandits"
text = """
been selected prior to time t (the denominator in (2.1)), and the number c > 0 controls\r
the degree of exploration. If Nt(a) = 0, then a is considered to be a maximizing action.\r
The idea of this upper confidence bound (UCB) action selection is that the square-root\r
term is a measure of the uncertainty or variance in the estimate of a’s value. The quantity\r
being max’ed over is thus a sort of upper bound on the possible true value of action a, with\r
c determining the confidence level. Each time a is selected the uncertainty is presumably\r
reduced: Nt(a) increments, and, as it appears in the denominator, the uncertainty term\r
decreases. On the other hand, each time an action other than a is selected, t increases but\r
Nt(a) does not; because t appears in the numerator, the uncertainty estimate increases.\r
The use of the natural logarithm means that the increases get smaller over time, but are\r
unbounded; all actions will eventually be selected, but actions with lower value estimates,\r
or that have already been selected frequently, will be selected with decreasing frequency\r
over time.\r
Results with UCB on the 10-armed testbed are shown in Figure 2.4. UCB often\r
performs well, as shown here, but is more dicult than "-greedy to extend beyond bandits\r
to the more general reinforcement learning settings considered in the rest of this book.\r
One diculty is in dealing with nonstationary problems; methods more complex than\r
those presented in Section 2.5 would be needed. Another diculty is dealing with large\r
state spaces, particularly when using function approximation as developed in Part II of\r
this book. In these more advanced settings the idea of UCB action selection is usually\r
not practical.\r
1 250 500 750 1000\r
0\r
0.5\r
1\r
1.5\r
-greedy  = 0.1\r
UCB c = 2\r
Average\r
reward\r
Steps\r
Figure 2.4: Average performance of UCB action selection on the 10-armed testbed. As shown,\r
UCB generally performs better than "-greedy action selection, except in the first k steps, when\r
it selects randomly among the as-yet-untried actions.\r
Exercise 2.8: UCB Spikes In Figure 2.4 the UCB algorithm shows a distinct spike\r
in performance on the 11th step. Why is this? Note that for your answer to be fully\r
satisfactory it must explain both why the reward increases on the 11th step and why it\r
decreases on the subsequent steps. Hint: If c = 1, then the spike is less prominent. ⇤"""

[[sections]]
number = "2.8"
title = "Gradient Bandit Algorithms 37"
text = ""

[[sections]]
number = "2.8"
title = "Gradient Bandit Algorithms"
text = """
So far in this chapter we have considered methods that estimate action values and use\r
those estimates to select actions. This is often a good approach, but it is not the only\r
one possible. In this section we consider learning a numerical preference for each action\r
a, which we denote Ht(a) 2 R. The larger the preference, the more often that action is\r
taken, but the preference has no interpretation in terms of reward. Only the relative\r
preference of one action over another is important; if we add 1000 to all the action\r
preferences there is no e↵ect on the action probabilities, which are determined according\r
to a soft-max distribution (i.e., Gibbs or Boltzmann distribution) as follows:\r
Pr{At =a} .= eHt(a)\r
Pk\r
b=1 eHt(b)\r
.\r
= ⇡t(a), (2.11)\r
where here we have also introduced a useful new notation, ⇡t(a), for the probability of\r
taking action a at time t. Initially all action preferences are the same (e.g., H1(a) = 0,\r
for all a) so that all actions have an equal probability of being selected.\r
Exercise 2.9 Show that in the case of two actions, the soft-max distribution is the same\r
as that given by the logistic, or sigmoid, function often used in statistics and artificial\r
neural networks. ⇤\r
There is a natural learning algorithm for soft-max action preferences based on the idea\r
of stochastic gradient ascent. On each step, after selecting action At and receiving the\r
reward Rt, the action preferences are updated by:\r
Ht+1(At) .= Ht(At) + ↵\r
\r
Rt  R¯t\r
1  ⇡t(At)\r
, and\r
Ht+1(a) .= Ht(a)  ↵\r
\r
Rt  R¯t\r
\r
⇡t(a), for all a 6= At, (2.12)\r
where ↵ > 0 is a step-size parameter, and R¯t 2 R is the average of the rewards up to but\r
not including time t (with R¯1\r
.\r
= R1), which can be computed incrementally as described\r
in Section 2.4 (or Section 2.5 if the problem is nonstationary).1 The R¯t term serves as a\r
baseline with which the reward is compared. If the reward is higher than the baseline,\r
then the probability of taking At in the future is increased, and if the reward is below\r
baseline, then the probability is decreased. The non-selected actions move in the opposite\r
direction.\r
Figure 2.5 shows results with the gradient bandit algorithm on a variant of the 10-\r
armed testbed in which the true expected rewards were selected according to a normal\r
distribution with a mean of +4 instead of zero (and with unit variance as before). This\r
shifting up of all the rewards has absolutely no e↵ect on the gradient bandit algorithm\r
because of the reward baseline term, which instantaneously adapts to the new level. But\r
if the baseline were omitted (that is, if R¯t was taken to be constant zero in (2.12)), then\r
performance would be significantly degraded, as shown in the figure.\r
1In the empirical results in this chapter, the baseline R¯t also included Rt."""

[[sections]]
number = "38"
title = "Chapter 2: Multi-armed Bandits"
text = """
%\r
Optimal\r
action\r
Steps\r
α = 0.1\r
100%\r
80%\r
60%\r
40%\r
20%\r
0%\r
α = 0.4\r
α = 0.1\r
α = 0.4\r
without baseline\r
with baseline\r
1 250 500 750 1000\r
Figure 2.5: Average performance of the gradient bandit algorithm with and without a reward\r
baseline on the 10-armed testbed when the q⇤(a) are chosen to be near +4 rather than near zero.\r
The Bandit Gradient Algorithm as Stochastic Gradient Ascent\r
One can gain a deeper insight into the gradient bandit algorithm by understanding\r
it as a stochastic approximation to gradient ascent. In exact gradient ascent, each\r
action preference Ht(a) would be incremented in proportion to the increment’s\r
e↵ect on performance:\r
Ht+1(a) .= Ht(a) + ↵\r
@E[Rt]\r
@Ht(a)\r
, (2.13)\r
where the measure of performance here is the expected reward:\r
E[Rt] = X\r
x\r
⇡t(x)q⇤(x),\r
and the measure of the increment’s e↵ect is the partial derivative of this performance\r
measure with respect to the action preference. Of course, it is not possible to\r
implement gradient ascent exactly in our case because by assumption we do not\r
know the q⇤(x), but in fact the updates of our algorithm (2.12) are equal to (2.13)\r
in expected value, making the algorithm an instance of stochastic gradient ascent.\r
The calculations showing this require only beginning calculus, but take several"""

[[sections]]
number = "2.8"
title = "Gradient Bandit Algorithms 39"
text = """
steps. First we take a closer look at the exact performance gradient:\r
@E[Rt]\r
@Ht(a) = @@Ht(a)\r
"\r
X\r
x\r
⇡t(x)q⇤(x)\r
#\r
= X\r
x\r
q⇤(x)\r
@ ⇡t(x)\r
@Ht(a)\r
= X\r
x\r
\r
q⇤(x)  Bt\r
 @ ⇡t(x)\r
@Ht(a)\r
,\r
where Bt, called the baseline, can be any scalar that does not depend on x. We can\r
include a baseline here without changing the equality because the gradient sums\r
to zero over all the actions, P\r
x\r
@ ⇡t(x)\r
@Ht(a) = 0. As Ht(a) is changed, some actions’\r
probabilities go up and some go down, but the sum of the changes must be zero\r
because the sum of the probabilities is always one.\r
Next we multiply each term of the sum by ⇡t(x)/⇡t(x):\r
@E[Rt]\r
@Ht(a) = X\r
x\r
⇡t(x)\r
\r
q⇤(x)  Bt\r
 @ ⇡t(x)\r
@Ht(a)\r
/⇡t(x).\r
The equation is now in the form of an expectation, summing over all possible values\r
x of the random variable At, then multiplying by the probability of taking those\r
values. Thus:\r
= E\r
\r
\r
q⇤(At)  Bt\r
@ ⇡t(At)\r
@Ht(a) /⇡t(At)\r
\r
= E\r
\r
\r
Rt  R¯t\r
@ ⇡t(At)\r
@Ht(a) /⇡t(At)\r
\r
,\r
where here we have chosen the baseline Bt = R¯t and substituted Rt for q⇤(At),\r
which is permitted because E[Rt|At] = q⇤(At). Shortly we will establish that\r
@ ⇡t(x)\r
@Ht(a) = ⇡t(x)\r
 a=x  ⇡t(a)\r
, where a=x is defined to be 1 if a = x, else 0.\r
Assuming that for now, we have\r
= E\r
⇥Rt  R¯t\r
⇡t(At)\r
 a=At  ⇡t(a)\r
/⇡t(At)\r
⇤\r
= E\r
⇥Rt  R¯t a=At  ⇡t(a)⇤ .\r
Recall that our plan has been to write the performance gradient as an expectation\r
of something that we can sample on each step, as we have just done, and then\r
update on each step in proportion to the sample. Substituting a sample of the\r
expectation above for the performance gradient in (2.13) yields:\r
Ht+1(a) = Ht(a) + ↵\r
\r
Rt  R¯t\r
 a=At  ⇡t(a)\r
, for all a,\r
which you may recognize as being equivalent to our original algorithm (2.12)."""

[[sections]]
number = "40"
title = "Chapter 2: Multi-armed Bandits"
text = """
Thus it remains only to show that @ ⇡t(x)\r
@Ht(a) = ⇡t(x)\r
 a=x  ⇡t(a)\r
, as we assumed.\r
Recall the standard quotient rule for derivatives:\r
@\r
@x\r
\r
f(x)\r
g(x)\r
\r
=\r
@f(x)\r
@x g(x)  f(x) @g(x)@x\r
g(x)2 .\r
Using this, we can write\r
@ ⇡t(x)\r
@Ht(a) = @@Ht(a)\r
⇡t(x)\r
= @\r
@Ht(a)\r
"\r
eHt(x)\r
Pk\r
y=1 eHt(y)\r
#\r
=\r
@eHt(x)\r
@Ht(a)\r
Pk\r
y=1 eHt(y)  eHt(x) @ Pk\r
y=1 eHt(y)\r
@Ht(a)\r
⇣Pk\r
y=1 eHt(y)\r
⌘2 (by the quotient rule)\r
= a=xeHt(x) Pk\r
y=1 eHt(y)  eHt(x)\r
eHt(a)\r
⇣Pk\r
y=1 eHt(y)\r
⌘2 (because @ex\r
@x = ex)\r
= a=xeHt(x)\r
Pk\r
y=1 eHt(y)  eHt(x)\r
eHt(a)\r
⇣Pk\r
y=1 eHt(y)\r
⌘2\r
= a=x⇡t(x)  ⇡t(x)⇡t(a)\r
= ⇡t(x)\r
 a=x  ⇡t(a)\r
. Q.E.D.\r
We have just shown that the expected update of the gradient bandit algorithm\r
is equal to the gradient of expected reward, and thus that the algorithm is an\r
instance of stochastic gradient ascent. This assures us that the algorithm has robust\r
convergence properties.\r
Note that we did not require any properties of the reward baseline other than\r
that it does not depend on the selected action. For example, we could have set\r
it to zero, or to 1000, and the algorithm would still be an instance of stochastic\r
gradient ascent. The choice of the baseline does not a↵ect the expected update\r
of the algorithm, but it does a↵ect the variance of the update and thus the rate of\r
convergence (as shown, for example, in Figure 2.5). Choosing it as the average of\r
the rewards may not be the very best, but it is simple and works well in practice."""

[[sections]]
number = "2.9"
title = "Associative Search (Contextual Bandits) 41"
text = ""

[[sections]]
number = "2.9"
title = "Associative Search (Contextual Bandits)"
text = """
So far in this chapter we have considered only nonassociative tasks, that is, tasks in which\r
there is no need to associate di↵erent actions with di↵erent situations. In these tasks\r
the learner either tries to find a single best action when the task is stationary, or tries to\r
track the best action as it changes over time when the task is nonstationary. However,\r
in a general reinforcement learning task there is more than one situation, and the goal\r
is to learn a policy: a mapping from situations to the actions that are best in those\r
situations. To set the stage for the full problem, we briefly discuss the simplest way in\r
which nonassociative tasks extend to the associative setting.\r
As an example, suppose there are several di↵erent k-armed bandit tasks, and that on\r
each step you confront one of these chosen at random. Thus, the bandit task changes\r
randomly from step to step. If the probabilities with which each task is selected for you\r
do not change over time, this would appear as a single stationary k-armed bandit task,\r
and you could use one of the methods described in this chapter. Now suppose, however,\r
that when a bandit task is selected for you, you are given some distinctive clue about its\r
identity (but not its action values). Maybe you are facing an actual slot machine that\r
changes the color of its display as it changes its action values. Now you can learn a policy\r
associating each task, signaled by the color you see, with the best action to take when\r
facing that task—for instance, if red, select arm 1; if green, select arm 2. With the right\r
policy you can usually do much better than you could in the absence of any information\r
distinguishing one bandit task from another.\r
This is an example of an associative search task, so called because it involves both\r
trial-and-error learning to search for the best actions, and association of these actions\r
with the situations in which they are best. Associative search tasks are often now called\r
contextual bandits in the literature. Associative search tasks are intermediate between\r
the k-armed bandit problem and the full reinforcement learning problem. They are like\r
the full reinforcement learning problem in that they involve learning a policy, but they\r
are also like our version of the k-armed bandit problem in that each action a↵ects only\r
the immediate reward. If actions are allowed to a↵ect the next situation as well as the\r
reward, then we have the full reinforcement learning problem. We present this problem\r
in the next chapter and consider its ramifications throughout the rest of the book.\r
Exercise 2.10 Suppose you face a 2-armed bandit task whose true action values change\r
randomly from time step to time step. Specifically, suppose that, for any time step,\r
the true values of actions 1 and 2 are respectively 10 and 20 with probability 0.5 (case\r
A), and 90 and 80 with probability 0.5 (case B). If you are not able to tell which case\r
you face at any step, what is the best expected reward you can achieve and how should\r
you behave to achieve it? Now suppose that on each step you are told whether you are\r
facing case A or case B (although you still don’t know the true action values). This is an\r
associative search task. What is the best expected reward you can achieve in this task,\r
and how should you behave to achieve it? ⇤"""

[[sections]]
number = "42"
title = "Chapter 2: Multi-armed Bandits"
text = ""

[[sections]]
number = "2.10"
title = "Summary"
text = """
We have presented in this chapter several simple ways of balancing exploration and\r
exploitation. The "-greedy methods choose randomly a small fraction of the time, whereas\r
UCB methods choose deterministically but achieve exploration by subtly favoring at each\r
step the actions that have so far received fewer samples. Gradient bandit algorithms\r
estimate not action values, but action preferences, and favor the more preferred actions\r
in a graded, probabilistic manner using a soft-max distribution. The simple expedient of\r
initializing estimates optimistically causes even greedy methods to explore significantly.\r
It is natural to ask which of these methods is best. Although this is a dicult question\r
to answer in general, we can certainly run them all on the 10-armed testbed that we\r
have used throughout this chapter and compare their performances. A complication is\r
that they all have a parameter; to get a meaningful comparison we have to consider\r
their performance as a function of their parameter. Our graphs so far have shown the\r
course of learning over time for each algorithm and parameter setting, to produce a\r
learning curve for that algorithm and parameter setting. If we plotted learning curves\r
for all algorithms and all parameter settings, then the graph would be too complex and\r
crowded to make clear comparisons. Instead we summarize a complete learning curve\r
by its average value over the 1000 steps; this value is proportional to the area under the\r
learning curve. Figure 2.6 shows this measure for the various bandit algorithms from\r
this chapter, each as a function of its own parameter shown on a single scale on the\r
x-axis. This kind of graph is called a parameter study. Note that the parameter values\r
are varied by factors of two and presented on a log scale. Note also the characteristic\r
inverted-U shapes of each algorithm’s performance; all the algorithms perform best at\r
an intermediate value of their parameter, neither too large nor too small. In assessing\r
Average\r
reward\r
over first \r
1000 steps\r
1.5\r
1.4\r
1.3\r
1.2\r
1.1\r
1\r
-greedy\r
UCB\r
gradient\r
bandit\r
greedy with\r
optimistic\r
initialization\r
α = 0.1\r
1/128 1/64 1/32 1/16 1/8 1/4 1/2 1 2 4\r
" ↵ c Q0\r
Figure 2.6: A parameter study of the various bandit algorithms presented in this chapter.\r
Each point is the average reward obtained over 1000 steps with a particular algorithm at a\r
particular setting of its parameter."""

[[sections]]
number = "2.10"
title = "Summary 43"
text = """
a method, we should attend not just to how well it does at its best parameter setting,\r
but also to how sensitive it is to its parameter value. All of these algorithms are fairly\r
insensitive, performing well over a range of parameter values varying by about an order\r
of magnitude. Overall, on this problem, UCB seems to perform best.\r
Despite their simplicity, in our opinion the methods presented in this chapter can\r
fairly be considered the state of the art. There are more sophisticated methods, but their\r
complexity and assumptions make them impractical for the full reinforcement learning\r
problem that is our real focus. Starting in Chapter 5 we present learning methods for\r
solving the full reinforcement learning problem that use in part the simple methods\r
explored in this chapter.\r
Although the simple methods explored in this chapter may be the best we can do\r
at present, they are far from a fully satisfactory solution to the problem of balancing\r
exploration and exploitation.\r
One well-studied approach to balancing exploration and exploitation in k-armed bandit\r
problems is to compute a special kind of action value called a Gittins index. In certain\r
important special cases, this computation is tractable and leads directly to optimal\r
solutions, although it does require complete knowledge of the prior distribution of possible\r
problems, which we generally assume is not available. In addition, neither the theory\r
nor the computational tractability of this approach appear to generalize to the full\r
reinforcement learning problem that we consider in the rest of the book.\r
The Gittins-index approach is an instance of Bayesian methods, which assume a known\r
initial distribution over the action values and then update the distribution exactly after\r
each step (assuming that the true action values are stationary). In general, the update\r
computations can be very complex, but for certain special distributions (called conjugate\r
priors) they are easy. One possibility is to then select actions at each step according\r
to their posterior probability of being the best action. This method, sometimes called\r
posterior sampling or Thompson sampling, often performs similarly to the best of the\r
distribution-free methods we have presented in this chapter.\r
In the Bayesian setting it is even conceivable to compute the optimal balance between\r
exploration and exploitation. One can compute for any possible action the probability\r
of each possible immediate reward and the resultant posterior distributions over action\r
values. This evolving distribution becomes the information state of the problem. Given\r
a horizon, say of 1000 steps, one can consider all possible actions, all possible resulting\r
rewards, all possible next actions, all next rewards, and so on for all 1000 steps. Given\r
the assumptions, the rewards and probabilities of each possible chain of events can be\r
determined; one need only pick the best. But the tree of possibilities grows extremely\r
rapidly; even if there were only two actions and two rewards, the tree would have 22000\r
leaves. It is generally not feasible to perform this immense computation exactly, but\r
perhaps it could be approximated eciently. This approach would e↵ectively turn the\r
bandit problem into an instance of the full reinforcement learning problem. In the end, we\r
may be able to use approximate reinforcement learning methods such as those presented\r
in Part II of this book to approach this optimal solution. But that is a topic for research\r
and beyond the scope of this introductory book."""

[[sections]]
number = "44"
title = "Chapter 2: Multi-armed Bandits"
text = """
Exercise 2.11 (programming) Make a figure analogous to Figure 2.6 for the nonstationary\r
case outlined in Exercise 2.5. Include the constant-step-size "-greedy algorithm with\r
↵= 0.1. Use runs of 200,000 steps and, as a performance measure for each algorithm and\r
parameter setting, use the average reward over the last 100,000 steps. ⇤\r
Bibliographical and Historical Remarks"""

[[sections]]
number = "2.1"
title = "Bandit problems have been studied in statistics, engineering, and psychology. In"
text = """
statistics, bandit problems fall under the heading “sequential design of experi\u0002ments,” introduced by Thompson (1933, 1934) and Robbins (1952), and studied\r
by Bellman (1956). Berry and Fristedt (1985) provide an extensive treatment of\r
bandit problems from the perspective of statistics. Narendra and Thathachar\r
(1989) treat bandit problems from the engineering perspective, providing a good\r
discussion of the various theoretical traditions that have focused on them. In\r
psychology, bandit problems have played roles in statistical learning theory (e.g.,\r
Bush and Mosteller, 1955; Estes, 1950).\r
The term greedy is often used in the heuristic search literature (e.g., Pearl, 1984).\r
The conflict between exploration and exploitation is known in control engineering\r
as the conflict between identification (or estimation) and control (e.g., Witten,\r
1976b). Feldbaum (1965) called it the dual control problem, referring to the\r
need to solve the two problems of identification and control simultaneously when\r
trying to control a system under uncertainty. In discussing aspects of genetic\r
algorithms, Holland (1975) emphasized the importance of this conflict, referring\r
to it as the conflict between the need to exploit and the need for new information."""

[[sections]]
number = "2.2"
title = "Action-value methods for our k-armed bandit problem were first proposed by"
text = """
Thathachar and Sastry (1985). These are often called estimator algorithms in the\r
learning automata literature. The term action value is due to Watkins (1989).\r
The first to use "-greedy methods may also have been Watkins (1989, p. 187),\r
but the idea is so simple that some earlier use seems likely.\r
2.4–5 This material falls under the general heading of stochastic iterative algorithms,\r
which is well covered by Bertsekas and Tsitsiklis (1996)."""

[[sections]]
number = "2.6"
title = "Optimistic initialization was used in reinforcement learning by Sutton (1996)."
text = ""

[[sections]]
number = "2.7"
title = "Early work on using estimates of the upper confidence bound to select actions"
text = """
was done by Lai and Robbins (1985), Kaelbling (1993b), and Agrawal (1995).\r
The UCB algorithm we present here is called UCB1 in the literature and was\r
first developed by Auer, Cesa-Bianchi and Fischer (2002).\r
2.8 Gradient bandit algorithms are a special case of the gradient-based reinforcement\r
learning algorithms introduced by Williams (1992) that later developed into the\r
actor–critic and policy-gradient algorithms that we treat later in this book. Our\r
development here was influenced by that by Balaraman Ravindran (personal"""

[[sections]]
number = "2.10"
title = "Summary 45"
text = """
communication). Further discussion of the choice of baseline is provided by\r
Greensmith, Bartlett, and Baxter (2002, 2004) and by Dick (2015). Early\r
systematic studies of algorithms like this were done by Sutton (1984).\r
The term soft-max for the action selection rule (2.11) is due to Bridle (1990).\r
This rule appears to have been first proposed by Luce (1959)."""

[[sections]]
number = "2.9"
title = "The term associative search and the corresponding problem were introduced by"
text = """
Barto, Sutton, and Brouwer (1981). The term associative reinforcement learning\r
has also been used for associative search (Barto and Anandan, 1985), but we\r
prefer to reserve that term as a synonym for the full reinforcement learning\r
problem (as in Sutton, 1984). (And, as we noted, the modern literature also\r
uses the term “contextual bandits” for this problem.) We note that Thorndike’s\r
Law of E↵ect (quoted in Chapter 1) describes associative search by referring\r
to the formation of associative links between situations (states) and actions.\r
According to the terminology of operant, or instrumental, conditioning (e.g.,\r
Skinner, 1938), a discriminative stimulus is a stimulus that signals the presence\r
of a particular reinforcement contingency. In our terms, di↵erent discriminative\r
stimuli correspond to di↵erent states."""

[[sections]]
number = "2.10"
title = "Bellman (1956) was the first to show how dynamic programming could be used"
text = """
to compute the optimal balance between exploration and exploitation within a\r
Bayesian formulation of the problem. The Gittins index approach is due to Gittins\r
and Jones (1974). Du↵ (1995) showed how it is possible to learn Gittins indices\r
for bandit problems through reinforcement learning. The survey by Kumar (1985)\r
provides a good discussion of Bayesian and non-Bayesian approaches to these\r
problems. The term information state comes from the literature on partially\r
observable MDPs; see, for example, Lovejoy (1991).\r
Other theoretical research focuses on the eciency of exploration, usually ex\u0002pressed as how quickly an algorithm can approach an optimal decision-making\r
policy. One way to formalize exploration eciency is by adapting to reinforcement\r
learning the notion of sample complexity for a supervised learning algorithm,\r
which is the number of training examples the algorithm needs to attain a desired\r
degree of accuracy in learning the target function. A definition of the sample\r
complexity of exploration for a reinforcement learning algorithm is the number of\r
time steps in which the algorithm does not select near-optimal actions (Kakade,\r
2003). Li (2012) discusses this and several other approaches in a survey of theo\u0002retical approaches to exploration eciency in reinforcement learning. A thorough\r
modern treatment of Thompson sampling is provided by Russo et al. (2018).

Chapter 3\r
Finite Markov Decision\r
Processes\r
In this chapter we introduce the formal problem of finite Markov decision processes, or\r
finite MDPs, which we try to solve in the rest of the book. This problem involves evaluative\r
feedback, as in bandits, but also an associative aspect—choosing di↵erent actions in\r
di↵erent situations. MDPs are a classical formalization of sequential decision making,\r
where actions influence not just immediate rewards, but also subsequent situations,\r
or states, and through those future rewards. Thus MDPs involve delayed reward and\r
the need to trade o↵ immediate and delayed reward. Whereas in bandit problems we\r
estimated the value q⇤(a) of each action a, in MDPs we estimate the value q⇤(s, a) of\r
each action a in each state s, or we estimate the value v⇤(s) of each state given optimal\r
action selections. These state-dependent quantities are essential to accurately assigning\r
credit for long-term consequences to individual action selections.\r
MDPs are a mathematically idealized form of the reinforcement learning problem\r
for which precise theoretical statements can be made. We introduce key elements of\r
the problem’s mathematical structure, such as returns, value functions, and Bellman\r
equations. We try to convey the wide range of applications that can be formulated as\r
finite MDPs. As in all of artificial intelligence, there is a tension between breadth of\r
applicability and mathematical tractability. In this chapter we introduce this tension\r
and discuss some of the trade-o↵s and challenges that it implies. Some ways in which\r
reinforcement learning can be taken beyond MDPs are treated in Chapter 17."""

[[sections]]
number = "3.1"
title = "The Agent–Environment Interface"
text = """
MDPs are meant to be a straightforward framing of the problem of learning from\r
interaction to achieve a goal. The learner and decision maker is called the agent. The\r
thing it interacts with, comprising everything outside the agent, is called the environment.\r
These interact continually, the agent selecting actions and the environment responding to"""

[[sections]]
number = "48"
title = "Chapter 3: Finite Markov Decision Processes"
text = """
these actions and presenting new situations to the agent.1 The environment also gives\r
rise to rewards, special numerical values that the agent seeks to maximize over time\r
through its choice of actions.\r
Agent\r
Environment\r
action\r
At\r
reward\r
Rt\r
state\r
St\r
Rt+1\r
St+1\r
Figure 3.1: The agent–environment interaction in a Markov decision process.\r
More specifically, the agent and environment interact at each of a sequence of discrete\r
time steps, t = 0, 1, 2, 3,...."""

[[sections]]
number = "2"
title = "At each time step t, the agent receives some representation"
text = """
of the environment’s state, St 2 S, and on that basis selects an action, At 2 A(s).3 One\r
time step later, in part as a consequence of its action, the agent receives a numerical\r
reward, Rt+1 2 R ⇢ R, and finds itself in a new state, St+1."""

[[sections]]
number = "4"
title = "The MDP and agent"
text = """
together thereby give rise to a sequence or trajectory that begins like this:\r
S0, A0, R1, S1, A1, R2, S2, A2, R3,... (3.1)\r
In a finite MDP, the sets of states, actions, and rewards (S, A, and R) all have a finite\r
number of elements. In this case, the random variables Rt and St have well defined\r
discrete probability distributions dependent only on the preceding state and action. That\r
is, for particular values of these random variables, s0 2 S and r 2 R, there is a probability\r
of those values occurring at time t, given particular values of the preceding state and\r
action:\r
p(s0, r|s, a) .= Pr{St =s0, Rt =r | St1 =s, At1 =a}, (3.2)\r
for all s0, s 2 S, r 2 R, and a 2 A(s). The function p defines the dynamics of the MDP.\r
The dot over the equals sign in the equation reminds us that it is a definition (in this\r
case of the function p) rather than a fact that follows from previous definitions. The\r
dynamics function p : S ⇥ R ⇥ S ⇥ A ! [0, 1] is an ordinary deterministic function of four\r
arguments. The ‘|’ in the middle of it comes from the notation for conditional probability,\r
1We use the terms agent, environment, and action instead of the engineers’ terms controller, controlled\r
system (or plant), and control signal because they are meaningful to a wider audience.\r
2We restrict attention to discrete time to keep things as simple as possible, even though many of the\r
ideas can be extended to the continuous-time case (e.g., see Bertsekas and Tsitsiklis, 1996; Doya, 1996).\r
3To simplify notation, we sometimes assume the special case in which the action set is the same in all\r
states and write it simply as A.\r
4We use Rt+1 instead of Rt to denote the reward due to At because it emphasizes that the next\r
reward and next state, Rt+1 and St+1, are jointly determined. Unfortunately, both conventions are\r
widely used in the literature."""

[[sections]]
number = "3.1"
title = "The Agent–Environment Interface 49"
text = """
but here it just reminds us that p specifies a probability distribution for each choice of s\r
and a, that is, that\r
X\r
s02S\r
X\r
r2R\r
p(s0, r|s, a)=1, for all s 2 S, a 2 A(s). (3.3)\r
In a Markov decision process, the probabilities given by p completely characterize the\r
environment’s dynamics. That is, the probability of each possible value for St and Rt\r
depends on the immediately preceding state and action, St1 and At1, and, given them,\r
not at all on earlier states and actions. This is best viewed as a restriction not on the\r
decision process, but on the state. The state must include information about all aspects\r
of the past agent–environment interaction that make a di↵erence for the future. If it\r
does, then the state is said to have the Markov property. We will assume the Markov\r
property throughout this book, though starting in Part II we will consider approximation\r
methods that do not rely on it, and in Chapter 17 we consider how a Markov state can\r
be eciently learned and constructed from non-Markov observations.\r
From the four-argument dynamics function, p, one can compute anything else one might\r
want to know about the environment, such as the state-transition probabilities (which we\r
denote, with a slight abuse of notation, as a three-argument function p : S⇥S⇥A ! [0, 1]),\r
p(s0|s, a) .= Pr{St =s0 | St1 =s, At1 =a} = X\r
r2R\r
p(s0, r|s, a). (3.4)\r
We can also compute the expected rewards for state–action pairs as a two-argument\r
function r : S ⇥ A ! R:\r
r(s, a) .= E[Rt | St1 =s, At1 =a] = X\r
r2R\r
r\r
X\r
s02S\r
p(s0, r|s, a), (3.5)\r
and the expected rewards for state–action–next-state triples as a three-argument function\r
r : S ⇥ A ⇥ S ! R,\r
r(s, a, s0) .= E[Rt | St1 =s, At1 =a, St = s0] = X\r
r2R\r
r\r
p(s0, r|s, a)\r
p(s0 |s, a) . (3.6)\r
In this book, we usually use the four-argument p function (3.2), but each of these other\r
notations are also occasionally convenient.\r
The MDP framework is abstract and flexible and can be applied to many di↵erent\r
problems in many di↵erent ways. For example, the time steps need not refer to fixed\r
intervals of real time; they can refer to arbitrary successive stages of decision making\r
and acting. The actions can be low-level controls, such as the voltages applied to the\r
motors of a robot arm, or high-level decisions, such as whether or not to have lunch or\r
to go to graduate school. Similarly, the states can take a wide variety of forms. They\r
can be completely determined by low-level sensations, such as direct sensor readings, or\r
they can be more high-level and abstract, such as symbolic descriptions of objects in a\r
room. Some of what makes up a state could be based on memory of past sensations or"""

[[sections]]
number = "50"
title = "Chapter 3: Finite Markov Decision Processes"
text = """
even be entirely mental or subjective. For example, an agent could be in the state of not\r
being sure where an object is, or of having just been surprised in some clearly defined\r
sense. Similarly, some actions might be totally mental or computational. For example,\r
some actions might control what an agent chooses to think about, or where it focuses its\r
attention. In general, actions can be any decisions we want to learn how to make, and\r
states can be anything we can know that might be useful in making them.\r
In particular, the boundary between agent and environment is typically not the same\r
as the physical boundary of a robot’s or an animal’s body. Usually, the boundary is\r
drawn closer to the agent than that. For example, the motors and mechanical linkages of\r
a robot and its sensing hardware should usually be considered parts of the environment\r
rather than parts of the agent. Similarly, if we apply the MDP framework to a person\r
or animal, the muscles, skeleton, and sensory organs should be considered part of the\r
environment. Rewards, too, presumably are computed inside the physical bodies of\r
natural and artificial learning systems, but are considered external to the agent.\r
The general rule we follow is that anything that cannot be changed arbitrarily by\r
the agent is considered to be outside of it and thus part of its environment. We do\r
not assume that everything in the environment is unknown to the agent. For example,\r
the agent often knows quite a bit about how its rewards are computed as a function of\r
its actions and the states in which they are taken. But we always consider the reward\r
computation to be external to the agent because it defines the task facing the agent and\r
thus must be beyond its ability to change arbitrarily. In fact, in some cases the agent may\r
know everything about how its environment works and still face a dicult reinforcement\r
learning task, just as we may know exactly how a puzzle like Rubik’s cube works, but\r
still be unable to solve it. The agent–environment boundary represents the limit of the\r
agent’s absolute control, not of its knowledge.\r
The agent–environment boundary can be located at di↵erent places for di↵erent\r
purposes. In a complicated robot, many di↵erent agents may be operating at once, each\r
with its own boundary. For example, one agent may make high-level decisions which form\r
part of the states faced by a lower-level agent that implements the high-level decisions. In\r
practice, the agent–environment boundary is determined once one has selected particular\r
states, actions, and rewards, and thus has identified a specific decision-making task of\r
interest.\r
The MDP framework is a considerable abstraction of the problem of goal-directed\r
learning from interaction. It proposes that whatever the details of the sensory, memory,\r
and control apparatus, and whatever objective one is trying to achieve, any problem of\r
learning goal-directed behavior can be reduced to three signals passing back and forth\r
between an agent and its environment: one signal to represent the choices made by the\r
agent (the actions), one signal to represent the basis on which the choices are made (the\r
states), and one signal to define the agent’s goal (the rewards). This framework may not\r
be sucient to represent all decision-learning problems usefully, but it has proved to be\r
widely useful and applicable.\r
Of course, the particular states and actions vary greatly from task to task, and how\r
they are represented can strongly a↵ect performance. In reinforcement learning, as in\r
other kinds of learning, such representational choices are at present more art than science."""

[[sections]]
number = "3.1"
title = "The Agent–Environment Interface 51"
text = """
In this book we o↵er some advice and examples regarding good ways of representing\r
states and actions, but our primary focus is on general principles for learning how to\r
behave once the representations have been selected.\r
Example 3.1: Bioreactor Suppose reinforcement learning is being applied to determine\r
moment-by-moment temperatures and stirring rates for a bioreactor (a large vat of\r
nutrients and bacteria used to produce useful chemicals). The actions in such an\r
application might be target temperatures and target stirring rates that are passed to\r
lower-level control systems that, in turn, directly activate heating elements and motors to\r
attain the targets. The states are likely to be thermocouple and other sensory readings,\r
perhaps filtered and delayed, plus symbolic inputs representing the ingredients in the\r
vat and the target chemical. The rewards might be moment-by-moment measures of the\r
rate at which the useful chemical is produced by the bioreactor. Notice that here each\r
state is a list, or vector, of sensor readings and symbolic inputs, and each action is a\r
vector consisting of a target temperature and a stirring rate. It is typical of reinforcement\r
learning tasks to have states and actions with such structured representations. Rewards,\r
on the other hand, are always single numbers.\r
Example 3.2: Pick-and-Place Robot Consider using reinforcement learning to\r
control the motion of a robot arm in a repetitive pick-and-place task. If we want to learn\r
movements that are fast and smooth, the learning agent will have to control the motors\r
directly and have low-latency information about the current positions and velocities\r
of the mechanical linkages. The actions in this case might be the voltages applied to\r
each motor at each joint, and the states might be the latest readings of joint angles and\r
velocities. The reward might be +1 for each object successfully picked up and placed. To\r
encourage smooth movements, on each time step a small, negative reward could be given\r
as a function of the moment-to-moment jerkiness of the motion.\r
Exercise 3.1 Devise three example tasks of your own that fit into the MDP framework,\r
identifying for each its states, actions, and rewards. Make the three examples as di↵erent\r
from each other as possible. The framework is abstract and flexible and can be applied in\r
many di↵erent ways. Stretch its limits in some way in at least one of your examples. ⇤\r
Exercise 3.2 Is the MDP framework adequate to usefully represent all goal-directed\r
learning tasks? Can you think of any clear exceptions? ⇤\r
Exercise 3.3 Consider the problem of driving. You could define the actions in terms of\r
the accelerator, steering wheel, and brake, that is, where your body meets the machine.\r
Or you could define them farther out—say, where the rubber meets the road, considering\r
your actions to be tire torques. Or you could define them farther in—say, where your\r
brain meets your body, the actions being muscle twitches to control your limbs. Or you\r
could go to a really high level and say that your actions are your choices of where to drive.\r
What is the right level, the right place to draw the line between agent and environment?\r
On what basis is one location of the line to be preferred over another? Is there any\r
fundamental reason for preferring one location over another, or is it a free choice? ⇤"""

[[sections]]
number = "52"
title = "Chapter 3: Finite Markov Decision Processes"
text = """
Example 3.3 Recycling Robot\r
A mobile robot has the job of collecting empty soda cans in an oce environment. It\r
has sensors for detecting cans, and an arm and gripper that can pick them up and place\r
them in an onboard bin; it runs on a rechargeable battery. The robot’s control system\r
has components for interpreting sensory information, for navigating, and for controlling\r
the arm and gripper. High-level decisions about how to search for cans are made by a\r
reinforcement learning agent based on the current charge level of the battery. To make a\r
simple example, we assume that only two charge levels can be distinguished, comprising\r
a small state set S = {high, low}. In each state, the agent can decide whether to (1)\r
actively search for a can for a certain period of time, (2) remain stationary and wait\r
for someone to bring it a can, or (3) head back to its home base to recharge its battery.\r
When the energy level is high, recharging would always be foolish, so we do not include it\r
in the action set for this state. The action sets are then A(high) = {search, wait} and\r
A(low) = {search, wait, recharge}.\r
The rewards are zero most of the time, but become positive when the robot secures an\r
empty can, or large and negative if the battery runs all the way down. The best way to\r
find cans is to actively search for them, but this runs down the robot’s battery, whereas\r
waiting does not. Whenever the robot is searching, the possibility exists that its battery\r
will become depleted. In this case the robot must shut down and wait to be rescued\r
(producing a low reward). If the energy level is high, then a period of active search can\r
always be completed without risk of depleting the battery. A period of searching that\r
begins with a high energy level leaves the energy level high with probability ↵ and reduces\r
it to low with probability 1  ↵. On the other hand, a period of searching undertaken\r
when the energy level is low leaves it low with probability  and depletes the battery with\r
probability 1  . In the latter case, the robot must be rescued, and the battery is then\r
recharged back to high. Each can collected by the robot counts as a unit reward, whereas\r
a reward of 3 results whenever the robot has to be rescued. Let rsearch and rwait, with\r
rsearch > rwait, denote the expected number of cans the robot will collect (and hence the\r
expected reward) while searching and while waiting respectively. Finally, suppose that no\r
cans can be collected during a run home for recharging, and that no cans can be collected\r
on a step in which the battery is depleted. This system is then a finite MDP, and we\r
can write down the transition probabilities and the expected rewards, with dynamics as\r
indicated in the table on the left:\r
sa s0 p(s0 |s, a) r(s, a, s0)\r
high search high ↵ rsearch high search low 1  ↵ rsearch low search high 1   3\r
low search low  rsearch high wait high 1 rwait high wait low 0 -\r
low wait high 0 -\r
low wait low 1 rwait low recharge high 1 0\r
low recharge low 0 -\r
search\r
high low 1, 0\r
search\r
recharge\r
wait\r
wait\r
, rsearch\r
↵, rsearch 1↵, rsearch\r
1, 3\r
1, rwait\r
1, rwait\r
Note that there is a row in the table for each possible combination of current state, s,\r
action, a 2 A(s), and next state, s0. Some transitions have zero probability of occurring,\r
so no expected reward is specified for them. Shown on the right is another useful way of"""

[[sections]]
number = "3.2"
title = "Goals and Rewards 53"
text = """
summarizing the dynamics of a finite MDP, as a transition graph. There are two kinds of\r
nodes: state nodes and action nodes. There is a state node for each possible state (a large\r
open circle labeled by the name of the state), and an action node for each state–action\r
pair (a small solid circle labeled by the name of the action and connected by a line to the\r
state node). Starting in state s and taking action a moves you along the line from state\r
node s to action node (s, a). Then the environment responds with a transition to the next\r
state’s node via one of the arrows leaving action node (s, a). Each arrow corresponds to\r
a triple (s, s0, a), where s0 is the next state, and we label the arrow with the transition\r
probability, p(s0|s, a), and the expected reward for that transition, r(s, a, s0). Note that\r
the transition probabilities labeling the arrows leaving an action node always sum to 1.\r
Exercise 3.4 Give a table analogous to that in Example 3.3, but for p(s0, r|s, a). It\r
should have columns for s, a, s0, r, and p(s0, r|s, a), and a row for every 4-tuple for which\r
p(s0, r|s, a) > 0. ⇤"""

[[sections]]
number = "3.2"
title = "Goals and Rewards"
text = """
In reinforcement learning, the purpose or goal of the agent is formalized in terms of a\r
special signal, called the reward, passing from the environment to the agent. At each time\r
step, the reward is a simple number, Rt 2 R. Informally, the agent’s goal is to maximize\r
the total amount of reward it receives. This means maximizing not immediate reward,\r
but cumulative reward in the long run. We can clearly state this informal idea as the\r
reward hypothesis:\r
That all of what we mean by goals and purposes can be well thought of as\r
the maximization of the expected value of the cumulative sum of a received\r
scalar signal (called reward).\r
The use of a reward signal to formalize the idea of a goal is one of the most distinctive\r
features of reinforcement learning.\r
Although formulating goals in terms of reward signals might at first appear limiting,\r
in practice it has proved to be flexible and widely applicable. The best way to see this is\r
to consider examples of how it has been, or could be, used. For example, to make a robot\r
learn to walk, researchers have provided reward on each time step proportional to the\r
robot’s forward motion. In making a robot learn how to escape from a maze, the reward\r
is often 1 for every time step that passes prior to escape; this encourages the agent to\r
escape as quickly as possible. To make a robot learn to find and collect empty soda cans\r
for recycling, one might give it a reward of zero most of the time, and then a reward of\r
+1 for each can collected. One might also want to give the robot negative rewards when\r
it bumps into things or when somebody yells at it. For an agent to learn to play checkers\r
or chess, the natural rewards are +1 for winning, 1 for losing, and 0 for drawing and\r
for all nonterminal positions.\r
You can see what is happening in all of these examples. The agent always learns to\r
maximize its reward. If we want it to do something for us, we must provide rewards\r
to it in such a way that in maximizing them the agent will also achieve our goals. It"""

[[sections]]
number = "54"
title = "Chapter 3: Finite Markov Decision Processes"
text = """
is thus critical that the rewards we set up truly indicate what we want accomplished.\r
In particular, the reward signal is not the place to impart to the agent prior knowledge\r
about how to achieve what we want it to do.5 For example, a chess-playing agent should\r
be rewarded only for actually winning, not for achieving subgoals such as taking its\r
opponent’s pieces or aining control of the center of the board. If achieving these sorts\r
of subgoals were rewarded, then the agent might find a way to achieve them without\r
achieving the real goal. For example, it might find a way to take the opponent’s pieces\r
even at the cost of losing the game. The reward signal is your way of communicating to\r
the agent what you want achieved, not how you want it achieved.6"""

[[sections]]
number = "3.3"
title = "Returns and Episodes"
text = """
So far we have discussed informally the objective of learning. We have said that the\r
agent’s goal is to maximize the cumulative reward it receives in the long run. How might\r
this be defined formally? If the sequence of rewards received after time step t is denoted\r
Rt+1, Rt+2, Rt+3,..., then what precise aspect of this sequence do we wish to maximize?\r
In general, we seek to maximize the expected return, where the return, denoted Gt, is\r
defined as some specific function of the reward sequence. In the simplest case the return\r
is the sum of the rewards:\r
Gt\r
.\r
= Rt+1 + Rt+2 + Rt+3 + ··· + RT , (3.7)\r
where T is a final time step. This approach makes sense in applications in which there\r
is a natural notion of final time step, that is, when the agent–environment interaction\r
breaks naturally into subsequences, which we call episodes,\r
7 such as plays of a game,\r
trips through a maze, or any sort of repeated interaction. Each episode ends in a special\r
state called the terminal state, followed by a reset to a standard starting state or to a\r
sample from a standard distribution of starting states. Even if you think of episodes as\r
ending in di↵erent ways, such as winning and losing a game, the next episode begins\r
independently of how the previous one ended. Thus the episodes can all be considered to\r
end in the same terminal state, with di↵erent rewards for the di↵erent outcomes. Tasks\r
with episodes of this kind are called episodic tasks. In episodic tasks we sometimes need\r
to distinguish the set of all nonterminal states, denoted S, from the set of all states plus\r
the terminal state, denoted S+. The time of termination, T, is a random variable that\r
normally varies from episode to episode.\r
On the other hand, in many cases the agent–environment interaction does not break\r
naturally into identifiable episodes, but goes on continually without limit. For example,\r
this would be the natural way to formulate an on-going process-control task, or an\r
application to a robot with a long life span. We call these continuing tasks. The return\r
formulation (3.7) is problematic for continuing tasks because the final time step would be\r
T = 1, and the return, which is what we are trying to maximize, could easily be infinite.\r
5Better places for imparting this kind of prior knowledge are the initial policy or initial value function.\r
6Section 17.4 delves further into the issue of designing e↵ective reward signals.\r
7Episodes are sometimes called “trials” in the literature."""

[[sections]]
number = "3.3"
title = "Returns and Episodes 55"
text = """
(For example, suppose the agent receives a reward of +1 at each time step.) Thus, in this\r
book we usually use a definition of return that is slightly more complex conceptually but\r
much simpler mathematically.\r
The additional concept that we need is that of discounting. According to this approach,\r
the agent tries to select actions so that the sum of the discounted rewards it receives over\r
the future is maximized. In particular, it chooses At to maximize the expected discounted\r
return:\r
Gt\r
.\r
= Rt+1 + Rt+2 + 2Rt+3 + ··· = X1\r
k=0\r
kRt+k+1, (3.8)\r
where  is a parameter, 0    1, called the discount rate.\r
The discount rate determines the present value of future rewards: a reward received\r
k time steps in the future is worth only k1 times what it would be worth if it were\r
received immediately. If  < 1, the infinite sum in (3.8) has a finite value as long as the\r
reward sequence {Rk} is bounded. If  = 0, the agent is “myopic” in being concerned\r
only with maximizing immediate rewards: its objective in this case is to learn how to\r
choose At so as to maximize only Rt+1. If each of the agent’s actions happened to\r
influence only the immediate reward, not future rewards as well, then a myopic agent\r
could maximize (3.8) by separately maximizing each immediate reward. But in general,\r
acting to maximize immediate reward can reduce access to future rewards so that the\r
return is reduced. As  approaches 1, the return objective takes future rewards into\r
account more strongly; the agent becomes more farsighted.\r
Returns at successive time steps are related to each other in a way that is important\r
for the theory and algorithms of reinforcement learning:\r
Gt\r
.\r
= Rt+1 + Rt+2 + 2Rt+3 + 3Rt+4 + ···\r
= Rt+1 + \r
\r
Rt+2 + Rt+3 + 2Rt+4 + ··· \r
= Rt+1 + Gt+1 (3.9)\r
Note that this works for all time steps t<T, even if termination occurs at t + 1, provided\r
we define GT = 0. This often makes it easy to compute returns from reward sequences.\r
Note that although the return (3.8) is a sum of an infinite number of terms, it is still\r
finite if the reward is nonzero and constant—if  < 1. For example, if the reward is a\r
constant +1, then the return is\r
Gt = X1\r
k=0\r
k = 1\r
1   . (3.10)\r
Exercise 3.5 The equations in Section 3.1 are for the continuing case and need to be\r
modified (very slightly) to apply to episodic tasks. Show that you know the modifications\r
needed by giving the modified version of (3.3). ⇤"""

[[sections]]
number = "56"
title = "Chapter 3: Finite Markov Decision Processes"
text = """
Example 3.4: Pole-Balancing\r
The objective in this task is to apply\r
forces to a cart moving along a track\r
so as to keep a pole hinged to the cart\r
from falling over: A failure is said to\r
occur if the pole falls past a given angle\r
from vertical or if the cart runs o↵ the\r
track. The pole is reset to vertical\r
after each failure. This task could be\r
treated as episodic, where the natural\r
episodes are the repeated attempts to balance the pole. The reward in this case could be\r
+1 for every time step on which failure did not occur, so that the return at each time\r
would be the number of steps until failure. In this case, successful balancing forever would\r
mean a return of infinity. Alternatively, we could treat pole-balancing as a continuing\r
task, using discounting. In this case the reward would be 1 on each failure and zero at\r
all other times. The return at each time would then be related to K1, where K is\r
the number of time steps before failure (as well as to the times of later failures). In either\r
case, the return is maximized by keeping the pole balanced for as long as possible.\r
Exercise 3.6 Suppose you treated pole-balancing as an episodic task but also used\r
discounting, with all rewards zero except for 1 upon failure. What then would the\r
return be at each time? How does this return di↵er from that in the discounted, continuing\r
formulation of this task? ⇤\r
Exercise 3.7 Imagine that you are designing a robot to run a maze. You decide to give it a\r
reward of +1 for escaping from the maze and a reward of zero at all other times. The task\r
seems to break down naturally into episodes—the successive runs through the maze—so\r
you decide to treat it as an episodic task, where the goal is to maximize expected total\r
reward (3.7). After running the learning agent for a while, you find that it is showing\r
no improvement in escaping from the maze. What is going wrong? Have you e↵ectively\r
communicated to the agent what you want it to achieve? ⇤\r
Exercise 3.8 Suppose  = 0.5 and the following sequence of rewards is received R1 = 1,\r
R2 = 2, R3 = 6, R4 = 3, and R5 = 2, with T = 5. What are G0, G1, ..., G5? Hint:\r
Work backwards. ⇤\r
Exercise 3.9 Suppose  = 0.9 and the reward sequence is R1 = 2 followed by an infinite\r
sequence of 7s. What are G1 and G0? ⇤\r
Exercise 3.10 Prove the second equality in (3.10). ⇤"""

[[sections]]
number = "3.4"
title = "Unified Notation for Episodic and Continuing Tasks 57"
text = ""

[[sections]]
number = "3.4"
title = "Unified Notation for Episodic and Continuing Tasks"
text = """
In the preceding section we described two kinds of reinforcement learning tasks, one\r
in which the agent–environment interaction naturally breaks down into a sequence of\r
separate episodes (episodic tasks), and one in which it does not (continuing tasks). The\r
former case is mathematically easier because each action a↵ects only the finite number of\r
rewards subsequently received during the episode. In this book we consider sometimes\r
one kind of problem and sometimes the other, but often both. It is therefore useful to\r
establish one notation that enables us to talk precisely about both cases simultaneously.\r
To be precise about episodic tasks requires some additional notation. Rather than one\r
long sequence of time steps, we need to consider a series of episodes, each of which consists\r
of a finite sequence of time steps. We number the time steps of each episode starting\r
anew from zero. Therefore, we have to refer not just to St, the state representation at\r
time t, but to St,i, the state representation at time t of episode i (and similarly for At,i,\r
Rt,i, ⇡t,i, Ti, etc.). However, it turns out that when we discuss episodic tasks we almost\r
never have to distinguish between di↵erent episodes. We are almost always considering\r
a particular episode, or stating something that is true for all episodes. Accordingly, in\r
practice we almost always abuse notation slightly by dropping the explicit reference to\r
episode number. That is, we write St to refer to St,i, and so on.\r
We need one other convention to obtain a single notation that covers both episodic\r
and continuing tasks. We have defined the return as a sum over a finite number of terms\r
in one case (3.7) and as a sum over an infinite number of terms in the other (3.8). These\r
two can be unified by considering episode termination to be the entering of a special\r
absorbing state that transitions only to itself and that generates only rewards of zero. For\r
example, consider the state transition diagram:\r
R1 = +1 S0 S1R2 = +1 S2\r
R3 = +1 R4 = 0\r
R5 = 0. . .\r
Here the solid square represents the special absorbing state corresponding to the end of an\r
episode. Starting from S0, we get the reward sequence +1, +1, +1, 0, 0, 0,.... Summing\r
these, we get the same return whether we sum over the first T rewards (here T = 3) or\r
over the full infinite sequence. This remains true even if we introduce discounting. Thus,\r
we can define the return, in general, according to (3.8), using the convention of omitting\r
episode numbers when they are not needed, and including the possibility that  = 1 if the\r
sum remains defined (e.g., because all episodes terminate). Alternatively, we can write\r
Gt\r
.\r
= X\r
T\r
k=t+1\r
kt1Rk, (3.11)\r
including the possibility that T = 1 or  = 1 (but not both). We use these conventions\r
throughout the rest of the book to simplify notation and to express the close parallels"""

[[sections]]
number = "58"
title = "Chapter 3: Finite Markov Decision Processes"
text = """
between episodic and continuing tasks. (Later, in Chapter 10, we will introduce a\r
formulation that is both continuing and undiscounted.)"""

[[sections]]
number = "3.5"
title = "Policies and Value Functions"
text = """
Almost all reinforcement learning algorithms involve estimating value functions—functions\r
of states (or of state–action pairs) that estimate how good it is for the agent to be in a\r
given state (or how good it is to perform a given action in a given state). The notion\r
of “how good” here is defined in terms of future rewards that can be expected, or, to\r
be precise, in terms of expected return. Of course the rewards the agent can expect to\r
receive in the future depend on what actions it will take. Accordingly, value functions\r
are defined with respect to particular ways of acting, called policies.\r
Formally, a policy is a mapping from states to probabilities of selecting each possible\r
action. If the agent is following policy ⇡ at time t, then ⇡(a|s) is the probability that\r
At = a if St = s. Like p, ⇡ is an ordinary function; the “|” in the middle of ⇡(a|s)\r
merely reminds us that it defines a probability distribution over a 2 A(s) for each s 2 S.\r
Reinforcement learning methods specify how the agent’s policy is changed as a result of\r
its experience.\r
Exercise 3.11 If the current state is St, and actions are selected according to a stochastic\r
policy ⇡, then what is the expectation of Rt+1 in terms of ⇡ and the four-argument\r
function p (3.2)? ⇤\r
The value function of a state s under a policy ⇡, denoted v⇡(s), is the expected return\r
when starting in s and following ⇡ thereafter. For MDPs, we can define v⇡ formally by\r
v⇡(s) .= E⇡[Gt | St =s] = E⇡\r
"\r
X1\r
k=0\r
kRt+k+1\r
\r
\r
\r
\r
\r
St =s\r
#\r
, for all s 2 S, (3.12)\r
where E⇡[·] denotes the expected value of a random variable given that the agent follows\r
policy ⇡, and t is any time step. Note that the value of the terminal state, if any, is\r
always zero. We call the function v⇡ the state-value function for policy ⇡.\r
Similarly, we define the value of taking action a in state s under a policy ⇡, denoted\r
q⇡(s, a), as the expected return starting from s, taking the action a, and thereafter\r
following policy ⇡:\r
q⇡(s, a) .= E⇡[Gt | St =s, At = a] = E⇡\r
"\r
X1\r
k=0\r
kRt+k+1\r
\r
\r
\r
\r
\r
St =s, At =a\r
#\r
. (3.13)\r
We call q⇡ the action-value function for policy ⇡.\r
Exercise 3.12 Give an equation for v⇡ in terms of q⇡ and ⇡. ⇤\r
Exercise 3.13 Give an equation for q⇡ in terms of v⇡ and the four-argument p. ⇤\r
The value functions v⇡ and q⇡ can be estimated from experience. For example, if an\r
agent follows policy ⇡ and maintains an average, for each state encountered, of the actual\r
returns that have followed that state, then the average will converge to the state’s value,\r
v⇡(s), as the number of times that state is encountered approaches infinity. If separate"""

[[sections]]
number = "3.5"
title = "Policies and Value Functions 59"
text = """
averages are kept for each action taken in each state, then these averages will similarly\r
converge to the action values, q⇡(s, a). We call estimation methods of this kind Monte\r
Carlo methods because they involve averaging over many random samples of actual returns.\r
These kinds of methods are presented in Chapter 5. Of course, if there are very many\r
states, then it may not be practical to keep separate averages for each state individually.\r
Instead, the agent would have to maintain v⇡ and q⇡ as parameterized functions (with\r
fewer parameters than states) and adjust the parameters to better match the observed\r
returns. This can also produce accurate estimates, although much depends on the nature\r
of the parameterized function approximator. These possibilities are discussed in Part II\r
of the book.\r
A fundamental property of value functions used throughout reinforcement learning and\r
dynamic programming is that they satisfy recursive relationships similar to that which\r
we have already established for the return (3.9). For any policy ⇡ and any state s, the\r
following consistency condition holds between the value of s and the value of its possible\r
successor states:\r
v⇡(s) .= E⇡[Gt | St =s]\r
= E⇡[Rt+1 + Gt+1 | St =s] (by (3.9))\r
= X\r
a\r
⇡(a|s)\r
X\r
s0\r
X\r
r\r
p(s0, r|s, a)\r
h\r
r + E⇡[Gt+1|St+1 =s0]\r
i\r
= X\r
a\r
⇡(a|s)\r
X\r
s0,r\r
p(s0, r|s, a)\r
h\r
r + v⇡(s0)\r
i\r
, for all s 2 S, (3.14)\r
where it is implicit that the actions, a, are taken from the set A(s), that the next states,\r
s0, are taken from the set S (or from S+ in the case of an episodic problem), and that\r
the rewards, r, are taken from the set R. Note also how in the last equation we have\r
merged the two sums, one over all the values of s0 and the other over all the values of r,\r
into one sum over all the possible values of both. We use this kind of merged sum often\r
to simplify formulas. Note how the final expression can be read easily as an expected\r
value. It is really a sum over all values of the three variables, a, s0, and r. For each triple,\r
we compute its probability, ⇡(a|s)p(s0, r|s, a), weight the quantity in brackets by that\r
probability, then sum over all possibilities to get an expected value.\r
⇡\r
s\r
s0\r
⇡\r
r p\r
a\r
Backup diagram for v⇡\r
Equation (3.14) is the Bellman equation for v⇡. It expresses\r
a relationship between the value of a state and the values of\r
its successor states. Think of looking ahead from a state to its\r
possible successor states, as suggested by the diagram to the\r
right. Each open circle represents a state and each solid circle\r
represents a state–action pair. Starting from state s, the root\r
node at the top, the agent could take any of some set of actions—\r
three are shown in the diagram—based on its policy ⇡. From\r
each of these, the environment could respond with one of several next states, s0 (two are\r
shown in the figure), along with a reward, r, depending on its dynamics given by the\r
function p. The Bellman equation (3.14) averages over all the possibilities, weighting each\r
by its probability of occurring. It states that the value of the start state must equal the\r
(discounted) value of the expected next state, plus the reward expected along the way."""

[[sections]]
number = "60"
title = "Chapter 3: Finite Markov Decision Processes"
text = """
The value function v⇡ is the unique solution to its Bellman equation. We show in\r
subsequent chapters how this Bellman equation forms the basis of a number of ways to\r
compute, approximate, and learn v⇡. We call diagrams like that above backup diagrams\r
because they diagram relationships that form the basis of the update or backup operations\r
that are at the heart of reinforcement learning methods. These operations transfer\r
value information back to a state (or a state–action pair) from its successor states (or\r
state–action pairs). We use backup diagrams throughout the book to provide graphical\r
summaries of the algorithms we discuss. (Note that, unlike transition graphs, the state\r
nodes of backup diagrams do not necessarily represent distinct states; for example, a\r
state might be its own successor.)\r
Example 3.5: Gridworld Figure 3.2 (left) shows a rectangular gridworld representation\r
of a simple finite MDP. The cells of the grid correspond to the states of the environment. At\r
each cell, four actions are possible: north, south, east, and west, which deterministically\r
cause the agent to move one cell in the respective direction on the grid. Actions that\r
would take the agent o↵ the grid leave its location unchanged, but also result in a reward\r
of 1. Other actions result in a reward of 0, except those that move the agent out of the\r
special states A and B. From state A, all four actions yield a reward of +10 and take the\r
agent to A0. From state B, all actions yield a reward of +5 and take the agent to B0.\r
the states of the environment. At each cell, four actions are possible: north,\r
south, east, and west, which deterministically cause the agent to move one\r
cell in the respective direction on the grid. Actions that would take the agent\r
o the grid leave its location unchanged, but also result in a reward of 1.\r
Other actions result in a reward of 0, except those that move the agent out\r
of the special states A and B. From state A, all four actions yield a reward of\r
+10 and take the agent to A. From state B, all actions yield a reward of +5\r
and take the agent to B.\r
Suppose the agent selects all four actions with equal probability in all\r
states. Figure 3.5b shows the value function, v⇡, for this policy, for the dis\u0002counted reward case with  = 0.9. This value function was computed by solv\u0002ing the system of equations (3.10). Notice the negative values near the lower\r
edge; these are the result of the high probability of hitting the edge of the grid\r
there under the random policy. State A is the best state to be in under this pol\u0002icy, but its expected return is less than 10, its immediate reward, because from\r
A the agent is taken to A, from which it is likely to run into the edge of the\r
grid. State B, on the other hand, is valued more than 5, its immediate reward,\r
because from B the agent is taken to B, which has a positive value. From B the\r
expected penalty (negative reward) for possibly running into an edge is more\r
3.3 8.8 4.4 5.3 1.5\r
1.5 3.0 2.3 1.9 0.5\r
0.1 0.7 0.7 0.4 -0.4\r
-1.0 -0.4 -0.4 -0.6 -1.2\r
-1.9 -1.3 -1.2 -1.4 -2.0\r
A B\r
A'\r
+10 B'\r
+5\r
Actions\r
(a) (b)\r
Figure 3.5: Grid example: (a) exceptional reward dynamics; (b) state-value\r
function for the equiprobable random policy.\r
Figure 3.2: Gridworld example: exceptional reward dynamics (left) and state-value function\r
for the equiprobable random policy (right).\r
Suppose the agent selects all four actions with equal probability in all states. Figure 3.2\r
(right) shows the value function, v⇡, for this policy, for the discounted reward case with\r
 = 0.9. This value function was computed by solving the system of linear equations\r
(3.14). Notice the negative values near the lower edge; these are the result of the high\r
probability of hitting the edge of the grid there under the random policy. State A is\r
the best state to be in under this policy. Note that A’s expected return is less than its\r
immediate reward of 10, because from A the agent is taken to state A0 from which it is\r
likely to run into the edge of the grid. State B, on the other hand, is valued more than\r
its immediate reward of 5, because from B the agent is taken to B0 which has a positive\r
value. From B0 the expected penalty (negative reward) for possibly running into an edge\r
is more than compensated for by the expected gain for possibly stumbling onto A or B.\r
Exercise 3.14 The Bellman equation (3.14) must hold for each state for the value function\r
v⇡ shown in Figure 3.2 (right) of Example 3.5. Show numerically that this equation holds\r
for the center state, valued at +0.7, with respect to its four neighboring states, valued at\r
+2.3, +0.4, 0.4, and +0.7. (These numbers are accurate only to one decimal place.) ⇤"""

[[sections]]
number = "3.5"
title = "Policies and Value Functions 61"
text = """
Exercise 3.15 In the gridworld example, rewards are positive for goals, negative for\r
running into the edge of the world, and zero the rest of the time. Are the signs of these\r
rewards important, or only the intervals between them? Prove, using (3.8), that adding a\r
constant c to all the rewards adds a constant, vc, to the values of all states, and thus\r
does not a↵ect the relative values of any states under any policies. What is vc in terms\r
of c and ? ⇤\r
Exercise 3.16 Now consider adding a constant c to all the rewards in an episodic task,\r
such as maze running. Would this have any e↵ect, or would it leave the task unchanged\r
as in the continuing task above? Why or why not? Give an example. ⇤\r
Example 3.6: Golf To formulate playing a hole of golf as a reinforcement learning\r
task, we count a penalty (negative reward) of 1 for each stroke until we hit the ball\r
into the hole. The state is the location of the ball. The value of a state is the negative of\r
the number of strokes to the hole from that location. Our actions are how we aim and\r
swing at the ball, of course, and which club we select. Let us take the former as given\r
and consider just the choice of club, which we assume is either a putter or a driver. The\r
upper part of Figure 3.3 shows a possible state-value function, vputt(s), for the policy that\r
Q*(s,driver)\r
Vputt\r
s a n d\r
green\r
!1\r
s a\r
n\r
d\r
!2 !2 !3\r
!4\r
!1\r
!5\r
!6\r
!4\r
!3\r
!3 !2\r
!4\r
s a n d\r
green\r
!1\r
s a\r
n\r
d\r
!2\r
!3\r
!2\r
0\r
0\r
!"\r
!"\r
vputt\r
q⇤(s, driver)\r
Figure 3.3: A golf example: the state-value func\u0002tion for putting (upper) and the optimal action\u0002value function for using the driver (lower).\r
always uses the putter. The terminal\r
state in-the-hole has a value of 0. From\r
anywhere on the green we assume we can\r
make a putt; these states have value 1.\r
O↵ the green we cannot reach the hole by\r
putting, and the value is lower. If we can\r
reach the green from a state by putting,\r
then that state must have value one less\r
than the green’s value, that is, 2. For\r
simplicity, let us assume we can putt very\r
precisely and deterministically, but with\r
a limited range. This gives us the sharp\r
contour line labeled 2 in the figure; all\r
locations between that line and the green\r
require exactly two strokes to complete\r
the hole. Similarly, any location within\r
putting range of the 2 contour line must\r
have a value of 3, and so on to get all the\r
contour lines shown in the figure. Putting\r
doesn’t get us out of sand traps, so they\r
have a value of 1. Overall, it takes us\r
six strokes to get from the tee to the hole\r
by putting.\r
r\r
s0\r
s, a\r
a0\r
⇡\r
p\r
q⇡ backup diagram\r
Exercise 3.17 What is the Bellman equation for action values, that\r
is, for q⇡? It must give the action value q⇡(s, a) in terms of the action\r
values, q⇡(s0, a0), of possible successors to the state–action pair (s, a).\r
Hint: The backup diagram to the right corresponds to this equation.\r
Show the sequence of equations analogous to (3.14), but for action\r
values. ⇤"""

[[sections]]
number = "62"
title = "Chapter 3: Finite Markov Decision Processes"
text = """
Exercise 3.18 The value of a state depends on the values of the actions possible in that\r
state and on how likely each action is to be taken under the current policy. We can\r
think of this in terms of a small backup diagram rooted at the state and considering each\r
possible action:\r
s\r
taken with\r
probability ⇡(a|s)\r
v⇡(s)\r
q⇡(s, a)\r
a1 a2 a3\r
Give the equation corresponding to this intuition and diagram for the value at the root\r
node, v⇡(s), in terms of the value at the expected leaf node, q⇡(s, a), given St = s. This\r
equation should include an expectation conditioned on following the policy, ⇡. Then give\r
a second equation in which the expected value is written out explicitly in terms of ⇡(a|s)\r
such that no expected value notation appears in the equation. ⇤\r
Exercise 3.19 The value of an action, q⇡(s, a), depends on the expected next reward and\r
the expected sum of the remaining rewards. Again we can think of this in terms of a\r
small backup diagram, this one rooted at an action (state–action pair) and branching to\r
the possible next states:\r
s, a q⇡(s, a)\r
s0\r
3 s0\r
2 s01\r
r1 r2 r3 v⇡(s0\r
)\r
expected\r
rewards\r
Give the equation corresponding to this intuition and diagram for the action value,\r
q⇡(s, a), in terms of the expected next reward, Rt+1, and the expected next state value,\r
v⇡(St+1), given that St =s and At =a. This equation should include an expectation but\r
not one conditioned on following the policy. Then give a second equation, writing out the\r
expected value explicitly in terms of p(s0, r|s, a) defined by (3.2), such that no expected\r
value notation appears in the equation. ⇤"""

[[sections]]
number = "3.6"
title = "Optimal Policies and Optimal Value Functions"
text = """
Solving a reinforcement learning task means, roughly, finding a policy that achieves a lot\r
of reward over the long run. For finite MDPs, we can precisely define an optimal policy\r
in the following way. Value functions define a partial ordering over policies. A policy ⇡ is\r
defined to be better than or equal to a policy ⇡0 if its expected return is greater than\r
or equal to that of ⇡0 for all states. In other words, ⇡  ⇡0 if and only if v⇡(s)  v⇡0 (s)\r
for all s 2 S. There is always at least one policy that is better than or equal to all other\r
policies. This is an optimal policy. Although there may be more than one, we denote all\r
the optimal policies by ⇡⇤. They share the same state-value function, called the optimal\r
state-value function, denoted v⇤, and defined as\r
v⇤(s) .= max ⇡ v⇡(s), (3.15)\r
for all s 2 S."""

[[sections]]
number = "3.6"
title = "Optimal Policies and Optimal Value Functions 63"
text = """
Optimal policies also share the same optimal action-value function, denoted q⇤, and\r
defined as\r
q⇤(s, a) .= max ⇡ q⇡(s, a), (3.16)\r
for all s 2 S and a 2 A(s). For the state–action pair (s, a), this function gives the\r
expected return for taking action a in state s and thereafter following an optimal policy.\r
Thus, we can write q⇤ in terms of v⇤ as follows:\r
q⇤(s, a) = E[Rt+1 + v⇤(St+1) | St =s, At =a] . (3.17)\r
Example 3.7: Optimal Value Functions for Golf The lower part of Figure 3.3\r
shows the contours of a possible optimal action-value function q⇤(s, driver). These are\r
the values of each state if we first play a stroke with the driver and afterward select either\r
the driver or the putter, whichever is better. The driver enables us to hit the ball farther,\r
but with less accuracy. We can reach the hole in one shot using the driver only if we\r
are already very close; thus the 1 contour for q⇤(s, driver) covers only a small portion\r
of the green. If we have two strokes, however, then we can reach the hole from much\r
farther away, as shown by the 2 contour. In this case we don’t have to drive all the way\r
to within the small 1 contour, but only to anywhere on the green; from there we can\r
use the putter. The optimal action-value function gives the values after committing to a\r
particular first action, in this case, to the driver, but afterward using whichever actions\r
are best. The 3 contour is still farther out and includes the starting tee. From the tee,\r
the best sequence of actions is two drives and one putt, sinking the ball in three strokes.\r
Because v⇤ is the value function for a policy, it must satisfy the self-consistency\r
condition given by the Bellman equation for state values (3.14). Because it is the optimal\r
value function, however, v⇤’s consistency condition can be written in a special form\r
without reference to any specific policy. This is the Bellman equation for v⇤, or the\r
Bellman optimality equation. Intuitively, the Bellman optimality equation expresses the\r
fact that the value of a state under an optimal policy must equal the expected return for\r
the best action from that state:\r
v⇤(s) = max a2A(s)q⇡⇤ (s, a)\r
= maxa E⇡⇤[Gt | St =s, At =a]\r
= maxa E⇡⇤[Rt+1 + Gt+1 | St =s, At =a] (by (3.9))\r
= maxa E[Rt+1 + v⇤(St+1) | St =s, At =a] (3.18)\r
= maxa\r
X\r
s0,r\r
p(s0, r|s, a)\r
⇥\r
r + v⇤(s0)\r
⇤\r
. (3.19)"""

[[sections]]
number = "64"
title = "Chapter 3: Finite Markov Decision Processes"
text = """
The last two equations are two forms of the Bellman optimality equation for v⇤. The\r
Bellman optimality equation for q⇤ is\r
q⇤(s, a) = E\r
h\r
Rt+1 +  max\r
a0 q⇤(St+1, a0\r
)\r
\r
\r
\r
St = s, At = a\r
i\r
= X\r
s0,r\r
p(s0, r|s, a)\r
h\r
r +  max\r
a0 q⇤(s0\r
, a0)\r
i\r
. (3.20)\r
The backup diagrams in the figure below show graphically the spans of future states\r
and actions considered in the Bellman optimality equations for v⇤ and q⇤. These are the\r
same as the backup diagrams for v⇡ and q⇡ presented earlier except that arcs have been\r
added at the agent’s choice points to represent that the maximum over that choice is\r
taken rather than the expected value given some policy. The backup diagram on the left\r
graphically represents the Bellman optimality equation (3.19) and the backup diagram\r
on the right graphically represents (3.20).\r
s\r
s0\r
a\r
r\r
r\r
s0\r
s, a\r
a0\r
max\r
max\r
(v⇤) (q⇤)\r
Figure 3.4: Backup diagrams for v⇤ and q⇤\r
For finite MDPs, the Bellman optimality equation for v⇤ (3.19) has a unique solution.\r
The Bellman optimality equation is actually a system of equations, one for each state, so\r
if there are n states, then there are n equations in n unknowns. If the dynamics p of the\r
environment are known, then in principle one can solve this system of equations for v⇤\r
using any one of a variety of methods for solving systems of nonlinear equations. One\r
can solve a related set of equations for q⇤.\r
Once one has v⇤, it is relatively easy to determine an optimal policy. For each state\r
s, there will be one or more actions at which the maximum is obtained in the Bellman\r
optimality equation. Any policy that assigns nonzero probability only to these actions is\r
an optimal policy. You can think of this as a one-step search. If you have the optimal\r
value function, v⇤, then the actions that appear best after a one-step search will be optimal\r
actions. Another way of saying this is that any policy that is greedy with respect to the\r
optimal evaluation function v⇤ is an optimal policy. The term greedy is used in computer\r
science to describe any search or decision procedure that selects alternatives based only\r
on local or immediate considerations, without considering the possibility that such a\r
selection may prevent future access to even better alternatives. Consequently, it describes\r
policies that select actions based only on their short-term consequences. The beauty of v⇤\r
is that if one uses it to evaluate the short-term consequences of actions—specifically, the\r
one-step consequences—then a greedy policy is actually optimal in the long-term sense in\r
which we are interested because v⇤ already takes into account the reward consequences of\r
all possible future behavior. By means of v⇤, the optimal expected long-term return is"""

[[sections]]
number = "3.6"
title = "Optimal Policies and Optimal Value Functions 65"
text = """
turned into a quantity that is locally and immediately available for each state. Hence, a\r
one-step-ahead search yields the long-term optimal actions.\r
Having q⇤ makes choosing optimal actions even easier. With q⇤, the agent does not\r
even have to do a one-step-ahead search: for any state s, it can simply find any action\r
that maximizes q⇤(s, a). The action-value function e↵ectively caches the results of all\r
one-step-ahead searches. It provides the optimal expected long-term return as a value\r
that is locally and immediately available for each state–action pair. Hence, at the cost of\r
representing a function of state–action pairs, instead of just of states, the optimal action\u0002value function allows optimal actions to be selected without having to know anything\r
about possible successor states and their values, that is, without having to know anything\r
about the environment’s dynamics.\r
Example 3.8: Solving the Gridworld Suppose we solve the Bellman equation for v⇤\r
for the simple grid task introduced in Example 3.5 and shown again in Figure 3.5 (left).\r
Recall that state A is followed by a reward of +10 and transition to state A0, while state\r
B is followed by a reward of +5 and transition to state B0. Figure 3.5 (middle) shows the\r
optimal value function, and Figure 3.5 (right) shows the corresponding optimal policies.\r
Where there are multiple arrows in a cell, all of the corresponding actions are optimal.\r
a) gridworld b) V* c) !*\r
22.0 24.4 22.0 19.4 17.5\r
19.8 22.0 19.8 17.8 16.0\r
17.8 19.8 17.8 16.0 14.4\r
16.0 17.8 16.0 14.4 13.0\r
14.4 16.0 14.4 13.0 11.7\r
A B\r
A'\r
+10 B'\r
+5\r
v* π* Gridworld v⇤ ⇡⇤\r
Figure 3.5: Optimal solutions to the gridworld example.\r
Example 3.9: Bellman Optimality Equations for the Recycling Robot Using\r
(3.19), we can explicitly give the Bellman optimality equation for the recycling robot\r
example. To make things more compact, we abbreviate the states high and low, and the\r
actions search, wait, and recharge respectively by h, l, s, w, and re. Because there are\r
only two states, the Bellman optimality equation consists of two equations. The equation\r
for v⇤(h) can be written as follows:\r
v⇤(h) = max ⇢ p(h|h, s)[r(h, s, h) + v⇤(h)] + p(l|h, s)[r(h, s, l) + v⇤(l)],\r
p(h|h, w)[r(h, w, h) + v⇤(h)] + p(l|h, w)[r(h, w, l) + v⇤(l)] \r
= max ⇢ ↵[rs + v⇤(h)] + (1  ↵)[rs + v⇤(l)],\r
1[rw + v⇤(h)] + 0[rw + v⇤(l)] \r
= max ⇢ rs + [↵v⇤(h) + (1  ↵)v⇤(l)],\r
rw + v⇤(h)\r
\r
."""

[[sections]]
number = "66"
title = "Chapter 3: Finite Markov Decision Processes"
text = """
Following the same procedure for v⇤(l) yields the equation\r
v⇤(l) = max\r
8\r
<\r
:\r
rs  3(1  ) + [(1  )v⇤(h) + v⇤(l)],\r
rw + v⇤(l),\r
v⇤(h)\r
9\r
=\r
; .\r
For any choice of rs, rw, ↵, , and , with 0   < 1, 0  ↵,   1, there is exactly\r
one pair of numbers, v⇤(h) and v⇤(l), that simultaneously satisfy these two nonlinear\r
equations.\r
Explicitly solving the Bellman optimality equation provides one route to finding an\r
optimal policy, and thus to solving the reinforcement learning problem. However, this\r
solution is rarely directly useful. It is akin to an exhaustive search, looking ahead at\r
all possibilities, computing their probabilities of occurrence and their desirabilities in\r
terms of expected rewards. This solution relies on at least three assumptions that are\r
rarely true in practice: (1) the dynamics of the environment are accurately known; (2)\r
computational resources are sucient to complete the calculation; and (3) the states\r
have the Markov property. For the kinds of tasks in which we are interested, one is\r
generally not able to implement this solution exactly because various combinations of\r
these assumptions are violated. For example, although the first and third assumptions\r
present no problems for the game of backgammon, the second is a major impediment.\r
Because the game has about 1020 states, it would take thousands of years on today’s\r
fastest computers to solve the Bellman equation for v⇤, and the same is true for finding\r
q⇤. In reinforcement learning one typically has to settle for approximate solutions.\r
Many di↵erent decision-making methods can be viewed as ways of approximately\r
solving the Bellman optimality equation. For example, heuristic search methods can be\r
viewed as expanding the right-hand side of (3.19) several times, up to some depth, forming\r
a “tree” of possibilities, and then using a heuristic evaluation function to approximate\r
v⇤ at the “leaf” nodes. (Heuristic search methods such as A⇤ are almost always based\r
on the episodic case.) The methods of dynamic programming can be related even more\r
closely to the Bellman optimality equation. Many reinforcement learning methods can\r
be clearly understood as approximately solving the Bellman optimality equation, using\r
actual experienced transitions in place of knowledge of the expected transitions. We\r
consider a variety of such methods in the following chapters.\r
Exercise 3.20 Draw or describe the optimal state-value function for the golf example. ⇤\r
Exercise 3.21 Draw or describe the contours of the optimal action-value function for\r
putting, q⇤(s, putter), for the golf example. ⇤\r
0 +2 +1 0\r
left right\r
Exercise 3.22 Consider the continuing MDP shown to the\r
right. The only decision to be made is that in the top state,\r
where two actions are available, left and right. The numbers\r
show the rewards that are received deterministically after\r
each action. There are exactly two deterministic policies,\r
⇡left and ⇡right. What policy is optimal if  = 0? If  = 0.9?\r
If  = 0.5? ⇤"""

[[sections]]
number = "3.7"
title = "Optimality and Approximation 67"
text = """
Exercise 3.23 Give the Bellman equation for q⇤ for the recycling robot. ⇤\r
Exercise 3.24 Figure 3.5 gives the optimal value of the best state of the gridworld as\r
24.4, to one decimal place. Use your knowledge of the optimal policy and (3.8) to express\r
this value symbolically, and then to compute it to three decimal places. ⇤\r
Exercise 3.25 Give an equation for v⇤ in terms of q⇤. ⇤\r
Exercise 3.26 Give an equation for q⇤ in terms of v⇤ and the four-argument p. ⇤\r
Exercise 3.27 Give an equation for ⇡⇤ in terms of q⇤. ⇤\r
Exercise 3.28 Give an equation for ⇡⇤ in terms of v⇤ and the four-argument p. ⇤\r
Exercise 3.29 Rewrite the four Bellman equations for the four value functions (v⇡, v⇤, q⇡,\r
and q⇤) in terms of the three argument function p (3.4) and the two-argument function r\r
(3.5). ⇤"""

[[sections]]
number = "3.7"
title = "Optimality and Approximation"
text = """
We have defined optimal value functions and optimal policies. Clearly, an agent that\r
learns an optimal policy has done very well, but in practice this rarely happens. For\r
the kinds of tasks in which we are interested, optimal policies can be generated only\r
with extreme computational cost. A well-defined notion of optimality organizes the\r
approach to learning we describe in this book and provides a way to understand the\r
theoretical properties of various learning algorithms, but it is an ideal that agents can\r
only approximate. As we discussed above, even if we have a complete and accurate model\r
of the environment’s dynamics, it is usually not possible to simply compute an optimal\r
policy by solving the Bellman optimality equation. For example, board games such as\r
chess are a tiny fraction of human experience, yet large, custom-designed computers still\r
cannot compute the optimal moves. A critical aspect of the problem facing the agent is\r
always the computational power available to it, in particular, the amount of computation\r
it can perform in a single time step.\r
The memory available is also an important constraint. A large amount of memory\r
is often required to build up approximations of value functions, policies, and models.\r
In tasks with small, finite state sets, it is possible to form these approximations using\r
arrays or tables with one entry for each state (or state–action pair). This we call the\r
tabular case, and the corresponding methods we call tabular methods. In many cases\r
of practical interest, however, there are far more states than could possibly be entries\r
in a table. In these cases the functions must be approximated, using some sort of more\r
compact parameterized function representation.\r
Our framing of the reinforcement learning problem forces us to settle for approxi\u0002mations. However, it also presents us with some unique opportunities for achieving\r
useful approximations. For example, in approximating optimal behavior, there may be\r
many states that the agent faces with such a low probability that selecting suboptimal\r
actions for them has little impact on the amount of reward the agent receives. Tesauro’s\r
backgammon player, for example, plays with exceptional skill even though it might make"""

[[sections]]
number = "68"
title = "Chapter 3: Finite Markov Decision Processes"
text = """
very bad decisions on board configurations that never occur in games against experts. In\r
fact, it is possible that TD-Gammon makes bad decisions for a large fraction of the game’s\r
state set. The online nature of reinforcement learning makes it possible to approximate\r
optimal policies in ways that put more e↵ort into learning to make good decisions for\r
frequently encountered states, at the expense of less e↵ort for infrequently encountered\r
states. This is one key property that distinguishes reinforcement learning from other\r
approaches to approximately solving MDPs."""

[[sections]]
number = "3.8"
title = "Summary"
text = """
Let us summarize the elements of the reinforcement learning problem that we have\r
presented in this chapter. Reinforcement learning is about learning from interaction\r
how to behave in order to achieve a goal. The reinforcement learning agent and its\r
environment interact over a sequence of discrete time steps. The specification of their\r
interface defines a particular task: the actions are the choices made by the agent; the\r
states are the basis for making the choices; and the rewards are the basis for evaluating\r
the choices. Everything inside the agent is known and controllable. Its environment, on\r
the other hand, is incompletely controllable and may or may not be completely known.\r
A policy is a stochastic rule by which the agent selects actions as a function of states.\r
The agent’s objective is to maximize the amount of reward it receives over time.\r
When the reinforcement learning setup described above is formulated with well defined\r
transition probabilities it constitutes a Markov decision process (MDP). A finite MDP is\r
an MDP with finite state, action, and (as we formulate it here) reward sets. Much of the\r
current theory of reinforcement learning is restricted to finite MDPs, but the methods\r
and ideas apply more generally.\r
The return is the function of future rewards that the agent seeks to maximize (in\r
expected value). It has several di↵erent definitions depending upon the nature of the\r
task and whether one wishes to discount delayed reward. The undiscounted formulation\r
is appropriate for episodic tasks, in which the agent–environment interaction breaks\r
naturally into episodes; the discounted formulation is appropriate for tabular continuing\r
tasks, in which the interaction does not naturally break into episodes but continues\r
without limit (but see Sections 10.3–4). We try to define the returns for the two kinds of\r
tasks such that one set of equations can apply to both the episodic and continuing cases.\r
A policy’s value functions (v⇡ and q⇡) assign to each state, or state–action pair, the\r
expected return from that state, or state–action pair, given that the agent uses the\r
policy. The optimal value functions (v⇤ and q⇤) assign to each state, or state–action pair,\r
the largest expected return achievable by any policy. A policy whose value functions\r
are optimal is an optimal policy. Whereas the optimal value functions for states and\r
state–action pairs are unique for a given MDP, there can be many optimal policies. Any\r
policy that is greedy with respect to the optimal value functions must be an optimal\r
policy. The Bellman optimality equations are special consistency conditions that the\r
optimal value functions must satisfy and that can, in principle, be solved for the optimal\r
value functions, from which an optimal policy can be determined with relative ease."""

[[sections]]
number = "3.8"
title = "Summary 69"
text = """
A reinforcement learning problem can be posed in a variety of di↵erent ways depending\r
on assumptions about the level of knowledge initially available to the agent. In problems\r
of complete knowledge, the agent has a complete and accurate model of the environment’s\r
dynamics. If the environment is an MDP, then such a model consists of the complete four\u0002argument dynamics function p (3.2). In problems of incomplete knowledge, a complete\r
and perfect model of the environment is not available.\r
Even if the agent had a complete and accurate environment model, the agent would\r
typically be unable to fully use it because of limitations on its memory and computation\r
per time step. In particular, extensive memory may be required to build up accurate\r
approximations of value functions, policies, and models. In most cases of practical interest\r
there are far more states than could possibly be entries in a table, and approximations\r
must be made.\r
A well-defined notion of optimality organizes the approach to learning we describe in\r
this book and provides a way to understand the theoretical properties of various learning\r
algorithms, but it is an ideal that reinforcement learning agents can only approximate\r
to varying degrees. In reinforcement learning we are very much concerned with cases in\r
which optimal solutions cannot be found but must be approximated in some way.\r
Bibliographical and Historical Remarks\r
The reinforcement learning problem is deeply indebted to the idea of Markov decision\r
processes (MDPs) from the field of optimal control. These historical influences and other\r
major influences from psychology are described in the brief history given in Chapter 1.\r
Reinforcement learning adds to MDPs a focus on approximation and incomplete infor\u0002mation for realistically large problems. MDPs and the reinforcement learning problem\r
are only weakly linked to traditional learning and decision-making problems in artificial\r
intelligence. However, artificial intelligence is now vigorously exploring MDP formulations\r
for planning and decision making from a variety of perspectives. MDPs are more general\r
than previous formulations used in artificial intelligence in that they permit more general\r
kinds of goals and uncertainty.\r
The theory of MDPs is treated by, for example, Bertsekas (2005), White (1969), Whittle\r
(1982, 1983), and Puterman (1994). A particularly compact treatment of the finite case\r
is given by Ross (1983). MDPs are also studied under the heading of stochastic optimal\r
control, where adaptive optimal control methods are most closely related to reinforcement\r
learning (e.g., Kumar, 1985; Kumar and Varaiya, 1986).\r
The theory of MDPs evolved from e↵orts to understand the problem of making sequences\r
of decisions under uncertainty, where each decision can depend on the previous decisions\r
and their outcomes. It is sometimes called the theory of multistage decision processes,\r
or sequential decision processes, and has roots in the statistical literature on sequential\r
sampling beginning with the papers by Thompson (1933, 1934) and Robbins (1952) that\r
we cited in Chapter 2 in connection with bandit problems (which are prototypical MDPs\r
if formulated as multiple-situation problems).\r
The earliest instance (that we are aware of) in which reinforcement learning was\r
discussed using the MDP formalism is Andreae’s (1969) description of a unified view of"""

[[sections]]
number = "70"
title = "Chapter 3: Finite Markov Decision Processes"
text = """
learning machines. Witten and Corbin (1973) experimented with a reinforcement learning\r
system later analyzed by Witten (1977, 1976a) using the MDP formalism. Although\r
he did not explicitly mention MDPs, Werbos (1977) suggested approximate solution\r
methods for stochastic optimal control problems that are related to modern reinforcement\r
learning methods (see also Werbos, 1982, 1987, 1988, 1989, 1992). Although Werbos’s\r
ideas were not widely recognized at the time, they were prescient in emphasizing the\r
importance of approximately solving optimal control problems in a variety of domains,\r
including artificial intelligence. The most influential integration of reinforcement learning\r
and MDPs is due to Watkins (1989)."""

[[sections]]
number = "3.1"
title = "Our characterization of the dynamics of an MDP in terms of p(s0, r|s, a) is"
text = """
slightly unusual. It is more common in the MDP literature to describe the\r
dynamics in terms of the state transition probabilities p(s0 |s, a) and expected\r
next rewards r(s, a). In reinforcement learning, however, we more often have\r
to refer to individual actual or sample rewards (rather than just their expected\r
values). Our notation also makes it plainer that St and Rt are in general jointly\r
determined, and thus must have the same time index. In teaching reinforcement\r
learning, we have found our notation to be more straightforward conceptually\r
and easier to understand.\r
For a good intuitive discussion of the system-theoretic concept of state, see\r
Minsky (1967).\r
The bioreactor example is based on the work of Ungar (1990) and Miller and\r
Williams (1992). The recycling robot example was inspired by the can-collecting\r
robot built by Jonathan Connell (1989). Kober and Peters (2012) present a\r
collection of robotics applications of reinforcement learning."""

[[sections]]
number = "3.2"
title = "An explicit statement of the reward hypothesis was suggested by Michael Littman"
text = """
(personal communication).\r
3.3–4 The terminology of episodic and continuing tasks is di↵erent from that usually\r
used in the MDP literature. In that literature it is common to distinguish\r
three types of tasks: (1) finite-horizon tasks, in which interaction terminates\r
after a particular fixed number of time steps; (2) indefinite-horizon tasks, in\r
which interaction can last arbitrarily long but must eventually terminate; and\r
(3) infinite-horizon tasks, in which interaction does not terminate. Our episodic\r
and continuing tasks are similar to indefinite-horizon and infinite-horizon tasks,\r
respectively, but we prefer to emphasize the di↵erence in the nature of the\r
interaction. This di↵erence seems more fundamental than the di↵erence in the\r
objective functions emphasized by the usual terms. Often episodic tasks use\r
an indefinite-horizon objective function and continuing tasks an infinite-horizon\r
objective function, but we see this as a common coincidence rather than a\r
fundamental di↵erence.\r
The pole-balancing example is from Michie and Chambers (1968) and Barto,\r
Sutton, and Anderson (1983)."""

[[sections]]
number = "3.8"
title = "Summary 71"
text = """
3.5–6 Assigning value on the basis of what is good or bad in the long run has ancient\r
roots. In control theory, mapping states to numerical values representing the\r
long-term consequences of control decisions is a key part of optimal control theory,\r
which was developed in the 1950s by extending nineteenth century state-function\r
theories of classical mechanics (see, for example, Schultz and Melsa, 1967). In\r
describing how a computer could be programmed to play chess, Shannon (1950)\r
suggested using an evaluation function that took into account the long-term\r
advantages and disadvantages of chess positions.\r
Watkins’s (1989) Q-learning algorithm for estimating q⇤ (Chapter 6) made action\u0002value functions an important part of reinforcement learning, and consequently\r
these functions are often called “Q-functions.” But the idea of an action-value\r
function is much older than this. Shannon (1950) suggested that a function\r
h(P,M) could be used by a chess-playing program to decide whether a move M\r
in position P is worth exploring. Michie’s (1961, 1963) MENACE system and\r
Michie and Chambers’s (1968) BOXES system can be understood as estimating\r
action-value functions. In classical physics, Hamilton’s principal function is\r
an action-value function; Newtonian dynamics are greedy with respect to this\r
function (e.g., Goldstein, 1957). Action-value functions also played a central role\r
in Denardo’s (1967) theoretical treatment of dynamic programming in terms of\r
contraction mappings.\r
The Bellman optimality equation (for v⇤) was popularized by Richard Bellman\r
(1957a), who called it the “basic functional equation.” The counterpart of the\r
Bellman optimality equation for continuous time and state problems is known\r
as the Hamilton–Jacobi–Bellman equation (or often just the Hamilton–Jacobi\r
equation), indicating its roots in classical physics (e.g., Schultz and Melsa, 1967).\r
The golf example was suggested by Chris Watkins.

Chapter 4\r
Dynamic Programming\r
The term dynamic programming (DP) refers to a collection of algorithms that can be\r
used to compute optimal policies given a perfect model of the environment as a Markov\r
decision process (MDP). Classical DP algorithms are of limited utility in reinforcement\r
learning both because of their assumption of a perfect model and because of their great\r
computational expense, but they are still important theoretically. DP provides an essential\r
foundation for the understanding of the methods presented in the rest of this book. In\r
fact, all of these methods can be viewed as attempts to achieve much the same e↵ect as\r
DP, only with less computation and without assuming a perfect model of the environment.\r
Starting with this chapter, we usually assume that the environment is a finite MDP.\r
That is, we assume that its state, action, and reward sets, S, A, and R, are finite, and\r
that its dynamics are given by a set of probabilities p(s0, r|s, a), for all s 2 S, a 2 A(s),\r
r 2 R, and s0 2 S+ (S+ is S plus a terminal state if the problem is episodic). Although\r
DP ideas can be applied to problems with continuous state and action spaces, exact\r
solutions are possible only in special cases. A common way of obtaining approximate\r
solutions for tasks with continuous states and actions is to quantize the state and action\r
spaces and then apply finite-state DP methods. The methods we explore in Part II are\r
applicable to continuous problems and are a significant extension of that approach.\r
The key idea of DP, and of reinforcement learning generally, is the use of value functions\r
to organize and structure the search for good policies. In this chapter we show how DP\r
can be used to compute the value functions defined in Chapter 3. As discussed there, we\r
can easily obtain optimal policies once we have found the optimal value functions, v⇤ or\r
q⇤, which satisfy the Bellman optimality equations:\r
v⇤(s) = maxa E[Rt+1 + v⇤(St+1) | St =s, At =a]\r
= maxa\r
X\r
s0,r\r
p(s0, r|s, a)\r
h\r
r + v⇤(s0)\r
i\r
, (4.1)\r
or\r
q⇤(s, a) = E\r
h\r
Rt+1 +  max\r
a0 q⇤(St+1, a0\r
)\r
\r
\r
\r
St =s, At =a\r
i\r
= X\r
s0,r\r
p(s0, r|s, a)\r
h\r
r +  max\r
a0 q⇤(s0\r
, a0)\r
i\r
, (4.2)"""

[[sections]]
number = "74"
title = "Chapter 4: Dynamic Programming"
text = """
for all s 2 S, a 2 A(s), and s0 2 S+. As we shall see, DP algorithms are obtained by\r
turning Bellman equations such as these into assignments, that is, into update rules for\r
improving approximations of the desired value functions."""

[[sections]]
number = "4.1"
title = "Policy Evaluation (Prediction)"
text = """
First we consider how to compute the state-value function v⇡ for an arbitrary policy ⇡.\r
This is called policy evaluation in the DP literature. We also refer to it as the prediction\r
problem. Recall from Chapter 3 that, for all s 2 S,\r
v⇡(s) .= E⇡[Gt | St =s]\r
= E⇡[Rt+1 + Gt+1 | St =s] (from (3.9))\r
= E⇡[Rt+1 + v⇡(St+1) | St =s] (4.3)\r
= X\r
a\r
⇡(a|s)\r
X\r
s0,r\r
p(s0, r|s, a)\r
h\r
r + v⇡(s0)\r
i\r
, (4.4)\r
where ⇡(a|s) is the probability of taking action a in state s under policy ⇡, and the\r
expectations are subscripted by ⇡ to indicate that they are conditional on ⇡ being followed.\r
The existence and uniqueness of v⇡ are guaranteed as long as either  < 1 or eventual\r
termination is guaranteed from all states under the policy ⇡.\r
If the environment’s dynamics are completely known, then (4.4) is a system of |S|\r
simultaneous linear equations in |S| unknowns (the v⇡(s), s 2 S). In principle, its solution\r
is a straightforward, if tedious, computation. For our purposes, iterative solution methods\r
are most suitable. Consider a sequence of approximate value functions v0, v1, v2,...,\r
each mapping S+ to R (the real numbers). The initial approximation, v0, is chosen\r
arbitrarily (except that the terminal state, if any, must be given value 0), and each\r
successive approximation is obtained by using the Bellman equation for v⇡ (4.4) as an\r
update rule:\r
vk+1(s) .= E⇡[Rt+1 + vk(St+1) | St =s]\r
= X\r
a\r
⇡(a|s)\r
X\r
s0,r\r
p(s0, r|s, a)\r
h\r
r + vk(s0)\r
i\r
, (4.5)\r
for all s 2 S. Clearly, vk = v⇡ is a fixed point for this update rule because the Bellman\r
equation for v⇡ assures us of equality in this case. Indeed, the sequence {vk} can be\r
shown in general to converge to v⇡ as k ! 1 under the same conditions that guarantee\r
the existence of v⇡. This algorithm is called iterative policy evaluation.\r
To produce each successive approximation, vk+1 from vk, iterative policy evaluation\r
applies the same operation to each state s: it replaces the old value of s with a new value\r
obtained from the old values of the successor states of s, and the expected immediate\r
rewards, along all the one-step transitions possible under the policy being evaluated. We\r
call this kind of operation an expected update. Each iteration of iterative policy evaluation\r
updates the value of every state once to produce the new approximate value function"""

[[sections]]
number = "4.1"
title = "Policy Evaluation (Prediction) 75"
text = """
vk+1. There are several di↵erent kinds of expected updates, depending on whether a\r
state (as here) or a state–action pair is being updated, and depending on the precise way\r
the estimated values of the successor states are combined. All the updates done in DP\r
algorithms are called expected updates because they are based on an expectation over all\r
possible next states rather than on a sample next state. The nature of an update can\r
be expressed in an equation, as above, or in a backup diagram like those introduced in\r
Chapter 3. For example, the backup diagram corresponding to the expected update used\r
in iterative policy evaluation is shown on page 59.\r
To write a sequential computer program to implement iterative policy evaluation as\r
given by (4.5) you would have to use two arrays, one for the old values, vk(s), and one\r
for the new values, vk+1(s). With two arrays, the new values can be computed one by\r
one from the old values without the old values being changed. Alternatively, you could\r
use one array and update the values “in place,” that is, with each new value immediately\r
overwriting the old one. Then, depending on the order in which the states are updated,\r
sometimes new values are used instead of old ones on the right-hand side of (4.5). This\r
in-place algorithm also converges to v⇡; in fact, it usually converges faster than the\r
two-array version, as you might expect, because it uses new data as soon as they are\r
available. We think of the updates as being done in a sweep through the state space. For\r
the in-place algorithm, the order in which states have their values updated during the\r
sweep has a significant influence on the rate of convergence. We usually have the in-place\r
version in mind when we think of DP algorithms.\r
A complete in-place version of iterative policy evaluation is shown in pseudocode in\r
the box below. Note how it handles termination. Formally, iterative policy evaluation\r
converges only in the limit, but in practice it must be halted short of this. The pseudocode\r
tests the quantity maxs2S |vk+1(s)vk(s)| after each sweep and stops when it is suciently\r
small.\r
Iterative Policy Evaluation, for estimating V ⇡ v⇡\r
Input ⇡, the policy to be evaluated\r
Algorithm parameter: a small threshold ✓ > 0 determining accuracy of estimation\r
Initialize V (s) arbitrarily, for s 2 S, and V (terminal) to 0\r
Loop:\r
 0\r
Loop for each s 2 S:\r
v V (s)\r
V (s) P\r
a ⇡(a|s)\r
P\r
s0,r p(s0\r
, r|s, a)\r
⇥\r
r + V (s0)\r
⇤\r
 max(, |v  V (s)|)\r
until  < ✓"""

[[sections]]
number = "76"
title = "Chapter 4: Dynamic Programming"
text = """
Example 4.1 Consider the 4⇥4 gridworld shown below.\r
actions\r
r = !1\r
on all transitions\r
1 2 3\r
4 5 6 7\r
8 9 10 11\r
12 13 14\r
Rt = 1\r
The nonterminal states are S = {1, 2,..., 14}. There are four actions possible in each\r
state, A = {up, down, right, left}, which deterministically cause the corresponding\r
state transitions, except that actions that would take the agent o↵ the grid in fact leave\r
the state unchanged. Thus, for instance, p(6, 1|5, right) = 1, p(7, 1|7, right) = 1,\r
and p(10, r|5, right) = 0 for all r 2 R. This is an undiscounted, episodic task. The\r
reward is 1 on all transitions until the terminal state is reached. The terminal state is\r
shaded in the figure (although it is shown in two places, it is formally one state). The\r
expected reward function is thus r(s, a, s0) = 1 for all states s, s0 and actions a. Suppose\r
the agent follows the equiprobable random policy (all actions equally likely). The left side\r
of Figure 4.1 shows the sequence of value functions {vk} computed by iterative policy\r
evaluation. The final estimate is in fact v⇡, which in this case gives for each state the\r
negation of the expected number of steps from that state until termination.\r
Exercise 4.1 In Example 4.1, if ⇡ is the equiprobable random policy, what is q⇡(11, down)?\r
What is q⇡(7, down)? ⇤\r
Exercise 4.2 In Example 4.1, suppose a new state 15 is added to the gridworld just below\r
state 13, and its actions, left, up, right, and down, take the agent to states 12, 13, 14,\r
and 15, respectively. Assume that the transitions from the original states are unchanged.\r
What, then, is v⇡(15) for the equiprobable random policy? Now suppose the dynamics of\r
state 13 are also changed, such that action down from state 13 takes the agent to the new\r
state 15. What is v⇡(15) for the equiprobable random policy in this case? ⇤\r
Exercise 4.3 What are the equations analogous to (4.3), (4.4), and (4.5), but for action\u0002value functions instead of state-value functions? ⇤"""

[[sections]]
number = "4.2"
title = "Policy Improvement"
text = """
Our reason for computing the value function for a policy is to help find better policies.\r
Suppose we have determined the value function v⇡ for an arbitrary deterministic policy\r
⇡. For some state s we would like to know whether or not we should change the policy\r
to deterministically choose an action a 6= ⇡(s). We know how good it is to follow the\r
current policy from s—that is v⇡(s)—but would it be better or worse to change to the\r
new policy? One way to answer this question is to consider selecting a in s and thereafter"""

[[sections]]
number = "4.2"
title = "Policy Improvement 77"
text = """
0.0 0.0 0.0\r
0.0 0.0 0.0 0.0\r
0.0 0.0 0.0 0.0\r
0.0 0.0 0.0\r
-1.0 -1.0 -1.0\r
-1.0 -1.0 -1.0 -1.0\r
-1.0 -1.0 -1.0 -1.0\r
-1.0 -1.0 -1.0\r
-1.7 -2.0 -2.0\r
-1.7 -2.0 -2.0 -2.0\r
-2.0 -2.0 -2.0 -1.7\r
-2.0 -2.0 -1.7\r
-2.4 -2.9 -3.0\r
-2.4 -2.9 -3.0 -2.9\r
-2.9 -3.0 -2.9 -2.4\r
-3.0 -2.9 -2.4\r
-6.1 -8.4 -9.0\r
-6.1 -7.7 -8.4 -8.4\r
-8.4 -8.4 -7.7 -6.1\r
-9.0 -8.4 -6.1\r
-14. -20. -22.\r
-14. -18. -20. -20.\r
-20. -20. -18. -14.\r
-22. -20. -14.\r
Vk for the\r
Random Policy\r
Greedy Policy\r
w.r.t. Vk\r
k = 0\r
k = 1\r
k = 2\r
k = 10\r
k = !\r
k = 3\r
optimal \r
policy\r
random\r
policy\r
0.0\r
0.0\r
0.0\r
0.0\r
0.0\r
0.0\r
0.0\r
0.0\r
0.0\r
0.0\r
0.0\r
0.0\r
vk\r
 for the\r
random policy\r
vk greedy policy\r
 w.r.t. vk\r
Figure 4.1: Convergence of iterative policy evaluation on a small gridworld. The left column is\r
the sequence of approximations of the state-value function for the random policy (all actions\r
equally likely). The right column is the sequence of greedy policies corresponding to the value\r
function estimates (arrows are shown for all actions achieving the maximum, and the numbers\r
shown are rounded to two significant digits). The last policy is guaranteed only to be an\r
improvement over the random policy, but in this case it, and all policies after the third iteration,\r
are optimal."""

[[sections]]
number = "78"
title = "Chapter 4: Dynamic Programming"
text = """
following the existing policy, ⇡. The value of this way of behaving is\r
q⇡(s, a) .= E[Rt+1 + v⇡(St+1) | St =s, At =a] (4.6)\r
= X\r
s0,r\r
p(s0, r|s, a)\r
h\r
r + v⇡(s0)\r
i\r
.\r
The key criterion is whether this is greater than or less than v⇡(s). If it is greater—that\r
is, if it is better to select a once in s and thereafter follow ⇡ than it would be to follow\r
⇡ all the time—then one would expect it to be better still to select a every time s is\r
encountered, and that the new policy would in fact be a better one overall.\r
That this is true is a special case of a general result called the policy improvement\r
theorem. Let ⇡ and ⇡0 be any pair of deterministic policies such that, for all s 2 S,\r
q⇡(s, ⇡0(s))  v⇡(s). (4.7)\r
Then the policy ⇡0 must be as good as, or better than, ⇡. That is, it must obtain greater\r
or equal expected return from all states s 2 S:\r
v⇡0 (s)  v⇡(s). (4.8)\r
Moreover, if there is strict inequality of (4.7) at any state, then there must be strict\r
inequality of (4.8) at that state.\r
The policy improvement theorem applies to the two policies that we considered at the\r
beginning of this section: an original deterministic policy, ⇡, and a changed policy, ⇡0,\r
that is identical to ⇡ except that ⇡0(s) = a 6= ⇡(s). For states other than s, (4.7) holds\r
because the two sides are equal. Thus, if q⇡(s, a) > v⇡(s), then the changed policy is\r
indeed better than ⇡.\r
The idea behind the proof of the policy improvement theorem is easy to understand.\r
Starting from (4.7), we keep expanding the q⇡ side with (4.6) and reapplying (4.7) until\r
we get v⇡0 (s):\r
v⇡(s)  q⇡(s, ⇡0(s))\r
= E[Rt+1 + v⇡(St+1) | St =s, At =⇡0(s)] (by (4.6))\r
= E⇡0[Rt+1 + v⇡(St+1) | St =s]\r
 E⇡0[Rt+1 + q⇡(St+1, ⇡0(St+1)) | St =s] (by (4.7))\r
= E⇡0[Rt+1 + E[Rt+2 + v⇡(St+2)|St+1, At+1 =⇡0(St+1)] | St =s]\r
= E⇡0\r
⇥\r
Rt+1 + Rt+2 + 2v⇡(St+2)\r
\r
 St =s\r
⇤\r
 E⇡0\r
⇥\r
Rt+1 + Rt+2 + 2Rt+3 + 3v⇡(St+3)\r
\r
 St =s\r
⇤\r
.\r
.\r
.\r
 E⇡0\r
⇥\r
Rt+1 + Rt+2 + 2Rt+3 + 3Rt+4 + ··· \r
 St =s\r
⇤\r
= v⇡0 (s).\r
So far we have seen how, given a policy and its value function, we can easily evaluate\r
a change in the policy at a single state. It is a natural extension to consider changes at"""

[[sections]]
number = "4.2"
title = "Policy Improvement 79"
text = """
all states, selecting at each state the action that appears best according to q⇡(s, a). In\r
other words, to consider the new greedy policy, ⇡0, given by\r
⇡0(s) .= argmax aq⇡(s, a)\r
= argmax aE[Rt+1 + v⇡(St+1) | St =s, At =a] (4.9)\r
= argmax a\r
X\r
s0,r\r
p(s0, r|s, a)\r
h\r
r + v⇡(s0)\r
i\r
,\r
where argmaxa denotes the value of a at which the expression that follows is maximized\r
(with ties broken arbitrarily). The greedy policy takes the action that looks best in the\r
short term—after one step of lookahead—according to v⇡. By construction, the greedy\r
policy meets the conditions of the policy improvement theorem (4.7), so we know that it\r
is as good as, or better than, the original policy. The process of making a new policy that\r
improves on an original policy, by making it greedy with respect to the value function of\r
the original policy, is called policy improvement.\r
Suppose the new greedy policy, ⇡0, is as good as, but not better than, the old policy ⇡.\r
Then v⇡ = v⇡0 , and from (4.9) it follows that for all s 2 S:\r
v⇡0 (s) = maxa E[Rt+1 + v⇡0 (St+1) | St =s, At =a]\r
= maxa\r
X\r
s0,r\r
p(s0, r|s, a)\r
h\r
r + v⇡0 (s0)\r
i\r
.\r
But this is the same as the Bellman optimality equation (4.1), and therefore, v⇡0 must be\r
v⇤, and both ⇡ and ⇡0 must be optimal policies. Policy improvement thus must give us a\r
strictly better policy except when the original policy is already optimal.\r
So far in this section we have considered the special case of deterministic policies.\r
In the general case, a stochastic policy ⇡ specifies probabilities, ⇡(a|s), for taking each\r
action, a, in each state, s. We will not go through the details, but in fact all the ideas of\r
this section extend easily to stochastic policies. In particular, the policy improvement\r
theorem carries through as stated for the stochastic case. In addition, if there are ties in\r
policy improvement steps such as (4.9)—that is, if there are several actions at which the\r
maximum is achieved—then in the stochastic case we need not select a single action from\r
among them. Instead, each maximizing action can be given a portion of the probability\r
of being selected in the new greedy policy. Any apportioning scheme is allowed as long\r
as all submaximal actions are given zero probability.\r
The last row of Figure 4.1 shows an example of policy improvement for stochastic\r
policies. Here the original policy, ⇡, is the equiprobable random policy, and the new\r
policy, ⇡0, is greedy with respect to v⇡. The value function v⇡ is shown in the bottom-left\r
diagram and the set of possible ⇡0 is shown in the bottom-right diagram. The states\r
with multiple arrows in the ⇡0 diagram are those in which several actions achieve the\r
maximum in (4.9); any apportionment of probability among these actions is permitted.\r
For any such policy, its state values v⇡0 (s) can be seen by inspection to be either 1, 2,\r
or 3, for all states s 2 S, whereas v⇡(s) is at most 14. Thus, v⇡0 (s)  v⇡(s), for all"""

[[sections]]
number = "80"
title = "Chapter 4: Dynamic Programming"
text = """
s 2 S, illustrating policy improvement. Although in this case the new policy ⇡0 happens\r
to be optimal, in general only an improvement is guaranteed."""

[[sections]]
number = "4.3"
title = "Policy Iteration"
text = """
Once a policy, ⇡, has been improved using v⇡ to yield a better policy, ⇡0, we can then\r
compute v⇡0 and improve it again to yield an even better ⇡00. We can thus obtain a\r
sequence of monotonically improving policies and value functions:\r
⇡0\r
E\r
! v⇡0\r
I\r
! ⇡1\r
E\r
! v⇡1\r
I\r
! ⇡2\r
E\r
! · · · I! ⇡⇤\r
E\r
! v⇤,\r
where E! denotes a policy evaluation and I! denotes a policy improvement. Each\r
policy is guaranteed to be a strict improvement over the previous one (unless it is already\r
optimal). Because a finite MDP has only a finite number of deterministic policies, this\r
process must converge to an optimal policy and the optimal value function in a finite\r
number of iterations.\r
This way of finding an optimal policy is called policy iteration. A complete algorithm is\r
given in the box below. Note that each policy evaluation, itself an iterative computation,\r
is started with the value function for the previous policy. This typically results in a great\r
increase in the speed of convergence of policy evaluation (presumably because the value\r
function changes little from one policy to the next).\r
Policy Iteration (using iterative policy evaluation) for estimating ⇡ ⇡ ⇡⇤"""

[[sections]]
number = "1"
title = "Initialization"
text = "V (s) 2 R and ⇡(s) 2 A(s) arbitrarily for all s 2 S; V (terminal) .= 0"

[[sections]]
number = "2"
title = "Policy Evaluation"
text = """
Loop:\r
 0\r
Loop for each s 2 S:\r
v V (s)\r
V (s) P\r
s0,r p(s0\r
, r|s, ⇡(s))⇥r + V (s0)\r
⇤\r
 max(, |v  V (s)|)\r
until  < ✓ (a small positive number determining the accuracy of estimation)"""

[[sections]]
number = "3"
title = "Policy Improvement"
text = """
policy-stable true\r
For each s 2 S:\r
old-action ⇡(s)\r
⇡(s) argmaxa\r
P\r
s0,r p(s0\r
, r|s, a)\r
⇥\r
r + V (s0)\r
⇤\r
If old-action 6= ⇡(s), then policy-stable f alse\r
If policy-stable, then stop and return V ⇡ v⇤ and ⇡ ⇡ ⇡⇤; else go to 2"""

[[sections]]
number = "4.3"
title = "Policy Iteration 81"
text = """
Example 4.2: Jack’s Car Rental Jack manages two locations for a nationwide car\r
rental company. Each day, some number of customers arrive at each location to rent cars.\r
If Jack has a car available, he rents it out and is credited $10 by the national company.\r
If he is out of cars at that location, then the business is lost. Cars become available for\r
renting the day after they are returned. To help ensure that cars are available where\r
they are needed, Jack can move them between the two locations overnight, at a cost of\r
$2 per car moved. We assume that the number of cars requested and returned at each\r
location are Poisson random variables, meaning that the probability that the number is\r
n is n\r
n! e, where  is the expected number. Suppose  is 3 and 4 for rental requests at\r
the first and second locations and 3 and 2 for returns. To simplify the problem slightly,\r
we assume that there can be no more than 20 cars at each location (any additional cars\r
are returned to the nationwide company, and thus disappear from the problem) and a\r
maximum of five cars can be moved from one location to the other in one night. We take\r
the discount rate to be  = 0.9 and formulate this as a continuing finite MDP, where\r
the time steps are days, the state is the number of cars at each location at the end of\r
the day, and the actions are the net numbers of cars moved between the two locations\r
overnight. Figure 4.2 shows the sequence of policies found by policy iteration starting\r
from the policy that never moves any cars.\r
V4\r
612\r
#Cars at second location\r
0 420\r
20 0\r
20\r
#Cars at first location\r
1\r
1\r
5\r
\u00021\r
\u00022\r
-4\r
4 3 2\r
4 3 2\r
\u00023\r
0\r
0\r
5\r
\u00021\r
\u00022\r
\u00023 \u00024\r
1 2 3 4\r
0\r
\u00010 \u00011 \u00012\r
\u00023 \u00024\r
\u00022\r
0\r
1 2\r
3 4\r
\u00021\r
\u00013\r
2\r
\u00023 \u00024\r
\u00022\r
0\r
1\r
3 4\r
5\r
\u00021\r
\u00014\r
#Cars at second location\r
#Cars at first location\r
5\r
0 20\r
0 20\r
v⇡4\r
⇡0 ⇡1 ⇡2\r
⇡3 ⇡4\r
Figure 4.2: The sequence of policies found by policy iteration on Jack’s car rental problem,\r
and the final state-value function. The first five diagrams show, for each number of cars at\r
each location at the end of the day, the number of cars to be moved from the first location to\r
the second (negative numbers indicate transfers from the second location to the first). Each\r
successive policy is a strict improvement over the previous policy, and the last policy is optimal."""

[[sections]]
number = "82"
title = "Chapter 4: Dynamic Programming"
text = """
Policy iteration often converges in surprisingly few iterations, as illustrated in the\r
example of Jack’s car rental and in the example in Figure 4.1. The bottom-left diagram of\r
Figure 4.1 shows the value function for the equiprobable random policy, and the bottom\u0002right diagram shows a greedy policy for this value function. The policy improvement\r
theorem assures us that these policies are better than the original random policy. In this\r
case, however, these policies are not just better, but optimal, proceeding to the terminal\r
states in the minimum number of steps. In this example, policy iteration would find the\r
optimal policy after just one iteration.\r
Exercise 4.4 The policy iteration algorithm on page 80 has a subtle bug in that it may\r
never terminate if the policy continually switches between two or more policies that are\r
equally good. This is okay for pedagogy, but not for actual use. Modify the pseudocode\r
so that convergence is guaranteed. ⇤\r
Exercise 4.5 How would policy iteration be defined for action values? Give a complete\r
algorithm for computing q⇤, analogous to that on page 80 for computing v⇤. Please pay\r
special attention to this exercise, because the ideas involved will be used throughout the\r
rest of the book. ⇤\r
Exercise 4.6 Suppose you are restricted to considering only policies that are "-soft,\r
meaning that the probability of selecting each action in each state, s, is at least "/|A(s)|.\r
Describe qualitatively the changes that would be required in each of the steps 3, 2, and 1,\r
in that order, of the policy iteration algorithm for v⇤ on page 80. ⇤\r
Exercise 4.7 (programming) Write a program for policy iteration and re-solve Jack’s car\r
rental problem with the following changes. One of Jack’s employees at the first location\r
rides a bus home each night and lives near the second location. She is happy to shuttle\r
one car to the second location for free. Each additional car still costs $2, as do all cars\r
moved in the other direction. In addition, Jack has limited parking space at each location.\r
If more than 10 cars are kept overnight at a location (after any moving of cars), then an\r
additional cost of $4 must be incurred to use a second parking lot (independent of how\r
many cars are kept there). These sorts of nonlinearities and arbitrary dynamics often\r
occur in real problems and cannot easily be handled by optimization methods other than\r
dynamic programming. To check your program, first replicate the results given for the\r
original problem. ⇤"""

[[sections]]
number = "4.4"
title = "Value Iteration"
text = """
One drawback to policy iteration is that each of its iterations involves policy evaluation,\r
which may itself be a protracted iterative computation requiring multiple sweeps through\r
the state set. If policy evaluation is done iteratively, then convergence exactly to v⇡\r
occurs only in the limit. Must we wait for exact convergence, or can we stop short of\r
that? The example in Figure 4.1 certainly suggests that it may be possible to truncate\r
policy evaluation. In that example, policy evaluation iterations beyond the first three\r
have no e↵ect on the corresponding greedy policy.\r
In fact, the policy evaluation step of policy iteration can be truncated in several ways\r
without losing the convergence guarantees of policy iteration. One important special"""

[[sections]]
number = "4.4"
title = "Value Iteration 83"
text = """
case is when policy evaluation is stopped after just one sweep (one update of each state).\r
This algorithm is called value iteration. It can be written as a particularly simple update\r
operation that combines the policy improvement and truncated policy evaluation steps:\r
vk+1(s) .= maxa E[Rt+1 + vk(St+1) | St =s, At =a]\r
= maxa\r
X\r
s0,r\r
p(s0, r|s, a)\r
h\r
r + vk(s0)\r
i\r
, (4.10)\r
for all s 2 S. For arbitrary v0, the sequence {vk} can be shown to converge to v⇤ under\r
the same conditions that guarantee the existence of v⇤.\r
Another way of understanding value iteration is by reference to the Bellman optimality\r
equation (4.1). Note that value iteration is obtained simply by turning the Bellman\r
optimality equation into an update rule. Also note how the value iteration update is\r
identical to the policy evaluation update (4.5) except that it requires the maximum to be\r
taken over all actions. Another way of seeing this close relationship is to compare the\r
backup diagrams for these algorithms on page 59 (policy evaluation) and on the left of\r
Figure 3.4 (value iteration). These two are the natural backup operations for computing\r
v⇡ and v⇤.\r
Finally, let us consider how value iteration terminates. Like policy evaluation, value\r
iteration formally requires an infinite number of iterations to converge exactly to v⇤. In\r
practice, we stop once the value function changes by only a small amount in a sweep.\r
The box below shows a complete algorithm with this kind of termination condition.\r
Value Iteration, for estimating ⇡ ⇡ ⇡⇤\r
Algorithm parameter: a small threshold ✓ > 0 determining accuracy of estimation\r
Initialize V (s), for all s 2 S+, arbitrarily except that V (terminal)=0\r
Loop:\r
|  0\r
| Loop for each s 2 S:\r
| v V (s)\r
| V (s) maxa\r
P\r
s0,r p(s0\r
, r|s, a)\r
⇥\r
r + V (s0)\r
⇤\r
|  max(, |v  V (s)|)\r
until  < ✓\r
Output a deterministic policy, ⇡ ⇡ ⇡⇤, such that\r
⇡(s) = argmaxa\r
P\r
s0,r p(s0\r
, r|s, a)\r
⇥\r
r + V (s0)\r
⇤\r
Value iteration e↵ectively combines, in each of its sweeps, one sweep of policy evaluation\r
and one sweep of policy improvement. Faster convergence is often achieved by interposing\r
multiple policy evaluation sweeps between each policy improvement sweep. In general,\r
the entire class of truncated policy iteration algorithms can be thought of as sequences\r
of sweeps, some of which use policy evaluation updates and some of which use value\r
iteration updates. Because the max operation in (4.10) is the only di↵erence between"""

[[sections]]
number = "84"
title = "Chapter 4: Dynamic Programming"
text = """
these updates, this just means that the max operation is added to some sweeps of policy\r
evaluation. All of these algorithms converge to an optimal policy for discounted finite\r
MDPs.\r
Example 4.3: Gambler’s Problem A gambler has the opportunity to make bets on the\r
outcomes of a sequence of coin flips. If the coin comes up heads, he wins as many dollars as\r
he has staked on that flip; if it is tails, he loses his stake. The game ends when the gambler\r
wins by reaching his goal of $100, or loses by running out of money. On each flip, the gam\u0002bler must decide what portion of his capital to stake, in integer numbers of dollars. This\r
problem can be formulated as an undiscounted, episodic, finite MDP. The state is the gam\u0002bler’s capital, s 2 {1, 2,..., 99} and the actions are stakes, a 2 {0, 1,..., min(s, 100  s)}.\r
1 25 50 75 99\r
1\r
10\r
20\r
30\r
40\r
50\r
1\r
0\r
0.2\r
0.4\r
0.6\r
0.8\r
1\r
25 50 75 99\r
Capital\r
Capital\r
Value\r
estimates\r
Final\r
policy\r
(stake)\r
sweep 1\r
sweep 2\r
sweep 3\r
sweep 32\r
Final value\r
function\r
Figure 4.3: The solution to the gambler’s problem for\r
ph = 0.4. The upper graph shows the value function\r
found by successive sweeps of value iteration. The\r
lower graph shows the final policy.\r
The reward is zero on all transi\u0002tions except those on which the gam\u0002bler reaches his goal, when it is +1.\r
The state-value function then gives\r
the probability of winning from each\r
state. A policy is a mapping from\r
levels of capital to stakes. The opti\u0002mal policy maximizes the probability\r
of reaching the goal. Let ph denote\r
the probability of the coin coming\r
up heads. If ph is known, then the\r
entire problem is known and it can\r
be solved, for instance, by value iter\u0002ation. Figure 4.3 shows the change\r
in the value function over successive\r
sweeps of value iteration, and the\r
final policy found, for the case of\r
ph = 0.4. This policy is optimal, but\r
not unique. In fact, there is a whole\r
family of optimal policies, all corre\u0002sponding to ties for the argmax ac\u0002tion selection with respect to the op\u0002timal value function. Can you guess\r
what the entire family looks like?\r
Exercise 4.8 Why does the optimal\r
policy for the gambler’s problem have such a curious form? In particular, for capital of 50\r
it bets it all on one flip, but for capital of 51 it does not. Why is this a good policy? ⇤\r
Exercise 4.9 (programming) Implement value iteration for the gambler’s problem and\r
solve it for ph = 0.25 and ph = 0.55. In programming, you may find it convenient to\r
introduce two dummy states corresponding to termination with capital of 0 and 100,\r
giving them values of 0 and 1 respectively. Show your results graphically, as in Figure 4.3.\r
Are your results stable as ✓ ! 0? ⇤\r
Exercise 4.10 What is the analog of the value iteration update (4.10) for action values,\r
qk+1(s, a)? ⇤"""

[[sections]]
number = "4.5"
title = "Asynchronous Dynamic Programming 85"
text = ""

[[sections]]
number = "4.5"
title = "Asynchronous Dynamic Programming"
text = """
A major drawback to the DP methods that we have discussed so far is that they involve\r
operations over the entire state set of the MDP, that is, they require sweeps of the state\r
set. If the state set is very large, then even a single sweep can be prohibitively expensive.\r
For example, the game of backgammon has over 1020 states. Even if we could perform\r
the value iteration update on a million states per second, it would take over a thousand\r
years to complete a single sweep.\r
Asynchronous DP algorithms are in-place iterative DP algorithms that are not organized\r
in terms of systematic sweeps of the state set. These algorithms update the values of\r
states in any order whatsoever, using whatever values of other states happen to be\r
available. The values of some states may be updated several times before the values of\r
others are updated once. To converge correctly, however, an asynchronous algorithm\r
must continue to update the values of all the states: it can’t ignore any state after some\r
point in the computation. Asynchronous DP algorithms allow great flexibility in selecting\r
states to update.\r
For example, one version of asynchronous value iteration updates the value, in place, of\r
only one state, sk, on each step, k, using the value iteration update (4.10). If 0   < 1,\r
asymptotic convergence to v⇤ is guaranteed given only that all states occur in the sequence\r
{sk} an infinite number of times (the sequence could even be random).1 Similarly, it is\r
possible to intermix policy evaluation and value iteration updates to produce a kind of\r
asynchronous truncated policy iteration. Although the details of this and other more\r
unusual DP algorithms are beyond the scope of this book, it is clear that a few di↵erent\r
updates form building blocks that can be used flexibly in a wide variety of sweepless DP\r
algorithms.\r
Of course, avoiding sweeps does not necessarily mean that we can get away with less\r
computation. It just means that an algorithm does not need to get locked into any\r
hopelessly long sweep before it can make progress improving a policy. We can try to\r
take advantage of this flexibility by selecting the states to which we apply updates so\r
as to improve the algorithm’s rate of progress. We can try to order the updates to let\r
value information propagate from state to state in an ecient way. Some states may not\r
need their values updated as often as others. We might even try to skip updating some\r
states entirely if they are not relevant to optimal behavior. Some ideas for doing this are\r
discussed in Chapter 8.\r
Asynchronous algorithms also make it easier to intermix computation with real-time\r
interaction. To solve a given MDP, we can run an iterative DP algorithm at the same\r
time that an agent is actually experiencing the MDP. The agent’s experience can be used\r
to determine the states to which the DP algorithm applies its updates. At the same time,\r
the latest value and policy information from the DP algorithm can guide the agent’s\r
decision making. For example, we can apply updates to states as the agent visits them.\r
This makes it possible to focus the DP algorithm’s updates onto parts of the state set\r
1In the undiscounted episodic case, it is possible that there are some orderings of updates that do not\r
result in convergence, but it is relatively easy to avoid these."""

[[sections]]
number = "86"
title = "Chapter 4: Dynamic Programming"
text = """
that are most relevant to the agent. This kind of focusing is a repeated theme in\r
reinforcement learning."""

[[sections]]
number = "4.6"
title = "Generalized Policy Iteration"
text = """
Policy iteration consists of two simultaneous, interacting processes, one making the value\r
function consistent with the current policy (policy evaluation), and the other making\r
the policy greedy with respect to the current value function (policy improvement). In\r
policy iteration, these two processes alternate, each completing before the other begins,\r
but this is not really necessary. In value iteration, for example, only a single iteration\r
of policy evaluation is performed in between each policy improvement. In asynchronous\r
DP methods, the evaluation and improvement processes are interleaved at an even\r
finer grain. In some cases a single state is updated in one process before returning\r
to the other. As long as both processes continue to update all states, the ultimate\r
result is typically the same—convergence to the optimal value function and an optimal\r
policy.\r
evaluation\r
improvement\r
⇡  greedy(V )\r
⇡ V\r
V  v⇡\r
⇡⇤ v⇤\r
We use the term generalized policy iteration (GPI) to refer\r
to the general idea of letting policy-evaluation and policy\u0002improvement processes interact, independent of the granularity\r
and other details of the two processes. Almost all reinforcement\r
learning methods are well described as GPI. That is, all have\r
identifiable policies and value functions, with the policy always\r
being improved with respect to the value function and the value\r
function always being driven toward the value function for the\r
policy, as suggested by the diagram to the right. If both the\r
evaluation process and the improvement process stabilize, that\r
is, no longer produce changes, then the value function and policy\r
must be optimal. The value function stabilizes only when it\r
is consistent with the current policy, and the policy stabilizes\r
only when it is greedy with respect to the current value function.\r
Thus, both processes stabilize only when a policy has been found that is greedy with\r
respect to its own evaluation function. This implies that the Bellman optimality equation\r
(4.1) holds, and thus that the policy and the value function are optimal.\r
The evaluation and improvement processes in GPI can be viewed as both competing\r
and cooperating. They compete in the sense that they pull in opposing directions. Making\r
the policy greedy with respect to the value function typically makes the value function\r
incorrect for the changed policy, and making the value function consistent with the policy\r
typically causes that policy no longer to be greedy. In the long run, however, these\r
two processes interact to find a single joint solution: the optimal value function and an\r
optimal policy.\r
One might also think of the interaction between the evaluation and improvement\r
processes in GPI in terms of two constraints or goals—for example, as two lines in"""

[[sections]]
number = "4.7"
title = "Eciency of Dynamic Programming 87"
text = """
v⇤, ⇡⇤\r
⇡ = greedy(v)\r
v, ⇡\r
v = v⇡\r
a two-dimensional space as suggested by the dia\u0002gram to the right. Although the real geometry is\r
much more complicated than this, the diagram sug\u0002gests what happens in the real case. Each process\r
drives the value function or policy toward one of\r
the lines representing a solution to one of the two\r
goals. The goals interact because the two lines are\r
not orthogonal. Driving directly toward one goal\r
causes some movement away from the other goal.\r
Inevitably, however, the joint process is brought closer to the overall goal of optimality.\r
The arrows in this diagram correspond to the behavior of policy iteration in that each\r
takes the system all the way to achieving one of the two goals completely. In GPI\r
one could also take smaller, incomplete steps toward each goal. In either case, the two\r
processes together achieve the overall goal of optimality even though neither is attempting\r
to achieve it directly."""

[[sections]]
number = "4.7"
title = "Eciency of Dynamic Programming"
text = """
DP may not be practical for very large problems, but compared with other methods\r
for solving MDPs, DP methods are actually quite ecient. If we ignore a few technical\r
details, then, in the worst case, the time that DP methods take to find an optimal policy\r
is polynomial in the number of states and actions. If n and k denote the number of states\r
and actions, this means that a DP method takes a number of computational operations\r
that is less than some polynomial function of n and k. A DP method is guaranteed to\r
find an optimal policy in polynomial time even though the total number of (deterministic)\r
policies is kn. In this sense, DP is exponentially faster than any direct search in policy\r
space could be, because direct search would have to exhaustively examine each policy\r
to provide the same guarantee. Linear programming methods can also be used to solve\r
MDPs, and in some cases their worst-case convergence guarantees are better than those\r
of DP methods. But linear programming methods become impractical at a much smaller\r
number of states than do DP methods (by a factor of about 100). For the largest problems,\r
only DP methods are feasible.\r
DP is sometimes thought to be of limited applicability because of the curse of dimen\u0002sionality, the fact that the number of states often grows exponentially with the number\r
of state variables. Large state sets do create diculties, but these are inherent diculties\r
of the problem, not of DP as a solution method. In fact, DP is comparatively better\r
suited to handling large state spaces than competing methods such as direct search and\r
linear programming.\r
In practice, DP methods can be used with today’s computers to solve MDPs with\r
millions of states. Both policy iteration and value iteration are widely used, and it is not\r
clear which, if either, is better in general. In practice, these methods usually converge\r
much faster than their theoretical worst-case run times, particularly if they are started\r
with good initial value functions or policies."""

[[sections]]
number = "88"
title = "Chapter 4: Dynamic Programming"
text = """
On problems with large state spaces, asynchronous DP methods are often preferred. To\r
complete even one sweep of a synchronous method requires computation and memory for\r
every state. For some problems, even this much memory and computation is impractical,\r
yet the problem is still potentially solvable because relatively few states occur along\r
optimal solution trajectories. Asynchronous methods and other variations of GPI can be\r
applied in such cases and may find good or optimal policies much faster than synchronous\r
methods can."""

[[sections]]
number = "4.8"
title = "Summary"
text = """
In this chapter we have become familiar with the basic ideas and algorithms of dynamic\r
programming as they relate to solving finite MDPs. Policy evaluation refers to the (typi\u0002cally) iterative computation of the value function for a given policy. Policy improvement\r
refers to the computation of an improved policy given the value function for that policy.\r
Putting these two computations together, we obtain policy iteration and value iteration,\r
the two most popular DP methods. Either of these can be used to reliably compute\r
optimal policies and value functions for finite MDPs given complete knowledge of the\r
MDP.\r
Classical DP methods operate in sweeps through the state set, performing an expected\r
update operation on each state. Each such operation updates the value of one state\r
based on the values of all possible successor states and their probabilities of occurring.\r
Expected updates are closely related to Bellman equations: they are little more than\r
these equations turned into assignment statements. When the updates no longer result in\r
any changes in value, convergence has occurred to values that satisfy the corresponding\r
Bellman equation. Just as there are four primary value functions (v⇡, v⇤, q⇡, and q⇤),\r
there are four corresponding Bellman equations and four corresponding expected updates.\r
An intuitive view of the operation of DP updates is given by their backup diagrams.\r
Insight into DP methods and, in fact, into almost all reinforcement learning methods,\r
can be gained by viewing them as generalized policy iteration (GPI). GPI is the general idea\r
of two interacting processes revolving around an approximate policy and an approximate\r
value function. One process takes the policy as given and performs some form of policy\r
evaluation, changing the value function to be more like the true value function for the\r
policy. The other process takes the value function as given and performs some form\r
of policy improvement, changing the policy to make it better, assuming that the value\r
function is its value function. Although each process changes the basis for the other,\r
overall they work together to find a joint solution: a policy and value function that are\r
unchanged by either process and, consequently, are optimal. In some cases, GPI can be\r
proved to converge, most notably for the classical DP methods that we have presented in\r
this chapter. In other cases convergence has not been proved, but still the idea of GPI\r
improves our understanding of the methods.\r
It is not necessary to perform DP methods in complete sweeps through the state\r
set. Asynchronous DP methods are in-place iterative methods that update states in an\r
arbitrary order, perhaps stochastically determined and using out-of-date information.\r
Many of these methods can be viewed as fine-grained forms of GPI."""

[[sections]]
number = "4.8"
title = "Summary 89"
text = """
Finally, we note one last special property of DP methods. All of them update estimates\r
of the values of states based on estimates of the values of successor states. That is, they\r
update estimates on the basis of other estimates. We call this general idea bootstrapping.\r
Many reinforcement learning methods perform bootstrapping, even those that do not\r
require, as DP requires, a complete and accurate model of the environment. In the next\r
chapter we explore reinforcement learning methods that do not require a model and do\r
not bootstrap. In the chapter after that we explore methods that do not require a model\r
but do bootstrap. These key features and properties are separable, yet can be mixed in\r
interesting combinations.\r
Bibliographical and Historical Remarks\r
The term “dynamic programming” is due to Bellman (1957a), who showed how these\r
methods could be applied to a wide range of problems. Extensive treatments of DP can\r
be found in many texts, including Bertsekas (2005, 2012), Bertsekas and Tsitsiklis (1996),\r
Dreyfus and Law (1977), Ross (1983), White (1969), and Whittle (1982, 1983). Our\r
interest in DP is restricted to its use in solving MDPs, but DP also applies to other types\r
of problems. Kumar and Kanal (1988) provide a more general look at DP.\r
To the best of our knowledge, the first connection between DP and reinforcement\r
learning was made by Minsky (1961) in commenting on Samuel’s checkers player. In\r
a footnote, Minsky mentioned that it is possible to apply DP to problems in which\r
Samuel’s backing-up process can be handled in closed analytic form. This remark may\r
have misled artificial intelligence researchers into believing that DP was restricted to\r
analytically tractable problems and therefore largely irrelevant to artificial intelligence.\r
Andreae (1969) mentioned DP in the context of reinforcement learning. Werbos (1977)\r
suggested an approach to approximating DP called “heuristic dynamic programming”\r
that emphasizes gradient-descent methods for continuous-state problems (Werbos, 1982,\r
1987, 1988, 1989, 1992). These methods are closely related to the reinforcement learning\r
algorithms that we discuss in this book. Watkins (1989) was explicit in connecting\r
reinforcement learning to DP, characterizing a class of reinforcement learning methods as\r
“incremental dynamic programming.”\r
4.1–4 These sections describe well-established DP algorithms that are covered in any of\r
the general DP references cited above. The policy improvement theorem and the\r
policy iteration algorithm are due to Bellman (1957a) and Howard (1960). Our\r
presentation was influenced by the local view of policy improvement taken by\r
Watkins (1989). Our discussion of value iteration as a form of truncated policy\r
iteration is based on the approach of Puterman and Shin (1978), who presented a\r
class of algorithms called modified policy iteration, which includes policy iteration\r
and value iteration as special cases. An analysis showing how value iteration can\r
be made to find an optimal policy in finite time is given by Bertsekas (1987).\r
Iterative policy evaluation is an example of a classical successive approximation\r
algorithm for solving a system of linear equations. The version of the algorithm"""

[[sections]]
number = "90"
title = "Chapter 4: Dynamic Programming"
text = """
that uses two arrays, one holding the old values while the other is updated, is\r
often called a Jacobi-style algorithm, after Jacobi’s classical use of this method.\r
It is also sometimes called a synchronous algorithm because the e↵ect is as if all\r
the values are updated at the same time. The second array is needed to simulate\r
this parallel computation sequentially. The in-place version of the algorithm\r
is often called a Gauss–Seidel-style algorithm after the classical Gauss–Seidel\r
algorithm for solving systems of linear equations. In addition to iterative policy\r
evaluation, other DP algorithms can be implemented in these di↵erent versions.\r
Bertsekas and Tsitsiklis (1989) provide excellent coverage of these variations and\r
their performance di↵erences."""

[[sections]]
number = "4.5"
title = "Asynchronous DP algorithms are due to Bertsekas (1982, 1983), who also called"
text = """
them distributed DP algorithms. The original motivation for asynchronous\r
DP was its implementation on a multiprocessor system with communication\r
delays between processors and no global synchronizing clock. These algorithms\r
are extensively discussed by Bertsekas and Tsitsiklis (1989). Jacobi-style and\r
Gauss–Seidel-style DP algorithms are special cases of the asynchronous version.\r
Williams and Baird (1990) presented DP algorithms that are asynchronous at a\r
finer grain than the ones we have discussed: the update operations themselves\r
are broken into steps that can be performed asynchronously."""

[[sections]]
number = "4.7"
title = "This section, written with the help of Michael Littman, is based on Littman,"
text = """
Dean, and Kaelbling (1995). The phrase “curse of dimensionality” is due to\r
Bellman (1957a).\r
Foundational work on the linear programming approach to reinforcement learning\r
was done by Daniela de Farias (de Farias, 2002; de Farias and Van Roy, 2003).

Chapter 5\r
Monte Carlo Methods\r
In this chapter we consider our first learning methods for estimating value functions and\r
discovering optimal policies. Unlike the previous chapter, here we do not assume complete\r
knowledge of the environment. Monte Carlo methods require only experience—sample\r
sequences of states, actions, and rewards from actual or simulated interaction with an\r
environment. Learning from actual experience is striking because it requires no prior\r
knowledge of the environment’s dynamics, yet can still attain optimal behavior. Learning\r
from simulated experience is also powerful. Although a model is required, the model need\r
only generate sample transitions, not the complete probability distributions of all possible\r
transitions that is required for dynamic programming (DP). In surprisingly many cases it\r
is easy to generate experience sampled according to the desired probability distributions,\r
but infeasible to obtain the distributions in explicit form.\r
Monte Carlo methods are ways of solving the reinforcement learning problem based on\r
averaging sample returns. To ensure that well-defined returns are available, here we define\r
Monte Carlo methods only for episodic tasks. That is, we assume experience is divided\r
into episodes, and that all episodes eventually terminate no matter what actions are\r
selected. Only on the completion of an episode are value estimates and policies changed.\r
Monte Carlo methods can thus be incremental in an episode-by-episode sense, but not in\r
a step-by-step (online) sense. The term “Monte Carlo” is often used more broadly for\r
any estimation method whose operation involves a significant random component. Here\r
we use it specifically for methods based on averaging complete returns (as opposed to\r
methods that learn from partial returns, considered in the next chapter).\r
Monte Carlo methods sample and average returns for each state–action pair much like\r
the bandit methods we explored in Chapter 2 sample and average rewards for each action.\r
The main di↵erence is that now there are multiple states, each acting like a di↵erent\r
bandit problem (like an associative-search or contextual bandit) and the di↵erent bandit\r
problems are interrelated. That is, the return after taking an action in one state depends\r
on the actions taken in later states in the same episode. Because all the action selections\r
are undergoing learning, the problem becomes nonstationary from the point of view of\r
the earlier state."""

[[sections]]
number = "92"
title = "Chapter 5: Monte Carlo Methods"
text = """
To handle the nonstationarity, we adapt the idea of general policy iteration (GPI)\r
developed in Chapter 4 for DP. Whereas there we computed value functions from knowledge\r
of the MDP, here we learn value functions from sample returns with the MDP. The value\r
functions and corresponding policies still interact to attain optimality in essentially the\r
same way (GPI). As in the DP chapter, first we consider the prediction problem (the\r
computation of v⇡ and q⇡ for a fixed arbitrary policy ⇡) then policy improvement, and,\r
finally, the control problem and its solution by GPI. Each of these ideas taken from DP\r
is extended to the Monte Carlo case in which only sample experience is available."""

[[sections]]
number = "5.1"
title = "Monte Carlo Prediction"
text = """
We begin by considering Monte Carlo methods for learning the state-value function for a\r
given policy. Recall that the value of a state is the expected return—expected cumulative\r
future discounted reward—starting from that state. An obvious way to estimate it from\r
experience, then, is simply to average the returns observed after visits to that state. As\r
more returns are observed, the average should converge to the expected value. This idea\r
underlies all Monte Carlo methods.\r
In particular, suppose we wish to estimate v⇡(s), the value of a state s under policy ⇡,\r
given a set of episodes obtained by following ⇡ and passing through s. Each occurrence\r
of state s in an episode is called a visit to s. Of course, s may be visited multiple times\r
in the same episode; let us call the first time it is visited in an episode the first visit\r
to s. The first-visit MC method estimates v⇡(s) as the average of the returns following\r
first visits to s, whereas the every-visit MC method averages the returns following all\r
visits to s. These two Monte Carlo (MC) methods are very similar but have slightly\r
di↵erent theoretical properties. First-visit MC has been most widely studied, dating back\r
to the 1940s, and is the one we focus on in this chapter. Every-visit MC extends more\r
naturally to function approximation and eligibility traces, as discussed in Chapters 9 and\r
12. First-visit MC is shown in procedural form in the box. Every-visit MC would be the\r
same except without the check for St having occurred earlier in the episode.\r
First-visit MC prediction, for estimating V ⇡ v⇡\r
Input: a policy ⇡ to be evaluated\r
Initialize:\r
V (s) 2 R, arbitrarily, for all s 2 S\r
Returns(s) an empty list, for all s 2 S\r
Loop forever (for each episode):\r
Generate an episode following ⇡: S0, A0, R1, S1, A1, R2,...,ST 1, AT 1, RT\r
G 0\r
Loop for each step of episode, t = T 1, T 2,..., 0:\r
G G + Rt+1\r
Unless St appears in S0, S1,...,St1:\r
Append G to Returns(St)\r
V (St) average(Returns(St))"""

[[sections]]
number = "5.1"
title = "Monte Carlo Prediction 93"
text = """
Both first-visit MC and every-visit MC converge to v⇡(s) as the number of visits (or\r
first visits) to s goes to infinity. This is easy to see for the case of first-visit MC. In\r
this case each return is an independent, identically distributed estimate of v⇡(s) with\r
finite variance. By the law of large numbers the sequence of averages of these estimates\r
converges to their expected value. Each average is itself an unbiased estimate, and the\r
standard deviation of its error falls as 1/\r
pn, where n is the number of returns averaged.\r
Every-visit MC is less straightforward, but its estimates also converge quadratically to\r
v⇡(s) (Singh and Sutton, 1996).\r
The use of Monte Carlo methods is best illustrated through an example.\r
Example 5.1: Blackjack The object of the popular casino card game of blackjack is to\r
obtain cards the sum of whose numerical values is as great as possible without exceeding\r
21. All face cards count as 10, and an ace can count either as 1 or as 11. We consider\r
the version in which each player competes independently against the dealer. The game\r
begins with two cards dealt to both dealer and player. One of the dealer’s cards is face\r
up and the other is face down. If the player has 21 immediately (an ace and a 10-card),\r
it is called a natural. He then wins unless the dealer also has a natural, in which case the\r
game is a draw. If the player does not have a natural, then he can request additional\r
cards, one by one (hits), until he either stops (sticks) or exceeds 21 (goes bust). If he goes\r
bust, he loses; if he sticks, then it becomes the dealer’s turn. The dealer hits or sticks\r
according to a fixed strategy without choice: he sticks on any sum of 17 or greater, and\r
hits otherwise. If the dealer goes bust, then the player wins; otherwise, the outcome—win,\r
lose, or draw—is determined by whose final sum is closer to 21.\r
Playing blackjack is naturally formulated as an episodic finite MDP. Each game of\r
blackjack is an episode. Rewards of +1, 1, and 0 are given for winning, losing, and\r
drawing, respectively. All rewards within a game are zero, and we do not discount ( = 1);\r
therefore these terminal rewards are also the returns. The player’s actions are to hit or\r
to stick. The states depend on the player’s cards and the dealer’s showing card. We\r
assume that cards are dealt from an infinite deck (i.e., with replacement) so that there is\r
no advantage to keeping track of the cards already dealt. If the player holds an ace that\r
he could count as 11 without going bust, then the ace is said to be usable. In this case\r
it is always counted as 11 because counting it as 1 would make the sum 11 or less, in\r
which case there is no decision to be made because, obviously, the player should always\r
hit. Thus, the player makes decisions on the basis of three variables: his current sum\r
(12–21), the dealer’s one showing card (ace–10), and whether or not he holds a usable\r
ace. This makes for a total of 200 states.\r
Consider the policy that sticks if the player’s sum is 20 or 21, and otherwise hits. To\r
find the state-value function for this policy by a Monte Carlo approach, one simulates\r
many blackjack games using the policy and averages the returns following each state.\r
In this way, we obtained the estimates of the state-value function shown in Figure 5.1.\r
The estimates for states with a usable ace are less certain and less regular because these\r
states are less common. In any event, after 500,000 games the value function is very well\r
approximated."""

[[sections]]
number = "94"
title = "Chapter 5: Monte Carlo Methods"
text = """
+1\r
!1\r
A\r
Dealer showing 10 12\r
Player sum"""

[[sections]]
number = "21"
title = "After 10,000 episodes After 500,000 episodes"
text = """
Usable\r
ace\r
No\r
usable\r
ace\r
Figure 5.1: Approximate state-value functions for the blackjack policy that sticks only on 20\r
or 21, computed by Monte Carlo policy evaluation.\r
Exercise 5.1 Consider the diagrams on the right in Figure 5.1. Why does the estimated\r
value function jump up for the last two rows in the rear? Why does it drop o↵ for the\r
whole last row on the left? Why are the frontmost values higher in the upper diagrams\r
than in the lower? ⇤\r
Exercise 5.2 Suppose every-visit MC was used instead of first-visit MC on the blackjack\r
task. Would you expect the results to be very di↵erent? Why or why not? ⇤\r
Although we have complete knowledge of the environment in the blackjack task, it\r
would not be easy to apply DP methods to compute the value function. DP methods\r
require the distribution of next events—in particular, they require the environments\r
dynamics as given by the four-argument function p—and it is not easy to determine\r
this for blackjack. For example, suppose the player’s sum is 14 and he chooses to stick.\r
What is his probability of terminating with a reward of +1 as a function of the dealer’s\r
showing card? All of the probabilities must be computed before DP can be applied, and\r
such computations are often complex and error-prone. In contrast, generating the sample\r
games required by Monte Carlo methods is easy. This is the case surprisingly often; the\r
ability of Monte Carlo methods to work with sample episodes alone can be a significant\r
advantage even when one has complete knowledge of the environment’s dynamics.\r
Can we generalize the idea of backup diagrams to Monte Carlo algorithms? The\r
general idea of a backup diagram is to show at the top the root node to be updated and\r
to show below all the transitions and leaf nodes whose rewards and estimated values\r
contribute to the update. For Monte Carlo estimation of v⇡, the root is a state node, and\r
below it is the entire trajectory of transitions along a particular single episode, ending"""

[[sections]]
number = "5.1"
title = "Monte Carlo Prediction 95"
text = """
at the terminal state, as shown to the right. Whereas the DP diagram (page 59)\r
shows all possible transitions, the Monte Carlo diagram shows only those sampled\r
on the one episode. Whereas the DP diagram includes only one-step transitions,\r
the Monte Carlo diagram goes all the way to the end of the episode. These\r
di↵erences in the diagrams accurately reflect the fundamental di↵erences between\r
the algorithms.\r
An important fact about Monte Carlo methods is that the estimates for each\r
state are independent. The estimate for one state does not build upon the estimate\r
of any other state, as is the case in DP. In other words, Monte Carlo methods do\r
not bootstrap as we defined it in the previous chapter.\r
In particular, note that the computational expense of estimating the value of\r
a single state is independent of the number of states. This can make Monte Carlo\r
methods particularly attractive when one requires the value of only one or a subset\r
of states. One can generate many sample episodes starting from the states of interest,\r
averaging returns from only these states, ignoring all others. This is a third advantage\r
Monte Carlo methods can have over DP methods (after the ability to learn from actual\r
experience and from simulated experience).\r
A bubble on a wire loop.\r
From Hersh and Griego (1969). Reproduced with\r
permission. ©1969 Scientific American, a divi\u0002sion of Nature America, Inc. All rights reserved.\r
Example 5.2: Soap Bubble Suppose a wire\r
frame forming a closed loop is dunked in soapy\r
water to form a soap surface or bubble conform\u0002ing at its edges to the wire frame. If the geom\u0002etry of the wire frame is irregular but known,\r
how can you compute the shape of the surface?\r
The shape has the property that the total force\r
on each point exerted by neighboring points is\r
zero (or else the shape would change). This\r
means that the surface’s height at any point is\r
the average of its heights at points in a small\r
circle around that point. In addition, the sur\u0002face must meet at its boundaries with the wire\r
frame. The usual approach to problems of this\r
kind is to put a grid over the area covered by\r
the surface and solve for its height at the grid points by an iterative computation. Grid\r
points at the boundary are forced to the wire frame, and all others are adjusted toward\r
the average of the heights of their four nearest neighbors. This process then iterates, much\r
like DP’s iterative policy evaluation, and ultimately converges to a close approximation\r
to the desired surface.\r
This is similar to the kind of problem for which Monte Carlo methods were originally\r
designed. Instead of the iterative computation described above, imagine standing on the\r
surface and taking a random walk, stepping randomly from grid point to neighboring\r
grid point, with equal probability, until you reach the boundary. It turns out that the\r
expected value of the height at the boundary is a close approximation to the height of\r
the desired surface at the starting point (in fact, it is exactly the value computed by the\r
iterative method described above). Thus, one can closely approximate the height of the"""

[[sections]]
number = "96"
title = "Chapter 5: Monte Carlo Methods"
text = """
surface at a point by simply averaging the boundary heights of many walks started at\r
the point. If one is interested in only the value at one point, or any fixed small set of\r
points, then this Monte Carlo method can be far more ecient than the iterative method\r
based on local consistency."""

[[sections]]
number = "5.2"
title = "Monte Carlo Estimation of Action Values"
text = """
If a model is not available, then it is particularly useful to estimate action values (the\r
values of state–action pairs) rather than state values. With a model, state values alone are\r
sucient to determine a policy; one simply looks ahead one step and chooses whichever\r
action leads to the best combination of reward and next state, as we did in the chapter on\r
DP. Without a model, however, state values alone are not sucient. One must explicitly\r
estimate the value of each action in order for the values to be useful in suggesting a policy.\r
Thus, one of our primary goals for Monte Carlo methods is to estimate q⇤. To achieve\r
this, we first consider the policy evaluation problem for action values.\r
The policy evaluation problem for action values is to estimate q⇡(s, a), the expected\r
return when starting in state s, taking action a, and thereafter following policy ⇡. The\r
Monte Carlo methods for this are essentially the same as just presented for state values,\r
except now we talk about visits to a state–action pair rather than to a state. A state–\r
action pair s, a is said to be visited in an episode if ever the state s is visited and action\r
a is taken in it. The every-visit MC method estimates the value of a state–action pair\r
as the average of the returns that have followed all the visits to it. The first-visit MC\r
method averages the returns following the first time in each episode that the state was\r
visited and the action was selected. These methods converge quadratically, as before, to\r
the true expected values as the number of visits to each state–action pair approaches\r
infinity.\r
The only complication is that many state–action pairs may never be visited. If ⇡ is\r
a deterministic policy, then in following ⇡ one will observe returns only for one of the\r
actions from each state. With no returns to average, the Monte Carlo estimates of the\r
other actions will not improve with experience. This is a serious problem because the\r
purpose of learning action values is to help in choosing among the actions available in\r
each state. To compare alternatives we need to estimate the value of all the actions from\r
each state, not just the one we currently favor.\r
This is the general problem of maintaining exploration, as discussed in the context\r
of the k-armed bandit problem in Chapter 2. For policy evaluation to work for action\r
values, we must assure continual exploration. One way to do this is by specifying that\r
the episodes start in a state–action pair, and that every pair has a nonzero probability of\r
being selected as the start. This guarantees that all state–action pairs will be visited an\r
infinite number of times in the limit of an infinite number of episodes. We call this the\r
assumption of exploring starts.\r
The assumption of exploring starts is sometimes useful, but of course it cannot be\r
relied upon in general, particularly when learning directly from actual interaction with an\r
environment. In that case the starting conditions are unlikely to be so helpful. The most\r
common alternative approach to assuring that all state–action pairs are encountered is"""

[[sections]]
number = "5.3"
title = "Monte Carlo Control 97"
text = """
to consider only policies that are stochastic with a nonzero probability of selecting all\r
actions in each state. We discuss two important variants of this approach in later sections.\r
For now, we retain the assumption of exploring starts and complete the presentation of a\r
full Monte Carlo control method.\r
Exercise 5.3 What is the backup diagram for Monte Carlo estimation of q⇡? ⇤"""

[[sections]]
number = "5.3"
title = "Monte Carlo Control"
text = """
We are now ready to consider how Monte Carlo estimation can be used in control, that\r
is, to approximate optimal policies. The overall idea is to proceed according to the same\r
pattern as in the DP chapter, that is, according to the idea of generalized policy iteration\r
evaluation\r
improvement\r
⇡ Q\r
⇡  greedy(Q)\r
Q  q⇡\r
(GPI). In GPI one maintains both an approximate policy and\r
an approximate value function. The value function is repeatedly\r
altered to more closely approximate the value function for the\r
current policy, and the policy is repeatedly improved with respect\r
to the current value function, as suggested by the diagram to\r
the right. These two kinds of changes work against each other to\r
some extent, as each creates a moving target for the other, but\r
together they cause both policy and value function to approach\r
optimality.\r
To begin, let us consider a Monte Carlo version of classical policy iteration. In\r
this method, we perform alternating complete steps of policy evaluation and policy\r
improvement, beginning with an arbitrary policy ⇡0 and ending with the optimal policy\r
and optimal action-value function:\r
⇡0\r
E\r
! q⇡0\r
I\r
! ⇡1\r
E\r
! q⇡1\r
I\r
! ⇡2\r
E\r
! · · · I! ⇡⇤\r
E\r
! q⇤,\r
where E! denotes a complete policy evaluation and I! denotes a complete policy\r
improvement. Policy evaluation is done exactly as described in the preceding section.\r
Many episodes are experienced, with the approximate action-value function approaching\r
the true function asymptotically. For the moment, let us assume that we do indeed\r
observe an infinite number of episodes and that, in addition, the episodes are generated\r
with exploring starts. Under these assumptions, the Monte Carlo methods will compute\r
each q⇡k exactly, for arbitrary ⇡k.\r
Policy improvement is done by making the policy greedy with respect to the current\r
value function. In this case we have an action-value function, and therefore no model is\r
needed to construct the greedy policy. For any action-value function q, the corresponding\r
greedy policy is the one that, for each s 2 S, deterministically chooses an action with\r
maximal action-value:\r
⇡(s) .= arg maxa q(s, a). (5.1)\r
Policy improvement then can be done by constructing each ⇡k+1 as the greedy policy\r
with respect to q⇡k . The policy improvement theorem (Section 4.2) then applies to ⇡k"""

[[sections]]
number = "98"
title = "Chapter 5: Monte Carlo Methods"
text = """
and ⇡k+1 because, for all s 2 S,\r
q⇡k (s, ⇡k+1(s)) = q⇡k (s, argmax aq⇡k (s, a))\r
= maxa q⇡k (s, a)\r
 q⇡k (s, ⇡k(s))\r
 v⇡k (s).\r
As we discussed in the previous chapter, the theorem assures us that each ⇡k+1 is uniformly\r
better than ⇡k, or just as good as ⇡k, in which case they are both optimal policies. This\r
in turn assures us that the overall process converges to the optimal policy and optimal\r
value function. In this way Monte Carlo methods can be used to find optimal policies\r
given only sample episodes and no other knowledge of the environment’s dynamics.\r
We made two unlikely assumptions above in order to easily obtain this guarantee of\r
convergence for the Monte Carlo method. One was that the episodes have exploring\r
starts, and the other was that policy evaluation could be done with an infinite number of\r
episodes. To obtain a practical algorithm we will have to remove both assumptions. We\r
postpone consideration of the first assumption until later in this chapter.\r
For now we focus on the assumption that policy evaluation operates on an infinite\r
number of episodes. This assumption is relatively easy to remove. In fact, the same issue\r
arises even in classical DP methods such as iterative policy evaluation, which also converge\r
only asymptotically to the true value function. In both DP and Monte Carlo cases there\r
are two ways to solve the problem. One is to hold firm to the idea of approximating q⇡k\r
in each policy evaluation. Measurements and assumptions are made to obtain bounds\r
on the magnitude and probability of error in the estimates, and then sucient steps are\r
taken during each policy evaluation to assure that these bounds are suciently small.\r
This approach can probably be made completely satisfactory in the sense of guaranteeing\r
correct convergence up to some level of approximation. However, it is also likely to require\r
far too many episodes to be useful in practice on any but the smallest problems.\r
There is a second approach to avoiding the infinite number of episodes nominally\r
required for policy evaluation, in which we give up trying to complete policy evaluation\r
before returning to policy improvement. On each evaluation step we move the value\r
function toward q⇡k , but we do not expect to actually get close except over many steps.\r
We used this idea when we first introduced the idea of GPI in Section 4.6. One extreme\r
form of the idea is value iteration, in which only one iteration of iterative policy evaluation\r
is performed between each step of policy improvement. The in-place version of value\r
iteration is even more extreme; there we alternate between improvement and evaluation\r
steps for single states.\r
For Monte Carlo policy iteration it is natural to alternate between evaluation and\r
improvement on an episode-by-episode basis. After each episode, the observed returns\r
are used for policy evaluation, and then the policy is improved at all the states visited in\r
the episode. A complete simple algorithm along these lines, which we call Monte Carlo\r
ES, for Monte Carlo with Exploring Starts, is given in pseudocode in the box on the next\r
page."""

[[sections]]
number = "5.3"
title = "Monte Carlo Control 99"
text = """
Monte Carlo ES (Exploring Starts), for estimating ⇡ ⇡ ⇡⇤\r
Initialize:\r
⇡(s) 2 A(s) (arbitrarily), for all s 2 S\r
Q(s, a) 2 R (arbitrarily), for all s 2 S, a 2 A(s)\r
Returns(s, a) empty list, for all s 2 S, a 2 A(s)\r
Loop forever (for each episode):\r
Choose S0 2 S, A0 2 A(S0) randomly such that all pairs have probability > 0\r
Generate an episode from S0, A0, following ⇡: S0, A0, R1,...,ST 1, AT 1, RT\r
G 0\r
Loop for each step of episode, t = T 1, T 2,..., 0:\r
G G + Rt+1\r
Unless the pair St, At appears in S0, A0, S1, A1 ...,St1, At1:\r
Append G to Returns(St, At)\r
Q(St, At) average(Returns(St, At))\r
⇡(St) argmaxa Q(St, a)\r
Exercise 5.4 The pseudocode for Monte Carlo ES is inecient because, for each state–\r
action pair, it maintains a list of all returns and repeatedly calculates their mean. It would\r
be more ecient to use techniques similar to those explained in Section 2.4 to maintain\r
just the mean and a count (for each state–action pair) and update them incrementally.\r
Describe how the pseudocode would be altered to achieve this. ⇤\r
In Monte Carlo ES, all the returns for each state–action pair are accumulated and\r
averaged, irrespective of what policy was in force when they were observed. It is easy\r
to see that Monte Carlo ES cannot converge to any suboptimal policy. If it did, then\r
the value function would eventually converge to the value function for that policy, and\r
that in turn would cause the policy to change. Stability is achieved only when both\r
the policy and the value function are optimal. Convergence to this optimal fixed point\r
seems inevitable as the changes to the action-value function decrease over time, but has\r
not yet been formally proved. In our opinion, this is one of the most fundamental open\r
theoretical questions in reinforcement learning (for a partial solution, see Tsitsiklis, 2002).\r
Example 5.3: Solving Blackjack It is straightforward to apply Monte Carlo ES to\r
blackjack. Because the episodes are all simulated games, it is easy to arrange for exploring\r
starts that include all possibilities. In this case one simply picks the dealer’s cards, the\r
player’s sum, and whether or not the player has a usable ace, all at random with equal\r
probability. As the initial policy we use the policy evaluated in the previous blackjack\r
example, that which sticks only on 20 or 21. The initial action-value function can be zero\r
for all state–action pairs. Figure 5.2 shows the optimal policy for blackjack found by\r
Monte Carlo ES. This policy is the same as the “basic” strategy of Thorp (1966) with the\r
sole exception of the leftmost notch in the policy for a usable ace, which is not present\r
in Thorp’s strategy. We are uncertain of the reason for this discrepancy, but confident\r
that what is shown here is indeed the optimal policy for the version of blackjack we have\r
described."""

[[sections]]
number = "100"
title = "Chapter 5: Monte Carlo Methods"
text = """
Usable\r
ace\r
No\r
usable\r
ace"""

[[sections]]
number = "20"
title = "A 2 3 4 5 6 7 8 9 10"
text = """
Dealer showing\r
Player sum\r
HIT\r
STICK 19\r
21\r
11\r
12\r
13\r
14\r
15\r
16\r
17\r
18\r
!\r
*\r
A 2 3 4 5 6 7 8 9 10\r
HIT\r
STICK 20\r
19\r
21\r
11\r
12\r
13\r
14\r
15\r
16\r
17"""

[[sections]]
number = "18"
title = "V*"
text = """
21\r
10 12\r
A\r
Dealer showing\r
Player sum"""

[[sections]]
number = "10"
title = "A 12"
text = """
21\r
+1\r
"1\r
v*\r
Usable\r
ace\r
No\r
usable\r
ace"""

[[sections]]
number = "20"
title = "A 2 3 4 5 6 7 8 9 10"
text = """
Dealer showing\r
Player sum\r
HIT\r
STICK 19\r
21\r
11\r
12\r
13\r
14\r
15\r
16\r
17\r
18\r
!\r
*\r
A 2 3 4 5 6 7 8 9 10\r
HIT\r
STICK 20\r
19\r
21\r
11\r
12\r
13\r
14\r
15\r
16\r
17"""

[[sections]]
number = "18"
title = "V*"
text = """
21\r
10 12\r
A\r
Dealer showing\r
Player sum"""

[[sections]]
number = "10"
title = "A 12"
text = """
21\r
+1\r
"1\r
v*\r
Usable\r
ace\r
No\r
usable\r
ace"""

[[sections]]
number = "20"
title = "A 2 3 4 5 6 7 8 9 10"
text = """
Dealer showing\r
Player sum\r
HIT\r
STICK 19\r
21\r
11\r
12\r
13\r
14\r
15\r
16\r
17\r
18\r
!\r
A 2 3 4 5 6 7 8 9 10\r
HIT\r
STICK 20\r
19\r
21\r
11\r
12\r
13\r
14\r
15\r
16\r
17\r
18\r
V\r
21\r
10 12\r
A\r
Dealer showing\r
Player sum"""

[[sections]]
number = "10"
title = "A 12"
text = """
21\r
+1\r
"1\r
v\r
Dealer showing\r
Player sum\r
* *\r
Figure 5.2: The optimal policy and state-value function for blackjack, found by Monte Carlo\r
ES. The state-value function shown was computed from the action-value function found by\r
Monte Carlo ES."""

[[sections]]
number = "5.4"
title = "Monte Carlo Control without Exploring Starts"
text = """
How can we avoid the unlikely assumption of exploring starts? The only general way to\r
ensure that all actions are selected infinitely often is for the agent to continue to select\r
them. There are two approaches to ensuring this, resulting in what we call on-policy\r
methods and o↵-policy methods. On-policy methods attempt to evaluate or improve the\r
policy that is used to make decisions, whereas o↵-policy methods evaluate or improve\r
a policy di↵erent from that used to generate the data. The Monte Carlo ES method\r
developed above is an example of an on-policy method. In this section we show how an\r
on-policy Monte Carlo control method can be designed that does not use the unrealistic\r
assumption of exploring starts. O↵-policy methods are considered in the next section.\r
In on-policy control methods the policy is generally soft, meaning that ⇡(a|s) > 0\r
for all s 2 S and all a 2 A(s), but gradually shifted closer and closer to a deterministic\r
optimal policy. Many of the methods discussed in Chapter 2 provide mechanisms for\r
this. The on-policy method we present in this section uses "-greedy policies, meaning\r
that most of the time they choose an action that has maximal estimated action value,\r
but with probability " they instead select an action at random. That is, all nongreedy\r
actions are given the minimal probability of selection, "\r
|A(s)| , and the remaining bulk of\r
the probability, 1  " + "\r
|A(s)| , is given to the greedy action. The "-greedy policies are\r
examples of "-soft policies, defined as policies for which ⇡(a|s)  "\r
|A(s)| for all states and\r
actions, for some " > 0. Among "-soft policies, "-greedy policies are in some sense those\r
that are closest to greedy."""

[[sections]]
number = "5.4"
title = "Monte Carlo Control without Exploring Starts 101"
text = """
The overall idea of on-policy Monte Carlo control is still that of GPI. As in Monte\r
Carlo ES, we use first-visit MC methods to estimate the action-value function for the\r
current policy. Without the assumption of exploring starts, however, we cannot simply\r
improve the policy by making it greedy with respect to the current value function, because\r
that would prevent further exploration of nongreedy actions. Fortunately, GPI does not\r
require that the policy be taken all the way to a greedy policy, only that it be moved\r
toward a greedy policy. In our on-policy method we will move it only to an "-greedy\r
policy. For any "-soft policy, ⇡, any "-greedy policy with respect to q⇡ is guaranteed to\r
be better than or equal to ⇡. The complete algorithm is given in the box below.\r
On-policy first-visit MC control (for "-soft policies), estimates ⇡ ⇡ ⇡⇤\r
Algorithm parameter: small " > 0\r
Initialize:\r
⇡ an arbitrary "-soft policy\r
Q(s, a) 2 R (arbitrarily), for all s 2 S, a 2 A(s)\r
Returns(s, a) empty list, for all s 2 S, a 2 A(s)\r
Repeat forever (for each episode):\r
Generate an episode following ⇡: S0, A0, R1,...,ST 1, AT 1, RT\r
G 0\r
Loop for each step of episode, t = T 1, T 2,..., 0:\r
G G + Rt+1\r
Unless the pair St, At appears in S0, A0, S1, A1 ...,St1, At1:\r
Append G to Returns(St, At)\r
Q(St, At) average(Returns(St, At))\r
A⇤ argmaxa Q(St, a) (with ties broken arbitrarily)\r
For all a 2 A(St):\r
⇡(a|St) \r
⇢ 1  " + "/|A(St)| if a = A⇤\r
"/|A(St)| if a 6= A⇤\r
That any "-greedy policy with respect to q⇡ is an improvement over any "-soft policy\r
⇡ is assured by the policy improvement theorem. Let ⇡0 be the "-greedy policy. The\r
conditions of the policy improvement theorem apply because for any s 2 S:\r
q⇡(s, ⇡0(s)) = X\r
a\r
⇡0(a|s)q⇡(s, a)\r
= "\r
|A(s)|\r
X\r
a\r
q⇡(s, a) + (1  ") maxa q⇡(s, a) (5.2)\r
\r
"\r
|A(s)|\r
X\r
a\r
q⇡(s, a) + (1  ")\r
X\r
a\r
⇡(a|s)  "\r
|A(s)|\r
1  "\r
q⇡(s, a)"""

[[sections]]
number = "102"
title = "Chapter 5: Monte Carlo Methods"
text = """
(the sum is a weighted average with nonnegative weights summing to 1, and as such it\r
must be less than or equal to the largest number averaged)\r
= "\r
|A(s)|\r
X\r
a\r
q⇡(s, a)  "\r
|A(s)|\r
X\r
a\r
q⇡(s, a) + X\r
a\r
⇡(a|s)q⇡(s, a)\r
= v⇡(s).\r
Thus, by the policy improvement theorem, ⇡0  ⇡ (i.e., v⇡0 (s)  v⇡(s), for all s 2 S). We\r
now prove that equality can hold only when both ⇡0 and ⇡ are optimal among the "-soft\r
policies, that is, when they are better than or equal to all other "-soft policies.\r
Consider a new environment that is just like the original environment, except with the\r
requirement that policies be "-soft “moved inside” the environment. The new environment\r
has the same action and state set as the original and behaves as follows. If in state s\r
and taking action a, then with probability 1  " the new environment behaves exactly\r
like the old environment. With probability " it repicks the action at random, with equal\r
probabilities, and then behaves like the old environment with the new, random action.\r
The best one can do in this new environment with general policies is the same as the\r
best one could do in the original environment with "-soft policies. Let ve⇤ and qe⇤ denote\r
the optimal value functions for the new environment. Then a policy ⇡ is optimal among\r
"-soft policies if and only if v⇡ = ve⇤. We know that ve⇤ is the unique solution to the\r
Bellman optimality equation (3.19) with altered transition probabilities:\r
ve⇤(s) = maxa\r
X\r
s0,r\r
h\r
(1  ")p(s0, r|s, a) +X\r
a0\r
"\r
|A(s)|\r
p(s0, r|s, a0)\r
ihr + ve⇤(s0\r
)\r
i\r
= (1  ") maxa\r
X\r
s0,r\r
p(s0, r|s, a)\r
h\r
r + ve⇤(s0)\r
i\r
+\r
"\r
|A(s)|\r
X\r
a\r
X\r
s0,r\r
p(s0, r|s, a)\r
h\r
r + ve⇤(s0)\r
i\r
.\r
When equality holds and the "-soft policy ⇡ is no longer improved, then we also know,\r
from (5.2), that\r
v⇡(s) = (1  ") maxa q⇡(s, a) + "\r
|A(s)|\r
X\r
a\r
q⇡(s, a)\r
= (1  ") maxa\r
X\r
s0,r\r
p(s0, r|s, a)\r
h\r
r + v⇡(s0)\r
i\r
+\r
"\r
|A(s)|\r
X\r
a\r
X\r
s0,r\r
p(s0, r|s, a)\r
h\r
r + v⇡(s0)\r
i\r
.\r
However, this equation is the same as the previous one, except for the substitution of v⇡\r
for ve⇤. Because ve⇤ is the unique solution, it must be that v⇡ = ve⇤.\r
In essence, we have shown in the last few pages that policy iteration works for "-soft\r
policies. Using the natural notion of greedy policy for "-soft policies, one is assured of\r
improvement on every step, except when the best policy has been found among the "-soft\r
policies. This analysis is independent of how the action-value functions are determined"""

[[sections]]
number = "5.5"
title = "O↵-policy Prediction via Importance Sampling 103"
text = """
at each stage, but it does assume that they are computed exactly. This brings us to\r
roughly the same point as in the previous section. Now we only achieve the best policy\r
among the "-soft policies, but on the other hand, we have eliminated the assumption of\r
exploring starts."""

[[sections]]
number = "5.5"
title = "O↵-policy Prediction via Importance Sampling"
text = """
All learning control methods face a dilemma: They seek to learn action values conditional\r
on subsequent optimal behavior, but they need to behave non-optimally in order to\r
explore all actions (to find the optimal actions). How can they learn about the optimal\r
policy while behaving according to an exploratory policy? The on-policy approach in the\r
preceding section is actually a compromise—it learns action values not for the optimal\r
policy, but for a near-optimal policy that still explores. A more straightforward approach\r
is to use two policies, one that is learned about and that becomes the optimal policy, and\r
one that is more exploratory and is used to generate behavior. The policy being learned\r
about is called the target policy, and the policy used to generate behavior is called the\r
behavior policy. In this case we say that learning is from data “o↵” the target policy, and\r
the overall process is termed o↵-policy learning.\r
Throughout the rest of this book we consider both on-policy and o↵-policy methods.\r
On-policy methods are generally simpler and are considered first. O↵-policy methods\r
require additional concepts and notation, and because the data is due to a di↵erent policy,\r
o↵-policy methods are often of greater variance and are slower to converge. On the other\r
hand, o↵-policy methods are more powerful and general. They include on-policy methods\r
as the special case in which the target and behavior policies are the same. O↵-policy\r
methods also have a variety of additional uses in applications. For example, they can\r
often be applied to learn from data generated by a conventional non-learning controller,\r
or from a human expert. O↵-policy learning is also seen by some as key to learning\r
multi-step predictive models of the world’s dynamics (see Section 17.2; Sutton, 2009;\r
Sutton et al., 2011).\r
In this section we begin the study of o↵-policy methods by considering the prediction\r
problem, in which both target and behavior policies are fixed. That is, suppose we wish\r
to estimate v⇡ or q⇡, but all we have are episodes following another policy b, where\r
b 6= ⇡. In this case, ⇡ is the target policy, b is the behavior policy, and both policies are\r
considered fixed and given.\r
In order to use episodes from b to estimate values for ⇡, we require that every action\r
taken under ⇡ is also taken, at least occasionally, under b. That is, we require that\r
⇡(a|s) > 0 implies b(a|s) > 0. This is called the assumption of coverage. It follows\r
from coverage that b must be stochastic in states where it is not identical to ⇡. The\r
target policy ⇡, on the other hand, may be deterministic, and, in fact, this is a case\r
of particular interest in control applications. In control, the target policy is typically\r
the deterministic greedy policy with respect to the current estimate of the action-value\r
function. This policy becomes a deterministic optimal policy while the behavior policy\r
remains stochastic and more exploratory, for example, an "-greedy policy. In this section,\r
however, we consider the prediction problem, in which ⇡ is unchanging and given."""

[[sections]]
number = "104"
title = "Chapter 5: Monte Carlo Methods"
text = """
Almost all o↵-policy methods utilize importance sampling, a general technique for\r
estimating expected values under one distribution given samples from another. We apply\r
importance sampling to o↵-policy learning by weighting returns according to the relative\r
probability of their trajectories occurring under the target and behavior policies, called\r
the importance-sampling ratio. Given a starting state St, the probability of the subsequent\r
state–action trajectory, At, St+1, At+1,...,ST , occurring under any policy ⇡ is\r
Pr{At, St+1, At+1,...,ST | St, At:T 1 ⇠ ⇡}\r
= ⇡(At|St)p(St+1 |St, At)⇡(At+1|St+1)··· p(ST |ST 1, AT 1)\r
=\r
T\r
Y1\r
k=t\r
⇡(Ak|Sk)p(Sk+1 |Sk, Ak),\r
where p here is the state-transition probability function defined by (3.4). Thus, the relative\r
probability of the trajectory under the target and behavior policies (the importance\u0002sampling ratio) is\r
⇢t:T 1\r
.\r
=\r
QT 1\r
k=t ⇡(Ak|Sk)p(Sk+1 |Sk, Ak)\r
QT 1\r
k=t b(Ak|Sk)p(Sk+1 |Sk, Ak) =\r
T\r
Y1\r
k=t\r
⇡(Ak|Sk)\r
b(Ak|Sk)\r
. (5.3)\r
Although the trajectory probabilities depend on the MDP’s transition probabilities, which\r
are generally unknown, they appear identically in both the numerator and denominator,\r
and thus cancel. The importance sampling ratio ends up depending only on the two\r
policies and the sequence, not on the MDP.\r
Recall that we wish to estimate the expected returns (values) under the target policy,\r
but all we have are returns Gt due to the behavior policy. These returns have the wrong\r
expectation E[Gt|St =s] = vb(s) and so cannot be averaged to obtain v⇡. This is where\r
importance sampling comes in. The ratio ⇢t:T 1 transforms the returns to have the right\r
expected value:\r
E[⇢t:T 1Gt | St =s] = v⇡(s). (5.4)\r
Now we are ready to give a Monte Carlo algorithm that averages returns from a batch\r
of observed episodes following policy b to estimate v⇡(s). It is convenient here to number\r
time steps in a way that increases across episode boundaries. That is, if the first episode\r
of the batch ends in a terminal state at time 100, then the next episode begins at time\r
t = 101. This enables us to use time-step numbers to refer to particular steps in particular\r
episodes. In particular, we can define the set of all time steps in which state s is visited,\r
denoted T(s). This is for an every-visit method; for a first-visit method, T(s) would only\r
include time steps that were first visits to s within their episodes. Also, let T(t) denote\r
the first time of termination following time t, and Gt denote the return after t up through\r
T(t). Then {Gt}t2T(s) are the returns that pertain to state s, and ⇢t:T(t)1\r
 \r
t2T(s) are\r
the corresponding importance-sampling ratios. To estimate v⇡(s), we simply scale the\r
returns by the ratios and average the results:\r
V (s) .=\r
P\r
t2T(s) ⇢t:T(t)1Gt\r
|T(s)| . (5.5)"""

[[sections]]
number = "5.5"
title = "O↵-policy Prediction via Importance Sampling 105"
text = """
When importance sampling is done as a simple average in this way it is called ordinary\r
importance sampling.\r
An important alternative is weighted importance sampling, which uses a weighted\r
average, defined as\r
V (s) .=\r
P\r
t2T(s) ⇢t:T(t)1Gt P\r
t2T(s) ⇢t:T(t)1\r
, (5.6)\r
or zero if the denominator is zero. To understand these two varieties of importance\r
sampling, consider the estimates of their first-visit methods after observing a single return\r
from state s. In the weighted-average estimate, the ratio ⇢t:T(t)1 for the single return\r
cancels in the numerator and denominator, so that the estimate is equal to the observed\r
return independent of the ratio (assuming the ratio is nonzero). Given that this return\r
was the only one observed, this is a reasonable estimate, but its expectation is vb(s) rather\r
than v⇡(s), and in this statistical sense it is biased. In contrast, the first-visit version\r
of the ordinary importance-sampling estimator (5.5) is always v⇡(s) in expectation (it\r
is unbiased), but it can be extreme. Suppose the ratio were ten, indicating that the\r
trajectory observed is ten times as likely under the target policy as under the behavior\r
policy. In this case the ordinary importance-sampling estimate would be ten times the\r
observed return. That is, it would be quite far from the observed return even though the\r
episode’s trajectory is considered very representative of the target policy.\r
Formally, the di↵erence between the first-visit methods of the two kinds of importance\r
sampling is expressed in their biases and variances. Ordinary importance sampling is\r
unbiased whereas weighted importance sampling is biased (though the bias converges\r
asymptotically to zero). On the other hand, the variance of ordinary importance sampling\r
is in general unbounded because the variance of the ratios can be unbounded, whereas in\r
the weighted estimator the largest weight on any single return is one. In fact, assuming\r
bounded returns, the variance of the weighted importance-sampling estimator converges\r
to zero even if the variance of the ratios themselves is infinite (Precup, Sutton, and\r
Dasgupta 2001). In practice, the weighted estimator usually has dramatically lower\r
variance and is strongly preferred. Nevertheless, we will not totally abandon ordinary\r
importance sampling as it is easier to extend to the approximate methods using function\r
approximation that we explore in the second part of this book.\r
The every-visit methods for ordinary and weighed importance sampling are both biased,\r
though, again, the bias falls asymptotically to zero as the number of samples increases.\r
In practice, every-visit methods are often preferred because they remove the need to keep\r
track of which states have been visited and because they are much easier to extend to\r
approximations. A complete every-visit MC algorithm for o↵-policy policy evaluation\r
using weighted importance sampling is given in the next section on page 110.\r
Exercise 5.5 Consider an MDP with a single nonterminal state and a single action\r
that transitions back to the nonterminal state with probability p and transitions to the\r
terminal state with probability 1p. Let the reward be +1 on all transitions, and let\r
 = 1. Suppose you observe one episode that lasts 10 steps, with a return of 10. What\r
are the first-visit and every-visit estimators of the value of the nonterminal state? ⇤"""

[[sections]]
number = "106"
title = "Chapter 5: Monte Carlo Methods"
text = """
Example 5.4: O↵-policy Estimation of a Blackjack State Value We applied\r
both ordinary and weighted importance-sampling methods to estimate the value of a single\r
blackjack state (Example 5.1) from o↵-policy data. Recall that one of the advantages of\r
Monte Carlo methods is that they can be used to evaluate a single state without forming\r
estimates for any other states. In this example, we evaluated the state in which the dealer\r
is showing a deuce, the sum of the player’s cards is 13, and the player has a usable ace (that\r
is, the player holds an ace and a deuce, or equivalently three aces). The data was generated\r
by starting in this state then choosing to hit or stick at random with equal probability\r
(the behavior policy). The target policy was to stick only on a sum of 20 or 21, as in\r
Example 5.1. The value of this state under the target policy is approximately 0.27726\r
(this was determined by separately generating one-hundred million episodes using the\r
target policy and averaging their returns). Both o↵-policy methods closely approximated\r
this value after 1000 o↵-policy episodes using the random policy. To make sure they did\r
this reliably, we performed 100 independent runs, each starting from estimates of zero\r
and learning for 10,000 episodes. Figure 5.3 shows the resultant learning curves—the\r
squared error of the estimates of each method as a function of number of episodes,\r
averaged over the 100 runs. The error approaches zero for both algorithms, but the\r
weighted importance-sampling method has much lower error at the beginning, as is typical\r
in practice.\r
Ordinary \r
importance \r
sampling\r
Weighted importance sampling\r
Episodes (log scale)\r
0 10 100 1000 10,000\r
Mean\r
square\r
error\r
(average over\r
100 runs)\r
0\r
5\r
Figure 5.3: Weighted importance sampling produces lower error estimates of the value of a\r
single blackjack state from o↵-policy episodes.\r
Example 5.5: Infinite Variance The estimates of ordinary importance sampling will\r
typically have infinite variance, and thus unsatisfactory convergence properties, whenever\r
the scaled returns have infinite variance—and this can easily happen in o↵-policy learning\r
when trajectories contain loops. A simple example is shown inset in Figure 5.4. There is\r
only one nonterminal state s and two actions, right and left. The right action causes a\r
deterministic transition to termination, whereas the left action transitions, with probability\r
0.9, back to s or, with probability 0.1, on to termination. The rewards are +1 on the\r
latter transition and otherwise zero. Consider the target policy that always selects left.\r
All episodes under this policy consist of some number (possibly zero) of transitions back"""

[[sections]]
number = "5.5"
title = "O↵-policy Prediction via Importance Sampling 107"
text = """
to s followed by termination with a reward and return of +1. Thus the value of s under\r
the target policy is 1 ( = 1). Suppose we are estimating this value from o↵-policy data\r
using the behavior policy that selects right and left with equal probability.\r
1\r
100,000 1,000,000 10,000,000 100,000,000\r
2\r
0.1"""

[[sections]]
number = "0.9"
title = "R = +1"
text = """
s\r
⇡(left|s)=1\r
left right\r
R = 0\r
R = 0\r
b(left|s) = 1\r
2\r
v⇡(s)\r
Monte-Carlo \r
estimate of \r
 with \r
ordinary\r
importance \r
sampling\r
(ten runs)\r
Episodes (log scale)\r
1 10 100 1000 10,000\r
0\r
Figure 5.4: Ordinary importance sampling produces surprisingly unstable estimates on the\r
one-state MDP shown inset (Example 5.5). The correct estimate here is 1 ( = 1), and, even\r
though this is the expected value of a sample return (after importance sampling), the variance\r
of the samples is infinite, and the estimates do not converge to this value. These results are for\r
o↵-policy first-visit MC.\r
The lower part of Figure 5.4 shows ten independent runs of the first-visit MC algorithm\r
using ordinary importance sampling. Even after millions of episodes, the estimates fail\r
to converge to the correct value of 1. In contrast, the weighted importance-sampling\r
algorithm would give an estimate of exactly 1 forever after the first episode that ended\r
with the left action. All returns not equal to 1 (that is, ending with the right action)\r
would be inconsistent with the target policy and thus would have a ⇢t:T(t)1 of zero and\r
contribute neither to the numerator nor denominator of (5.6). The weighted importance\u0002sampling algorithm produces a weighted average of only the returns consistent with the\r
target policy, and all of these would be exactly 1.\r
We can verify that the variance of the importance-sampling-scaled returns is infinite\r
in this example by a simple calculation. The variance of any random variable X is the\r
expected value of the deviation from its mean X¯, which can be written\r
Var[X] .= E\r
h\r
X  X¯2\r
i\r
= E\r
⇥\r
X2  2XX¯ + X¯ 2⇤= E\r
⇥\r
X2⇤ X¯ 2.\r
Thus, if the mean is finite, as it is in our case, the variance is infinite if and only if the\r
expectation of the square of the random variable is infinite. Thus, we need only show"""

[[sections]]
number = "108"
title = "Chapter 5: Monte Carlo Methods"
text = """
that the expected square of the importance-sampling-scaled return is infinite:\r
E\r
2\r
4\r
 T\r
Y1\r
t=0\r
⇡(At|St)\r
b(At|St)\r
G0\r
!23\r
5 .\r
To compute this expectation, we break it down into cases based on episode length and\r
termination. First note that, for any episode ending with the right action, the importance\r
sampling ratio is zero, because the target policy would never take this action; these\r
episodes thus contribute nothing to the expectation (the quantity in parenthesis will be\r
zero) and can be ignored. We need only consider episodes that involve some number\r
(possibly zero) of left actions that transition back to the nonterminal state, followed by a\r
left action transitioning to termination. All of these episodes have a return of 1, so the\r
G0 factor can be ignored. To get the expected square we need only consider each length\r
of episode, multiplying the probability of the episode’s occurrence by the square of its\r
importance-sampling ratio, and add these up:\r
= 1\r
2 · 0.1\r
✓ 1\r
0.5\r
◆2\r
(the length 1 episode)\r
+\r
1\r
2 · 0.9 ·\r
1\r
2 · 0.1\r
✓ 1\r
0.5\r
1\r
0.5\r
◆2\r
(the length 2 episode)\r
+\r
1\r
2 · 0.9 ·\r
1\r
2 · 0.9 ·\r
1\r
2 · 0.1\r
✓ 1\r
0.5\r
1\r
0.5\r
1\r
0.5\r
◆2\r
(the length 3 episode)\r
+ ···\r
= 0.1\r
X1\r
k=0\r
0.9k · 2k · 2= 0.2\r
X1\r
k=0\r
1.8k = 1.\r
Exercise 5.6 What is the equation analogous to (5.6) for action values Q(s, a) instead of\r
state values V (s), again given returns generated using b? ⇤\r
Exercise 5.7 In learning curves such as those shown in Figure 5.3 error generally decreases\r
with training, as indeed happened for the ordinary importance-sampling method. But for\r
the weighted importance-sampling method error first increased and then decreased. Why\r
do you think this happened? ⇤\r
Exercise 5.8 The results with Example 5.5 and shown in Figure 5.4 used a first-visit MC\r
method. Suppose that instead an every-visit MC method was used on the same problem.\r
Would the variance of the estimator still be infinite? Why or why not? ⇤"""

[[sections]]
number = "5.7"
title = "O↵-policy Monte Carlo Control 109"
text = ""

[[sections]]
number = "5.6"
title = "Incremental Implementation"
text = """
Monte Carlo prediction methods can be implemented incrementally, on an episode-by\u0002episode basis, using extensions of the techniques described in Chapter 2 (Section 2.4).\r
Whereas in Chapter 2 we averaged rewards, in Monte Carlo methods we average returns.\r
In all other respects exactly the same methods as used in Chapter 2 can be used for on\u0002policy Monte Carlo methods. For o↵-policy Monte Carlo methods, we need to separately\r
consider those that use ordinary importance sampling and those that use weighted\r
importance sampling.\r
In ordinary importance sampling, the returns are scaled by the importance sampling\r
ratio ⇢t:T(t)1 (5.3), then simply averaged, as in (5.5). For these methods we can again\r
use the incremental methods of Chapter 2, but using the scaled returns in place of\r
the rewards of that chapter. This leaves the case of o↵-policy methods using weighted\r
importance sampling. Here we have to form a weighted average of the returns, and a\r
slightly di↵erent incremental algorithm is required.\r
Suppose we have a sequence of returns G1, G2,...,Gn1, all starting in the same state\r
and each with a corresponding random weight Wi (e.g., Wi = ⇢ti:T(ti)1). We wish to\r
form the estimate\r
Vn\r
.\r
=\r
Pn1\r
k=1 WkGk\r
Pn1\r
k=1 Wk\r
, n  2, (5.7)\r
and keep it up-to-date as we obtain a single additional return Gn. In addition to keeping\r
track of Vn, we must maintain for each state the cumulative sum Cn of the weights given\r
to the first n returns. The update rule for Vn is\r
Vn+1\r
.\r
= Vn +\r
Wn\r
Cn\r
h\r
Gn  Vn\r
i\r
, n  1, (5.8)\r
and\r
Cn+1\r
.\r
= Cn + Wn+1,\r
where C0\r
.\r
= 0 (and V1 is arbitrary and thus need not be specified). The box on the\r
next page contains a complete episode-by-episode incremental algorithm for Monte Carlo\r
policy evaluation. The algorithm is nominally for the o↵-policy case, using weighted\r
importance sampling, but applies as well to the on-policy case just by choosing the\r
target and behavior policies as the same (in which case (⇡ = b), W is always 1). The\r
approximation Q converges to q⇡ (for all encountered state–action pairs) while actions\r
are selected according to a potentially di↵erent policy, b.\r
Exercise 5.9 Modify the algorithm for first-visit MC policy evaluation (Section 5.1) to\r
use the incremental implementation for sample averages described in Section 2.4. ⇤\r
Exercise 5.10 Derive the weighted-average update rule (5.8) from (5.7). Follow the\r
pattern of the derivation of the unweighted rule (2.3). ⇤"""

[[sections]]
number = "110"
title = "Chapter 5: Monte Carlo Methods"
text = """
O↵-policy MC prediction (policy evaluation) for estimating Q ⇡ q⇡\r
Input: an arbitrary target policy ⇡\r
Initialize, for all s 2 S, a 2 A(s):\r
Q(s, a) 2 R (arbitrarily)\r
C(s, a) 0\r
Loop forever (for each episode):\r
b any policy with coverage of ⇡\r
Generate an episode following b: S0, A0, R1,...,ST 1, AT 1, RT\r
G 0\r
W 1\r
Loop for each step of episode, t = T 1, T 2,..., 0, while W 6= 0:\r
G G + Rt+1\r
C(St, At) C(St, At) + W\r
Q(St, At) Q(St, At) + W\r
C(St,At) [G  Q(St, At)]\r
W W ⇡(At|St)\r
b(At|St)"""

[[sections]]
number = "5.7"
title = "O↵-policy Monte Carlo Control"
text = """
We are now ready to present an example of the second class of learning control methods\r
we consider in this book: o↵-policy methods. Recall that the distinguishing feature of\r
on-policy methods is that they estimate the value of a policy while using it for control.\r
In o↵-policy methods these two functions are separated. The policy used to generate\r
behavior, called the behavior policy, may in fact be unrelated to the policy that is\r
evaluated and improved, called the target policy. An advantage of this separation is\r
that the target policy may be deterministic (e.g., greedy), while the behavior policy can\r
continue to sample all possible actions.\r
O↵-policy Monte Carlo control methods use one of the techniques presented in the\r
preceding two sections. They follow the behavior policy while learning about and\r
improving the target policy. These techniques require that the behavior policy has a\r
nonzero probability of selecting all actions that might be selected by the target policy\r
(coverage). To explore all possibilities, we require that the behavior policy be soft (i.e.,\r
that it select all actions in all states with nonzero probability).\r
The box on the next page shows an o↵-policy Monte Carlo control method, based on\r
GPI and weighted importance sampling, for estimating ⇡⇤ and q⇤. The target policy\r
⇡ ⇡ ⇡⇤ is the greedy policy with respect to Q, which is an estimate of q⇡. The behavior\r
policy b can be anything, but in order to assure convergence of ⇡ to the optimal policy, an\r
infinite number of returns must be obtained for each pair of state and action. This can be\r
assured by choosing b to be "-soft. The policy ⇡ converges to optimal at all encountered\r
states even though actions are selected according to a di↵erent soft policy b, which may\r
change between or even within episodes."""

[[sections]]
number = "5.7"
title = "O↵-policy Monte Carlo Control 111"
text = """
O↵-policy MC control, for estimating ⇡ ⇡ ⇡⇤\r
Initialize, for all s 2 S, a 2 A(s):\r
Q(s, a) 2 R (arbitrarily)\r
C(s, a) 0\r
⇡(s) argmaxa Q(s, a) (with ties broken consistently)\r
Loop forever (for each episode):\r
b any soft policy\r
Generate an episode using b: S0, A0, R1,...,ST 1, AT 1, RT\r
G 0\r
W 1\r
Loop for each step of episode, t = T 1, T 2,..., 0:\r
G G + Rt+1\r
C(St, At) C(St, At) + W\r
Q(St, At) Q(St, At) + W\r
C(St,At) [G  Q(St, At)]\r
⇡(St) argmaxa Q(St, a) (with ties broken consistently)\r
If At 6= ⇡(St) then exit inner Loop (proceed to next episode)\r
W W 1\r
b(At|St)\r
A potential problem is that this method learns only from the tails of episodes, when\r
all of the remaining actions in the episode are greedy. If nongreedy actions are common,\r
then learning will be slow, particularly for states appearing in the early portions of\r
long episodes. Potentially, this could greatly slow learning. There has been insucient\r
experience with o↵-policy Monte Carlo methods to assess how serious this problem is. If\r
it is serious, the most important way to address it is probably by incorporating temporal\u0002di↵erence learning, the algorithmic idea developed in the next chapter. Alternatively, if \r
is less than 1, then the idea developed in the next section may also help significantly.\r
Exercise 5.11 In the boxed algorithm for o↵-policy MC control, you may have been\r
expecting the W update to have involved the importance-sampling ratio ⇡(At|St)\r
b(At|St) , but\r
instead it involves 1\r
b(At|St) . Why is this nevertheless correct? ⇤\r
Exercise 5.12: Racetrack (programming) Consider driving a race car around a turn\r
like those shown in Figure 5.5. You want to go as fast as possible, but not so fast as\r
to run o↵ the track. In our simplified racetrack, the car is at one of a discrete set of\r
grid positions, the cells in the diagram. The velocity is also discrete, a number of grid\r
cells moved horizontally and vertically per time step. The actions are increments to the\r
velocity components. Each may be changed by +1, 1, or 0 in each step, for a total of\r
nine (3 ⇥ 3) actions. Both velocity components are restricted to be nonnegative and less\r
than 5, and they cannot both be zero except at the starting line. Each episode begins\r
in one of the randomly selected start states with both velocity components zero and\r
ends when the car crosses the finish line. The rewards are 1 for each step until the car\r
crosses the finish line. If the car hits the track boundary, it is moved back to a random\r
position on the starting line, both velocity components are reduced to zero, and the"""

[[sections]]
number = "112"
title = "Chapter 5: Monte Carlo Methods"
text = """
Starting line\r
Finish\r
line\r
Starting line\r
Finish\r
line\r
Figure 5.5: A couple of right turns for the racetrack task.\r
episode continues. Before updating the car’s location at each time step, check to see if\r
the projected path of the car intersects the track boundary. If it intersects the finish line,\r
the episode ends; if it intersects anywhere else, the car is considered to have hit the track\r
boundary and is sent back to the starting line. To make the task more challenging, with\r
probability 0.1 at each time step the velocity increments are both zero, independently of\r
the intended increments. Apply a Monte Carlo control method to this task to compute\r
the optimal policy from each starting state. Exhibit several trajectories following the\r
optimal policy (but turn the noise o↵ for these trajectories). ⇤\r
5.8 *Discounting-aware Importance Sampling\r
The o↵-policy methods that we have considered so far are based on forming importance\u0002sampling weights for returns considered as unitary wholes, without taking into account\r
the returns’ internal structures as sums of discounted rewards. We now briefly consider\r
cutting-edge research ideas for using this structure to significantly reduce the variance of\r
o↵-policy estimators.\r
For example, consider the case where episodes are long and  is significantly less than\r
1. For concreteness, say that episodes last 100 steps and that  = 0. The return from\r
time 0 will then be just G0 = R1, but its importance sampling ratio will be a product of\r
100 factors, ⇡(A0|S0)\r
b(A0|S0)\r
⇡(A1|S1)\r
b(A1|S1) ··· ⇡(A99|S99)b(A99|S99) . In ordinary importance sampling, the return\r
will be scaled by the entire product, but it is really only necessary to scale by the first\r
factor, by ⇡(A0|S0)\r
b(A0|S0) . The other 99 factors ⇡(A1|S1)b(A1|S1) ··· ⇡(A99|S99)b(A99|S99) are irrelevant because\r
after the first reward the return has already been determined. These later factors are\r
all independent of the return and of expected value 1; they do not change the expected\r
update, but they add enormously to its variance. In some cases they could even make the\r
variance infinite. Let us now consider an idea for avoiding this large extraneous variance."""

[[sections]]
number = "5.9"
title = "Per-decision Importance Sampling 113"
text = """
The essence of the idea is to think of discounting as determining a probability of\r
termination or, equivalently, a degree of partial termination. For any  2 [0, 1), we can\r
think of the return G0 as partly terminating in one step, to the degree 1  , producing\r
a return of just the first reward, R1, and as partly terminating after two steps, to the\r
degree (1  ), producing a return of R1 + R2, and so on. The latter degree corresponds\r
to terminating on the second step, 1  , and not having already terminated on the\r
first step, . The degree of termination on the third step is thus (1  )2, with the 2\r
reflecting that termination did not occur on either of the first two steps. The partial\r
returns here are called flat partial returns:\r
G¯t:h\r
.\r
= Rt+1 + Rt+2 + ··· + Rh, 0  t<h  T,\r
where “flat” denotes the absence of discounting, and “partial” denotes that these returns\r
do not extend all the way to termination but instead stop at h, called the horizon (and T\r
is the time of termination of the episode). The conventional full return Gt can be viewed\r
as a sum of flat partial returns as suggested above as follows:\r
Gt\r
.\r
= Rt+1 + Rt+2 + 2Rt+3 + ··· + T t1RT\r
= (1  )Rt+1\r
+ (1  ) (Rt+1 + Rt+2)\r
+ (1  )2 (Rt+1 + Rt+2 + Rt+3)\r
.\r
.\r
.\r
+ (1  )T t2 (Rt+1 + Rt+2 + ··· + RT 1)\r
+ T t1 (Rt+1 + Rt+2 + ··· + RT )\r
= (1  )\r
T\r
X1\r
h=t+1\r
ht1G¯t:h + T t1G¯t:T .\r
Now we need to scale the flat partial returns by an importance sampling ratio that\r
is similarly truncated. As G¯t:h only involves rewards up to a horizon h, we only need\r
the ratio of the probabilities up to h  1. We define an ordinary importance-sampling\r
estimator, analogous to (5.5), as\r
V (s) .=\r
P\r
t2T(s)\r
⇣\r
(1  )\r
PT(t)1\r
h=t+1 ht1⇢t:h1G¯t:h + T(t)t1⇢t:T(t)1G¯t:T(t)\r
⌘\r
|T(s)| , (5.9)\r
and a weighted importance-sampling estimator, analogous to (5.6), as\r
V (s) .=\r
P\r
t2T(s)\r
⇣\r
(1  )\r
PT(t)1\r
h=t+1 ht1⇢t:h1G¯t:h + T(t)t1⇢t:T(t)1G¯t:T(t)\r
⌘\r
P\r
t2T(s)\r
⇣\r
(1  )\r
PT(t)1\r
h=t+1 ht1⇢t:h1 + T(t)t1⇢t:T(t)1\r
⌘ . (5.10)\r
We call these two estimators discounting-aware importance sampling estimators. They\r
take into account the discount rate but have no e↵ect (are the same as the o↵-policy\r
estimators from Section 5.5) if  = 1."""

[[sections]]
number = "114"
title = "Chapter 5: Monte Carlo Methods"
text = """
5.9 *Per-decision Importance Sampling\r
There is one more way in which the structure of the return as a sum of rewards can be\r
taken into account in o↵-policy importance sampling, a way that may be able to reduce\r
variance even in the absence of discounting (that is, even if  = 1). In the o↵-policy\r
estimators (5.5) and (5.6), each term of the sum in the numerator is itself a sum:\r
⇢t:T 1Gt = ⇢t:T 1\r
\r
Rt+1 + Rt+2 + ··· + T t1RT\r
\r
= ⇢t:T 1Rt+1 + ⇢t:T 1Rt+2 + ··· + T t1⇢t:T 1RT . (5.11)\r
The o↵-policy estimators rely on the expected values of these terms, which can be written\r
in a simpler way. Note that each sub-term of (5.11) is a product of a random reward and\r
a random importance-sampling ratio. For example, the first sub-term can be written,\r
using (5.3), as\r
⇢t:T 1Rt+1 = ⇡(At|St)\r
b(At|St)\r
⇡(At+1|St+1)\r
b(At+1|St+1)\r
⇡(At+2|St+2)\r
b(At+2|St+2) ··· ⇡(AT 1|ST 1)b(AT 1|ST 1)\r
Rt+1. (5.12)\r
Of all these factors, one might suspect that only the first and the last (the reward)\r
are related; all the others are for events that occurred after the reward. Moreover, the\r
expected value of all these other factors is one:\r
E\r
\r
⇡(Ak|Sk)\r
b(Ak|Sk)\r
 .\r
= X\r
a\r
b(a|Sk)\r
⇡(a|Sk)\r
b(a|Sk) = X\r
a\r
⇡(a|Sk)=1. (5.13)\r
With a few more steps, one can show that, as suspected, all of these other factors have\r
no e↵ect in expectation, in other words, that\r
E[⇢t:T 1Rt+1] = E[⇢t:tRt+1] . (5.14)\r
If we repeat this process for the kth sub-term of (5.11), we get\r
E[⇢t:T 1Rt+k] = E[⇢t:t+k1Rt+k] .\r
It follows then that the expectation of our original term (5.11) can be written\r
E[⇢t:T 1Gt] = E\r
h\r
G˜t\r
i\r
,\r
where\r
G˜t = ⇢t:tRt+1 + ⇢t:t+1Rt+2 + 2⇢t:t+2Rt+3 + ··· + T t1⇢t:T 1RT .\r
We call this idea per-decision importance sampling."""

[[sections]]
number = "5.10"
title = "Summary 115"
text = """
It follows immediately that there is an alternate importance-sampling estimator, with\r
the same unbiased expectation (in the first-visit case) as the ordinary-importance-sampling\r
estimator (5.5), using G˜t:\r
V (s) .=\r
P\r
t2T(s) G˜t\r
|T(s)| , (5.15)\r
which we might expect to sometimes be of lower variance.\r
Is there a per-decision version of weighted importance sampling? This is less clear. So\r
far, all the estimators that have been proposed for this that we know of are not consistent\r
(that is, they do not converge to the true value with infinite data).\r
⇤\r
Exercise 5.13 Show the steps to derive (5.14) from (5.12). ⇤\r
⇤\r
Exercise 5.14 Modify the algorithm for o↵-policy Monte Carlo control (page 111) to use\r
the idea of the truncated weighted-average estimator (5.10). Note that you will first need\r
to convert this equation to action values. ⇤"""

[[sections]]
number = "5.10"
title = "Summary"
text = """
The Monte Carlo methods presented in this chapter learn value functions and optimal\r
policies from experience in the form of sample episodes. This gives them at least three\r
kinds of advantages over DP methods. First, they can be used to learn optimal behavior\r
directly from interaction with the environment, with no model of the environment’s\r
dynamics. Second, they can be used with simulation or sample models. For surprisingly\r
many applications it is easy to simulate sample episodes even though it is dicult to\r
construct the kind of explicit model of transition probabilities required by DP methods.\r
Third, it is easy and ecient to focus Monte Carlo methods on a small subset of the states.\r
A region of special interest can be accurately evaluated without going to the expense of\r
accurately evaluating the rest of the state set (we explore this further in Chapter 8).\r
A fourth advantage of Monte Carlo methods, which we discuss later in the book, is\r
that they may be less harmed by violations of the Markov property. This is because they\r
do not update their value estimates on the basis of the value estimates of successor states.\r
In other words, it is because they do not bootstrap.\r
In designing Monte Carlo control methods we have followed the overall schema of\r
generalized policy iteration (GPI) introduced in Chapter 4. GPI involves interacting\r
processes of policy evaluation and policy improvement. Monte Carlo methods provide an\r
alternative policy evaluation process. Rather than use a model to compute the value of\r
each state, they simply average many returns that start in the state. Because a state’s\r
value is the expected return, this average can become a good approximation to the\r
value. In control methods we are particularly interested in approximating action-value\r
functions, because these can be used to improve the policy without requiring a model of\r
the environment’s transition dynamics. Monte Carlo methods intermix policy evaluation\r
and policy improvement steps on an episode-by-episode basis, and can be incrementally\r
implemented on an episode-by-episode basis."""

[[sections]]
number = "116"
title = "Chapter 5: Monte Carlo Methods"
text = """
Maintaining sucient exploration is an issue in Monte Carlo control methods. It is\r
not enough just to select the actions currently estimated to be best, because then no\r
returns will be obtained for alternative actions, and it may never be learned that they\r
are actually better. One approach is to ignore this problem by assuming that episodes\r
begin with state–action pairs randomly selected to cover all possibilities. Such exploring\r
starts can sometimes be arranged in applications with simulated episodes, but are unlikely\r
in learning from real experience. In on-policy methods, the agent commits to always\r
exploring and tries to find the best policy that still explores. In o↵-policy methods, the\r
agent also explores, but learns a deterministic optimal policy that may be unrelated to\r
the policy followed.\r
O↵-policy prediction refers to learning the value function of a target policy from data\r
generated by a di↵erent behavior policy. Such learning methods are based on some form\r
of importance sampling, that is, on weighting returns by the ratio of the probabilities of\r
taking the observed actions under the two policies, thereby transforming their expectations\r
from the behavior policy to the target policy. Ordinary importance sampling uses a\r
simple average of the weighted returns, whereas weighted importance sampling uses a\r
weighted average. Ordinary importance sampling produces unbiased estimates, but has\r
larger, possibly infinite, variance, whereas weighted importance sampling always has\r
finite variance and is preferred in practice. Despite their conceptual simplicity, o↵-policy\r
Monte Carlo methods for both prediction and control remain unsettled and are a subject\r
of ongoing research.\r
The Monte Carlo methods treated in this chapter di↵er from the DP methods treated\r
in the previous chapter in two major ways. First, they operate on sample experience,\r
and thus can be used for direct learning without a model. Second, they do not bootstrap.\r
That is, they do not update their value estimates on the basis of other value estimates.\r
These two di↵erences are not tightly linked, and can be separated. In the next chapter\r
we consider methods that learn from experience, like Monte Carlo methods, but also\r
bootstrap, like DP methods.\r
Bibliographical and Historical Remarks\r
The term “Monte Carlo” dates from the 1940s, when physicists at Los Alamos devised\r
games of chance that they could study to help understand complex physical phenomena\r
relating to the atom bomb. Coverage of Monte Carlo methods in this sense can be found\r
in several textbooks (e.g., Kalos and Whitlock, 1986; Rubinstein, 1981).\r
5.1–2 Singh and Sutton (1996) distinguished between every-visit and first-visit MC\r
methods and proved results relating these methods to reinforcement learning\r
algorithms. The blackjack example is based on an example used by Widrow,\r
Gupta, and Maitra (1973). The soap bubble example is a classical Dirichlet\r
problem whose Monte Carlo solution was first proposed by Kakutani (1945; see\r
Hersh and Griego, 1969; Doyle and Snell, 1984).\r
Barto and Du↵ (1994) discussed policy evaluation in the context of classical\r
Monte Carlo algorithms for solving systems of linear equations. They used the"""

[[sections]]
number = "5.10"
title = "Summary 117"
text = """
analysis of Curtiss (1954) to point out the computational advantages of Monte\r
Carlo policy evaluation for large problems.\r
5.3–4 Monte Carlo ES was introduced in the 1998 edition of this book. That may have\r
been the first explicit connection between Monte Carlo estimation and control\r
methods based on policy iteration. An early use of Monte Carlo methods to\r
estimate action values in a reinforcement learning context was by Michie and\r
Chambers (1968). In pole balancing (page 56), they used averages of episode\r
durations to assess the worth (expected balancing “life”) of each possible action\r
in each state, and then used these assessments to control action selections. Their\r
method is similar in spirit to Monte Carlo ES with every-visit MC estimates.\r
Narendra and Wheeler (1986) studied a Monte Carlo method for ergodic finite\r
Markov chains that used the return accumulated between successive visits to the\r
same state as a reward for adjusting a learning automaton’s action probabilities."""

[[sections]]
number = "5.5"
title = "Ecient o↵-policy learning has become recognized as an important challenge"
text = """
that arises in several fields. For example, it is closely related to the idea of\r
“interventions” and “counterfactuals” in probabilistic graphical (Bayesian) models\r
(e.g., Pearl, 1995; Balke and Pearl, 1994). O↵-policy methods using importance\r
sampling have a long history and yet still are not well understood. Weighted\r
importance sampling, which is also sometimes called normalized importance\r
sampling (e.g., Koller and Friedman, 2009), is discussed by Rubinstein (1981),\r
Hesterberg (1988), Shelton (2001), and Liu (2001) among others.\r
The target policy in o↵-policy learning is sometimes referred to in the literature\r
as the “estimation” policy, as it was in the first edition of this book."""

[[sections]]
number = "5.7"
title = "The racetrack exercise is adapted from Barto, Bradtke, and Singh (1995), and"
text = "from Gardner (1973)."

[[sections]]
number = "5.8"
title = "Our treatment of the idea of discounting-aware importance sampling is based on"
text = """
the analysis of Sutton, Mahmood, Precup, and van Hasselt (2014). It has been\r
worked out most fully to date by Mahmood (2017; Mahmood, van Hasselt, and\r
Sutton, 2014)."""

[[sections]]
number = "5.9"
title = "Per-decision importance sampling was introduced by Precup, Sutton, and Singh"
text = """
(2000). They also combined o↵-policy learning with temporal-di↵erence learning,\r
eligibility traces, and approximation methods, introducing subtle issues that we\r
consider in later chapters.\r
Exercise 5.15 Make new equations analogous to the importance-sampling Monte Carlo\r
estimates (5.5) and (5.6), but for action value estimates Q(s, a). You will need new\r
notation T(s, a) for the time steps on which the state–action pair s, a is visited on the\r
episode. Do these estimates involve more or less importance-sampling correction?

Chapter 6\r
Temporal-Di↵erence Learning\r
If one had to identify one idea as central and novel to reinforcement learning, it would\r
undoubtedly be temporal-di↵erence (TD) learning. TD learning is a combination of\r
Monte Carlo ideas and dynamic programming (DP) ideas. Like Monte Carlo methods,\r
TD methods can learn directly from raw experience without a model of the environment’s\r
dynamics. Like DP, TD methods update estimates based in part on other learned\r
estimates, without waiting for a final outcome (they bootstrap). The relationship between\r
TD, DP, and Monte Carlo methods is a recurring theme in the theory of reinforcement\r
learning; this chapter is the beginning of our exploration of it. Before we are done, we\r
will see that these ideas and methods blend into each other and can be combined in many\r
ways. In particular, in Chapter 7 we introduce n-step algorithms, which provide a bridge\r
from TD to Monte Carlo methods, and in Chapter 12 we introduce the TD() algorithm,\r
which seamlessly unifies them.\r
As usual, we start by focusing on the policy evaluation or prediction problem, the\r
problem of estimating the value function v⇡ for a given policy ⇡. For the control problem\r
(finding an optimal policy), DP, TD, and Monte Carlo methods all use some variation of\r
generalized policy iteration (GPI). The di↵erences in the methods are primarily di↵erences\r
in their approaches to the prediction problem."""

[[sections]]
number = "6.1"
title = "TD Prediction"
text = """
Both TD and Monte Carlo methods use experience to solve the prediction problem. Given\r
some experience following a policy ⇡, both methods update their estimate V of v⇡ for\r
the nonterminal states St occurring in that experience. Roughly speaking, Monte Carlo\r
methods wait until the return following the visit is known, then use that return as a\r
target for V (St). A simple every-visit Monte Carlo method suitable for nonstationary\r
environments is\r
V (St) V (St) + ↵\r
h\r
Gt  V (St)\r
i\r
, (6.1)"""

[[sections]]
number = "120"
title = "Chapter 6: Temporal-Di↵erence Learning"
text = """
where Gt is the actual return following time t, and ↵ is a constant step-size parameter (c.f.,\r
Equation 2.4). Let us call this method constant-↵ MC. Whereas Monte Carlo methods\r
must wait until the end of the episode to determine the increment to V (St) (only then is\r
Gt known), TD methods need to wait only until the next time step. At time t + 1 they\r
immediately form a target and make a useful update using the observed reward Rt+1 and\r
the estimate V (St+1). The simplest TD method makes the update\r
V (St) V (St) + ↵\r
h\r
Rt+1 + V (St+1)  V (St)\r
i\r
(6.2)\r
immediately on transition to St+1 and receiving Rt+1. In e↵ect, the target for the Monte\r
Carlo update is Gt, whereas the target for the TD update is Rt+1 + V (St+1). This TD\r
method is called TD(0), or one-step TD, because it is a special case of the TD() and\r
n-step TD methods developed in Chapter 12 and Chapter 7. The box below specifies\r
TD(0) completely in procedural form.\r
Tabular TD(0) for estimating v⇡\r
Input: the policy ⇡ to be evaluated\r
Algorithm parameter: step size ↵ 2 (0, 1]\r
Initialize V (s), for all s 2 S+, arbitrarily except that V (terminal)=0\r
Loop for each episode:\r
Initialize S\r
Loop for each step of episode:\r
A action given by ⇡ for S\r
Take action A, observe R, S0\r
V (S) V (S) + ↵\r
⇥\r
R + V (S0)  V (S)\r
⇤\r
S S0\r
until S is terminal\r
Because TD(0) bases its update in part on an existing estimate, we say that it is a\r
bootstrapping method, like DP. We know from Chapter 3 that\r
v⇡(s) .= E⇡[Gt | St =s] (6.3)\r
= E⇡[Rt+1 + Gt+1 | St =s] (from (3.9))\r
= E⇡[Rt+1 + v⇡(St+1) | St =s] . (6.4)\r
Roughly speaking, Monte Carlo methods use an estimate of (6.3) as a target, whereas\r
DP methods use an estimate of (6.4) as a target. The Monte Carlo target is an estimate\r
because the expected value in (6.3) is not known; a sample return is used in place of the\r
real expected return. The DP target is an estimate not because of the expected values,\r
which are assumed to be completely provided by a model of the environment, but because\r
v⇡(St+1) is not known and the current estimate, V (St+1), is used instead. The TD target\r
is an estimate for both reasons: it samples the expected values in (6.4) and it uses the\r
current estimate V instead of the true v⇡. Thus, TD methods combine the sampling of"""

[[sections]]
number = "6.1"
title = "TD Prediction 121"
text = """
Monte Carlo with the bootstrapping of DP. As we shall see, with care and imagination\r
this can take us a long way toward obtaining the advantages of both Monte Carlo and\r
DP methods.\r
TD(0)\r
Shown to the right is the backup diagram for tabular TD(0). The value\r
estimate for the state node at the top of the backup diagram is updated on\r
the basis of the one sample transition from it to the immediately following\r
state. We refer to TD and Monte Carlo updates as sample updates because\r
they involve looking ahead to a sample successor state (or state–action pair),\r
using the value of the successor and the reward along the way to compute a\r
backed-up value, and then updating the value of the original state (or state–\r
action pair) accordingly. Sample updates di↵er from the expected updates\r
of DP methods in that they are based on a single sample successor rather than on a\r
complete distribution of all possible successors.\r
Finally, note that the quantity in brackets in the TD(0) update is a sort of error,\r
measuring the di↵erence between the estimated value of St and the better estimate\r
Rt+1 + V (St+1). This quantity, called the TD error, arises in various forms throughout\r
reinforcement learning:\r
t\r
.\r
= Rt+1 + V (St+1)  V (St). (6.5)\r
Notice that the TD error at each time is the error in the estimate made at that time.\r
Because the TD error depends on the next state and next reward, it is not actually\r
available until one time step later. That is, t is the error in V (St), available at time\r
t + 1. Also note that if the array V does not change during the episode (as it does not in\r
Monte Carlo methods), then the Monte Carlo error can be written as a sum of TD errors:\r
Gt  V (St) = Rt+1 + Gt+1  V (St) + V (St+1)  V (St+1) (from (3.9))\r
= t + \r
\r
Gt+1  V (St+1)\r
\r
= t + t+1 + 2Gt+2  V (St+2)\r
\r
= t + t+1 + 2t+2 + ··· + T t1T 1 + T t\r
\r
GT  V (ST )\r
\r
= t + t+1 + 2t+2 + ··· + T t1T 1 + T t\r
\r
0  0\r
\r
=\r
T\r
X1\r
k=t\r
ktk. (6.6)\r
This identity is not exact if V is updated during the episode (as it is in TD(0)), but if the\r
step size is small then it may still hold approximately. Generalizations of this identity\r
play an important role in the theory and algorithms of temporal-di↵erence learning.\r
Exercise 6.1 If V changes during the episode, then (6.6) only holds approximately; what\r
would the di↵erence be between the two sides? Let Vt denote the array of state values\r
used at time t in the TD error (6.5) and in the TD update (6.2). Redo the derivation\r
above to determine the additional amount that must be added to the sum of TD errors\r
in order to equal the Monte Carlo error. ⇤"""

[[sections]]
number = "122"
title = "Chapter 6: Temporal-Di↵erence Learning"
text = """
Example 6.1: Driving Home Each day as you drive home from work, you try to\r
predict how long it will take to get home. When you leave your oce, you note the time,\r
the day of week, the weather, and anything else that might be relevant. Say on this\r
Friday you are leaving at exactly 6 o’clock, and you estimate that it will take 30 minutes\r
to get home. As you reach your car it is 6:05, and you notice it is starting to rain. Trac\r
is often slower in the rain, so you reestimate that it will take 35 minutes from then, or a\r
total of 40 minutes. Fifteen minutes later you have completed the highway portion of\r
your journey in good time. As you exit onto a secondary road you cut your estimate of\r
total travel time to 35 minutes. Unfortunately, at this point you get stuck behind a slow\r
truck, and the road is too narrow to pass. You end up having to follow the truck until\r
you turn onto the side street where you live at 6:40. Three minutes later you are home.\r
The sequence of states, times, and predictions is thus as follows:\r
Elapsed Time Predicted Predicted\r
State (minutes) Time to Go Total Time\r
leaving oce, friday at 6 0 30 30\r
reach car, raining 5 35 40\r
exiting highway 20 15 35\r
2ndary road, behind truck 30 10 40\r
entering home street 40 3 43\r
arrive home 43 0 43\r
The rewards in this example are the elapsed times on each leg of the journey.1 We are\r
not discounting ( = 1), and thus the return for each state is the actual time to go from\r
that state. The value of each state is the expected time to go. The second column of\r
numbers gives the current estimated value for each state encountered.\r
A simple way to view the operation of Monte Carlo methods is to plot the predicted\r
total time (the last column) over the sequence, as in Figure 6.1 (left). The red arrows\r
show the changes in predictions recommended by the constant-↵ MC method (6.1), for\r
↵ = 1. These are exactly the errors between the estimated value (predicted time to go)\r
in each state and the actual return (actual time to go). For example, when you exited\r
the highway you thought it would take only 15 minutes more to get home, but in fact it\r
took 23 minutes. Equation 6.1 applies at this point and determines an increment in the\r
estimate of time to go after exiting the highway. The error, Gt  V (St), at this time is\r
eight minutes. Suppose the step-size parameter, ↵, is 1/2. Then the predicted time to go\r
after exiting the highway would be revised upward by four minutes as a result of this\r
experience. This is probably too large a change in this case; the truck was probably just\r
an unlucky break. In any event, the change can only be made o↵-line, that is, after you\r
have reached home. Only at this point do you know any of the actual returns.\r
Is it necessary to wait until the final outcome is known before learning can begin?\r
Suppose on another day you again estimate when leaving your oce that it will take 30\r
minutes to drive home, but then you become stuck in a massive trac jam. Twenty-five\r
minutes after leaving the oce you are still bumper-to-bumper on the highway. You now\r
1If this were a control problem with the objective of minimizing travel time, then we would of course\r
make the rewards the negative of the elapsed time. But because we are concerned here only with\r
prediction (policy evaluation), we can keep things simple by using positive numbers."""

[[sections]]
number = "6.1"
title = "TD Prediction 123"
text = """
road\r
30\r
35\r
40"""

[[sections]]
number = "45"
title = "Predicted"
text = """
total\r
travel\r
time\r
leaving\r
office\r
exiting\r
highway\r
2ndary home arrive\r
Situation\r
actual outcome\r
reach\r
car street home\r
actual\r
outcome\r
Situation\r
30\r
35\r
40"""

[[sections]]
number = "45"
title = "Predicted"
text = """
total\r
travel\r
time\r
road\r
leaving\r
office\r
exiting\r
highway\r
reach 2ndary home arrive\r
car street home\r
Figure 6.1: Changes recommended in the driving home example by Monte Carlo methods (left)\r
and TD methods (right).\r
estimate that it will take another 25 minutes to get home, for a total of 50 minutes. As\r
you wait in trac, you already know that your initial estimate of 30 minutes was too\r
optimistic. Must you wait until you get home before increasing your estimate for the\r
initial state? According to the Monte Carlo approach you must, because you don’t yet\r
know the true return.\r
According to a TD approach, on the other hand, you would learn immediately, shifting\r
your initial estimate from 30 minutes toward 50. In fact, each estimate would be shifted\r
toward the estimate that immediately follows it. Returning to our first day of driving,\r
Figure 6.1 (right) shows the changes in the predictions recommended by the TD rule\r
(6.2) (these are the changes made by the rule if ↵ = 1). Each error is proportional to the\r
change over time of the prediction, that is, to the temporal di↵erences in predictions.\r
Besides giving you something to do while waiting in trac, there are several computa\u0002tional reasons why it is advantageous to learn based on your current predictions rather\r
than waiting until termination when you know the actual return. We briefly discuss some\r
of these in the next section.\r
Exercise 6.2 This is an exercise to help develop your intuition about why TD methods\r
are often more ecient than Monte Carlo methods. Consider the driving home example\r
and how it is addressed by TD and Monte Carlo methods. Can you imagine a scenario\r
in which a TD update would be better on average than a Monte Carlo update? Give\r
an example scenario—a description of past experience and a current state—in which\r
you would expect the TD update to be better. Here’s a hint: Suppose you have lots\r
of experience driving home from work. Then you move to a new building and a new\r
parking lot (but you still enter the highway at the same place). Now you are starting\r
to learn predictions for the new building. Can you see why TD updates are likely to be\r
much better, at least initially, in this case? Might the same sort of thing happen in the\r
original scenario? ⇤"""

[[sections]]
number = "124"
title = "Chapter 6: Temporal-Di↵erence Learning"
text = ""

[[sections]]
number = "6.2"
title = "Advantages of TD Prediction Methods"
text = """
TD methods update their estimates based in part on other estimates. They learn a\r
guess from a guess—they bootstrap. Is this a good thing to do? What advantages do\r
TD methods have over Monte Carlo and DP methods? Developing and answering such\r
questions will take the rest of this book and more. In this section we briefly anticipate\r
some of the answers.\r
Obviously, TD methods have an advantage over DP methods in that they do not\r
require a model of the environment, of its reward and next-state probability distributions.\r
The next most obvious advantage of TD methods over Monte Carlo methods is that\r
they are naturally implemented in an online, fully incremental fashion. With Monte\r
Carlo methods one must wait until the end of an episode, because only then is the return\r
known, whereas with TD methods one need wait only one time step. Surprisingly often\r
this turns out to be a critical consideration. Some applications have very long episodes, so\r
that delaying all learning until the end of the episode is too slow. Other applications are\r
continuing tasks and have no episodes at all. Finally, as we noted in the previous chapter,\r
some Monte Carlo methods must ignore or discount episodes on which experimental\r
actions are taken, which can greatly slow learning. TD methods are much less susceptible\r
to these problems because they learn from each transition regardless of what subsequent\r
actions are taken.\r
But are TD methods sound? Certainly it is convenient to learn one guess from the\r
next, without waiting for an actual outcome, but can we still guarantee convergence\r
to the correct answer? Happily, the answer is yes. For any fixed policy ⇡, TD(0) has\r
been proved to converge to v⇡, in the mean for a constant step-size parameter if it is\r
suciently small, and with probability 1 if the step-size parameter decreases according to\r
the usual stochastic approximation conditions (2.7). Most convergence proofs apply only\r
to the table-based case of the algorithm presented above (6.2), but some also apply to\r
the case of general linear function approximation. These results are discussed in a more\r
general setting in Section 9.4.\r
If both TD and Monte Carlo methods converge asymptotically to the correct predictions,\r
then a natural next question is “Which gets there first?” In other words, which method\r
learns faster? Which makes the more ecient use of limited data? At the current time\r
this is an open question in the sense that no one has been able to prove mathematically\r
that one method converges faster than the other. In fact, it is not even clear what is the\r
most appropriate formal way to phrase this question! In practice, however, TD methods\r
have usually been found to converge faster than constant-↵ MC methods on stochastic\r
tasks, as illustrated in Example 6.2."""

[[sections]]
number = "6.2"
title = "Advantages of TD Prediction Methods 125"
text = """
Example 6.2 Random Walk\r
In this example we empirically compare the prediction abilities of TD(0) and\r
constant-↵ MC when applied to the following Markov reward process:\r
A B CDE\r
0 0 0 0 0 1\r
start\r
A Markov reward process, or MRP, is a Markov decision process without actions.\r
We will often use MRPs when focusing on the prediction problem, in which there is\r
no need to distinguish the dynamics due to the environment from those due to the\r
agent. In this MRP, all episodes start in the center state, C, then proceed either left\r
or right by one state on each step, with equal probability. Episodes terminate either\r
on the extreme left or the extreme right. When an episode terminates on the right,\r
a reward of +1 occurs; all other rewards are zero. For example, a typical episode\r
might consist of the following state-and-reward sequence: C, 0,B, 0, C, 0, D, 0, E, 1.\r
Because this task is undiscounted, the true value of each state is the probability of\r
terminating on the right if starting from that state. Thus, the true value of the\r
center state is v⇡(C)=0.5. The true values of all the states, A through E, are\r
1\r
6 , 26 , 36 , 46 , and 56 .\r
0.8\r
0\r
0.2\r
0.4"""

[[sections]]
number = "0.6"
title = "A B C D E"
text = """
0\r
10\r
1"""

[[sections]]
number = "100"
title = "State"
text = """
Estimated\r
value\r
True \r
values\r
Estimated\r
value\r
0\r
0.05\r
0.1\r
0.15\r
0.2\r
0.25\r
0 25 50 75 100\r
Walks / Episodes\r
TD\r
MC\r
RMS error,\r
averaged\r
over states\r
_\u001B\f\u000E\u000F\r
_\u001B\f\u000F\r
_\u001B\f\u000E\u0010\r
_\u001B\f\u000E\u0011\r
_\u001B\f\u000E\u0012\r
_\u001B\f\u000F\u0013\r
_\u001B\f\u000E\u0013\r
Empirical RMS error, \r
averaged over states\r
The left graph above shows the values learned after various numbers of episodes on\r
a single run of TD(0). The estimates after 100 episodes are about as close as they\r
ever come to the true values—with a constant step-size parameter (↵ = 0.1 in this\r
example), the values fluctuate indefinitely in response to the outcomes of the most\r
recent episodes. The right graph shows learning curves for the two methods for\r
various values of ↵. The performance measure shown is the root mean square (RMS)\r
error between the value function learned and the true value function, averaged over\r
the five states, then averaged over 100 runs. In all cases the approximate value\r
function was initialized to the intermediate value V (s)=0.5, for all s. The TD\r
method was consistently better than the MC method on this task."""

[[sections]]
number = "126"
title = "Chapter 6: Temporal-Di↵erence Learning"
text = """
Exercise 6.3 From the results shown in the left graph of the random walk example it\r
appears that the first episode results in a change in only V (A). What does this tell you\r
about what happened on the first episode? Why was only the estimate for this one state\r
changed? By exactly how much was it changed? ⇤\r
Exercise 6.4 The specific results shown in the right graph of the random walk example\r
are dependent on the value of the step-size parameter, ↵. Do you think the conclusions\r
about which algorithm is better would be a↵ected if a wider range of ↵ values were used?\r
Is there a di↵erent, fixed value of ↵ at which either algorithm would have performed\r
significantly better than shown? Why or why not? ⇤\r
⇤\r
Exercise 6.5 In the right graph of the random walk example, the RMS error of the\r
TD method seems to go down and then up again, particularly at high ↵’s. What could\r
have caused this? Do you think this always occurs, or might it be a function of how the\r
approximate value function was initialized? ⇤\r
Exercise 6.6 In Example 6.2 we stated that the true values for the random walk example\r
are 1\r
6 , 26 , 36 , 46 , and 56 , for states A through E. Describe at least two di↵erent ways that\r
these could have been computed. Which would you guess we actually used? Why? ⇤"""

[[sections]]
number = "6.3"
title = "Optimality of TD(0)"
text = """
Suppose there is available only a finite amount of experience, say 10 episodes or 100\r
time steps. In this case, a common approach with incremental learning methods is to\r
present the experience repeatedly until the method converges upon an answer. Given an\r
approximate value function, V , the increments specified by (6.1) or (6.2) are computed\r
for every time step t at which a nonterminal state is visited, but the value function is\r
changed only once, by the sum of all the increments. Then all the available experience is\r
processed again with the new value function to produce a new overall increment, and so\r
on, until the value function converges. We call this batch updating because updates are\r
made only after processing each complete batch of training data.\r
Under batch updating, TD(0) converges deterministically to a single answer independent\r
of the step-size parameter, ↵, as long as ↵ is chosen to be suciently small. The constant-\r
↵ MC method also converges deterministically under the same conditions, but to a\r
di↵erent answer. Understanding these two answers will help us understand the di↵erence\r
between the two methods. Under normal updating the methods do not move all the way\r
to their respective batch answers, but in some sense they take steps in these directions.\r
Before trying to understand the two answers in general, for all possible tasks, we first\r
look at a few examples.\r
Example 6.3: Random walk under batch updating Batch-updating versions of\r
TD(0) and constant-↵ MC were applied as follows to the random walk prediction example\r
(Example 6.2). After each new episode, all episodes seen so far were treated as a batch.\r
They were repeatedly presented to the algorithm, either TD(0) or constant-↵ MC, with\r
↵ suciently small that the value function converged. The resulting value function was\r
then compared with v⇡, and the average root mean square error across the five states\r
(and across 100 independent repetitions of the whole experiment) was plotted to obtain"""

[[sections]]
number = "6.3"
title = "Optimality of TD(0) 127"
text = """
the learning curves shown in Figure 6.2. Note that the batch TD method was consistently\r
better than the batch Monte Carlo method.\r
. 0\r
.05\r
. 1\r
.15\r
. 2\r
.25\r
0 25 50 75 100\r
TD\r
MC\r
BATCH TRAINING\r
Walks / Episodes\r
RMS error,\r
averaged\r
over states\r
Figure 6.2: Performance of TD(0) and constant-↵\r
MC under batch training on the random walk task.\r
Under batch training, constant-↵\r
MC converges to values, V (s), that\r
are sample averages of the actual re\u0002turns experienced after visiting each\r
state s. These are optimal estimates\r
in the sense that they minimize the\r
mean square error from the actual\r
returns in the training set. In this\r
sense it is surprising that the batch\r
TD method was able to perform\r
better according to the root mean\r
square error measure shown in the\r
figure to the right. How is it that\r
batch TD was able to perform better\r
than this optimal method? The an\u0002swer is that the Monte Carlo method\r
is optimal only in a limited way, and\r
that TD is optimal in a way that is more relevant to predicting returns.\r
Example 6.4: You are the Predictor Place yourself now in the role of the predictor\r
of returns for an unknown Markov reward process. Suppose you observe the following\r
eight episodes:\r
A, 0, B, 0 B, 1\r
B, 1 B, 1\r
B, 1 B, 1\r
B, 1 B, 0\r
This means that the first episode started in state A, transitioned to B with a reward of\r
0, and then terminated from B with a reward of 0. The other seven episodes were even\r
shorter, starting from B and terminating immediately. Given this batch of data, what\r
would you say are the optimal predictions, the best values for the estimates V (A) and\r
V (B)? Everyone would probably agree that the optimal value for V (B) is 3\r
4 , because six\r
out of the eight times in state B the process terminated immediately with a return of 1,\r
and the other two times in B the process terminated immediately with a return of 0.\r
But what is the optimal value for the estimate V (A) given this data? Here there are\r
A B\r
r = 1\r
100%\r
75%\r
25%\r
r = 0\r
r = 0\r
two reasonable answers. One is to observe that 100% of the\r
times the process was in state A it traversed immediately to\r
B (with a reward of 0); and because we have already decided\r
that B has value 3\r
4 , therefore A must have value 34 as well.\r
One way of viewing this answer is that it is based on first\r
modeling the Markov process, in this case as shown to the\r
right, and then computing the correct estimates given the\r
model, which indeed in this case gives V (A) = 3"""

[[sections]]
number = "4"
title = "This is"
text = "also the answer that batch TD(0) gives."

[[sections]]
number = "128"
title = "Chapter 6: Temporal-Di↵erence Learning"
text = """
The other reasonable answer is simply to observe that we have seen A once and the\r
return that followed it was 0; we therefore estimate V (A) as 0. This is the answer that\r
batch Monte Carlo methods give. Notice that it is also the answer that gives minimum\r
squared error on the training data. In fact, it gives zero error on the data. But still we\r
expect the first answer to be better. If the process is Markov, we expect that the first\r
answer will produce lower error on future data, even though the Monte Carlo answer is\r
better on the existing data.\r
Example 6.4 illustrates a general di↵erence between the estimates found by batch\r
TD(0) and batch Monte Carlo methods. Batch Monte Carlo methods always find the\r
estimates that minimize mean square error on the training set, whereas batch TD(0)\r
always finds the estimates that would be exactly correct for the maximum-likelihood\r
model of the Markov process. In general, the maximum-likelihood estimate of a parameter\r
is the parameter value whose probability of generating the data is greatest. In this case,\r
the maximum-likelihood estimate is the model of the Markov process formed in the\r
obvious way from the observed episodes: the estimated transition probability from i to j\r
is the fraction of observed transitions from i that went to j, and the associated expected\r
reward is the average of the rewards observed on those transitions. Given this model,\r
we can compute the estimate of the value function that would be exactly correct if the\r
model were exactly correct. This is called the certainty-equivalence estimate because it\r
is equivalent to assuming that the estimate of the underlying process was known with\r
certainty rather than being approximated. In general, batch TD(0) converges to the\r
certainty-equivalence estimate.\r
This helps explain why TD methods converge more quickly than Monte Carlo methods.\r
In batch form, TD(0) is faster than Monte Carlo methods because it computes the\r
true certainty-equivalence estimate. This explains the advantage of TD(0) shown in the\r
batch results on the random walk task (Figure 6.2). The relationship to the certainty\u0002equivalence estimate may also explain in part the speed advantage of nonbatch TD(0)\r
(e.g., Example 6.2, page 125, right graph). Although the nonbatch methods do not achieve\r
either the certainty-equivalence or the minimum squared error estimates, they can be\r
understood as moving roughly in these directions. Nonbatch TD(0) may be faster than\r
constant-↵ MC because it is moving toward a better estimate, even though it is not\r
getting all the way there. At the current time nothing more definite can be said about\r
the relative eciency of online TD and Monte Carlo methods.\r
Finally, it is worth noting that although the certainty-equivalence estimate is in some\r
sense an optimal solution, it is almost never feasible to compute it directly. If n = |S| is\r
the number of states, then just forming the maximum-likelihood estimate of the process\r
may require on the order of n2 memory, and computing the corresponding value function\r
requires on the order of n3 computational steps if done conventionally. In these terms it\r
is indeed striking that TD methods can approximate the same solution using memory\r
no more than order n and repeated computations over the training set. On tasks with\r
large state spaces, TD methods may be the only feasible way of approximating the\r
certainty-equivalence solution.\r
⇤\r
Exercise 6.7 Design an o↵-policy version of the TD(0) update that can be used with\r
arbitrary target policy ⇡ and covering behavior policy b, using at each step t the importance\r
sampling ratio ⇢t:t (5.3). ⇤"""

[[sections]]
number = "6.4"
title = "Sarsa: On-policy TD Control 129"
text = ""

[[sections]]
number = "6.4"
title = "Sarsa: On-policy TD Control"
text = """
We turn now to the use of TD prediction methods for the control problem. As usual, we\r
follow the pattern of generalized policy iteration (GPI), only this time using TD methods\r
for the evaluation or prediction part. As with Monte Carlo methods, we face the need to\r
trade o↵ exploration and exploitation, and again approaches fall into two main classes:\r
on-policy and o↵-policy. In this section we present an on-policy TD control method.\r
The first step is to learn an action-value function rather than a state-value function.\r
In particular, for an on-policy method we must estimate q⇡(s, a) for the current behavior\r
policy ⇡ and for all states s and actions a. This can be done using essentially the same TD\r
method described above for learning v⇡. Recall that an episode consists of an alternating\r
sequence of states and state–action pairs:\r
At\r
Rt+1 St At+1Rt+2 St+1 At+2Rt+3 St+2 At+3\r
St+3 . . . . . .\r
In the previous section we considered transitions from state to state and learned the\r
values of states. Now we consider transitions from state–action pair to state–action pair,\r
and learn the values of state–action pairs. Formally these cases are identical: they are\r
both Markov chains with a reward process. The theorems assuring the convergence of\r
state values under TD(0) also apply to the corresponding algorithm for action values:\r
Q(St, At) Q(St, At) + ↵\r
h\r
Rt+1 + Q(St+1, At+1)  Q(St, At)\r
i\r
. (6.7)\r
Sarsa\r
This update is done after every transition from a nonterminal state St. If\r
St+1 is terminal, then Q(St+1, At+1) is defined as zero. This rule uses every\r
element of the quintuple of events, (St, At, Rt+1, St+1, At+1), that make up a\r
transition from one state–action pair to the next. This quintuple gives rise to\r
the name Sarsa for the algorithm. The backup diagram for Sarsa is as shown\r
to the right.\r
It is straightforward to design an on-policy control algorithm based on the Sarsa\r
prediction method. As in all on-policy methods, we continually estimate q⇡ for the\r
behavior policy ⇡, and at the same time change ⇡ toward greediness with respect to q⇡.\r
The general form of the Sarsa control algorithm is given in the box on the next page.\r
The convergence properties of the Sarsa algorithm depend on the nature of the policy’s\r
dependence on Q. For example, one could use "-greedy or "-soft policies. Sarsa converges\r
with probability 1 to an optimal policy and action-value function, under the usual\r
conditions on the step sizes (2.7), as long as all state–action pairs are visited an infinite\r
number of times and the policy converges in the limit to the greedy policy (which can be\r
arranged, for example, with "-greedy policies by setting " = 1/t).\r
Exercise 6.8 Show that an action-value version of (6.6) holds for the action-value form\r
of the TD error t = Rt+1 + Q(St+1, At+1)  Q(St, At), again assuming that the values\r
don’t change from step to step. ⇤"""

[[sections]]
number = "130"
title = "Chapter 6: Temporal-Di↵erence Learning"
text = """
Sarsa (on-policy TD control) for estimating Q ⇡ q⇤\r
Algorithm parameters: step size ↵ 2 (0, 1], small " > 0\r
Initialize Q(s, a), for all s 2 S+, a 2 A(s), arbitrarily except that Q(terminal, ·)=0\r
Loop for each episode:\r
Initialize S\r
Choose A from S using policy derived from Q (e.g., "-greedy)\r
Loop for each step of episode:\r
Take action A, observe R, S0\r
Choose A0 from S0 using policy derived from Q (e.g., "-greedy)\r
Q(S, A) Q(S, A) + ↵\r
⇥\r
R + Q(S0, A0)  Q(S, A)\r
⇤\r
S S0; A A0;\r
until S is terminal\r
Example 6.5: Windy Gridworld Shown inset below is a standard gridworld, with\r
start and goal states, but with one di↵erence: there is a crosswind running upward\r
through the middle of the grid. The actions are the standard four—up, down, right,\r
and left—but in the middle region the resultant next states are shifted upward by a\r
“wind,” the strength of which varies from column to column. The strength of the wind\r
0 1000 2000 3000 4000 5000 6000 7000 8000\r
0\r
50\r
100\r
150"""

[[sections]]
number = "170"
title = "Time steps"
text = """
S G\r
000 1 1 1 12 2 0\r
Actions\r
Episodes\r
is given below each column, in num\u0002ber of cells shifted upward. For ex\u0002ample, if you are one cell to the\r
right of the goal, then the action\r
left takes you to the cell just above\r
the goal. This is an undiscounted\r
episodic task, with constant rewards\r
of 1 until the goal state is reached.\r
The graph to the right shows the\r
results of applying "-greedy Sarsa to\r
this task, with " = 0.1, ↵ = 0.5,\r
and the initial values Q(s, a)=0\r
for all s, a. The increasing slope of\r
the graph shows that the goal was\r
reached more quickly over time. By\r
8000 time steps, the greedy policy was long since optimal (a trajectory from it is shown\r
inset); continued "-greedy exploration kept the average episode length at about 17 steps,\r
two more than the minimum of 15. Note that Monte Carlo methods cannot easily be\r
used here because termination is not guaranteed for all policies. If a policy was ever\r
found that caused the agent to stay in the same state, then the next episode would\r
never end. Online learning methods such as Sarsa do not have this problem because they\r
quickly learn during the episode that such policies are poor, and switch to something\r
else."""

[[sections]]
number = "6.5"
title = "Q-learning: O↵-policy TD Control 131"
text = """
Exercise 6.9: Windy Gridworld with King’s Moves (programming) Re-solve the windy\r
gridworld assuming eight possible actions, including the diagonal moves, rather than four.\r
How much better can you do with the extra actions? Can you do even better by including\r
a ninth action that causes no movement at all other than that caused by the wind? ⇤\r
Exercise 6.10: Stochastic Wind (programming) Re-solve the windy gridworld task with\r
King’s moves, assuming that the e↵ect of the wind, if there is any, is stochastic, sometimes\r
varying by 1 from the mean values given for each column. That is, a third of the time\r
you move exactly according to these values, as in the previous exercise, but also a third\r
of the time you move one cell above that, and another third of the time you move one\r
cell below that. For example, if you are one cell to the right of the goal and you move\r
left, then one-third of the time you move one cell above the goal, one-third of the time\r
you move two cells above the goal, and one-third of the time you move to the goal. ⇤"""

[[sections]]
number = "6.5"
title = "Q-learning: O↵-policy TD Control"
text = """
One of the early breakthroughs in reinforcement learning was the development of an\r
o↵-policy TD control algorithm known as Q-learning (Watkins, 1989), defined by\r
Q(St, At) Q(St, At) + ↵\r
h\r
Rt+1 +  maxa Q(St+1, a)  Q(St, At)\r
i\r
. (6.8)\r
In this case, the learned action-value function, Q, directly approximates q⇤, the optimal\r
action-value function, independent of the policy being followed. This dramatically\r
simplifies the analysis of the algorithm and enabled early convergence proofs. The policy\r
still has an e↵ect in that it determines which state–action pairs are visited and updated.\r
However, all that is required for correct convergence is that all pairs continue to be\r
updated. As we observed in Chapter 5, this is a minimal requirement in the sense that\r
any method guaranteed to find optimal behavior in the general case must require it.\r
Under this assumption and a variant of the usual stochastic approximation conditions on\r
the sequence of step-size parameters, Q has been shown to converge with probability 1 to\r
q⇤. The Q-learning algorithm is shown below in procedural form.\r
Q-learning (o↵-policy TD control) for estimating ⇡ ⇡ ⇡⇤\r
Algorithm parameters: step size ↵ 2 (0, 1], small " > 0\r
Initialize Q(s, a), for all s 2 S+, a 2 A(s), arbitrarily except that Q(terminal, ·)=0\r
Loop for each episode:\r
Initialize S\r
Loop for each step of episode:\r
Choose A from S using policy derived from Q (e.g., "-greedy)\r
Take action A, observe R, S0\r
Q(S, A) Q(S, A) + ↵\r
⇥\r
R +  maxa Q(S0, a)  Q(S, A)\r
⇤\r
S S0\r
until S is terminal"""

[[sections]]
number = "132"
title = "Chapter 6: Temporal-Di↵erence Learning"
text = """
What is the backup diagram for Q-learning? The rule (6.8) updates a state–action\r
pair, so the top node, the root of the update, must be a small, filled action node. The\r
update is also from action nodes, maximizing over all those actions possible in the next\r
state. Thus the bottom nodes of the backup diagram should be all these action nodes.\r
Finally, remember that we indicate taking the maximum of these “next action” nodes\r
with an arc across them (Figure 3.4-right). Can you guess now what the diagram is? If\r
so, please do make a guess before turning to the answer in Figure 6.4 on page 134.\r
Example 6.6: Cli↵ Walking This gridworld example compares Sarsa and Q-learning,\r
highlighting the di↵erence between on-policy (Sarsa) and o↵-policy (Q-learning) methods.\r
Reward\r
per\r
epsiode\r
\u0004 \u0004 \r
\u0004 \u0005\r
\u0004 \r
\u0004 \u0006\r
 \u0004 \u0006 \u0007 ~ \r
Episodes\r
Sarsa\r
Q-learning\r
S T h e C l i f f G\r
R\r
Sum of \r
rewards\r
during\r
episode\r
R = -1\r
Safer path\r
Optimal path\r
R = -100\r
Episodes\r
Sarsa\r
Q-learning\r
S G\r
r = \u0004 \u0004 \r
T h e C l i f f\r
r =\b\u0004 \u0004 sa\r
op\r
R\r
R\r
Sum of \r
rewards\r
during\r
episode\r
R = -1\r
safe path\r
optimal path\r
R = -100\r
Episodes\r
-25\r
-50\r
-75\r
-100\r
0 100 200 300 400 500\r
Consider the gridworld shown to the\r
right. This is a standard undis\u0002counted, episodic task, with start\r
and goal states, and the usual ac\u0002tions causing movement up, down,\r
right, and left. Reward is 1 on all\r
transitions except those into the re\u0002gion marked “The Cli↵.” Stepping\r
into this region incurs a reward of\r
100 and sends the agent instantly\r
back to the start.\r
The graph to the right shows the\r
performance of the Sarsa and Q\u0002learning methods with "-greedy ac\u0002tion selection, " = 0.1. After an\r
initial transient, Q-learning learns\r
values for the optimal policy, that\r
which travels right along the edge\r
of the cli↵. Unfortunately, this re\u0002sults in its occasionally falling o↵\r
the cli↵ because of the "-greedy ac\u0002tion selection. Sarsa, on the other\r
hand, takes the action selection into\r
account and learns the longer but\r
safer path through the upper part\r
of the grid. Although Q-learning ac\u0002tually learns the values of the opti\u0002mal policy, its online performance\r
is worse than that of Sarsa, which\r
learns the roundabout policy. Of course, if " were gradually reduced, then both methods\r
would asymptotically converge to the optimal policy.\r
Exercise 6.11 Why is Q-learning considered an o↵-policy control method? ⇤\r
Exercise 6.12 Suppose action selection is greedy. Is Q-learning then exactly the same\r
algorithm as Sarsa? Will they make exactly the same action selections and weight\r
updates? ⇤"""

[[sections]]
number = "6.6"
title = "Expected Sarsa 133"
text = ""

[[sections]]
number = "6.6"
title = "Expected Sarsa"
text = """
Consider the learning algorithm that is just like Q-learning except that instead of the\r
maximum over next state–action pairs it uses the expected value, taking into account\r
how likely each action is under the current policy. That is, consider the algorithm with\r
the update rule\r
Q(St, At) Q(St, At) + ↵\r
h\r
Rt+1 + E⇡[Q(St+1, At+1) | St+1]  Q(St, At)\r
i\r
= Q(St, At) + ↵\r
h\r
Rt+1 + \r
X\r
a\r
⇡(a|St+1)Q(St+1, a)  Q(St, At)\r
i\r
, (6.9)\r
but that otherwise follows the schema of Q-learning. Given the next state, St+1, this\r
algorithm moves deterministically in the same direction as Sarsa moves in expectation,\r
and accordingly it is called Expected Sarsa. Its backup diagram is shown on the right in\r
Figure 6.4.\r
Expected Sarsa is more complex computationally than Sarsa but, in return, it eliminates\r
the variance due to the random selection of At+1. Given the same amount of experience\r
we might expect it to perform slightly better than Sarsa, and indeed it generally does.\r
Figure 6.3 shows summary results on the cli↵-walking task with Expected Sarsa compared\r
to Sarsa and Q-learning. Expected Sarsa retains the significant advantage of Sarsa over\r
Q-learning on this problem. In addition, Expected Sarsa shows a significant improvement\r
ts on two versions of the windy\r
with a deterministic environment\r
c environment. We do so in order\r
e of environment stochasticity on\r
nce between Expected Sarsa and\r
rst part of Hypothesis 2. We then\r
ent amounts of policy stochasticity\r
of Hypothesis 2. For completeness,\r
ance of Q-learning on this problem.\r
ts in other domains verifying the\r
arsa in a broader setting. All results\r
raged over numerous independent\r
ard error becomes negligible.\r
ypothesis 1 using the cliff walking\r
isodic navigation task in which the\r
from start to goal in a deterministic\r
ge of the grid world is a cliff (see\r
take any of four movement actions:\r
each of which moves the agent one\r
ng direction. Each step results in a\r
n the agent steps into the cliff area,\r
d of -100 and an immediate return\r
isode ends upon reaching the goal\r
G\r
. The agent has to move from the start [S]\r
stepping into the cliff (grey area).\r
rmance over the first n episodes as\r
g rate ↵ using an -greedy policy\r
hows the result for n = 100 and\r
ed the results over 50,000 runs and\r
on the edge of the cliff immediately, resulting in a slightly\r
better on-line performance.\r
For n = 100, 000, the average return is equal for all\r
↵ values in case of Expected Sarsa and Q-learning. This\r
indicates that the algorithms have converged long before the\r
end of the run for all ↵ values, since we do not see any\r
effect of the initial learning phase. For Sarsa the performance\r
comes close to the performance of Expected Sarsa only for\r
↵ = 0.1, while for large ↵, the performance for n = 100, 000\r
even drops below the performance for n = 100. The reason\r
is that for large values of ↵ the Q values of Sarsa diverge.\r
Although the policy is still improved over the initial random\r
policy during the early stages of learning, divergence causes\r
the policy to get worse in the long run.\r
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 −160\r
−140\r
−120\r
−100\r
−80\r
−60\r
−40\r
−20\r
0\r
alpha\r
average return\r
n = 100, Sarsa\r
n = 100, Q−learning\r
n = 100, Expected Sarsa\r
n = 1E5, Sarsa\r
n = 1E5, Q−learning\r
n = 1E5, Expected Sarsa\r
Fig. 2. Average return on the cliff walking task over the first n episodes\r
for n = 100 and n = 100, 000 using an -greedy policy with  = 0.1. The\r
big dots indicate the maximal values.\r
B. Windy Grid World\r
We turn to the windy grid world task to further test Hy\u0002pothesis 2. The windy grid world task is another navigation\r
task, where the agent has to find its way from start to goal.\r
Expected Sarsa\r
Sarsa Q-learning\r
Asymptotic Performance\r
Interim Performance\r
Q-learning\r
Sum of rewards\r
per episode\r
↵\r
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1\r
0\r
-40\r
-80\r
-120\r
Figure 6.3: Interim and asymptotic performance of TD control methods on the cli↵-walking\r
task as a function of ↵. All algorithms used an "-greedy policy with " = 0.1. Asymptotic\r
performance is an average over 100,000 episodes whereas interim performance is an average\r
over the first 100 episodes. These data are averages of over 50,000 and 10 runs for the interim\r
and asymptotic cases respectively. The solid circles mark the best interim performance of each\r
method. Adapted from van Seijen et al. (2009)."""

[[sections]]
number = "134"
title = "Chapter 6: Temporal-Di↵erence Learning"
text = """
Q-learning Expected Sarsa\r
Figure 6.4: The backup diagrams for Q-learning and Expected Sarsa.\r
over Sarsa over a wide range of values for the step-size parameter ↵. In cli↵ walking\r
the state transitions are all deterministic and all randomness comes from the policy. In\r
such cases, Expected Sarsa can safely set ↵ = 1 without su↵ering any degradation of\r
asymptotic performance, whereas Sarsa can only perform well in the long run at a small\r
value of ↵, at which short-term performance is poor. In this and other examples there is\r
a consistent empirical advantage of Expected Sarsa over Sarsa.\r
In these cli↵ walking results Expected Sarsa was used on-policy, but in general it\r
might use a policy di↵erent from the target policy ⇡ to generate behavior, in which case\r
it becomes an o↵-policy algorithm. For example, suppose ⇡ is the greedy policy while\r
behavior is more exploratory; then Expected Sarsa is exactly Q-learning. In this sense\r
Expected Sarsa subsumes and generalizes Q-learning while reliably improving over Sarsa.\r
Except for the small additional computational cost, Expected Sarsa may completely\r
dominate both of the other more-well-known TD control algorithms."""

[[sections]]
number = "6.7"
title = "Maximization Bias and Double Learning"
text = """
All the control algorithms that we have discussed so far involve maximization in the\r
construction of their target policies. For example, in Q-learning the target policy is\r
the greedy policy given the current action values, which is defined with a max, and in\r
Sarsa the policy is often "-greedy, which also involves a maximization operation. In these\r
algorithms, a maximum over estimated values is used implicitly as an estimate of the\r
maximum value, which can lead to a significant positive bias. To see why, consider a\r
single state s where there are many actions a whose true values, q(s, a), are all zero but\r
whose estimated values, Q(s, a), are uncertain and thus distributed some above and some\r
below zero. The maximum of the true values is zero, but the maximum of the estimates\r
is positive, a positive bias. We call this maximization bias.\r
Example 6.7: Maximization Bias Example The small MDP shown inset in\r
Figure 6.5 provides a simple example of how maximization bias can harm the performance\r
of TD control algorithms. The MDP has two non-terminal states A and B. Episodes\r
always start in A with a choice between two actions, left and right. The right action\r
transitions immediately to the terminal state with a reward and return of zero. The\r
left action transitions to B, also with a reward of zero, from which there are many\r
possible actions all of which cause immediate termination with a reward drawn from a\r
normal distribution with mean 0.1 and variance 1.0. Thus, the expected return for\r
any trajectory starting with left is 0.1, and thus taking left in state A is always a"""

[[sections]]
number = "6.7"
title = "Maximization Bias and Double Learning 135"
text = "B A left right"

[[sections]]
number = "0"
title = "N(0.1, 1)"
text = ""

[[sections]]
number = "0"
title = "Q-learning"
text = """
Double\r
Q-learning\r
Episodes\r
1 100 200 300\r
% left\r
actions\r
from A\r
100%\r
75%\r
50%\r
25%\r
5%\r
0\r
optimal\r
Figure 6.5: Comparison of Q-learning and Double Q-learning on a simple episodic MDP (shown\r
inset). Q-learning initially learns to take the left action much more often than the right action,\r
and always takes it significantly more often than the 5% minimum probability enforced by\r
"-greedy action selection with " = 0.1. In contrast, Double Q-learning is essentially una↵ected by\r
maximization bias. These data are averaged over 10,000 runs. The initial action-value estimates\r
were zero. Any ties in "-greedy action selection were broken randomly.\r
mistake. Nevertheless, our control methods may favor left because of maximization bias\r
making B appear to have a positive value. Figure 6.5 shows that Q-learning with "-greedy\r
action selection initially learns to strongly favor the left action on this example. Even at\r
asymptote, Q-learning takes the left action about 5% more often than is optimal at our\r
parameter settings (" = 0.1, ↵ = 0.1, and  = 1).\r
Are there algorithms that avoid maximization bias? To start, consider a bandit case in\r
which we have noisy estimates of the value of each of many actions, obtained as sample\r
averages of the rewards received on all the plays with each action. As we discussed above,\r
there will be a positive maximization bias if we use the maximum of the estimates as\r
an estimate of the maximum of the true values. One way to view the problem is that\r
it is due to using the same samples (plays) both to determine the maximizing action\r
and to estimate its value. Suppose we divided the plays in two sets and used them to\r
learn two independent estimates, call them Q1(a) and Q2(a), each an estimate of the\r
true value q(a), for all a 2 A. We could then use one estimate, say Q1, to determine\r
the maximizing action A⇤ = argmaxa Q1(a), and the other, Q2, to provide the estimate\r
of its value, Q2(A⇤) = Q2(argmaxa Q1(a)). This estimate will then be unbiased in the\r
sense that E[Q2(A⇤)] = q(A⇤). We can also repeat the process with the role of the two\r
estimates reversed to yield a second unbiased estimate Q1(argmaxa Q2(a)). This is the\r
idea of double learning. Note that although we learn two estimates, only one estimate is\r
updated on each play; double learning doubles the memory requirements, but does not\r
increase the amount of computation per step.\r
The idea of double learning extends naturally to algorithms for full MDPs. For example,\r
the double learning algorithm analogous to Q-learning, called Double Q-learning, divides\r
the time steps in two, perhaps by flipping a coin on each step. If the coin comes up heads,\r
the update is"""

[[sections]]
number = "136"
title = "Chapter 6: Temporal-Di↵erence Learning"
text = """
Q1(St, At) Q1(St, At)+↵\r
h\r
Rt+1+Q2\r
\r
St+1, argmax aQ1(St+1, a)\r
\r
Q1(St, At)\r
i\r
. (6.10)\r
If the coin comes up tails, then the same update is done with Q1 and Q2 switched,\r
so that Q2 is updated. The two approximate value functions are treated completely\r
symmetrically. The behavior policy can use both action-value estimates. For example, an\r
"-greedy policy for Double Q-learning could be based on the average (or sum) of the two\r
action-value estimates. A complete algorithm for Double Q-learning is given in the box\r
below. This is the algorithm used to produce the results in Figure 6.5. In that example,\r
double learning seems to eliminate the harm caused by maximization bias. Of course\r
there are also double versions of Sarsa and Expected Sarsa.\r
Double Q-learning, for estimating Q1 ⇡ Q2 ⇡ q⇤\r
Algorithm parameters: step size ↵ 2 (0, 1], small " > 0\r
Initialize Q1(s, a) and Q2(s, a), for all s 2 S+, a 2 A(s), such that Q(terminal, ·)=0\r
Loop for each episode:\r
Initialize S\r
Loop for each step of episode:\r
Choose A from S using the policy "-greedy in Q1 + Q2\r
Take action A, observe R, S0\r
With 0.5 probabilility:\r
Q1(S, A) Q1(S, A) + ↵\r
⇣\r
R + Q2\r
\r
S0, argmaxa Q1(S0, a)\r
\r
 Q1(S, A)\r
⌘\r
else:\r
Q2(S, A) Q2(S, A) + ↵\r
⇣\r
R + Q1\r
\r
S0, argmaxa Q2(S0, a)\r
\r
 Q2(S, A)\r
⌘\r
S S0\r
until S is terminal\r
⇤\r
Exercise 6.13 What are the update equations for Double Expected Sarsa with an\r
"-greedy target policy? ⇤"""

[[sections]]
number = "6.8"
title = "Games, Afterstates, and Other Special Cases"
text = """
In this book we try to present a uniform approach to a wide class of tasks, but of\r
course there are always exceptional tasks that are better treated in a specialized way. For\r
example, our general approach involves learning an action-value function, but in Chapter 1\r
we presented a TD method for learning to play tic-tac-toe that learned something much\r
more like a state-value function. If we look closely at that example, it becomes apparent\r
that the function learned there is neither an action-value function nor a state-value\r
function in the usual sense. A conventional state-value function evaluates states in which\r
the agent has the option of selecting an action, but the state-value function used in"""

[[sections]]
number = "6.8"
title = "Games, Afterstates, and Other Special Cases 137"
text = """
tic-tac-toe evaluates board positions after the agent has made its move. Let us call these\r
afterstates, and value functions over these, afterstate value functions. Afterstates are\r
useful when we have knowledge of an initial part of the environment’s dynamics but not\r
necessarily of the full dynamics. For example, in games we typically know the immediate\r
e↵ects of our moves. We know for each possible chess move what the resulting position\r
will be, but not how our opponent will reply. Afterstate value functions are a natural\r
way to take advantage of this kind of knowledge and thereby produce a more ecient\r
learning method.\r
The reason it is more ecient to design algorithms in terms of afterstates is ap\u0002parent from the tic-tac-toe example. A conventional action-value function would map\r
from positions and moves to an estimate of the value. But many position—move\r
pairs produce the same resulting position, as in the example below: In such cases the\r
X\r
O X\r
X\r
O + X O X + X\r
position–move pairs are di↵er\u0002ent but produce the same “af\u0002terposition,” and thus must have\r
the same value. A conventional\r
action-value function would have\r
to separately assess both pairs,\r
whereas an afterstate value func\u0002tion would immediately assess\r
both equally. Any learning about\r
the position–move pair on the left\r
would immediately transfer to the\r
pair on the right.\r
Afterstates arise in many tasks,\r
not just games. For example, in\r
queuing tasks there are actions\r
such as assigning customers to servers, rejecting customers, or discarding information. In\r
such cases the actions are in fact defined in terms of their immediate e↵ects, which are\r
completely known.\r
It is impossible to describe all the possible kinds of specialized problems and corre\u0002sponding specialized learning algorithms. However, the principles developed in this book\r
should apply widely. For example, afterstate methods are still aptly described in terms\r
of generalized policy iteration, with a policy and (afterstate) value function interacting in\r
essentially the same way. In many cases one will still face the choice between on-policy\r
and o↵-policy methods for managing the need for persistent exploration.\r
Exercise 6.14 Describe how the task of Jack’s Car Rental (Example 4.2) could be\r
reformulated in terms of afterstates. Why, in terms of this specific task, would such a\r
reformulation be likely to speed convergence? ⇤"""

[[sections]]
number = "138"
title = "Chapter 6: Temporal-Di↵erence Learning"
text = ""

[[sections]]
number = "6.9"
title = "Summary"
text = """
In this chapter we introduced a new kind of learning method, temporal-di↵erence (TD)\r
learning, and showed how it can be applied to the reinforcement learning problem. As\r
usual, we divided the overall problem into a prediction problem and a control problem.\r
TD methods are alternatives to Monte Carlo methods for solving the prediction problem.\r
In both cases, the extension to the control problem is via the idea of generalized policy\r
iteration (GPI) that we abstracted from dynamic programming. This is the idea that\r
approximate policy and value functions should interact in such a way that they both\r
move toward their optimal values.\r
One of the two processes making up GPI drives the value function to accurately predict\r
returns for the current policy; this is the prediction problem. The other process drives\r
the policy to improve locally (e.g., to be "-greedy) with respect to the current value\r
function. When the first process is based on experience, a complication arises concerning\r
maintaining sucient exploration. We can classify TD control methods according to\r
whether they deal with this complication by using an on-policy or o↵-policy approach.\r
Sarsa is an on-policy method, and Q-learning is an o↵-policy method. Expected Sarsa\r
is also an o↵-policy method as we present it here. There is a third way in which TD\r
methods can be extended to control which we did not include in this chapter, called\r
actor–critic methods. These methods are covered in full in Chapter 13.\r
The methods presented in this chapter are today the most widely used reinforcement\r
learning methods. This is probably due to their great simplicity: they can be applied\r
online, with a minimal amount of computation, to experience generated from interaction\r
with an environment; they can be expressed nearly completely by single equations that\r
can be implemented with small computer programs. In the next few chapters we extend\r
these algorithms, making them slightly more complicated and significantly more powerful.\r
All the new algorithms will retain the essence of those introduced here: they will be able\r
to process experience online, with relatively little computation, and they will be driven\r
by TD errors. The special cases of TD methods introduced in the present chapter should\r
rightly be called one-step, tabular, model-free TD methods. In the next two chapters we\r
extend them to n-step forms (a link to Monte Carlo methods) and forms that include\r
a model of the environment (a link to planning and dynamic programming). Then, in\r
the second part of the book we extend them to various forms of function approximation\r
rather than tables (a link to deep learning and artificial neural networks).\r
Finally, in this chapter we have discussed TD methods entirely within the context of\r
reinforcement learning problems, but TD methods are actually more general than this.\r
They are general methods for learning to make long-term predictions about dynamical\r
systems. For example, TD methods may be relevant to predicting financial data, life\r
spans, election outcomes, weather patterns, animal behavior, demands on power stations,\r
or customer purchases. It was only when TD methods were analyzed as pure prediction\r
methods, independent of their use in reinforcement learning, that their theoretical\r
properties first came to be well understood. Even so, these other potential applications\r
of TD learning methods have not yet been extensively explored."""

[[sections]]
number = "6.9"
title = "Summary 139"
text = """
Bibliographical and Historical Remarks\r
As we outlined in Chapter 1, the idea of TD learning has its early roots in animal learning\r
psychology and artificial intelligence, most notably the work of Samuel (1959) and Klopf\r
(1972). Samuel’s work is described as a case study in Section 16.2. Also related to TD\r
learning are Holland’s (1975, 1976) early ideas about consistency among value predictions.\r
These influenced one of the authors (Barto), who was a graduate student from 1970 to\r
1975 at the University of Michigan, where Holland was teaching. Holland’s ideas led to\r
a number of TD-related systems, including the work of Booker (1982) and the bucket\r
brigade of Holland (1986), which is related to Sarsa as discussed below.\r
6.1–2 Most of the specific material from these sections is from Sutton (1988), includ\u0002ing the TD(0) algorithm, the random walk example, and the term “temporal\u0002di↵erence learning.” The characterization of the relationship to dynamic pro\u0002gramming and Monte Carlo methods was influenced by Watkins (1989), Werbos\r
(1987), and others. The use of backup diagrams was new to the first edition of\r
this book.\r
Tabular TD(0) was proved to converge in the mean by Sutton (1988) and with\r
probability 1 by Dayan (1992), based on the work of Watkins and Dayan (1992).\r
These results were extended and strengthened by Jaakkola, Jordan, and Singh\r
(1994) and Tsitsiklis (1994) by using extensions of the powerful existing theory\r
of stochastic approximation. Other extensions and generalizations are covered in\r
later chapters."""

[[sections]]
number = "6.3"
title = "The optimality of the TD algorithm under batch training was established by"
text = """
Sutton (1988). Illuminating this result is Barnard’s (1993) derivation of the TD\r
algorithm as a combination of one step of an incremental method for learning a\r
model of the Markov chain and one step of a method for computing predictions\r
from the model. The term certainty equivalence is from the adaptive control\r
literature (e.g., Goodwin and Sin, 1984)."""

[[sections]]
number = "6.4"
title = "The Sarsa algorithm was introduced by Rummery and Niranjan (1994). They"
text = """
explored it in conjunction with artificial neural networks and called it “Modified\r
Connectionist Q-learning”. The name “Sarsa” was introduced by Sutton (1996).\r
The convergence of one-step tabular Sarsa (the form treated in this chapter) has\r
been proved by Singh, Jaakkola, Littman, and Szepesv´ari (2000). The “windy\r
gridworld” example was suggested by Tom Kalt.\r
Holland’s (1986) bucket brigade idea evolved into an algorithm closely related to\r
Sarsa. The original idea of the bucket brigade involved chains of rules triggering\r
each other; it focused on passing credit back from the current rule to the rules\r
that triggered it. Over time, the bucket brigade came to be more like TD learning\r
in passing credit back to any temporally preceding rule, not just to the ones\r
that triggered the current rule. The modern form of the bucket brigade, when\r
simplified in various natural ways, is nearly identical to one-step Sarsa, as detailed\r
by Wilson (1994)."""

[[sections]]
number = "140"
title = "Chapter 6: Temporal-Di↵erence Learning"
text = ""

[[sections]]
number = "6.5"
title = "Q-learning was introduced by Watkins (1989), whose outline of a convergence"
text = """
proof was made rigorous by Watkins and Dayan (1992). More general convergence\r
results were proved by Jaakkola, Jordan, and Singh (1994) and Tsitsiklis (1994)."""

[[sections]]
number = "6.6"
title = "The Expected Sarsa algorithm was introduced by George John (1994), who"
text = """
called it “Q-learning” and stressed its advantages over Q-learning as an o↵-policy\r
algorithm. John’s work was not known to us when we presented Expected\r
Sarsa in the first edition of this book as an exercise, or to van Seijen, van\r
Hasselt, Whiteson, and Weiring (2009) when they established Expected Sarsa’s\r
convergence properties and conditions under which it will outperform regular\r
Sarsa and Q-learning. Our Figure 6.3 is adapted from their results. Van Seijen\r
et al. defined “Expected Sarsa” to be an on-policy method exclusively (as we\r
did in the first edition), whereas now we use this name for the general algorithm\r
in which the target and behavior policies may di↵er. The general o↵-policy\r
view of Expected Sarsa was noted by van Hasselt (2011), who called it “General\r
Q-learning.”\r
6.7 Maximization bias and double learning were introduced and extensively investi\u0002gated by van Hasselt (2010, 2011). The example MDP in Figure 6.5 was adapted\r
from that in his Figure 4.1 (van Hasselt, 2011)."""

[[sections]]
number = "6.8"
title = "The notion of an afterstate is the same as that of a “post-decision state” (Van"
text = """
Roy, Bertsekas, Lee, and Tsitsiklis, 1997; Powell, 2011).

Chapter 7\r
n-step Bootstrapping\r
In this chapter we unify the Monte Carlo (MC) methods and the one-step temporal\u0002di↵erence (TD) methods presented in the previous two chapters. Neither MC methods nor\r
one-step TD methods are always the best. In this chapter we present n-step TD methods\r
that generalize both methods so that one can shift from one to the other smoothly as\r
needed to meet the demands of a particular task. n-step methods span a spectrum with\r
MC methods at one end and one-step TD methods at the other. The best methods are\r
often intermediate between the two extremes.\r
Another way of looking at the benefits of n-step methods is that they free you from\r
the tyranny of the time step. With one-step TD methods the same time step determines\r
how often the action can be changed and the time interval over which bootstrapping\r
is done. In many applications one wants to be able to update the action very fast to\r
take into account anything that has changed, but bootstrapping works best if it is over a\r
length of time in which a significant and recognizable state change has occurred. With\r
one-step TD methods, these time intervals are the same, and so a compromise must be\r
made. n-step methods enable bootstrapping to occur over multiple steps, freeing us from\r
the tyranny of the single time step.\r
The idea of n-step methods is usually used as an introduction to the algorithmic\r
idea of eligibility traces (Chapter 12), which enable bootstrapping over multiple time\r
intervals simultaneously. Here we instead consider the n-step bootstrapping idea on its\r
own, postponing the treatment of eligibility-trace mechanisms until later. This allows us\r
to separate the issues better, dealing with as many of them as possible in the simpler\r
n-step setting.\r
As usual, we first consider the prediction problem and then the control problem. That\r
is, we first consider how n-step methods can help in predicting returns as a function of\r
state for a fixed policy (i.e., in estimating v⇡). Then we extend the ideas to action values\r
and control methods."""

[[sections]]
number = "142"
title = "Chapter 7: n-step Bootstrapping"
text = """
7.1 n-step TD Prediction\r
What is the space of methods lying between Monte Carlo and TD methods? Consider\r
estimating v⇡ from sample episodes generated using ⇡. Monte Carlo methods perform\r
an update for each state based on the entire sequence of observed rewards from that\r
state until the end of the episode. The update of one-step TD methods, on the other\r
hand, is based on just the one next reward, bootstrapping from the value of the state\r
one step later as a proxy for the remaining rewards. One kind of intermediate method,\r
then, would perform an update based on an intermediate number of rewards: more than\r
one, but less than all of them until termination. For example, a two-step update would\r
be based on the first two rewards and the estimated value of the state two steps later.\r
Similarly, we could have three-step updates, four-step updates, and so on. Figure 7.1\r
shows the backup diagrams of the spectrum of n-step updates for v⇡, with the one-step\r
TD update on the left and the up-until-termination Monte Carlo update on the right.\r
1-step TD\r
and TD(0) 2-step TD 3-step TD n-step TD\r
∞-step TD\r
and Monte Carlo\r
···\r
···\r
···\r
···\r
Figure 7.1: The backup diagrams of n-step methods. These methods form a spectrum ranging\r
from one-step TD methods to Monte Carlo methods.\r
The methods that use n-step updates are still TD methods because they still change\r
an earlier estimate based on how it di↵ers from a later estimate. Now the later estimate\r
is not one step later, but n steps later. Methods in which the temporal di↵erence extends\r
over n steps are called n-step TD methods. The TD methods introduced in the previous\r
chapter all used one-step updates, which is why we called them one-step TD methods.\r
More formally, consider the update of the estimated value of state St as a result of the\r
state–reward sequence, St, Rt+1, St+1, Rt+2,...,RT , ST (omitting the actions). We know\r
that in Monte Carlo updates the estimate of v⇡(St) is updated in the direction of the

7.1. n-step TD Prediction 143\r
complete return:\r
Gt\r
.\r
= Rt+1 + Rt+2 + 2Rt+3 + ··· + T t1RT ,\r
where T is the last time step of the episode. Let us call this quantity the target of the\r
update. Whereas in Monte Carlo updates the target is the return, in one-step updates\r
the target is the first reward plus the discounted estimated value of the next state, which\r
we call the one-step return:\r
Gt:t+1\r
.\r
= Rt+1 + Vt(St+1),\r
where Vt : S ! R here is the estimate at time t of v⇡. The subscripts on Gt:t+1 indicate\r
that it is a truncated return for time t using rewards up until time t+1, with the discounted\r
estimate Vt(St+1) taking the place of the other terms Rt+2 + 2Rt+3 + ···+ T t1RT\r
of the full return, as discussed in the previous chapter. Our point now is that this idea\r
makes just as much sense after two steps as it does after one. The target for a two-step\r
update is the two-step return:\r
Gt:t+2\r
.\r
= Rt+1 + Rt+2 + 2Vt+1(St+2),\r
where now 2Vt+1(St+2) corrects for the absence of the terms 2Rt+3 + 3Rt+4 + ··· +\r
T t1RT . Similarly, the target for an arbitrary n-step update is the n-step return:\r
Gt:t+n\r
.\r
= Rt+1 + Rt+2 + ··· + n1Rt+n + nVt+n1(St+n), (7.1)\r
for all n, t such that n  1 and 0  t<T  n. All n-step returns can be considered\r
approximations to the full return, truncated after n steps and then corrected for the\r
remaining missing terms by Vt+n1(St+n). If t + n  T (if the n-step return extends\r
to or beyond termination), then all the missing terms are taken as zero, and the n-step\r
return defined to be equal to the ordinary full return (Gt:t+n\r
.\r
= Gt if t + n  T).\r
Note that n-step returns for n > 1 involve future rewards and states that are not\r
available at the time of transition from t to t + 1. No real algorithm can use the n-step\r
return until after it has seen Rt+n and computed Vt+n1. The first time these are\r
available is t + n. The natural state-value learning algorithm for using n-step returns is\r
thus\r
Vt+n(St) .= Vt+n1(St) + ↵\r
⇥\r
Gt:t+n  Vt+n1(St)\r
⇤\r
, 0  t < T, (7.2)\r
while the values of all other states remain unchanged: Vt+n(s) = Vt+n1(s), for all s6=St.\r
We call this algorithm n-step TD. Note that no changes at all are made during the first\r
n  1 steps of each episode. To make up for that, an equal number of additional updates\r
are made at the end of the episode, after termination and before starting the next episode.\r
Complete pseudocode is given in the box on the next page.\r
Exercise 7.1 In Chapter 6 we noted that the Monte Carlo error can be written as the\r
sum of TD errors (6.6) if the value estimates don’t change from step to step. Show that\r
the n-step error used in (7.2) can also be written as a sum of TD errors (again if the\r
value estimates don’t change) generalizing the earlier result. ⇤\r
Exercise 7.2 (programming) With an n-step method, the value estimates do change from\r
step to step, so an algorithm that used the sum of TD errors (see previous exercise) in"""

[[sections]]
number = "144"
title = "Chapter 7: n-step Bootstrapping"
text = """
n-step TD for estimating V ⇡ v⇡\r
Input: a policy ⇡\r
Algorithm parameters: step size ↵ 2 (0, 1], a positive integer n\r
Initialize V (s) arbitrarily, for all s 2 S\r
All store and access operations (for St and Rt) can take their index mod n + 1\r
Loop for each episode:\r
Initialize and store S0 6= terminal\r
T 1\r
Loop for t = 0, 1, 2,... :\r
| If t<T, then:\r
| Take an action according to ⇡(·|St)\r
| Observe and store the next reward as Rt+1 and the next state as St+1\r
| If St+1 is terminal, then T t + 1\r
| ⌧ t  n +1 (⌧ is the time whose state’s estimate is being updated)\r
| If ⌧  0:\r
| G Pmin(⌧+n,T)\r
i=⌧+1 i⌧1Ri\r
| If ⌧ + n<T, then: G G + nV (S⌧+n) (G⌧:⌧+n)\r
| V (S⌧ ) V (S⌧ ) + ↵ [G  V (S⌧ )]\r
Until ⌧ = T  1\r
place of the error in (7.2) would actually be a slightly di↵erent algorithm. Would it be a\r
better algorithm or a worse one? Devise and program a small experiment to answer this\r
question empirically. ⇤\r
The n-step return uses the value function Vt+n1 to correct for the missing rewards\r
beyond Rt+n. An important property of n-step returns is that their expectation is\r
guaranteed to be a better estimate of v⇡ than Vt+n1 is, in a worst-state sense. That is,\r
the worst error of the expected n-step return is guaranteed to be less than or equal to n\r
times the worst error under Vt+n1:\r
maxs\r
\r
\r
\r
E⇡[Gt:t+n|St =s]  v⇡(s)\r
\r
\r
  n maxs\r
\r
\r
\r
Vt+n1(s)  v⇡(s)\r
\r
\r
, (7.3)\r
for all n  1. This is called the error reduction property of n-step returns. Because of the\r
error reduction property, one can show formally that all n-step TD methods converge to\r
the correct predictions under appropriate technical conditions. The n-step TD methods\r
thus form a family of sound methods, with one-step TD methods and Monte Carlo\r
methods as extreme members.\r
Example 7.1: n-step TD Methods on the Random Walk Consider using n-step\r
TD methods on the 5-state random walk task described in Example 6.2 (page 125).\r
Suppose the first episode progressed directly from the center state, C, to the right,\r
through D and E, and then terminated on the right with a return of 1. Recall that the\r
estimated values of all the states started at an intermediate value, V (s)=0.5. As a result\r
of this experience, a one-step method would change only the estimate for the last state,

7.2. n-step Sarsa 145\r
V (E), which would be incremented toward 1, the observed return. A two-step method,\r
on the other hand, would increment the values of the two states preceding termination:\r
V (D) and V (E) both would be incremented toward 1. A three-step method, or any n-step\r
method for n > 2, would increment the values of all three of the visited states toward 1,\r
all by the same amount.\r
Which value of n is better? Figure 7.2 shows the results of a simple empirical test for\r
a larger random walk process, with 19 states instead of 5 (and with a 1 outcome on the\r
left, all values initialized to 0), which we use as a running example in this chapter. Results\r
are shown for n-step TD methods with a range of values for n and ↵. The performance\r
measure for each parameter setting, shown on the vertical axis, is the square-root of\r
the average squared error between the predictions at the end of the episode for the 19\r
states and their true values, then averaged over the first 10 episodes and 100 repetitions\r
of the whole experiment (the same sets of walks were used for all parameter settings).\r
Note that methods with an intermediate value of n worked best. This illustrates how\r
the generalization of TD and Monte Carlo methods to n-step methods can potentially\r
perform better than either of the two extreme methods.\r
↵\r
Average\r
RMS error\r
over 19 states\r
and first 10 \r
episodes n=1\r
n=2\r
n=4\r
n=8\r
n=16\r
n=32\r
n=32 n=64 128 512256 0.55\r
0.5\r
0.45\r
0.35\r
0.3\r
0.25\r
0.4\r
0 0.2 0.4 0.6 0.8 1\r
Figure 7.2: Performance of n-step TD methods as a function of ↵, for various values of n, on\r
a 19-state random walk task (Example 7.1).\r
Exercise 7.3 Why do you think a larger random walk task (19 states instead of 5) was\r
used in the examples of this chapter? Would a smaller walk have shifted the advantage\r
to a di↵erent value of n? How about the change in left-side outcome from 0 to 1 made\r
in the larger walk? Do you think that made any di↵erence in the best value of n? ⇤\r
7.2 n-step Sarsa\r
How can n-step methods be used not just for prediction, but for control? In this section\r
we show how n-step methods can be combined with Sarsa in a straightforward way to"""

[[sections]]
number = "146"
title = "Chapter 7: n-step Bootstrapping"
text = """
produce an on-policy TD control method. The n-step version of Sarsa we call n-step\r
Sarsa, and the original version presented in the previous chapter we henceforth call\r
one-step Sarsa, or Sarsa(0).\r
The main idea is to simply switch states for actions (state–action pairs) and then use\r
an "-greedy policy. The backup diagrams for n-step Sarsa (shown in Figure 7.3), like\r
those of n-step TD (Figure 7.1), are strings of alternating states and actions, except that\r
the Sarsa ones all start and end with an action rather a state. We redefine n-step returns\r
(update targets) in terms of estimated action values:\r
Gt:t+n\r
.\r
= Rt+1+Rt+2+···+n1Rt+n+nQt+n1(St+n, At+n), n  1, 0  t<T n,\r
(7.4)\r
with Gt:t+n\r
.\r
= Gt if t + n  T. The natural algorithm is then\r
Qt+n(St, At) .= Qt+n1(St, At) + ↵ [Gt:t+n  Qt+n1(St, At)] , 0  t < T, (7.5)\r
while the values of all other states remain unchanged: Qt+n(s, a) = Qt+n1(s, a), for all\r
s, a such that s 6= St or a 6= At. This is the algorithm we call n-step Sarsa. Pseudocode\r
is shown in the box on the next page, and an example of why it can speed up learning\r
compared to one-step methods is given in Figure 7.4.\r
1-step Sarsa\r
aka Sarsa(0) 2-step Sarsa 3-step Sarsa n-step Sarsa\r
∞-step Sarsa\r
aka Monte Carlo\r
n-step \r
Expected Sarsa\r
Figure 7.3: The backup diagrams for the spectrum of n-step methods for state–action values.\r
They range from the one-step update of Sarsa(0) to the up-until-termination update of the\r
Monte Carlo method. In between are the n-step updates, based on n steps of real rewards and\r
the estimated value of the nth next state–action pair, all appropriately discounted. On the far\r
right is the backup diagram for n-step Expected Sarsa.

7.2. n-step Sarsa 147\r
n-step Sarsa for estimating Q ⇡ q⇤ or q⇡\r
Initialize Q(s, a) arbitrarily, for all s 2 S, a 2 A\r
Initialize ⇡ to be "-greedy with respect to Q, or to a fixed given policy\r
Algorithm parameters: step size ↵ 2 (0, 1], small " > 0, a positive integer n\r
All store and access operations (for St, At, and Rt) can take their index mod n + 1\r
Loop for each episode:\r
Initialize and store S0 6= terminal\r
Select and store an action A0 ⇠ ⇡(·|S0)\r
T 1\r
Loop for t = 0, 1, 2,... :\r
| If t<T, then:\r
| Take action At\r
| Observe and store the next reward as Rt+1 and the next state as St+1\r
| If St+1 is terminal, then:\r
| T t + 1\r
| else:\r
| Select and store an action At+1 ⇠ ⇡(·|St+1)\r
| ⌧ t  n +1 (⌧ is the time whose estimate is being updated)\r
| If ⌧  0:\r
| G Pmin(⌧+n,T)\r
i=⌧+1 i⌧1Ri\r
| If ⌧ + n<T, then G G + nQ(S⌧+n, A⌧+n) (G⌧:⌧+n)\r
| Q(S⌧ , A⌧ ) Q(S⌧ , A⌧ ) + ↵ [G  Q(S⌧ , A⌧ )]\r
| If ⇡ is being learned, then ensure that ⇡(·|S⌧ ) is "-greedy wrt Q\r
Until ⌧ = T  1\r
Path taken\r
Action values increased\r
by one-step Sarsa\r
Action values increased\r
by 10-step Sarsa\r
G G G\r
Figure 7.4: Gridworld example of the speedup of policy learning due to the use of n-step\r
methods. The first panel shows the path taken by an agent in a single episode, ending at a\r
location of high reward, marked by the G. In this example the values were all initially 0, and all\r
rewards were zero except for a positive reward at G. The arrows in the other two panels show\r
which action values were strengthened as a result of this path by one-step and n-step Sarsa\r
methods. The one-step method strengthens only the last action of the sequence of actions that\r
led to the high reward, whereas the n-step method strengthens the last n actions of the sequence,\r
so that much more is learned from the one episode."""

[[sections]]
number = "148"
title = "Chapter 7: n-step Bootstrapping"
text = """
Exercise 7.4 Prove that the n-step return of Sarsa (7.4) can be written exactly in terms\r
of a novel TD error, as\r
Gt:t+n = Qt1(St, At)+\r
min(t\r
X\r
+n,T)1\r
k=t\r
kt [Rk+1 + Qk(Sk+1, Ak+1)  Qk1(Sk, Ak)] .\r
(7.6)\r
⇤\r
What about Expected Sarsa? The backup diagram for the n-step version of Expected\r
Sarsa is shown on the far right in Figure 7.3. It consists of a linear string of sample\r
actions and states, just as in n-step Sarsa, except that its last element is a branch over\r
all action possibilities weighted, as always, by their probability under ⇡. This algorithm\r
can be described by the same equation as n-step Sarsa (above) except with the n-step\r
return redefined as\r
Gt:t+n\r
.\r
= Rt+1 + ··· + n1Rt+n + nV¯t+n1(St+n), t + n < T, (7.7)\r
(with Gt:t+n\r
.\r
=Gt for t + n  T) where V¯t(s) is the expected approximate value of state s,\r
using the estimated action values at time t, under the target policy:\r
V¯t(s) .= X\r
a\r
⇡(a|s)Qt(s, a), for all s 2 S. (7.8)\r
Expected approximate values are used in developing many of the action-value methods\r
in the rest of this book. If s is terminal, then its expected approximate value is defined\r
to be 0.\r
7.3 n-step O↵-policy Learning\r
Recall that o↵-policy learning is learning the value function for one policy, ⇡, while\r
following another policy, b. Often, ⇡ is the greedy policy for the current action-value\u0002function estimate, and b is a more exploratory policy, perhaps "-greedy. In order to\r
use the data from b we must take into account the di↵erence between the two policies,\r
using their relative probability of taking the actions that were taken (see Section 5.5). In\r
n-step methods, returns are constructed over n steps, so we are interested in the relative\r
probability of just those n actions. For example, to make a simple o↵-policy version of\r
n-step TD, the update for time t (actually made at time t + n) can simply be weighted\r
by ⇢t:t+n1:\r
Vt+n(St) .= Vt+n1(St) + ↵⇢t:t+n1 [Gt:t+n  Vt+n1(St)] , 0  t < T, (7.9)\r
where ⇢t:t+n1, called the importance sampling ratio, is the relative probability under\r
the two policies of taking the n actions from At to At+n1 (cf. Eq. 5.3):\r
⇢t:h\r
.\r
=\r
min(\r
Y\r
h,T 1)\r
k=t\r
⇡(Ak|Sk)\r
b(Ak|Sk)\r
. (7.10)

7.3. n-step O↵-policy Learning 149\r
For example, if any one of the actions would never be taken by ⇡ (i.e., ⇡(Ak|Sk) = 0) then\r
the n-step return should be given zero weight and be totally ignored. On the other hand,\r
if by chance an action is taken that ⇡ would take with much greater probability than b\r
does, then this will increase the weight that would otherwise be given to the return. This\r
makes sense because that action is characteristic of ⇡ (and therefore we want to learn\r
about it) but is selected only rarely by b and thus rarely appears in the data. To make\r
up for this we have to over-weight it when it does occur. Note that if the two policies\r
are actually the same (the on-policy case) then the importance sampling ratio is always\r
1. Thus our new update (7.9) generalizes and can completely replace our earlier n-step\r
TD update. Similarly, our previous n-step Sarsa update can be completely replaced by a\r
simple o↵-policy form:\r
Qt+n(St, At) .= Qt+n1(St, At) + ↵⇢t+1:t+n [Gt:t+n  Qt+n1(St, At)] , (7.11)\r
for 0  t<T. Note that the importance sampling ratio here starts and ends one step\r
later than for n-step TD (7.9). This is because here we are updating a state–action\r
pair. We do not have to care how likely we were to select the action; now that we have\r
selected it we want to learn fully from what happens, with importance sampling only for\r
subsequent actions. Pseudocode for the full algorithm is shown in the box below.\r
O↵-policy n-step Sarsa for estimating Q ⇡ q⇤ or q⇡\r
Input: an arbitrary behavior policy b such that b(a|s) > 0, for all s 2 S, a 2 A\r
Initialize Q(s, a) arbitrarily, for all s 2 S, a 2 A\r
Initialize ⇡ to be greedy with respect to Q, or as a fixed given policy\r
Algorithm parameters: step size ↵ 2 (0, 1], a positive integer n\r
All store and access operations (for St, At, and Rt) can take their index mod n + 1\r
Loop for each episode:\r
Initialize and store S0 6= terminal\r
Select and store an action A0 ⇠ b(·|S0)\r
T 1\r
Loop for t = 0, 1, 2,... :\r
| If t<T, then:\r
| Take action At\r
| Observe and store the next reward as Rt+1 and the next state as St+1\r
| If St+1 is terminal, then:\r
| T t + 1\r
| else:\r
| Select and store an action At+1 ⇠ b(·|St+1)\r
| ⌧ t  n +1 (⌧ is the time whose estimate is being updated)\r
| If ⌧  0:\r
| ⇢ Qmin(⌧+n,T 1)\r
i=⌧+1\r
⇡(Ai|Si)\r
b(Ai|Si) (⇢⌧+1:⌧+n)\r
| G Pmin(⌧+n,T )\r
i=⌧+1 i⌧1Ri\r
| If ⌧ + n<T, then: G G + nQ(S⌧+n, A⌧+n) (G⌧:⌧+n)\r
| Q(S⌧ , A⌧ ) Q(S⌧ , A⌧ ) + ↵⇢ [G  Q(S⌧ , A⌧ )]\r
| If ⇡ is being learned, then ensure that ⇡(·|S⌧ ) is greedy wrt Q\r
Until ⌧ = T  1"""

[[sections]]
number = "150"
title = "Chapter 7: n-step Bootstrapping"
text = """
The o↵-policy version of n-step Expected Sarsa would use the same update as above\r
for n-step Sarsa except that the importance sampling ratio would have one less factor in\r
it. That is, the above equation would use ⇢t+1:t+n1 instead of ⇢t+1:t+n, and of course\r
it would use the Expected Sarsa version of the n-step return (7.7). This is because in\r
Expected Sarsa all possible actions are taken into account in the last state; the one\r
actually taken has no e↵ect and does not have to be corrected for.\r
7.4 *Per-decision Methods with Control Variates\r
The multi-step o↵-policy methods presented in the previous section are simple and\r
conceptually clear, but are probably not the most ecient. A more sophisticated approach\r
would use per-decision importance sampling ideas such as were introduced in Section 5.9.\r
To understand this approach, first note that the ordinary n-step return (7.1), like all\r
returns, can be written recursively. For the n steps ending at horizon h, the n-step return\r
can be written\r
Gt:h = Rt+1 + Gt+1:h, t < h < T, (7.12)\r
where Gh:h\r
.\r
= Vh1(Sh). (Recall that this return is used at time h, previously denoted\r
t + n.) Now consider the e↵ect of following a behavior policy b that is not the same\r
as the target policy ⇡. All of the resulting experience, including the first reward Rt+1\r
and the next state St+1, must be weighted by the importance sampling ratio for time t,\r
⇢t = ⇡(At|St)\r
b(At|St) . One might be tempted to simply weight the righthand side of the above\r
equation, but one can do better. Suppose the action at time t would never be selected by\r
⇡, so that ⇢t is zero. Then a simple weighting would result in the n-step return being\r
zero, which could result in high variance when it was used as a target. Instead, in this\r
more sophisticated approach, one uses an alternate, o↵-policy definition of the n-step\r
return ending at horizon h, as\r
Gt:h\r
.\r
= ⇢t (Rt+1 + Gt+1:h) + (1  ⇢t)Vh1(St), t < h < T, (7.13)\r
where again Gh:h\r
.\r
= Vh1(Sh). In this approach, if ⇢t is zero, then instead of the target\r
being zero and causing the estimate to shrink, the target is the same as the estimate and\r
causes no change. The importance sampling ratio being zero means we should ignore the\r
sample, so leaving the estimate unchanged seems appropriate. The second, additional\r
term in (7.13) is called a control variate (for obscure reasons). Notice that the control\r
variate does not change the expected update; the importance sampling ratio has expected\r
value one (Section 5.9) and is uncorrelated with the estimate, so the expected value\r
of the control variate is zero. Also note that the o↵-policy definition (7.13) is a strict\r
generalization of the earlier on-policy definition of the n-step return (7.1), as the two are\r
identical in the on-policy case, in which ⇢t is always 1.\r
For a conventional n-step method, the learning rule to use in conjunction with (7.13)\r
is the n-step TD update (7.2), which has no explicit importance sampling ratios other\r
than those embedded in the return.\r
Exercise 7.5 Write the pseudocode for the o↵-policy state-value prediction algorithm\r
described above. ⇤"""

[[sections]]
number = "7.4"
title = "Per-decision Methods with Control Variates 151"
text = """
For action values, the o↵-policy definition of the n-step return is a little di↵erent\r
because the first action does not play a role in the importance sampling. That first action\r
is the one being learned; it does not matter if it was unlikely or even impossible under the\r
target policy—it has been taken and now full unit weight must be given to the reward\r
and state that follows it. Importance sampling will apply only to the actions that follow\r
it.\r
First note that for action values the n-step on-policy return ending at horizon h,\r
expectation form (7.7), can be written recursively just as in (7.12), except that for action\r
values the recursion ends with Gh:h\r
.\r
= V¯h1(Sh) as in (7.8). An o↵-policy form with\r
control variates is\r
Gt:h\r
.\r
= Rt+1 + \r
⇣\r
⇢t+1Gt+1:h + V¯h1(St+1)  ⇢t+1Qh1(St+1, At+1)\r
⌘\r
,\r
= Rt+1 + ⇢t+1⇣Gt+1:h  Qh1(St+1, At+1)\r
⌘\r
+ V¯h1(St+1), t<h  T.\r
(7.14)\r
If h<T, then the recursion ends with Gh:h\r
.\r
= Qh1(Sh, Ah), whereas, if h  T,\r
the recursion ends with and GT 1:h\r
.\r
= RT . The resultant prediction algorithm (after\r
combining with (7.5)) is analogous to Expected Sarsa.\r
Exercise 7.6 Prove that the control variate in the above equations does not change the\r
expected value of the return. ⇤\r
⇤\r
Exercise 7.7 Write the pseudocode for the o↵-policy action-value prediction algorithm\r
described immediately above. Pay particular attention to the termination conditions for\r
the recursion upon hitting the horizon or the end of episode. ⇤\r
Exercise 7.8 Show that the general (o↵-policy) version of the n-step return (7.13) can\r
still be written exactly and compactly as the sum of state-based TD errors (6.5) if the\r
approximate state value function does not change. ⇤\r
Exercise 7.9 Repeat the above exercise for the action version of the o↵-policy n-step\r
return (7.14) and the Expected Sarsa TD error (the quantity in brackets in Equation 6.9).\r
⇤\r
Exercise 7.10 (programming) Devise a small o↵-policy prediction problem and use it to\r
show that the o↵-policy learning algorithm using (7.13) and (7.2) is more data ecient\r
than the simpler algorithm using (7.1) and (7.9). ⇤\r
The importance sampling that we have used in this section, the previous section, and\r
in Chapter 5, enables sound o↵-policy learning, but also results in high variance updates,\r
forcing the use of a small step-size parameter and thereby causing learning to be slow. It\r
is probably inevitable that o↵-policy training is slower than on-policy training—after all,\r
the data is less relevant to what is being learned. However, it is probably also true that\r
these methods can be improved on. The control variates are one way of reducing the\r
variance. Another is to rapidly adapt the step sizes to the observed variance, as in the\r
Autostep method (Mahmood, Sutton, Degris and Pilarski, 2012). Yet another promising\r
approach is the invariant updates of Karampatziakis and Langford (2010) as extended\r
to TD by Tian (in preparation). The usage technique of Mahmood (2017; Mahmood"""

[[sections]]
number = "152"
title = "Chapter 7: n-step Bootstrapping"
text = """
and Sutton, 2015) may also be part of the solution. In the next section we consider an\r
o↵-policy learning method that does not use importance sampling."""

[[sections]]
number = "7.5"
title = "O↵-policy Learning Without Importance Sampling:"
text = """
The n-step Tree Backup Algorithm\r
Is o↵-policy learning possible without importance sampling? Q-learning and Expected\r
Sarsa from Chapter 6 do this for the one-step case, but is there a corresponding multi-step\r
algorithm? In this section we present just such an n-step method, called the tree-backup\r
algorithm.\r
St, At\r
At+1\r
Rt+1\r
St+1\r
St+2\r
Rt+2\r
At+2 Rt+3\r
St+3\r
the 3-step\r
tree-backup\r
update\r
The idea of the algorithm is suggested by the 3-step tree-backup backup\r
diagram shown to the right. Down the central spine and labeled in the\r
diagram are three sample states and rewards, and two sample actions.\r
These are the random variables representing the events occurring after the\r
initial state–action pair St, At. Hanging o↵ to the sides of each state are\r
the actions that were not selected. (For the last state, all the actions are\r
considered to have not (yet) been selected.) Because we have no sample\r
data for the unselected actions, we bootstrap and use the estimates of\r
their values in forming the target for the update. This slightly extends the\r
idea of a backup diagram. So far we have always updated the estimated\r
value of the node at the top of the diagram toward a target combining\r
the rewards along the way (appropriately discounted) and the estimated\r
values of the nodes at the bottom. In the tree-backup update, the target\r
includes all these things plus the estimated values of the dangling action\r
nodes hanging o↵ the sides, at all levels. This is why it is called a tree\u0002backup update; it is an update from the entire tree of estimated action\r
values.\r
More precisely, the update is from the estimated action values of the\r
leaf nodes of the tree. The action nodes in the interior, corresponding to\r
the actual actions taken, do not participate. Each leaf node contributes to the target\r
with a weight proportional to its probability of occurring under the target policy ⇡. Thus\r
each first-level action a contributes with a weight of ⇡(a|St+1), except that the action\r
actually taken, At+1, does not contribute at all. Its probability, ⇡(At+1|St+1), is used\r
to weight all the second-level action values. Thus, each non-selected second-level action\r
a0 contributes with weight ⇡(At+1|St+1)⇡(a0|St+2). Each third-level action contributes\r
with weight ⇡(At+1|St+1)⇡(At+2|St+2)⇡(a00|St+3), and so on. It is as if each arrow to an\r
action node in the diagram is weighted by the action’s probability of being selected under\r
the target policy and, if there is a tree below the action, then that weight applies to all\r
the leaf nodes in the tree."""

[[sections]]
number = "7.5"
title = "O↵-policy Learning Without Importance Sampling: n-step Tree Backup 153"
text = """
We can think of the 3-step tree-backup update as consisting of 6 half-steps, alternating\r
between sample half-steps from an action to a subsequent state, and expected half-steps\r
considering from that state all possible actions with their probabilities of occurring under\r
the policy.\r
Now let us develop the detailed equations for the n-step tree-backup algorithm. The\r
one-step return (target) is the same as that of Expected Sarsa,\r
Gt:t+1\r
.\r
= Rt+1 + \r
X\r
a\r
⇡(a|St+1)Qt(St+1, a), (7.15)\r
for t<T  1, and the two-step tree-backup return is\r
Gt:t+2\r
.\r
= Rt+1 + \r
X\r
a6=At+1\r
⇡(a|St+1)Qt+1(St+1, a)\r
+ ⇡(At+1|St+1)\r
⇣\r
Rt+2 + \r
X\r
a\r
⇡(a|St+2)Qt+1(St+2, a)\r
⌘\r
= Rt+1 + \r
X\r
a6=At+1\r
⇡(a|St+1)Qt+1(St+1, a) + ⇡(At+1|St+1)Gt+1:t+2,\r
for t<T  2. The latter form suggests the general recursive definition of the tree-backup\r
n-step return:\r
Gt:t+n\r
.\r
= Rt+1 + \r
X\r
a6=At+1\r
⇡(a|St+1)Qt+n1(St+1, a) +  ⇡(At+1|St+1)Gt+1:t+n, (7.16)\r
for t<T  1, n  2, with the n = 1 case handled by (7.15) except for GT 1:t+n\r
.\r
= RT .\r
This target is then used with the usual action-value update rule from n-step Sarsa:\r
Qt+n(St, At) .= Qt+n1(St, At) + ↵ [Gt:t+n  Qt+n1(St, At)] ,\r
for 0  t<T, while the values of all other state–action pairs remain unchanged:\r
Qt+n(s, a) = Qt+n1(s, a), for all s, a such that s 6= St or a 6= At. Pseudocode for this\r
algorithm is shown in the box on the next page.\r
Exercise 7.11 Show that if the approximate action values are unchanging, then the\r
tree-backup return (7.16) can be written as a sum of expectation-based TD errors:\r
Gt:t+n = Q(St, At) +\r
min(t+\r
Xn1,T 1)\r
k=t\r
k\r
Y\r
k\r
i=t+1\r
⇡(Ai|Si),\r
where t\r
.\r
= Rt+1 + V¯t(St+1)  Q(St, At) and V¯t is given by (7.8). ⇤"""

[[sections]]
number = "154"
title = "Chapter 7: n-step Bootstrapping"
text = """
n-step Tree Backup for estimating Q ⇡ q⇤ or q⇡\r
Initialize Q(s, a) arbitrarily, for all s 2 S, a 2 A\r
Initialize ⇡ to be greedy with respect to Q, or as a fixed given policy\r
Algorithm parameters: step size ↵ 2 (0, 1], a positive integer n\r
All store and access operations can take their index mod n + 1\r
Loop for each episode:\r
Initialize and store S0 6= terminal\r
Choose an action A0 arbitrarily as a function of S0; Store A0\r
T 1\r
Loop for t = 0, 1, 2,... :\r
| If t<T:\r
| Take action At; observe and store the next reward and state as Rt+1, St+1\r
| If St+1 is terminal:\r
| T t + 1\r
| else:\r
| Choose an action At+1 arbitrarily as a function of St+1; Store At+1\r
| ⌧ t + 1  n (⌧ is the time whose estimate is being updated)\r
| If ⌧  0:\r
| If t + 1  T:\r
| G RT\r
| else\r
| G Rt+1 + \r
P\r
a ⇡(a|St+1)Q(St+1, a)\r
| Loop for k = min(t, T  1) down through ⌧ + 1:\r
| G Rk + \r
P\r
a6=Ak ⇡(a|Sk)Q(Sk, a) + ⇡(Ak|Sk)G\r
| Q(S⌧ , A⌧ ) Q(S⌧ , A⌧ ) + ↵ [G  Q(S⌧ , A⌧ )]\r
| If ⇡ is being learned, then ensure that ⇡(·|S⌧ ) is greedy wrt Q\r
Until ⌧ = T  1\r
7.6 *A Unifying Algorithm: n-step Q()\r
So far in this chapter we have considered three di↵erent kinds of action-value algorithms,\r
corresponding to the first three backup diagrams shown in Figure 7.5. n-step Sarsa has\r
all sample transitions, the tree-backup algorithm has all state-to-action transitions fully\r
branched without sampling, and n-step Expected Sarsa has all sample transitions except\r
for the last state-to-action one, which is fully branched with an expected value. To what\r
extent can these algorithms be unified?\r
One idea for unification is suggested by the fourth backup diagram in Figure 7.5. This\r
is the idea that one might decide on a step-by-step basis whether one wanted to take the\r
action as a sample, as in Sarsa, or consider the expectation over all actions instead, as in\r
the tree-backup update. Then, if one chose always to sample, one would obtain Sarsa,\r
whereas if one chose never to sample, one would get the tree-backup algorithm. Expected\r
Sarsa would be the case where one chose to sample for all steps except for the last one."""

[[sections]]
number = "7.6"
title = "A Unifying Algorithm: n-step Q() 155"
text = """
⇢\r
⇢\r
⇢\r
⇢\r
⇢\r
⇢\r
⇢\r
⇢\r
⇢\r
 = 1\r
 = 0\r
 = 1\r
 = 0\r
4-step\r
Sarsa\r
4-step\r
Tree backup\r
4-step\r
Expected Sarsa\r
4-step\r
Q()\r
Figure 7.5: The backup diagrams of the three kinds of n-step action-value updates considered\r
so far in this chapter (4-step case) plus the backup diagram of a fourth kind of update that unifies\r
them all. The label ‘⇢’ indicates half transitions on which importance sampling is required in the\r
o↵-policy case. The fourth kind of update unifies all the others by choosing on a state-by-state\r
basis whether to sample (t = 1) or not (t = 0).\r
And of course there would be many other possibilities, as suggested by the last diagram\r
in the figure. To increase the possibilities even further we can consider a continuous\r
variation between sampling and expectation. Let t 2 [0, 1] denote the degree of sampling\r
on step t, with  = 1 denoting full sampling and  = 0 denoting a pure expectation with\r
no sampling. The random variable t might be set as a function of the state, action, or\r
state–action pair at time t. We call this proposed new algorithm n-step Q().\r
Now let us develop the equations of n-step Q(). First we write the tree-backup\r
n-step return (7.16) in terms of the horizon h = t + n and then in terms of the expected\r
approximate value V¯ (7.8):\r
Gt:h = Rt+1 + \r
X\r
a6=At+1\r
⇡(a|St+1)Qh1(St+1, a) +  ⇡(At+1|St+1)Gt+1:h\r
= Rt+1 + V¯h1(St+1)  ⇡(At+1|St+1)Qh1(St+1, At+1) + ⇡(At+1|St+1)Gt+1:h\r
= Rt+1 + ⇡(At+1|St+1)\r
⇣\r
Gt+1:h  Qh1(St+1, At+1)\r
⌘\r
+ V¯h1(St+1),\r
after which it is exactly like the n-step return for Sarsa with control variates (7.14) except\r
with the action probability ⇡(At+1|St+1) substituted for the importance-sampling ratio\r
⇢t+1. For Q(), we slide linearly between these two cases:\r
Gt:h\r
.\r
= Rt+1 + \r
⇣\r
t+1⇢t+1 + (1  t+1)⇡(At+1|St+1)\r
⌘⇣Gt+1:h  Qh1(St+1, At+1)⌘\r
+ V¯h1(St+1), (7.17)"""

[[sections]]
number = "156"
title = "Chapter 7: n-step Bootstrapping"
text = """
for t<h  T. The recursion ends with Gh:h\r
.\r
= Qh1(Sh, Ah) if h<T, or with\r
GT 1:T\r
.\r
= RT if h = T. Then we use the earlier update for n-step Sarsa without\r
importance-sampling ratios (7.5) instead of (7.11), because now the ratios are incorporated\r
in the n-step return. A complete algorithm is given in the box.\r
O↵-policy n-step Q() for estimating Q ⇡ q⇤ or q⇡\r
Input: an arbitrary behavior policy b such that b(a|s) > 0, for all s 2 S, a 2 A\r
Initialize Q(s, a) arbitrarily, for all s 2 S, a 2 A\r
Initialize ⇡ to be greedy with respect to Q, or else it is a fixed given policy\r
Algorithm parameters: step size ↵ 2 (0, 1], a positive integer n\r
All store and access operations can take their index mod n + 1\r
Loop for each episode:\r
Initialize and store S0 6= terminal\r
Choose and store an action A0 ⇠ b(·|S0)\r
T 1\r
Loop for t = 0, 1, 2,... :\r
| If t<T:\r
| Take action At; observe and store the next reward and state as Rt+1, St+1\r
| If St+1 is terminal:\r
| T t + 1\r
| else:\r
| Choose and store an action At+1 ⇠ b(·|St+1)\r
| Select and store t+1\r
| Store ⇡(At+1|St+1)\r
b(At+1|St+1) as ⇢t+1\r
| ⌧ t  n +1 (⌧ is the time whose estimate is being updated)\r
| If ⌧  0:\r
| If t + 1 < T:\r
| G Q(St+1, At+1)\r
| Loop for k = min(t + 1, T) down through ⌧ + 1:\r
| if k = T:\r
| G RT\r
| else:\r
| V¯ P\r
a ⇡(a|Sk)Q(Sk, a)\r
| G Rk + \r
\r
k⇢k + (1  k)⇡(Ak|Sk)\r
G  Q(Sk, Ak)\r
+ V¯\r
| Q(S⌧ , A⌧ ) Q(S⌧ , A⌧ ) + ↵ [G  Q(S⌧ , A⌧ )]\r
| If ⇡ is being learned, then ensure that ⇡(·|S⌧ ) is greedy wrt Q\r
Until ⌧ = T  1"""

[[sections]]
number = "7.7"
title = "Summary 157"
text = ""

[[sections]]
number = "7.7"
title = "Summary"
text = """
In this chapter we have developed a range of temporal-di↵erence learning methods that lie\r
in between the one-step TD methods of the previous chapter and the Monte Carlo methods\r
of the chapter before. Methods that involve an intermediate amount of bootstrapping\r
are important because they will typically perform better than either extreme.\r
⇢\r
⇢\r
= 1\r
= 0\r
= 1\r
= 0\r
4-step\r
Q()\r
⇢\r
⇢\r
⇢\r
⇢\r
4-step\r
TD\r
Our focus in this chapter has been on n-step methods, which\r
look ahead to the next n rewards, states, and actions. The two\r
4-step backup diagrams to the right together summarize most of the\r
methods introduced. The state-value update shown is for n-step\r
TD with importance sampling, and the action-value update is for\r
n-step Q(), which generalizes Expected Sarsa and Q-learning. All\r
n-step methods involve a delay of n time steps before updating,\r
as only then are all the required future events known. A further\r
drawback is that they involve more computation per time step\r
than previous methods. Compared to one-step methods, n-step\r
methods also require more memory to record the states, actions,\r
rewards, and sometimes other variables over the last n time steps.\r
Eventually, in Chapter 12, we will see how multi-step TD methods\r
can be implemented with minimal memory and computational\r
complexity using eligibility traces, but there will always be some\r
additional computation beyond one-step methods. Such costs can\r
be well worth paying to escape the tyranny of the single time step.\r
Although n-step methods are more complex than those using\r
eligibility traces, they have the great benefit of being conceptually\r
clear. We have sought to take advantage of this by developing two\r
approaches to o↵-policy learning in the n-step case. One, based on\r
importance sampling is conceptually simple but can be of high variance. If the target and\r
behavior policies are very di↵erent it probably needs some new algorithmic ideas before\r
it can be ecient and practical. The other, based on tree-backup updates, is the natural\r
extension of Q-learning to the multi-step case with stochastic target policies. It involves\r
no importance sampling but, again if the target and behavior policies are substantially\r
di↵erent, the bootstrapping may span only a few steps even if n is large."""

[[sections]]
number = "158"
title = "Chapter 7: n-step Bootstrapping"
text = """
Bibliographical and Historical Remarks\r
The notion of n-step returns is due to Watkins (1989), who also first discussed their error\r
reduction property. n-step algorithms were explored in the first edition of this book,\r
in which they were treated as of conceptual interest, but not feasible in practice. The\r
work of Cichosz (1995) and particularly van Seijen (2016) showed that they are actually\r
completely practical algorithms. Given this, and their conceptual clarity and simplicity,\r
we have chosen to highlight them here in the second edition. In particular, we now\r
postpone all discussion of the backward view and of eligibility traces until Chapter 12.\r
7.1–2 The results in the random walk examples were made for this text based on work\r
of Sutton (1988) and Singh and Sutton (1996). The use of backup diagrams to\r
describe these and other algorithms in this chapter is new.\r
7.3–5 The developments in these sections are based on the work of Precup, Sutton,\r
and Singh (2000), Precup, Sutton, and Dasgupta (2001), and Sutton, Mahmood,\r
Precup, and van Hasselt (2014).\r
The tree-backup algorithm is due to Precup, Sutton, and Singh (2000), but the\r
presentation of it here is new."""

[[sections]]
number = "7.6"
title = "The Q() algorithm is new to this text, but closely related algorithms have been"
text = """
explored further by De Asis, Hernandez-Garcia, Holland, and Sutton (2017).

Chapter 8\r
Planning and Learning with\r
Tabular Methods\r
In this chapter we develop a unified view of reinforcement learning methods that require\r
a model of the environment, such as dynamic programming and heuristic search, and\r
methods that can be used without a model, such as Monte Carlo and temporal-di↵erence\r
methods. These are respectively called model-based and model-free reinforcement learning\r
methods. Model-based methods rely on planning as their primary component, while\r
model-free methods primarily rely on learning. Although there are real di↵erences between\r
these two kinds of methods, there are also great similarities. In particular, the heart of\r
both kinds of methods is the computation of value functions. Moreover, all the methods\r
are based on looking ahead to future events, computing a backed-up value, and then\r
using it as an update target for an approximate value function. Earlier in this book we\r
presented Monte Carlo and temporal-di↵erence methods as distinct alternatives, then\r
showed how they can be unified by n-step methods. Our goal in this chapter is a similar\r
integration of model-based and model-free methods. Having established these as distinct\r
in earlier chapters, we now explore the extent to which they can be intermixed."""

[[sections]]
number = "8.1"
title = "Models and Planning"
text = """
By a model of the environment we mean anything that an agent can use to predict how the\r
environment will respond to its actions. Given a state and an action, a model produces a\r
prediction of the resultant next state and next reward. If the model is stochastic, then\r
there are several possible next states and next rewards, each with some probability of\r
occurring. Some models produce a description of all possibilities and their probabilities;\r
these we call distribution models. Other models produce just one of the possibilities,\r
sampled according to the probabilities; these we call sample models. For example, consider\r
modeling the sum of a dozen dice. A distribution model would produce all possible sums\r
and their probabilities of occurring, whereas a sample model would produce an individual"""

[[sections]]
number = "160"
title = "Chapter 8: Planning and Learning with Tabular Methods"
text = """
sum drawn according to this probability distribution. The kind of model assumed in\r
dynamic programming—estimates of the MDP’s dynamics, p(s0, r|s, a)—is a distribution\r
model. The kind of model used in the blackjack example in Chapter 5 is a sample model.\r
Distribution models are stronger than sample models in that they can always be used\r
to produce samples. However, in many applications it is much easier to obtain sample\r
models than distribution models. The dozen dice are a simple example of this. It would\r
be easy to write a computer program to simulate the dice rolls and return the sum, but\r
harder and more error-prone to figure out all the possible sums and their probabilities.\r
Models can be used to mimic or simulate experience. Given a starting state and action,\r
a sample model produces a possible transition, and a distribution model generates all\r
possible transitions weighted by their probabilities of occurring. Given a starting state\r
and a policy, a sample model could produce an entire episode, and a distribution model\r
could generate all possible episodes and their probabilities. In either case, we say the\r
model is used to simulate the environment and produce simulated experience.\r
The word planning is used in several di↵erent ways in di↵erent fields. We use the\r
term to refer to any computational process that takes a model as input and produces or\r
improves a policy for interacting with the modeled environment:\r
planning model policy\r
In artificial intelligence, there are two distinct approaches to planning according to our\r
definition. State-space planning, which includes the approach we take in this book,\r
is viewed primarily as a search through the state space for an optimal policy or an\r
optimal path to a goal. Actions cause transitions from state to state, and value functions\r
are computed over states. In what we call plan-space planning, planning is instead a\r
search through the space of plans. Operators transform one plan into another, and\r
value functions, if any, are defined over the space of plans. Plan-space planning includes\r
evolutionary methods and “partial-order planning,” a common kind of planning in artificial\r
intelligence in which the ordering of steps is not completely determined at all stages of\r
planning. Plan-space methods are dicult to apply eciently to the stochastic sequential\r
decision problems that are the focus in reinforcement learning, and we do not consider\r
them further (but see, e.g., Russell and Norvig, 2010).\r
The unified view we present in this chapter is that all state-space planning methods\r
share a common structure, a structure that is also present in the learning methods\r
presented in this book. It takes the rest of the chapter to develop this view, but there are\r
two basic ideas: (1) all state-space planning methods involve computing value functions\r
as a key intermediate step toward improving the policy, and (2) they compute value\r
functions by updates or backup operations applied to simulated experience. This common\r
structure can be diagrammed as follows:\r
values\r
backups model simulated\r
experience policy backups updates\r
Dynamic programming methods clearly fit this structure: they make sweeps through the\r
space of states, generating for each state the distribution of possible transitions. Each\r
distribution is then used to compute a backed-up value (update target) and update the"""

[[sections]]
number = "8.2"
title = "Dyna: Integrated Planning, Acting, and Learning 161"
text = """
state’s estimated value. In this chapter we argue that various other state-space planning\r
methods also fit this structure, with individual methods di↵ering only in the kinds of\r
updates they do, the order in which they do them, and in how long the backed-up\r
information is retained.\r
Viewing planning methods in this way emphasizes their relationship to the learning\r
methods that we have described in this book. The heart of both learning and planning\r
methods is the estimation of value functions by backing-up update operations. The\r
di↵erence is that whereas planning uses simulated experience generated by a model,\r
learning methods use real experience generated by the environment. Of course this\r
di↵erence leads to a number of other di↵erences, for example, in how performance is\r
assessed and in how flexibly experience can be generated. But the common structure\r
means that many ideas and algorithms can be transferred between planning and learning.\r
In particular, in many cases a learning algorithm can be substituted for the key update\r
step of a planning method. Learning methods require only experience as input, and in\r
many cases they can be applied to simulated experience just as well as to real experience.\r
The box below shows a simple example of a planning method based on one-step tabular\r
Q-learning and on random samples from a sample model. This method, which we call\r
random-sample one-step tabular Q-planning, converges to the optimal policy for the model\r
under the same conditions that one-step tabular Q-learning converges to the optimal\r
policy for the real environment (each state–action pair must be selected an infinite number\r
of times in Step 1, and ↵ must decrease appropriately over time).\r
Random-sample one-step tabular Q-planning\r
Loop forever:"""

[[sections]]
number = "1"
title = "Select a state, S 2 S, and an action, A 2 A(S), at random"
text = ""

[[sections]]
number = "2"
title = "Send S, A to a sample model, and obtain"
text = "a sample next reward, R, and a sample next state, S0"

[[sections]]
number = "3"
title = "Apply one-step tabular Q-learning to S, A, R, S0:"
text = """
Q(S, A) Q(S, A) + ↵\r
⇥\r
R +  maxa Q(S0, a)  Q(S, A)\r
⇤\r
In addition to the unified view of planning and learning methods, a second theme in\r
this chapter is the benefits of planning in small, incremental steps. This enables planning\r
to be interrupted or redirected at any time with little wasted computation, which appears\r
to be a key requirement for eciently intermixing planning with acting and with learning\r
of the model. Planning in very small steps may be the most ecient approach even on\r
pure planning problems if the problem is too large to be solved exactly."""

[[sections]]
number = "8.2"
title = "Dyna: Integrated Planning, Acting, and Learning"
text = """
When planning is done online, while interacting with the environment, a number of\r
interesting issues arise. New information gained from the interaction may change the\r
model and thereby interact with planning. It may be desirable to customize the planning\r
process in some way to the states or decisions currently under consideration, or expected"""

[[sections]]
number = "162"
title = "Chapter 8: Planning and Learning with Tabular Methods"
text = """
in the near future. If decision making and model learning are both computation-intensive\r
processes, then the available computational resources may need to be divided between\r
them. To begin exploring these issues, in this section we present Dyna-Q, a simple\r
architecture integrating the major functions needed in an online planning agent. Each\r
function appears in Dyna-Q in a simple, almost trivial, form. In subsequent sections we\r
elaborate some of the alternate ways of achieving each function and the trade-o↵s between\r
them. For now, we seek merely to illustrate the ideas and stimulate your intuition.\r
Within a planning agent, there are at least two roles for real experience: it can be\r
used to improve the model (to make it more accurately match the real environment)\r
and it can be used to directly improve the value function and policy using the kinds of\r
planning\r
value/policy\r
model experience\r
model\r
learning\r
acting\r
direct\r
RL\r
reinforcement learning methods we have discussed\r
in previous chapters. The former we call model\u0002learning, and the latter we call direct reinforcement\r
learning (direct RL). The possible relationships\r
between experience, model, values, and policy are\r
summarized in the diagram to the right. Each ar\u0002row shows a relationship of influence and presumed\r
improvement. Note how experience can improve\r
value functions and policies either directly or in\u0002directly via the model. It is the latter, which is\r
sometimes called indirect reinforcement learning,\r
that is involved in planning.\r
Both direct and indirect methods have advantages and disadvantages. Indirect methods\r
often make fuller use of a limited amount of experience and thus achieve a better policy\r
with fewer environmental interactions. On the other hand, direct methods are much\r
simpler and are not a↵ected by biases in the design of the model. Some have argued\r
that indirect methods are always superior to direct ones, while others have argued that\r
direct methods are responsible for most human and animal learning. Related debates\r
in psychology and artificial intelligence concern the relative importance of cognition as\r
opposed to trial-and-error learning, and of deliberative planning as opposed to reactive\r
decision making (see Chapter 14 for discussion of some of these issues from the perspective\r
of psychology). Our view is that the contrast between the alternatives in all these debates\r
has been exaggerated, that more insight can be gained by recognizing the similarities\r
between these two sides than by opposing them. For example, in this book we have\r
emphasized the deep similarities between dynamic programming and temporal-di↵erence\r
methods, even though one was designed for planning and the other for model-free learning.\r
Dyna-Q includes all of the processes shown in the diagram above—planning, acting,\r
model-learning, and direct RL—all occurring continually. The planning method is the\r
random-sample one-step tabular Q-planning method on page 161. The direct RL method\r
is one-step tabular Q-learning. The model-learning method is also table-based and assumes\r
the environment is deterministic. After each transition St, At ! Rt+1, St+1, the model\r
records in its table entry for St, At the prediction that Rt+1, St+1 will deterministically\r
follow. Thus, if the model is queried with a state–action pair that has been experienced\r
before, it simply returns the last-observed next state and next reward as its prediction."""

[[sections]]
number = "8.2"
title = "Dyna: Integrated Planning, Acting, and Learning 163"
text = """
During planning, the Q-planning algorithm randomly samples only from state–action\r
pairs that have previously been experienced (in Step 1), so the model is never queried\r
with a pair about which it has no information.\r
The overall architecture of Dyna agents, of which the Dyna-Q algorithm is one example,\r
is shown in Figure 8.1. The central column represents the basic interaction between\r
agent and environment, giving rise to a trajectory of real experience. The arrow on the\r
left of the figure represents direct reinforcement learning operating on real experience to\r
improve the value function and the policy. On the right are model-based processes. The\r
model is learned from real experience and gives rise to simulated experience. We use the\r
term search control to refer to the process that selects the starting states and actions\r
for the simulated experiences generated by the model. Finally, planning is achieved by\r
applying reinforcement learning methods to the simulated experiences just as if they had\r
really happened. Typically, as in Dyna-Q, the same reinforcement learning method is\r
used both for learning from real experience and for planning from simulated experience.\r
The reinforcement learning method is thus the “final common path” for both learning\r
and planning. Learning and planning are deeply integrated in the sense that they share\r
almost all the same machinery, di↵ering only in the source of their experience.\r
real\r
direct RL\r
update\r
Model\r
planning update\r
search\r
control\r
Policy/value functions\r
experience\r
model\r
learning\r
Environment\r
simulated\r
experience\r
Figure 8.1: The general Dyna Architecture. Real experience, passing back and forth between\r
the environment and the policy, a↵ects policy and value functions in much the same way as does\r
simulated experience generated by the model of the environment.\r
Conceptually, planning, acting, model-learning, and direct RL occur simultaneously\r
and in parallel in Dyna agents. For concreteness and implementation on a serial computer,\r
however, we fully specify the order in which they occur within a time step. In Dyna-Q,\r
the acting, model-learning, and direct RL processes require little computation, and we\r
assume they consume just a fraction of the time. The remaining time in each step can be\r
devoted to the planning process, which is inherently computation-intensive. Let us assume\r
that there is time in each step, after acting, model-learning, and direct RL, to complete"""

[[sections]]
number = "164"
title = "Chapter 8: Planning and Learning with Tabular Methods"
text = """
n iterations (Steps 1–3) of the Q-planning algorithm. In the pseudocode algorithm for\r
Dyna-Q in the box below, Model(s, a) denotes the contents of the model (predicted\r
next state and reward) for state–action pair (s, a). Direct reinforcement learning, model\u0002learning, and planning are implemented by steps (d), (e), and (f), respectively. If (e) and\r
(f) were omitted, the remaining algorithm would be one-step tabular Q-learning.\r
Tabular Dyna-Q\r
Initialize Q(s, a) and Model(s, a) for all s 2 S and a 2 A(s)\r
Loop forever:\r
(a) S current (nonterminal) state\r
(b) A "-greedy(S, Q)\r
(c) Take action A; observe resultant reward, R, and state, S0\r
(d) Q(S, A) Q(S, A) + ↵\r
⇥\r
R +  maxa Q(S0, a)  Q(S, A)\r
⇤\r
(e) Model(S, A) R, S0 (assuming deterministic environment)\r
(f) Loop repeat n times:\r
S random previously observed state\r
A random action previously taken in S\r
R, S0 Model(S, A)\r
Q(S, A) Q(S, A) + ↵\r
⇥\r
R +  maxa Q(S0, a)  Q(S, A)\r
⇤\r
Example 8.1: Dyna Maze Consider the simple maze shown inset in Figure 8.2. In\r
each of the 47 states there are four actions, up, down, right, and left, which take the\r
agent deterministically to the corresponding neighboring states, except when movement\r
is blocked by an obstacle or the edge of the maze, in which case the agent remains where\r
it is. Reward is zero on all transitions, except those into the goal state, on which it is +1.\r
After reaching the goal state (G), the agent returns to the start state (S) to begin a new\r
episode. This is a discounted, episodic task with  = 0.95.\r
The main part of Figure 8.2 shows average learning curves from an experiment in\r
which Dyna-Q agents were applied to the maze task. The initial action values were zero,\r
the step-size parameter was ↵ = 0.1, and the exploration parameter was " = 0.1. When\r
selecting greedily among actions, ties were broken randomly. The agents varied in the\r
number of planning steps, n, they performed per real step. For each n, the curves show\r
the number of steps taken by the agent to reach the goal in each episode, averaged over 30\r
repetitions of the experiment. In each repetition, the initial seed for the random number\r
generator was held constant across algorithms. Because of this, the first episode was\r
exactly the same (about 1700 steps) for all values of n, and its data are not shown in\r
the figure. After the first episode, performance improved for all values of n, but much\r
more rapidly for larger values. Recall that the n = 0 agent is a nonplanning agent, using\r
only direct reinforcement learning (one-step tabular Q-learning). This was by far the\r
slowest agent on this problem, despite the fact that the parameter values (↵ and ") were\r
optimized for it. The nonplanning agent took about 25 episodes to reach ("-)optimal\r
performance, whereas the n = 5 agent took about five episodes, and the n = 50 agent\r
took only three episodes."""

[[sections]]
number = "8.2"
title = "Dyna: Integrated Planning, Acting, and Learning 165"
text = """
2\r
800\r
600\r
400\r
200\r
14\r
10 20 30 40 50\r
0 planning steps\r
(direct RL only)\r
Episodes\r
Steps\r
per\r
episode 5 planning steps\r
50 planning steps\r
S\r
G\r
actions\r
Figure 8.2: A simple maze (inset) and the average learning curves for Dyna-Q agents varying\r
in their number of planning steps (n) per real step. The task is to travel from S to G as quickly\r
as possible.\r
Figure 8.3 shows why the planning agents found the solution so much faster than\r
the nonplanning agent. Shown are the policies found by the n = 0 and n = 50 agents\r
halfway through the second episode. Without planning (n = 0), each episode adds only\r
one additional step to the policy, and so only one step (the last) has been learned so far.\r
With planning, again only one step is learned during the first episode, but here during\r
the second episode an extensive policy has been developed that by the end of the episode\r
will reach almost back to the start state. This policy is built by the planning process\r
while the agent is still wandering near the start state. By the end of the third episode a\r
complete optimal policy will have been found and perfect performance attained.\r
S\r
G\r
S\r
G\r
WITHOUT PLANNING (n=0) WITH PLANNING (n=50)\r
Figure 8.3: Policies found by planning and nonplanning Dyna-Q agents halfway through the\r
second episode. The arrows indicate the greedy action in each state; if no arrow is shown for a\r
state, then all of its action values were equal. The black square indicates the location of the\r
agent."""

[[sections]]
number = "166"
title = "Chapter 8: Planning and Learning with Tabular Methods"
text = """
In Dyna-Q, learning and planning are accomplished by exactly the same algorithm,\r
operating on real experience for learning and on simulated experience for planning.\r
Because planning proceeds incrementally, it is trivial to intermix planning and acting.\r
Both proceed as fast as they can. The agent is always reactive and always deliberative,\r
responding instantly to the latest sensory information and yet always planning in the\r
background. Also ongoing in the background is the model-learning process. As new\r
information is gained, the model is updated to better match reality. As the model changes,\r
the ongoing planning process will gradually compute a di↵erent way of behaving to match\r
the new model.\r
Exercise 8.1 The nonplanning method looks particularly poor in Figure 8.3 because it is\r
a one-step method; a method using multi-step bootstrapping would do better. Do you\r
think one of the multi-step bootstrapping methods from Chapter 7 could do as well as\r
the Dyna method? Explain why or why not. ⇤"""

[[sections]]
number = "8.3"
title = "When the Model Is Wrong"
text = """
In the maze example presented in the previous section, the changes in the model were\r
relatively modest. The model started out empty, and was then filled only with exactly\r
correct information. In general, we cannot expect to be so fortunate. Models may be\r
incorrect because the environment is stochastic and only a limited number of samples\r
have been observed, or because the model was learned using function approximation that\r
has generalized imperfectly, or simply because the environment has changed and its new\r
behavior has not yet been observed. When the model is incorrect, the planning process is\r
likely to compute a suboptimal policy.\r
In some cases, the suboptimal policy computed by planning quickly leads to the\r
discovery and correction of the modeling error. This tends to happen when the model\r
is optimistic in the sense of predicting greater reward or better state transitions than\r
are actually possible. The planned policy attempts to exploit these opportunities and in\r
doing so discovers that they do not exist.\r
Example 8.2: Blocking Maze A maze example illustrating this relatively minor\r
kind of modeling error and recovery from it is shown in Figure 8.4. Initially, there is a\r
short path from start to goal, to the right of the barrier, as shown in the upper left of the\r
figure. After 1000 time steps, the short path is “blocked,” and a longer path is opened up\r
along the left-hand side of the barrier, as shown in upper right of the figure. The graph\r
shows average cumulative reward for a Dyna-Q agent and an enhanced Dyna-Q+ agent\r
to be described shortly. The first part of the graph shows that both Dyna agents found\r
the short path within 1000 steps. When the environment changed, the graphs become\r
flat, indicating a period during which the agents obtained no reward because they were\r
wandering around behind the barrier. After a while, however, they were able to find the\r
new opening and the new optimal behavior.\r
Greater diculties arise when the environment changes to become better than it was\r
before, and yet the formerly correct policy does not reveal the improvement. In these\r
cases the modeling error may not be detected for a long time, if ever."""

[[sections]]
number = "8.3"
title = "When the Model Is Wrong 167"
text = """
Cumulative\r
reward\r
0 1000 2000 3000\r
Time steps\r
150"""

[[sections]]
number = "0"
title = "Dyna-Q+"
text = """
S\r
G G\r
S\r
Dyna-Q\r
Figure 8.4: Average performance of Dyna agents on a blocking task. The left environment\r
was used for the first 1000 steps, the right environment for the rest. Dyna-Q+ is Dyna-Q with\r
an exploration bonus that encourages exploration.\r
Cumulative\r
reward\r
S\r
G G\r
S\r
0 3000 6000\r
Time steps\r
400"""

[[sections]]
number = "0"
title = "Dyna-Q+"
text = """
Dyna-Q\r
Figure 8.5: Average performance of Dyna agents on\r
a shortcut task. The left environment was used for the\r
first 3000 steps, the right environment for the rest.\r
Example 8.3: Shortcut Maze\r
The problem caused by this kind of\r
environmental change is illustrated\r
by the maze example shown in Fig\u0002ure 8.5. Initially, the optimal path is\r
to go around the left side of the bar\u0002rier (upper left). After 3000 steps,\r
however, a shorter path is opened up\r
along the right side, without disturb\u0002ing the longer path (upper right).\r
The graph shows that the regular\r
Dyna-Q agent never switched to the\r
shortcut. In fact, it never realized\r
that it existed. Its model said that\r
there was no shortcut, so the more it\r
planned, the less likely it was to step\r
to the right and discover it. Even\r
with an "-greedy policy, it is very\r
unlikely that an agent will take so\r
many exploratory actions as to dis\u0002cover the shortcut.\r
The general problem here is another version of the conflict between exploration and\r
exploitation. In a planning context, exploration means trying actions that improve the\r
model, whereas exploitation means behaving in the optimal way given the current model."""

[[sections]]
number = "168"
title = "Chapter 8: Planning and Learning with Tabular Methods"
text = """
We want the agent to explore to find changes in the environment, but not so much that\r
performance is greatly degraded. As in the earlier exploration/exploitation conflict, there\r
probably is no solution that is both perfect and practical, but simple heuristics are often\r
e↵ective.\r
The Dyna-Q+ agent that did solve the shortcut maze uses one such heuristic. This\r
agent keeps track for each state–action pair of how many time steps have elapsed since\r
the pair was last tried in a real interaction with the environment. The more time that\r
has elapsed, the greater (we might presume) the chance that the dynamics of this pair\r
has changed and that the model of it is incorrect. To encourage behavior that tests\r
long-untried actions, a special “bonus reward” is given on simulated experiences involving\r
these actions. In particular, if the modeled reward for a transition is r, and the transition\r
has not been tried in ⌧ time steps, then planning updates are done as if that transition\r
produced a reward of r + \r
p⌧ , for some small . This encourages the agent to keep\r
testing all accessible state transitions and even to find long sequences of actions in order\r
to carry out such tests.1 Of course all this testing has its cost, but in many cases, as in the\r
shortcut maze, this kind of computational curiosity is well worth the extra exploration.\r
Exercise 8.2 Why did the Dyna agent with exploration bonus, Dyna-Q+, perform\r
better in the first phase as well as in the second phase of the blocking and shortcut\r
experiments? ⇤\r
Exercise 8.3 Careful inspection of Figure 8.5 reveals that the di↵erence between Dyna-Q+\r
and Dyna-Q narrowed slightly over the first part of the experiment. What is the reason\r
for this? ⇤\r
Exercise 8.4 (programming) The exploration bonus described above actually changes\r
the estimated values of states and actions. Is this necessary? Suppose the bonus \r
p⌧\r
was used not in updates, but solely in action selection. That is, suppose the action\r
selected was always that for which Q(St, a) + \r
p⌧ (St, a) was maximal. Carry out a\r
gridworld experiment that tests and illustrates the strengths and weaknesses of this\r
alternate approach. ⇤\r
Exercise 8.5 How might the tabular Dyna-Q algorithm shown on page 164 be modified\r
to handle stochastic environments? How might this modification perform poorly on\r
changing environments such as considered in this section? How could the algorithm be\r
modified to handle stochastic environments and changing environments? ⇤"""

[[sections]]
number = "8.4"
title = "Prioritized Sweeping"
text = """
In the Dyna agents presented in the preceding sections, simulated transitions are started in\r
state–action pairs selected uniformly at random from all previously experienced pairs. But\r
a uniform selection is usually not the best; planning can be much more ecient if simulated\r
transitions and updates are focused on particular state–action pairs. For example, consider\r
1The Dyna-Q+ agent was changed in two other ways as well. First, actions that had never been\r
tried before from a state were allowed to be considered in the planning step (f) of the Tabular Dyna-Q\r
algorithm in the box above. Second, the initial model for such actions was that they would lead back to\r
the same state with a reward of zero."""

[[sections]]
number = "8.4"
title = "Prioritized Sweeping 169"
text = """
what happens during the second episode of the first maze task (Figure 8.3). At the\r
beginning of the second episode, only the state–action pair leading directly into the goal\r
has a positive value; the values of all other pairs are still zero. This means that it is\r
pointless to perform updates along almost all transitions, because they take the agent\r
from one zero-valued state to another, and thus the updates would have no e↵ect. Only\r
an update along a transition into the state just prior to the goal, or from it, will change\r
any values. If simulated transitions are generated uniformly, then many wasteful updates\r
will be made before stumbling onto one of these useful ones. As planning progresses, the\r
region of useful updates grows, but planning is still far less ecient than it would be if\r
focused where it would do the most good. In the much larger problems that are our real\r
objective, the number of states is so large that an unfocused search would be extremely\r
inecient.\r
This example suggests that search might be usefully focused by working backward from\r
goal states. Of course, we do not really want to use any methods specific to the idea of\r
“goal state.” We want methods that work for general reward functions. Goal states are\r
just a special case, convenient for stimulating intuition. In general, we want to work back\r
not just from goal states but from any state whose value has changed. Suppose that the\r
values are initially correct given the model, as they were in the maze example prior to\r
discovering the goal. Suppose now that the agent discovers a change in the environment\r
and changes its estimated value of one state, either up or down. Typically, this will imply\r
that the values of many other states should also be changed, but the only useful one-step\r
updates are those of actions that lead directly into the one state whose value has been\r
changed. If the values of these actions are updated, then the values of the predecessor\r
states may change in turn. If so, then actions leading into them need to be updated, and\r
then their predecessor states may have changed. In this way one can work backward\r
from arbitrary states that have changed in value, either performing useful updates or\r
terminating the propagation. This general idea might be termed backward focusing of\r
planning computations.\r
As the frontier of useful updates propagates backward, it often grows rapidly, producing\r
many state–action pairs that could usefully be updated. But not all of these will be\r
equally useful. The values of some states may have changed a lot, whereas others may\r
have changed little. The predecessor pairs of those that have changed a lot are more\r
likely to also change a lot. In a stochastic environment, variations in estimated transition\r
probabilities also contribute to variations in the sizes of changes and in the urgency with\r
which pairs need to be updated. It is natural to prioritize the updates according to a\r
measure of their urgency, and perform them in order of priority. This is the idea behind\r
prioritized sweeping. A queue is maintained of every state–action pair whose estimated\r
value would change nontrivially if updated , prioritized by the size of the change. When\r
the top pair in the queue is updated, the e↵ect on each of its predecessor pairs is computed.\r
If the e↵ect is greater than some small threshold, then the pair is inserted in the queue\r
with the new priority (if there is a previous entry of the pair in the queue, then insertion\r
results in only the higher priority entry remaining in the queue). In this way the e↵ects\r
of changes are eciently propagated backward until quiescence. The full algorithm for\r
the case of deterministic environments is given in the box on the next page."""

[[sections]]
number = "170"
title = "Chapter 8: Planning and Learning with Tabular Methods"
text = """
Prioritized sweeping for a deterministic environment\r
Initialize Q(s, a), Model(s, a), for all s, a, and P Queue to empty\r
Loop forever:\r
(a) S current (nonterminal) state\r
(b) A policy(S, Q)\r
(c) Take action A; observe resultant reward, R, and state, S0\r
(d) Model(S, A) R, S0\r
(e) P |R +  maxa Q(S0, a)  Q(S, A)|.\r
(f) if P > ✓, then insert S, A into P Queue with priority P\r
(g) Loop repeat n times, while P Queue is not empty:\r
S, A f irst(P Queue)\r
R, S0 Model(S, A)\r
Q(S, A) Q(S, A) + ↵\r
⇥\r
R +  maxa Q(S0, a)  Q(S, A)\r
⇤\r
Loop for all S, ¯ A¯ predicted to lead to S:\r
R¯ predicted reward for S, ¯ A, S ¯\r
P |R¯ +  maxa Q(S, a)  Q(S, ¯ A¯)|.\r
if P > ✓ then insert S, ¯ A¯ into P Queue with priority P\r
Backups\r
until\r
optimal\r
solution\r
10\r
103\r
104\r
105\r
106\r
107\r
102\r
0 47 94 186 376 752 1504 3008 6016\r
Gridworld size (#states)\r
Dyna-Q\r
Prioritized\r
sweeping\r
Updates Updates\r
until\r
optimal\r
solution\r
Example 8.4: Prioritized Sweeping\r
on Mazes Prioritized sweeping has been\r
found to dramatically increase the speed\r
at which optimal solutions are found in\r
maze tasks, often by a factor of 5 to 10.\r
A typical example is shown to the right.\r
These data are for a sequence of maze\r
tasks of exactly the same structure as the\r
one shown in Figure 8.2, except that they\r
vary in the grid resolution. Prioritized\r
sweeping maintained a decisive advantage\r
over unprioritized Dyna-Q. Both systems\r
made at most n = 5 updates per environ\u0002mental interaction. Adapted from Peng\r
and Williams (1993).\r
Extensions of prioritized sweeping to stochastic environments are straightforward. The\r
model is maintained by keeping counts of the number of times each state–action pair has\r
been experienced and of what the next states were. It is natural then to update each pair\r
not with a sample update, as we have been using so far, but with an expected update,\r
taking into account all possible next states and their probabilities of occurring.\r
Prioritized sweeping is just one way of distributing computations to improve planning\r
eciency, and probably not the best way. One of prioritized sweeping’s limitations is that\r
it uses expected updates, which in stochastic environments may waste lots of computation\r
on low-probability transitions. As we show in the following section, sample updates"""

[[sections]]
number = "8.4"
title = "Prioritized Sweeping 171"
text = """
Example 8.5 Prioritized Sweeping for Rod Maneuvering\r
Start\r
Goal\r
The objective in this task is to\r
maneuver a rod around some awk\u0002wardly placed obstacles within a\r
limited rectangular work space to a\r
goal position in the fewest number\r
of steps. The rod can be translated\r
along its long axis or perpendicu\u0002lar to that axis, or it can be ro\u0002tated in either direction around its\r
center. The distance of each move\u0002ment is approximately 1/20 of the\r
work space, and the rotation incre\u0002ment is 10 degrees. Translations\r
are deterministic and quantized to\r
one of 20 ⇥ 20 positions. To the\r
right is shown the obstacles and the\r
shortest solution from start to goal,\r
found by prioritized sweeping. This problem is deterministic, but has four actions\r
and 14,400 potential states (some of these are unreachable because of the obstacles).\r
This problem is probably too large to be solved with unprioritized methods. Figure\r
reprinted from Moore and Atkeson (1993).\r
can in many cases get closer to the true value function with less computation despite\r
the variance introduced by sampling. Sample updates can win because they break the\r
overall backing-up computation into smaller pieces—those corresponding to individual\r
transitions—which then enables it to be focused more narrowly on the pieces that will\r
have the largest impact. This idea was taken to what may be its logical limit in the “small\r
backups” introduced by van Seijen and Sutton (2013). These are updates along a single\r
transition, like a sample update, but based on the probability of the transition without\r
sampling, as in an expected update. By selecting the order in which small updates\r
are done it is possible to greatly improve planning eciency beyond that possible with\r
prioritized sweeping.\r
We have suggested in this chapter that all kinds of state-space planning can be viewed\r
as sequences of value updates, varying only in the type of update, expected or sample,\r
large or small, and in the order in which the updates are done. In this section we have\r
emphasized backward focusing, but this is just one strategy. For example, another would\r
be to focus on states according to how easily they can be reached from the states that\r
are visited frequently under the current policy, which might be called forward focusing.\r
Peng and Williams (1993) and Barto, Bradtke and Singh (1995) have explored versions\r
of forward focusing, and the methods introduced in the next few sections take it to an\r
extreme form."""

[[sections]]
number = "172"
title = "Chapter 8: Planning and Learning with Tabular Methods"
text = ""

[[sections]]
number = "8.5"
title = "Expected vs. Sample Updates"
text = """
The examples in the previous sections give some idea of the range of possibilities for\r
combining methods of learning and planning. In the rest of this chapter, we analyze some\r
of the component ideas involved, starting with the relative advantages of expected and\r
sample updates.\r
Much of this book has been about di↵erent kinds of value-function updates, and we\r
have considered a great many varieties. Focusing for the moment on one-step updates,\r
they vary primarily along three binary dimensions. The first two dimensions are whether\r
they update state values or action values and whether they estimate the value for the\r
optimal policy or for an arbitrary given policy. These two dimensions give rise to four\r
classes of updates for approximating the four value functions, q⇤, v⇤, q⇡, and v⇡. The\r
Value\r
estimated\r
Expected updates\r
(DP)\r
Sample updates \r
(one-step TD)\r
⇡\r
s\r
s0\r
⇡\r
r p\r
a\r
q⇡(s, a)\r
q⇤(s, a)\r
v⇡(s)\r
v⇤(s)\r
s\r
s0\r
r\r
max\r
a\r
p\r
policy evaluation\r
value iteration\r
r\r
s0\r
s, a\r
a0\r
⇡\r
p\r
q-policy evaluation\r
r\r
s0\r
s, a\r
a0\r
max\r
p\r
q-value iteration\r
s\r
A\r
S0\r
R\r
R\r
S0\r
s, a\r
A0\r
R\r
S0\r
s, a\r
max\r
TD(0)\r
Sarsa\r
Q-learning\r
a0\r
Figure 8.6: Backup diagrams for all the one-step\r
updates considered in this book.\r
other binary dimension is whether the\r
updates are expected updates, consider\u0002ing all possible events that might hap\u0002pen, or sample updates, considering a\r
single sample of what might happen.\r
These three binary dimensions give rise\r
to eight cases, seven of which corre\u0002spond to specific algorithms, as shown\r
in the figure to the right. (The eighth\r
case does not seem to correspond to\r
any useful update.) Any of these one\u0002step updates can be used in planning\r
methods. The Dyna-Q agents discussed\r
earlier use q⇤ sample updates, but they\r
could just as well use q⇤ expected up\u0002dates, or either expected or sample q⇡\r
updates. The Dyna-AC system uses v⇡\r
sample updates together with a learning\r
policy structure (as in Chapter 13). For\r
stochastic problems, prioritized sweep\u0002ing is always done using one of the ex\u0002pected updates.\r
When we introduced one-step sam\u0002ple updates in Chapter 6, we presented\r
them as substitutes for expected up\u0002dates. In the absence of a distribution\r
model, expected updates are not pos\u0002sible, but sample updates can be done\r
using sample transitions from the envi\u0002ronment or a sample model. Implicit in\r
that point of view is that expected up\u0002dates, if possible, are preferable to sam\u0002ple updates. But are they? Expected"""

[[sections]]
number = "8.5"
title = "Expected vs. Sample Updates 173"
text = """
updates certainly yield a better estimate because they are uncorrupted by sampling error,\r
but they also require more computation, and computation is often the limiting resource\r
in planning. To properly assess the relative merits of expected and sample updates for\r
planning we must control for their di↵erent computational requirements.\r
For concreteness, consider the expected and sample updates for approximating q⇤,\r
and the special case of discrete states and actions, a table-lookup representation of\r
the approximate value function, Q, and a model in the form of estimated dynamics,\r
pˆ(s0, r|s, a). The expected update for a state–action pair, s, a, is:\r
Q(s, a) X\r
s0,r\r
pˆ(s0, r|s, a)\r
h\r
r +  max\r
a0 Q(s0\r
, a0)\r
i\r
. (8.1)\r
The corresponding sample update for s, a, given a sample next state and reward, S0 and\r
R (from the model), is the Q-learning-like update:\r
Q(s, a) Q(s, a) + ↵\r
h\r
R +  max\r
a0 Q(S0\r
, a0)  Q(s, a)\r
i\r
, (8.2)\r
where ↵ is the usual positive step-size parameter.\r
The di↵erence between these expected and sample updates is significant to the extent\r
that the environment is stochastic, specifically, to the extent that, given a state and\r
action, many possible next states may occur with various probabilities. If only one next\r
state is possible, then the expected and sample updates given above are identical (taking\r
↵ = 1). If there are many possible next states, then there may be significant di↵erences.\r
In favor of the expected update is that it is an exact computation, resulting in a new\r
Q(s, a) whose correctness is limited only by the correctness of the Q(s0, a0) at successor\r
states. The sample update is in addition a↵ected by sampling error. On the other hand,\r
the sample update is cheaper computationally because it considers only one next state,\r
not all possible next states. In practice, the computation required by update operations\r
is usually dominated by the number of state–action pairs at which Q is evaluated. For a\r
particular starting pair, s, a, let b be the branching factor (i.e., the number of possible\r
next states, s0, for which pˆ(s0 |s, a) > 0). Then an expected update of this pair requires\r
roughly b times as much computation as a sample update.\r
If there is enough time to complete an expected update, then the resulting estimate is\r
generally better than that of b sample updates because of the absence of sampling error.\r
But if there is insucient time to complete an expected update, then sample updates are\r
always preferable because they at least make some improvement in the value estimate\r
with fewer than b updates. In a large problem with many state–action pairs, we are often\r
in the latter situation. With so many state–action pairs, expected updates of all of them\r
would take a very long time. Before that we may be much better o↵ with a few sample\r
updates at many state–action pairs than with expected updates at a few pairs. Given a\r
unit of computational e↵ort, is it better devoted to a few expected updates or to b times\r
as many sample updates?\r
Figure 8.7 shows the results of an analysis that suggests an answer to this question. It\r
shows the estimation error as a function of computation time for expected and sample\r
updates for a variety of branching factors, b. The case considered is that in which all"""

[[sections]]
number = "174"
title = "Chapter 8: Planning and Learning with Tabular Methods"
text = """
b=2 (branching factor)\r
b=10\r
b=100\r
b=1000 b=10,000\r
sample\r
updates\r
expected\r
updates\r
1\r
0\r
0 1b 2b\r
RMS error\r
in value\r
estimate\r
Number of computations max\r
a0 Q(s0\r
, a0)\r
Figure 8.7: Comparison of eciency of expected and sample updates.\r
b successor states are equally likely and in which the error in the initial estimate is\r
1. The values at the next states are assumed correct, so the expected update reduces\r
the error to zero upon its completion. In this case, sample updates reduce the error\r
according to qb1\r
bt where t is the number of sample updates that have been performed\r
(assuming sample averages, i.e., ↵ = 1/t). The key observation is that for moderately\r
large b the error falls dramatically with a tiny fraction of b updates. For these cases,\r
many state–action pairs could have their values improved dramatically, to within a few\r
percent of the e↵ect of an expected update, in the same time that a single state–action\r
pair could undergo an expected update.\r
The advantage of sample updates shown in Figure 8.7 is probably an underestimate of\r
the real e↵ect. In a real problem, the values of the successor states would be estimates\r
that are themselves updated. By causing estimates to be more accurate sooner, sample\r
updates will have a second advantage in that the values backed up from the successor\r
states will be more accurate. These results suggest that sample updates are likely to be\r
superior to expected updates on problems with large stochastic branching factors and\r
too many states to be solved exactly.\r
Exercise 8.6 The analysis above assumed that all of the b possible next states were\r
equally likely to occur. Suppose instead that the distribution was highly skewed, that\r
some of the b states were much more likely to occur than most. Would this strengthen or\r
weaken the case for sample updates over expected updates? Support your answer. ⇤"""

[[sections]]
number = "8.6"
title = "Trajectory Sampling"
text = """
In this section we compare two ways of distributing updates. The classical approach, from\r
dynamic programming, is to perform sweeps through the entire state (or state–action)\r
space, updating each state (or state–action pair) once per sweep. This is problematic"""

[[sections]]
number = "8.6"
title = "Trajectory Sampling 175"
text = """
on large tasks because there may not be time to complete even one sweep. In many\r
tasks the vast majority of the states are irrelevant because they are visited only under\r
very poor policies or with very low probability. Exhaustive sweeps implicitly devote\r
equal time to all parts of the state space rather than focusing where it is needed. As we\r
discussed in Chapter 4, exhaustive sweeps and the equal treatment of all states that they\r
imply are not necessary properties of dynamic programming. In principle, updates can\r
be distributed any way one likes (to assure convergence, all states or state–action pairs\r
must be visited in the limit an infinite number of times; although an exception to this is\r
discussed in Section 8.7 below), but in practice exhaustive sweeps are often used.\r
The second approach is to sample from the state or state–action space according\r
to some distribution. One could sample uniformly, as in the Dyna-Q agent, but this\r
would su↵er from some of the same problems as exhaustive sweeps. More appealing\r
is to distribute updates according to the on-policy distribution, that is, according to\r
the distribution observed when following the current policy. One advantage of this\r
distribution is that it is easily generated; one simply interacts with the model, following\r
the current policy. In an episodic task, one starts in a start state (or according to the\r
starting-state distribution) and simulates until the terminal state. In a continuing task,\r
one starts anywhere and just keeps simulating. In either case, sample state transitions\r
and rewards are given by the model, and sample actions are given by the current policy.\r
In other words, one simulates explicit individual trajectories and performs updates at the\r
state or state–action pairs encountered along the way. We call this way of generating\r
experience and updates trajectory sampling.\r
It is hard to imagine any ecient way of distributing updates according to the on-policy\r
distribution other than by trajectory sampling. If one had an explicit representation\r
of the on-policy distribution, then one could sweep through all states, weighting the\r
update of each according to the on-policy distribution, but this leaves us again with all\r
the computational costs of exhaustive sweeps. Possibly one could sample and update\r
individual state–action pairs from the distribution, but even if this could be done eciently,\r
what benefit would this provide over simulating trajectories? Even knowing the on-policy\r
distribution in an explicit form is unlikely. The distribution changes whenever the policy\r
changes, and computing the distribution requires computation comparable to a complete\r
policy evaluation. Consideration of such other possibilities makes trajectory sampling\r
seem both ecient and elegant.\r
Is the on-policy distribution of updates a good one? Intuitively it seems like a good\r
choice, at least better than the uniform distribution. For example, if you are learning to\r
play chess, you study positions that might arise in real games, not random positions of\r
chess pieces. The latter may be valid states, but to be able to accurately value them is a\r
di↵erent skill from evaluating positions in real games. We will also see in Part II that the\r
on-policy distribution has significant advantages when function approximation is used.\r
Whether or not function approximation is used, one might expect on-policy focusing to\r
significantly improve the speed of planning.\r
Focusing on the on-policy distribution could be beneficial because it causes vast,\r
uninteresting parts of the space to be ignored, or it could be detrimental because it causes\r
the same old parts of the space to be updated over and over. We conducted a small"""

[[sections]]
number = "176"
title = "Chapter 8: Planning and Learning with Tabular Methods"
text = """
experiment to assess the e↵ect empirically. To isolate the e↵ect of the update distribution,\r
we used entirely one-step expected tabular updates, as defined by (8.1). In the uniform\r
case, we cycled through all state–action pairs, updating each in place, and in the on-policy\r
case we simulated episodes, all starting in the same state, updating each state–action pair\r
that occurred under the current "-greedy policy ("= 0.1). The tasks were undiscounted\r
episodic tasks, generated randomly as follows. From each of the |S| states, two actions\r
were possible, each of which resulted in one of b next states, all equally likely, with a\r
di↵erent random selection of b states for each state–action pair. The branching factor, b,\r
was the same for all state–action pairs. In addition, on all transitions there was a 0.1\r
probability of transition to the terminal state, ending the episode. The expected reward\r
on each transition was selected from a Gaussian distribution with mean 0 and variance 1.\r
b=10\r
b=3\r
b=1\r
b=1\r
ion-pol cy\r
ion-pol cy\r
uniform\r
uniform\r
0\r
1\r
2"""

[[sections]]
number = "3"
title = "Value of"
text = """
start state\r
under\r
greedy\r
policy\r
0 5,000 10,000 15,000 20,000\r
Computation time, in full backups\r
0\r
1\r
2"""

[[sections]]
number = "3"
title = "Value of"
text = """
start state\r
under\r
greedy\r
policy\r
0 50,000 100,000 150,000 200,000\r
Computation time, in full backups\r
uniform\r
uniform\r
on-policy\r
on-policy\r
expected updates\r
expected updates\r
1,000 STATES\r
10,000 STATES\r
Figure 8.8: Relative eciency of updates dis\u0002tributed uniformly across the state space versus\r
focused on simulated on-policy trajectories, each\r
starting in the same state. Results are for randomly\r
generated tasks of two sizes and various branching\r
factors, b.\r
At any point in the planning process\r
one can stop and exhaustively compute\r
v⇡˜(s0), the true value of the start state\r
under the greedy policy, ⇡˜, given the cur\u0002rent action-value function Q, as an indi\u0002cation of how well the agent would do on\r
a new episode on which it acted greed\u0002ily (all the while assuming the model is\r
correct).\r
The upper part of the figure to\r
the right shows results averaged over\r
200 sample tasks with 1000 states and\r
branching factors of 1, 3, and 10. The\r
quality of the policies found is plotted as\r
a function of the number of expected up\u0002dates completed. In all cases, sampling\r
according to the on-policy distribution\r
resulted in faster planning initially and\r
retarded planning in the long run. The\r
e↵ect was stronger, and the initial pe\u0002riod of faster planning was longer, at\r
smaller branching factors. In other ex\u0002periments, we found that these e↵ects\r
also became stronger as the number of\r
states increased. For example, the lower\r
part of the figure shows results for a\r
branching factor of 1 for tasks with\r
10,000 states. In this case the advan\u0002tage of on-policy focusing is large and\r
long-lasting.\r
All of these results make sense. In the\r
short term, sampling according to the\r
on-policy distribution helps by focusing\r
on states that are near descendants of"""

[[sections]]
number = "8.7"
title = "Real-time Dynamic Programming 177"
text = """
the start state. If there are many states and a small branching factor, this e↵ect will be\r
large and long-lasting. In the long run, focusing on the on-policy distribution may hurt\r
because the commonly occurring states all already have their correct values. Sampling\r
them is useless, whereas sampling other states may actually perform some useful work.\r
This presumably is why the exhaustive, unfocused approach does better in the long run,\r
at least for small problems. These results are not conclusive because they are only for\r
problems generated in a particular, random way, but they do suggest that sampling\r
according to the on-policy distribution can be a great advantage for large problems, in\r
particular for problems in which a small subset of the state–action space is visited under\r
the on-policy distribution.\r
Exercise 8.7 Some of the graphs in Figure 8.8 seem to be scalloped in their early portions,\r
particularly the upper graph for b = 1 and the uniform distribution. Why do you think\r
this is? What aspects of the data shown support your hypothesis? ⇤\r
Exercise 8.8 (programming) Replicate the experiment whose results are shown in the\r
lower part of Figure 8.8, then try the same experiment but with b = 3. Discuss the\r
meaning of your results. ⇤"""

[[sections]]
number = "8.7"
title = "Real-time Dynamic Programming"
text = """
Real-time dynamic programming, or RTDP, is an on-policy trajectory-sampling version of\r
the value-iteration algorithm of dynamic programming (DP). Because it is closely related\r
to conventional sweep-based policy iteration, RTDP illustrates in a particularly clear way\r
some of the advantages that on-policy trajectory sampling can provide. RTDP updates\r
the values of states visited in actual or simulated trajectories by means of expected\r
tabular value-iteration updates as defined by (4.10). It is basically the algorithm that\r
produced the on-policy results shown in Figure 8.8.\r
The close connection between RTDP and conventional DP makes it possible to derive\r
some theoretical results by adapting existing theory. RTDP is an example of an asyn\u0002chronous DP algorithm as described in Section 4.5. Asynchronous DP algorithms are\r
not organized in terms of systematic sweeps of the state set; they update state values in\r
any order whatsoever, using whatever values of other states happen to be available. In\r
RTDP, the update order is dictated by the order states are visited in real or simulated\r
trajectories.\r
Start States\r
Irrelevant States: \r
unreachable from any start state\r
under any optimal policy\r
Relevant States\r
reachable from some start state \r
under some optimal policy\r
If trajectories can start only from a designated\r
set of start states, and if you are interested in\r
the prediction problem for a given policy, then on\u0002policy trajectory sampling allows the algorithm to\r
completely skip states that cannot be reached by\r
the given policy from any of the start states: such\r
states are irrelevant to the prediction problem.\r
For a control problem, where the goal is to find\r
an optimal policy instead of evaluating a given\r
policy, there might well be states that cannot be"""

[[sections]]
number = "178"
title = "Chapter 8: Planning and Learning with Tabular Methods"
text = """
reached by any optimal policy from any of the start states, and there is no need to specify\r
optimal actions for these irrelevant states. What is needed is an optimal partial policy,\r
meaning a policy that is optimal for the relevant states but can specify arbitrary actions,\r
or even be undefined, for the irrelevant states.\r
But finding such an optimal partial policy with an on-policy trajectory-sampling\r
control method, such as Sarsa (Section 6.4), in general requires visiting all state–action\r
pairs—even those that will turn out to be irrelevant—an infinite number of times. This\r
can be done, for example, by using exploring starts (Section 5.3). This is true for RTDP\r
as well: for episodic tasks with exploring starts, RTDP is an asynchronous value-iteration\r
algorithm that converges to optimal policies for discounted finite MDPs (and for the\r
undiscounted case under certain conditions). Unlike the situation for a prediction problem,\r
it is generally not possible to stop updating any state or state–action pair if convergence\r
to an optimal policy is important.\r
The most interesting result for RTDP is that for certain types of problems satisfying\r
reasonable conditions, RTDP is guaranteed to find a policy that is optimal on the relevant\r
states without visiting every state infinitely often, or even without visiting some states at\r
all. Indeed, in some problems, only a small fraction of the states need to be visited. This\r
can be a great advantage for problems with very large state sets, where even a single\r
sweep may not be feasible.\r
The tasks for which this result holds are undiscounted episodic tasks for MDPs with\r
absorbing goal states that generate zero rewards, as described in Section 3.4. At every step\r
of a real or simulated trajectory, RTDP selects a greedy action (breaking ties randomly)\r
and applies the expected value-iteration update operation to the current state. It can\r
also update the values of an arbitrary collection of other states at each step; for example,\r
it can update the values of states visited in a limited-horizon look-ahead search from the\r
current state.\r
For these problems, with each episode beginning in a state randomly chosen from the\r
set of start states and ending at a goal state, RTDP converges with probability one to a\r
policy that is optimal for all the relevant states provided: (1) the initial value of every\r
goal state is zero, (2) there exists at least one policy that guarantees that a goal state\r
will be reached with probability one from any start state, (3) all rewards for transitions\r
from non-goal states are strictly negative, and (4) all the initial values are equal to, or\r
greater than, their optimal values (which can be satisfied by simply setting the initial\r
values of all states to zero). This result was proved by Barto, Bradtke, and Singh (1995)\r
by combining results for asynchronous DP with results about a heuristic search algorithm\r
known as learning real-time A* due to Korf (1990).\r
Tasks having these properties are examples of stochastic optimal path problems, which\r
are usually stated in terms of cost minimization instead of as reward maximization as\r
we do here. Maximizing the negative returns in our version is equivalent to minimizing\r
the costs of paths from a start state to a goal state. Examples of this kind of task are\r
minimum-time control tasks, where each time step required to reach a goal produces a\r
reward of 1, or problems like the Golf example in Section 3.5, whose objective is to hit\r
the hole with the fewest strokes."""

[[sections]]
number = "8.7"
title = "Real-time Dynamic Programming 179"
text = """
Example 8.6: RTDP on the Racetrack The racetrack problem of Exercise 5.12\r
(page 111) is a stochastic optimal path problem. Comparing RTDP and the conventional\r
DP value iteration algorithm on an example racetrack problem illustrates some of the\r
advantages of on-policy trajectory sampling.\r
Recall from the exercise that an agent has to learn how to drive a car around a turn\r
like those shown in Figure 5.5 and cross the finish line as quickly as possible while staying\r
on the track. Start states are all the zero-speed states on the starting line; the goal states\r
are all the states that can be reached in one time step by crossing the finish line from\r
inside the track. Unlike Exercise 5.12, here there is no limit on the car’s speed, so the\r
state set is potentially infinite. However, the set of states that can be reached from the\r
set of start states via any policy is finite and can be considered to be the state set of the\r
problem. Each episode begins in a randomly selected start state and ends when the car\r
crosses the finish line. The rewards are 1 for each step until the car crosses the finish\r
line. If the car hits the track boundary, it is moved back to a random start state, and the\r
episode continues.\r
A racetrack similar to the small racetrack on the left of Figure 5.5 has 9,115 states\r
reachable from start states by any policy, only 599 of which are relevant, meaning that\r
they are reachable from some start state via some optimal policy. (The number of relevant\r
states was estimated by counting the states visited while executing optimal actions for\r
107 episodes.)\r
The table below compares solving this task by conventional DP and by RTDP. These\r
results are averages over 25 runs, each begun with a di↵erent random number seed.\r
Conventional DP in this case is value iteration using exhaustive sweeps of the state set,\r
with values updated one state at a time in place, meaning that the update for each state\r
uses the most recent values of the other states (This is the Gauss-Seidel version of value\r
iteration, which was found to be approximately twice as fast as the Jacobi version on\r
this problem. See Section 4.8.) No special attention was paid to the ordering of the\r
updates; other orderings could have produced faster convergence. Initial values were all\r
zero for each run of both methods. DP was judged to have converged when the maximum\r
change in a state value over a sweep was less than 104, and RTDP was judged to have\r
converged when the average time to cross the finish line over 20 episodes appeared to\r
stabilize at an asymptotic number of steps. This version of RTDP updated only the value\r
of the current state on each step.\r
DP RTDP\r
Average computation to convergence 28 sweeps 4000 episodes\r
Average number of updates to convergence 252,784 127,600\r
Average number of updates per episode — 31.9\r
% of states updated  100 times — 98.45\r
% of states updated  10 times — 80.51\r
% of states updated 0 times — 3.18\r
Both methods produced policies averaging between 14 and 15 steps to cross the finish\r
line, but RTDP required only roughly half of the updates that DP did. This is the result\r
of RTDP’s on-policy trajectory sampling. Whereas the value of every state was updated"""

[[sections]]
number = "180"
title = "Chapter 8: Planning and Learning with Tabular Methods"
text = """
in each sweep of DP, RTDP focused updates on fewer states. In an average run, RTDP\r
updated the values of 98.45% of the states no more than 100 times and 80.51% of the\r
states no more than 10 times; the values of about 290 states were not updated at all in\r
an average run.\r
Another advantage of RTDP is that as the value function approaches the optimal\r
value function v⇤, the policy used by the agent to generate trajectories approaches an\r
optimal policy because it is always greedy with respect to the current value function.\r
This is in contrast to the situation in conventional value iteration. In practice, value\r
iteration terminates when the value function changes by only a small amount in a sweep,\r
which is how we terminated it to obtain the results in the table above. At this point,\r
the value function closely approximates v⇤, and a greedy policy is close to an optimal\r
policy. However, it is possible that policies that are greedy with respect to the latest\r
value function were optimal, or nearly so, long before value iteration terminates. (Recall\r
from Chapter 4 that optimal policies can be greedy with respect to many di↵erent\r
value functions, not just v⇤.) Checking for the emergence of an optimal policy before\r
value iteration converges is not a part of the conventional DP algorithm and requires\r
considerable additional computation.\r
In the racetrack example, by running many test episodes after each DP sweep, with\r
actions selected greedily according to the result of that sweep, it was possible to estimate\r
the earliest point in the DP computation at which the approximated optimal evaluation\r
function was good enough so that the corresponding greedy policy was nearly optimal.\r
For this racetrack, a close-to-optimal policy emerged after 15 sweeps of value iteration, or\r
after 136,725 value-iteration updates. This is considerably less than the 252,784 updates\r
DP needed to converge to v⇤, but still more than the 127,600 updates RTDP required.\r
Although these simulations are certainly not definitive comparisons of the RTDP with\r
conventional sweep-based value iteration, they illustrate some of advantages of on-policy\r
trajectory sampling. Whereas conventional value iteration continued to update the value\r
of all the states, RTDP strongly focused on subsets of the states that were relevant to\r
the problem’s objective. This focus became increasingly narrow as learning continued.\r
Because the convergence theorem for RTDP applies to the simulations, we know that\r
RTDP eventually would have focused only on relevant states, i.e., on states making up\r
optimal paths. RTDP achieved nearly optimal control with about 50% of the computation\r
required by sweep-based value iteration."""

[[sections]]
number = "8.8"
title = "Planning at Decision Time"
text = """
Planning can be used in at least two ways. The one we have considered so far in this\r
chapter, typified by dynamic programming and Dyna, is to use planning to gradually\r
improve a policy or value function on the basis of simulated experience obtained from a\r
model (either a sample or a distribution model). Selecting actions is then a matter of\r
comparing the current state’s action values obtained from a table in the tabular case we\r
have thus far considered, or by evaluating a mathematical expression in the approximate\r
methods we consider in Part II below. Well before an action is selected for any current\r
state St, planning has played a part in improving the table entries, or the function"""

[[sections]]
number = "8.9"
title = "Heuristic Search 181"
text = """
approximation parameters, needed to select actions for many states, including St. Used\r
this way, planning is not focused on the current state. We call planning used in this way\r
background planning.\r
The other way to use planning is to begin and complete it after encountering each\r
new state St, as a computation whose output is the selection of a single action At; on\r
the next step planning begins anew with St+1 to produce At+1, and so on. The simplest,\r
and almost degenerate, example of this use of planning is when only state values are\r
available, and an action is selected by comparing the values of model-predicted next states\r
for each action (or by comparing the values of afterstates as in the tic-tac-toe example\r
in Chapter 1). More generally, planning used in this way can look much deeper than\r
one-step-ahead and evaluate action choices leading to many di↵erent predicted state and\r
reward trajectories. Unlike the first use of planning, here planning focuses on a particular\r
state. We call this decision-time planning.\r
These two ways of thinking about planning—using simulated experience to gradually\r
improve a policy or value function, or using simulated experience to select an action for\r
the current state—can blend together in natural and interesting ways, but they have\r
tended to be studied separately, and that is a good way to first understand them. Let us\r
now take a closer look at decision-time planning.\r
Even when planning is only done at decision time, we can still view it, as we did\r
in Section 8.1, as proceeding from simulated experience to updates and values, and\r
ultimately to a policy. It is just that now the values and policy are specific to the current\r
state and the action choices available there, so much so that the values and policy created\r
by the planning process are typically discarded after being used to select the current\r
action. In many applications this is not a great loss because there are very many states\r
and we are unlikely to return to the same state for a long time. In general, one may\r
want to do a mix of both: focus planning on the current state and store the results\r
of planning so as to be that much farther along should one return to the same state\r
later. Decision-time planning is most useful in applications in which fast responses are\r
not required. In chess playing programs, for example, one may be permitted seconds or\r
minutes of computation for each move, and strong programs may plan dozens of moves\r
ahead within this time. On the other hand, if low latency action selection is the priority,\r
then one is generally better o↵ doing planning in the background to compute a policy\r
that can then be rapidly applied to each newly encountered state."""

[[sections]]
number = "8.9"
title = "Heuristic Search"
text = """
The classical state-space planning methods in artificial intelligence are decision-time\r
planning methods collectively known as heuristic search. In heuristic search, for each\r
state encountered, a large tree of possible continuations is considered. The approximate\r
value function is applied to the leaf nodes and then backed up toward the current state\r
at the root. The backing up within the search tree is just the same as in the expected\r
updates with maxes (those for v⇤ and q⇤) discussed throughout this book. The backing\r
up stops at the state–action nodes for the current state. Once the backed-up values of\r
these nodes are computed, the best of them is chosen as the current action, and then all\r
backed-up values are discarded."""

[[sections]]
number = "182"
title = "Chapter 8: Planning and Learning with Tabular Methods"
text = """
In conventional heuristic search no e↵ort is made to save the backed-up values by\r
changing the approximate value function. In fact, the value function is generally designed\r
by people and never changed as a result of search. However, it is natural to consider\r
allowing the value function to be improved over time, using either the backed-up values\r
computed during heuristic search or any of the other methods presented throughout\r
this book. In a sense we have taken this approach all along. Our greedy, "-greedy, and\r
UCB (Section 2.7) action-selection methods are not unlike heuristic search, albeit on a\r
smaller scale. For example, to compute the greedy action given a model and a state-value\r
function, we must look ahead from each possible action to each possible next state, take\r
into account the rewards and estimated values, and then pick the best action. Just as\r
in conventional heuristic search, this process computes backed-up values of the possible\r
actions, but does not attempt to save them. Thus, heuristic search can be viewed as an\r
extension of the idea of a greedy policy beyond a single step.\r
The point of searching deeper than one step is to obtain better action selections. If one\r
has a perfect model and an imperfect action-value function, then in fact deeper search\r
will usually yield better policies.2 Certainly, if the search is all the way to the end of\r
the episode, then the e↵ect of the imperfect value function is eliminated, and the action\r
determined in this way must be optimal. If the search is of sucient depth k such that k\r
is very small, then the actions will be correspondingly near optimal. On the other hand,\r
the deeper the search, the more computation is required, usually resulting in a slower\r
response time. A good example is provided by Tesauro’s grandmaster-level backgammon\r
player, TD-Gammon (Section 16.1). This system used TD learning to learn an afterstate\r
value function through many games of self-play, using a form of heuristic search to make\r
its moves. As a model, TD-Gammon used a priori knowledge of the probabilities of dice\r
rolls and the assumption that the opponent always selected the actions that TD-Gammon\r
rated as best for it. Tesauro found that the deeper the heuristic search, the better the\r
moves made by TD-Gammon, but the longer it took to make each move. Backgammon\r
has a large branching factor, yet moves must be made within a few seconds. It was\r
only feasible to search ahead selectively a few steps, but even so the search resulted in\r
significantly better action selections.\r
We should not overlook the most obvious way in which heuristic search focuses updates:\r
on the current state. Much of the e↵ectiveness of heuristic search is due to its search tree\r
being tightly focused on the states and actions that might immediately follow the current\r
state. You may spend more of your life playing chess than checkers, but when you play\r
checkers, it pays to think about checkers and about your particular checkers position,\r
your likely next moves, and successor positions. No matter how you select actions, it\r
is these states and actions that are of highest priority for updates and where you most\r
urgently want your approximate value function to be accurate. Not only should your\r
computation be preferentially devoted to imminent events, but so should your limited\r
memory resources. In chess, for example, there are far too many possible positions to\r
store distinct value estimates for each of them, but chess programs based on heuristic\r
search can easily store distinct estimates for the millions of positions they encounter\r
2There are interesting exceptions to this (see Pearl, 1984)."""

[[sections]]
number = "8.10"
title = "Rollout Algorithms 183"
text = """
looking ahead from a single position. This great focusing of memory and computational\r
resources on the current decision is presumably the reason why heuristic search can be so\r
e↵ective.\r
The distribution of updates can be altered in similar ways to focus on the current\r
state and its likely successors. As a limiting case we might use exactly the methods of\r
heuristic search to construct a search tree, and then perform the individual, one-step\r
updates from bottom up, as suggested by Figure 8.9. If the updates are ordered in this\r
way and a tabular representation is used, then exactly the same overall update would\r
be achieved as in depth-first heuristic search. Any state-space search can be viewed in\r
this way as the piecing together of a large number of individual one-step updates. Thus,\r
the performance improvement observed with deeper searches is not due to the use of\r
multistep updates as such. Instead, it is due to the focus and concentration of updates\r
on states and actions immediately downstream from the current state. By devoting a\r
large amount of computation specifically relevant to the candidate actions, decision-time\r
planning can produce better decisions than can be produced by relying on unfocused\r
updates.\r
1 2\r
3\r
4 5\r
6\r
7\r
8 9\r
10\r
Figure 8.9: Heuristic search can be implemented as a sequence of one-step updates (shown\r
here outlined in blue) backing up values from the leaf nodes toward the root. The ordering\r
shown here is for a selective depth-first search."""

[[sections]]
number = "8.10"
title = "Rollout Algorithms"
text = """
Rollout algorithms are decision-time planning algorithms based on Monte Carlo control\r
applied to simulated trajectories that all begin at the current environment state. They\r
estimate action values for a given policy by averaging the returns of many simulated\r
trajectories that start with each possible action and then follow the given policy. When\r
the action-value estimates are considered to be accurate enough, the action (or one of the"""

[[sections]]
number = "184"
title = "Chapter 8: Planning and Learning with Tabular Methods"
text = """
actions) having the highest estimated value is executed, after which the process is carried\r
out anew from the resulting next state. As explained by Tesauro and Galperin (1997),\r
who experimented with rollout algorithms for playing backgammon, the term “rollout”\r
comes from estimating the value of a backgammon position by playing out, i.e., “rolling\r
out,” the position many times to the game’s end with randomly generated sequences of\r
dice rolls, where the moves of both players are made by some fixed policy.\r
Unlike the Monte Carlo control algorithms described in Chapter 5, the goal of a\r
rollout algorithm is not to estimate a complete optimal action-value function, q⇤, or a\r
complete action-value function, q⇡, for a given policy ⇡. Instead, they produce Monte\r
Carlo estimates of action values only for each current state and for a given policy usually\r
called the rollout policy. As decision-time planning algorithms, rollout algorithms make\r
immediate use of these action-value estimates, then discard them. This makes rollout\r
algorithms relatively simple to implement because there is no need to sample outcomes\r
for every state-action pair, and there is no need to approximate a function over either\r
the state space or the state-action space.\r
What then do rollout algorithms accomplish? The policy improvement theorem\r
described in Section 4.2 tells us that given any two policies ⇡ and ⇡0 that are identical\r
except that ⇡0(s) = a 6= ⇡(s) for some state s, if q⇡(s, a)  v⇡(s), then policy ⇡0 is as good\r
as, or better, than ⇡. Moreover, if the inequality is strict, then ⇡0 is in fact better than ⇡.\r
This applies to rollout algorithms where s is the current state and ⇡ is the rollout policy.\r
Averaging the returns of the simulated trajectories produces estimates of q⇡(s, a0) for\r
each action a0 2 A(s). Then the policy that selects an action in s that maximizes these\r
estimates and thereafter follows ⇡ is a good candidate for a policy that improves over\r
⇡. The result is like one step of the policy-iteration algorithm of dynamic programming\r
discussed in Section 4.3 (though it is more like one step of asynchronous value iteration\r
described in Section 4.5 because it changes the action for just the current state).\r
In other words, the aim of a rollout algorithm is to improve upon the rollout policy;\r
not to find an optimal policy. Experience has shown that rollout algorithms can be\r
surprisingly e↵ective. For example, Tesauro and Galperin (1997) were surprised by the\r
dramatic improvements in backgammon playing ability produced by the rollout method.\r
In some applications, a rollout algorithm can produce good performance even if the\r
rollout policy is completely random. But the performance of the improved policy depends\r
on properties of the rollout policy and the ranking of actions produced by the Monte\r
Carlo value estimates. Intuition suggests that the better the rollout policy and the more\r
accurate the value estimates, the better the policy produced by a rollout algorithm is\r
likely be (but see Gelly and Silver, 2007).\r
This involves important tradeo↵s because better rollout policies typically mean that\r
more time is needed to simulate enough trajectories to obtain good value estimates.\r
As decision-time planning methods, rollout algorithms usually have to meet strict time\r
constraints. The computation time needed by a rollout algorithm depends on the number\r
of actions that have to be evaluated for each decision, the number of time steps in the\r
simulated trajectories needed to obtain useful sample returns, the time it takes the rollout\r
policy to make decisions, and the number of simulated trajectories needed to obtain good\r
Monte Carlo action-value estimates."""

[[sections]]
number = "8.11"
title = "Monte Carlo Tree Search 185"
text = """
Balancing these factors is important in any application of rollout methods, though there\r
are several ways to ease the challenge. Because the Monte Carlo trials are independent of\r
one another, it is possible to run many trials in parallel on separate processors. Another\r
approach is to truncate the simulated trajectories short of complete episodes, correcting\r
the truncated returns by means of a stored evaluation function (which brings into play\r
all that we have said about truncated returns and updates in the preceding chapters).\r
It is also possible, as Tesauro and Galperin (1997) suggest, to monitor the Monte Carlo\r
simulations and prune away candidate actions that are unlikely to turn out to be the\r
best, or whose values are close enough to that of the current best that choosing them\r
instead would make no real di↵erence (though Tesauro and Galperin point out that this\r
would complicate a parallel implementation).\r
We do not ordinarily think of rollout algorithms as learning algorithms because they\r
do not maintain long-term memories of values or policies. However, these algorithms take\r
advantage of some of the features of reinforcement learning that we have emphasized\r
in this book. As instances of Monte Carlo control, they estimate action values by\r
averaging the returns of a collection of sample trajectories, in this case trajectories of\r
simulated interactions with a sample model of the environment. In this way they are\r
like reinforcement learning algorithms in avoiding the exhaustive sweeps of dynamic\r
programming by trajectory sampling, and in avoiding the need for distribution models\r
by relying on sample, instead of expected, updates. Finally, rollout algorithms take\r
advantage of the policy improvement property by acting greedily with respect to the\r
estimated action values."""

[[sections]]
number = "8.11"
title = "Monte Carlo Tree Search"
text = """
Monte Carlo Tree Search (MCTS) is a recent and strikingly successful example of decision\u0002time planning. At its base, MCTS is a rollout algorithm as described above, but enhanced\r
by the addition of a means for accumulating value estimates obtained from the Monte\r
Carlo simulations in order to successively direct simulations toward more highly-rewarding\r
trajectories. MCTS is largely responsible for the improvement in computer Go from\r
a weak amateur level in 2005 to a grandmaster level (6 dan or more) in 2015. Many\r
variations of the basic algorithm have been developed, including a variant that we discuss\r
in Section 16.6 that was critical for the stunning 2016 victories of the program AlphaGo\r
over an 18-time world champion Go player. MCTS has proved to be e↵ective in a wide\r
variety of competitive settings, including general game playing (e.g., see Finnsson and\r
Bj¨ornsson, 2008; Genesereth and Thielscher, 2014), but it is not limited to games; it can\r
be e↵ective for single-agent sequential decision problems if there is an environment model\r
simple enough for fast multistep simulation.\r
MCTS is executed after encountering each new state to select the agent’s action for\r
that state; it is executed again to select the action for the next state, and so on. As in a\r
rollout algorithm, each execution is an iterative process that simulates many trajectories\r
starting from the current state and running to a terminal state (or until discounting\r
makes any further reward negligible as a contribution to the return). The core idea\r
of MCTS is to successively focus multiple simulations starting at the current state by"""

[[sections]]
number = "186"
title = "Chapter 8: Planning and Learning with Tabular Methods"
text = """
extending the initial portions of trajectories that have received high evaluations from\r
earlier simulations. MCTS does not have to retain approximate value functions or policies\r
from one action selection to the next, though in many implementations it retains selected\r
action values likely to be useful for its next execution.\r
For the most part, the actions in the simulated trajectories are generated using a simple\r
policy, usually called a rollout policy as it is for simpler rollout algorithms. When both\r
the rollout policy and the model do not require a lot of computation, many simulated\r
trajectories can be generated in a short period of time. As in any tabular Monte Carlo\r
method, the value of a state–action pair is estimated as the average of the (simulated)\r
returns from that pair. Monte Carlo value estimates are maintained only for the subset\r
of state–action pairs that are most likely to be reached in a few steps, which form a tree\r
rooted at the current state, as illustrated in Figure 8.10. MCTS incrementally extends\r
the tree by adding nodes representing states that look promising based on the results\r
of the simulated trajectories. Any simulated trajectory will pass through the tree and\r
then exit it at some leaf node. Outside the tree and at the leaf nodes the rollout policy is\r
used for action selections, but at the states inside the tree something better is possible.\r
For these states we have value estimates for at least some of the actions, so we can pick\r
among them using an informed policy, called the tree policy, that balances exploration\r
Selection Expansion Simulation Backup\r
Repeat while time remains \r
Tree\r
 Policy\r
Rollout\r
Policy\r
Figure 8.10: Monte Carlo Tree Search. When the environment changes to a new state, MCTS\r
executes as many iterations as possible before an action needs to be selected, incrementally\r
building a tree whose root node represents the current state. Each iteration consists of the four\r
operations Selection, Expansion (though possibly skipped on some iterations), Simulation,\r
and Backup, as explained in the text and illustrated by the bold arrows in the trees. Adapted\r
from Chaslot, Bakkes, Szita, and Spronck (2008)."""

[[sections]]
number = "8.11"
title = "Monte Carlo Tree Search 187"
text = """
and exploitation. For example, the tree policy could select actions using an "-greedy or\r
UCB selection rule (Chapter 2).\r
In more detail, each iteration of a basic version of MCTS consists of the following four\r
steps as illustrated in Figure 8.10:"""

[[sections]]
number = "1"
title = "Selection. Starting at the root node, a tree policy based on the action values"
text = """
attached to the edges of the tree traverses the tree to select a leaf node.\r
2. Expansion. On some iterations (depending on details of the application), the tree\r
is expanded from the selected leaf node by adding one or more child nodes reached\r
from the selected node via unexplored actions."""

[[sections]]
number = "3"
title = "Simulation. From the selected node, or from one of its newly-added child nodes"
text = """
(if any), simulation of a complete episode is run with actions selected by the rollout\r
policy. The result is a Monte Carlo trial with actions selected first by the tree\r
policy and beyond the tree by the rollout policy."""

[[sections]]
number = "4"
title = "Backup. The return generated by the simulated episode is backed up to update,"
text = """
or to initialize, the action values attached to the edges of the tree traversed by\r
the tree policy in this iteration of MCTS. No values are saved for the states and\r
actions visited by the rollout policy beyond the tree. Figure 8.10 illustrates this by\r
showing a backup from the terminal state of the simulated trajectory directly to the\r
state–action node in the tree where the rollout policy began (though in general, the\r
entire return over the simulated trajectory is backed up to this state–action node).\r
MCTS continues executing these four steps, starting each time at the tree’s root node,\r
until no more time is left, or some other computational resource is exhausted. Then,\r
finally, an action from the root node (which still represents the current state of the\r
environment) is selected according to some mechanism that depends on the accumulated\r
statistics in the tree; for example, it may be an action having the largest action value\r
of all the actions available from the root state, or perhaps the action with the largest\r
visit count to avoid selecting outliers. This is the action MCTS actually selects. After\r
the environment transitions to a new state, MCTS is run again, sometimes starting\r
with a tree of a single root node representing the new state, but often starting with a\r
tree containing any descendants of this node left over from the tree constructed by the\r
previous execution of MCTS; all the remaining nodes are discarded, along with the action\r
values associated with them.\r
MCTS was first proposed to select moves in programs playing two-person competitive\r
games, such as Go. For game playing, each simulated episode is one complete play of the\r
game in which both players select actions by the tree and rollout policies. Section 16.6\r
describes an extension of MCTS used in the AlphaGo program that combines the Monte\r
Carlo evaluations of MCTS with action values learned by a deep artificial neural network\r
via self-play reinforcement learning.\r
Relating MCTS to the reinforcement learning principles we describe in this book\r
provides some insight into how it achieves such impressive results. At its base, MCTS is\r
a decision-time planning algorithm based on Monte Carlo control applied to simulations"""

[[sections]]
number = "188"
title = "Chapter 8: Planning and Learning with Tabular Methods"
text = """
that start from the root state; that is, it is a kind of rollout algorithm as described in\r
the previous section. It therefore benefits from online, incremental, sample-based value\r
estimation and policy improvement. Beyond this, it saves action-value estimates attached\r
to the tree edges and updates them using reinforcement learning’s sample updates. This\r
has the e↵ect of focusing the Monte Carlo trials on trajectories whose initial segments\r
are common to high-return trajectories previously simulated. Further, by incrementally\r
expanding the tree, MCTS e↵ectively grows a lookup table to store a partial action-value\r
function, with memory allocated to the estimated values of state–action pairs visited in\r
the initial segments of high-yielding sample trajectories. MCTS thus avoids the problem\r
of globally approximating an action-value function while it retains the benefit of using\r
past experience to guide exploration.\r
The striking success of decision-time planning by MCTS has deeply influenced artificial\r
intelligence, and many researchers are studying modifications and extensions of the basic\r
procedure for use in both games and single-agent applications."""

[[sections]]
number = "8.12"
title = "Summary of the Chapter"
text = """
Planning requires a model of the environment. A distribution model consists of the\r
probabilities of next states and rewards for possible actions; a sample model produces\r
single transitions and rewards generated according to these probabilities. Dynamic\r
programming requires a distribution model because it uses expected updates, which involve\r
computing expectations over all the possible next states and rewards. A sample model,\r
on the other hand, is what is needed to simulate interacting with the environment during\r
which sample updates, like those used by many reinforcement learning algorithms, can be\r
used. Sample models are generally much easier to obtain than distribution models.\r
We have presented a perspective emphasizing the surprisingly close relationships be\u0002tween planning optimal behavior and learning optimal behavior. Both involve estimating\r
the same value functions, and in both cases it is natural to update the estimates incre\u0002mentally, in a long series of small backing-up operations. This makes it straightforward\r
to integrate learning and planning processes simply by allowing both to update the same\r
estimated value function. In addition, any of the learning methods can be converted into\r
planning methods simply by applying them to simulated (model-generated) experience\r
rather than to real experience. In this case learning and planning become even more\r
similar; they are possibly identical algorithms operating on two di↵erent sources of\r
experience.\r
It is straightforward to integrate incremental planning methods with acting and model\u0002learning. Planning, acting, and model-learning interact in a circular fashion (as in\r
the diagram on page 162), each producing what the other needs to improve; no other\r
interaction among them is either required or prohibited. The most natural approach\r
is for all processes to proceed asynchronously and in parallel. If the processes must\r
share computational resources, then the division can be handled almost arbitrarily—by\r
whatever organization is most convenient and ecient for the task at hand.\r
In this chapter we have touched upon a number of dimensions of variation among\r
state-space planning methods. One dimension is the variation in the size of updates. The"""

[[sections]]
number = "8.13"
title = "Summary of Part I: Dimensions 189"
text = """
smaller the updates, the more incremental the planning methods can be. Among the\r
smallest updates are one-step sample updates, as in Dyna. Another important dimension\r
is the distribution of updates, that is, of the focus of search. Prioritized sweeping focuses\r
backward on the predecessors of states whose values have recently changed. On-policy\r
trajectory sampling focuses on states or state–action pairs that the agent is likely to\r
encounter when controlling its environment. This can allow computation to skip over\r
parts of the state space that are irrelevant to the prediction or control problem. Real\u0002time dynamic programming, an on-policy trajectory sampling version of value iteration,\r
illustrates some of the advantages this strategy has over conventional sweep-based policy\r
iteration.\r
Planning can also focus forward from pertinent states, such as states actually encoun\u0002tered during an agent-environment interaction. The most important form of this is when\r
planning is done at decision time, that is, as part of the action-selection process. Classical\r
heuristic search as studied in artificial intelligence is an example of this. Other examples\r
are rollout algorithms and Monte Carlo Tree Search that benefit from online, incremental,\r
sample-based value estimation and policy improvement."""

[[sections]]
number = "8.13"
title = "Summary of Part I: Dimensions"
text = """
This chapter concludes Part I of this book. In it we have tried to present reinforcement\r
learning not as a collection of individual methods, but as a coherent set of ideas cutting\r
across methods. Each idea can be viewed as a dimension along which methods vary. The\r
set of such dimensions spans a large space of possible methods. By exploring this space\r
at the level of dimensions we hope to obtain the broadest and most lasting understanding.\r
In this section we use the concept of dimensions in method space to recapitulate the view\r
of reinforcement learning developed so far in this book.\r
All of the methods we have explored so far in this book have three key ideas in common:\r
first, they all seek to estimate value functions; second, they all operate by backing up\r
values along actual or possible state trajectories; and third, they all follow the general\r
strategy of generalized policy iteration (GPI), meaning that they maintain an approximate\r
value function and an approximate policy, and they continually try to improve each on the\r
basis of the other. These three ideas are central to the subjects covered in this book. We\r
suggest that value functions, backing up value updates, and GPI are powerful organizing\r
principles potentially relevant to any model of intelligence, whether artificial or natural.\r
Two of the most important dimensions along which the methods vary are shown in\r
Figure 8.11. These dimensions have to do with the kind of update used to improve the\r
value function. The horizontal dimension is whether they are sample updates (based on a\r
sample trajectory) or expected updates (based on a distribution of possible trajectories).\r
Expected updates require a distribution model, whereas sample updates need only a\r
sample model, or can be done from actual experience with no model at all (another\r
dimension of variation). The vertical dimension of Figure 8.11 corresponds to the depth\r
of updates, that is, to the degree of bootstrapping. At three of the four corners of the\r
space are the three primary methods for estimating values: dynamic programming, TD,\r
and Monte Carlo. Along the left edge of the space are the sample-update methods,"""

[[sections]]
number = "190"
title = "Chapter 8: Planning and Learning with Tabular Methods"
text = """
width\r
of update\r
depth\r
(length)\r
of update\r
Temporal\u0002difference\r
learning\r
Dynamic\r
programming\r
Monte\r
Carlo\r
...\r
Exhaustive\r
search\r
Figure 8.11: A slice through the space of reinforcement learning methods, highlighting the\r
two of the most important dimensions explored in Part I of this book: the depth and width of\r
the updates.\r
ranging from one-step TD updates to full-return Monte Carlo updates. Between these\r
is a spectrum including methods based on n-step updates (and in Chapter 12 we will\r
extend this to mixtures of n-step updates such as the -updates implemented by eligibility\r
traces).\r
Dynamic programming methods are shown in the extreme upper-right corner of the\r
space because they involve one-step expected updates. The lower-right corner is the\r
extreme case of expected updates so deep that they run all the way to terminal states\r
(or, in a continuing task, until discounting has reduced the contribution of any further\r
rewards to a negligible level). This is the case of exhaustive search. Intermediate methods\r
along this dimension include heuristic search and related methods that search and update\r
up to a limited depth, perhaps selectively. There are also methods that are intermediate\r
along the horizontal dimension. These include methods that mix expected and sample\r
updates, as well as the possibility of methods that mix samples and distributions within\r
a single update. The interior of the square is filled in to represent the space of all such\r
intermediate methods.\r
A third dimension that we have emphasized in this book is the binary distinction\r
between on-policy and o↵-policy methods. In the former case, the agent learns the value\r
function for the policy it is currently following, whereas in the latter case it learns the"""

[[sections]]
number = "8.13"
title = "Summary of Part I: Dimensions 191"
text = """
value function for the policy for a di↵erent policy, often the one that the agent currently\r
thinks is best. The policy generating behavior is typically di↵erent from what is currently\r
thought best because of the need to explore. This third dimension might be visualized as\r
perpendicular to the plane of the page in Figure 8.11.\r
In addition to the three dimensions just discussed, we have identified a number of\r
others throughout the book:\r
Definition of return Is the task episodic or continuing, discounted or undiscounted?\r
Action values vs. state values vs. afterstate values What kind of values should\r
be estimated? If only state values are estimated, then either a model or a separate\r
policy (as in actor–critic methods) is required for action selection.\r
Action selection/exploration How are actions selected to ensure a suitable trade-o↵\r
between exploration and exploitation? We have considered only the simplest ways to\r
do this: "-greedy, optimistic initialization of values, soft-max, and upper confidence\r
bound.\r
Synchronous vs. asynchronous Are the updates for all states performed simultane\u0002ously or one by one in some order?\r
Real vs. simulated Should one update based on real experience or simulated experi\u0002ence? If both, how much of each?\r
Location of updates What states or state–action pairs should be updated? Model\u0002free methods can choose only among the states and state–action pairs actually\r
encountered, but model-based methods can choose arbitrarily. There are many\r
possibilities here.\r
Timing of updates Should updates be done as part of selecting actions, or only after\u0002ward?\r
Memory for updates How long should updated values be retained? Should they be\r
retained permanently, or only while computing an action selection, as in heuristic\r
search?\r
Of course, these dimensions are neither exhaustive nor mutually exclusive. Individual\r
algorithms di↵er in many other ways as well, and many algorithms lie in several places\r
along several dimensions. For example, Dyna methods use both real and simulated\r
experience to a↵ect the same value function. It is also perfectly sensible to maintain\r
multiple value functions computed in di↵erent ways or over di↵erent state and action\r
representations. These dimensions do, however, constitute a coherent set of ideas for\r
describing and exploring a wide space of possible methods.\r
The most important dimension not mentioned here, and not covered in Part I of\r
this book, is that of function approximation. Function approximation can be viewed as\r
an orthogonal spectrum of possibilities ranging from tabular methods at one extreme\r
through state aggregation, a variety of linear methods, and then a diverse set of nonlinear\r
methods. This dimension is explored in Part II."""

[[sections]]
number = "192"
title = "Chapter 8: Planning and Learning with Tabular Methods"
text = "Bibliographical and Historical Remarks"

[[sections]]
number = "8.1"
title = "The overall view of planning and learning presented here has developed gradually"
text = """
over a number of years, in part by the authors (Sutton, 1990, 1991a, 1991b;\r
Barto, Bradtke, and Singh, 1991, 1995; Sutton and Pinette, 1985; Sutton and\r
Barto, 1981b); it has been strongly influenced by Agre and Chapman (1990; Agre\r
1988), Bertsekas and Tsitsiklis (1989), Singh (1993), and others. The authors\r
were also strongly influenced by psychological studies of latent learning (Tolman,\r
1932) and by psychological views of the nature of thought (e.g., Galanter and\r
Gerstenhaber, 1956; Craik, 1943; Campbell, 1960; Dennett, 1978). In Part\r
III of the book, Section 14.6 relates model-based and model-free methods to\r
psychological theories of learning and behavior, and Section 15.11 discusses ideas\r
about how the brain might implement these types of methods."""

[[sections]]
number = "8.2"
title = "The terms direct and indirect, which we use to describe di↵erent kinds of"
text = """
reinforcement learning, are from the adaptive control literature (e.g., Goodwin\r
and Sin, 1984), where they are used to make the same kind of distinction. The\r
term system identification is used in adaptive control for what we call model\u0002learning (e.g., Goodwin and Sin, 1984; Ljung and S¨oderstrom, 1983; Young,\r
1984). The Dyna architecture is due to Sutton (1990), and the results in this\r
and the next section are based on results reported there. Barto and Singh\r
(1990) consider some of the issues in comparing direct and indirect reinforcement\r
learning methods. Early work extending Dyna to linear function approximation\r
was done by Sutton, Szepesv´ari, Geramifard, and Bowling (2008) and by Parr,\r
Li, Taylor, Painter-Wakefield, and Littman (2008)."""

[[sections]]
number = "8.3"
title = "There have been several works with model-based reinforcement learning that take"
text = """
the idea of exploration bonuses and optimistic initialization to its logical extreme,\r
in which all incompletely explored choices are assumed maximally rewarding\r
and optimal paths are computed to test them. The E3 algorithm of Kearns and\r
Singh (2002) and the R-max algorithm of Brafman and Tennenholtz (2003) are\r
guaranteed to find a near-optimal solution in time polynomial in the number\r
of states and actions. This is usually too slow for practical algorithms but is\r
probably the best that can be done in the worst case."""

[[sections]]
number = "8.4"
title = "Prioritized sweeping was developed simultaneously and independently by Moore"
text = """
and Atkeson (1993) and Peng and Williams (1993). The results in the box on\r
page 170 are due to Peng and Williams (1993). The results in the box on page 171\r
are due to Moore and Atkeson. Key subsequent work in this area includes that\r
by McMahan and Gordon (2005) and by van Seijen and Sutton (2013)."""

[[sections]]
number = "8.5"
title = "This section was strongly influenced by the experiments of Singh (1993)."
text = """
8.6–7 Trajectory sampling has implicitly been a part of reinforcement learning from\r
the outset, but it was most explicitly emphasized by Barto, Bradtke, and Singh\r
(1995) in their introduction of RTDP. They recognized that Korf’s (1990) learning"""

[[sections]]
number = "8.13"
title = "Summary of Part I: Dimensions 193"
text = """
real-time A* (LRTA*) algorithm is an asynchronous DP algorithm that applies\r
to stochastic problems as well as the deterministic problems on which Korf\r
focused. Beyond LRTA*, RTDP includes the option of updating the values of\r
many states in the time intervals between the execution of actions. Barto et\r
al. (1995) proved the convergence result described here by combining Korf’s (1990)\r
convergence proof for LRTA* with the result of Bertsekas (1982) (also Bertsekas\r
and Tsitsiklis, 1989) ensuring convergence of asynchronous DP for stochastic\r
shortest path problems in the undiscounted case. Combining model-learning\r
with RTDP is called Adaptive RTDP, also presented by Barto et al. (1995) and\r
discussed by Barto (2011).\r
8.9 For further reading on heuristic search, the reader is encouraged to consult texts\r
and surveys such as those by Russell and Norvig (2009) and Korf (1988). Peng\r
and Williams (1993) explored a forward focusing of updates much as is suggested\r
in this section.\r
8.10 Abramson’s (1990) expected-outcome model is a rollout algorithm applied to two\u0002person games in which the play of both simulated players is random. He argued\r
that even with random play, it is a “powerful heuristic” that is “precise, accurate,\r
easily estimable, eciently calculable, and domain-independent.” Tesauro and\r
Galperin (1997) demonstrated the e↵ectiveness of rollout algorithms for improving\r
the play of backgammon programs, adopting the term “rollout” from its use\r
in evaluating backgammon positions by playing out positions with di↵erent\r
randomly generating sequences of dice rolls. Bertsekas, Tsitsiklis, and Wu (1997)\r
examine rollout algorithms applied to combinatorial optimization problems, and\r
Bertsekas (2013) surveys their use in discrete deterministic optimization problems,\r
remarking that they are “often surprisingly e↵ective.”"""

[[sections]]
number = "8.11"
title = "The central ideas of MCTS were introduced by Coulom (2006) and by Kocsis"
text = """
and Szepesv´ari (2006). They built upon previous research with Monte Carlo\r
planning algorithms as reviewed by these authors. Browne, Powley, Whitehouse,\r
Lucas, Cowling, Rohlfshagen, Tavener, Perez, Samothrakis, and Colton (2012)\r
is an excellent survey of MCTS methods and their applications. David Silver\r
contributed to the ideas and presentation in this section.

Part II:\r
Approximate Solution Methods\r
In the second part of the book we extend the tabular methods presented in the first part\r
to apply to problems with arbitrarily large state spaces. In many of the tasks to which we\r
would like to apply reinforcement learning the state space is combinatorial and enormous;\r
the number of possible camera images, for example, is much larger than the number of\r
atoms in the universe. In such cases we cannot expect to find an optimal policy or the\r
optimal value function even in the limit of infinite time and data; our goal instead is to\r
find a good approximate solution using limited computational resources. In this part of\r
the book we explore such approximate solution methods.\r
The problem with large state spaces is not just the memory needed for large tables,\r
but the time and data needed to fill them accurately. In many of our target tasks, almost\r
every state encountered will never have been seen before. To make sensible decisions in\r
such states it is necessary to generalize from previous encounters with di↵erent states\r
that are in some sense similar to the current one. In other words, the key issue is that of\r
generalization. How can experience with a limited subset of the state space be usefully\r
generalized to produce a good approximation over a much larger subset?\r
Fortunately, generalization from examples has already been extensively studied, and\r
we do not need to invent totally new methods for use in reinforcement learning. To some\r
extent we need only combine reinforcement learning methods with existing generalization\r
methods. The kind of generalization we require is often called function approximation\r
because it takes examples from a desired function (e.g., a value function) and attempts\r
to generalize from them to construct an approximation of the entire function. Function\r
approximation is an instance of supervised learning, the primary topic studied in machine\r
learning, artificial neural networks, pattern recognition, and statistical curve fitting. In\r
theory, any of the methods studied in these fields can be used in the role of function\r
approximator within reinforcement learning algorithms, although in practice some fit\r
more easily into this role than others.\r
Reinforcement learning with function approximation involves a number of new issues\r
that do not normally arise in conventional supervised learning, such as nonstationarity,\r
bootstrapping, and delayed targets. We introduce these and other issues successively over\r
the five chapters of this part. Initially we restrict attention to on-policy training, treating\r
in Chapter 9 the prediction case, in which the policy is given and only its value function\r
is approximated, and then in Chapter 10 the control case, in which an approximation to\r
the optimal policy is found. The challenging problem of o↵-policy learning with function\r
approximation is treated in Chapter 11. In each of these three chapters we will have"""

[[sections]]
number = "196"
title = "Part II: Approximate Solution Methods"
text = """
to return to first principles and re-examine the objectives of the learning to take into\r
account function approximation. Chapter 12 introduces and analyzes the algorithmic\r
mechanism of eligibility traces, which dramatically improves the computational properties\r
of multi-step reinforcement learning methods in many cases. The final chapter of this\r
part explores a di↵erent approach to control, policy-gradient methods, which approximate\r
the optimal policy directly and need never form an approximate value function (although\r
they may be much more ecient if they do approximate a value function as well the\r
policy).

Chapter 9\r
On-policy Prediction with\r
Approximation\r
In this chapter, we begin our study of function approximation in reinforcement learning\r
by considering its use in estimating the state-value function from on-policy data, that is,\r
in approximating v⇡ from experience generated using a known policy ⇡. The novelty in\r
this chapter is that the approximate value function is represented not as a table but as a\r
parameterized functional form with weight vector w 2 Rd. We will write vˆ(s,w) ⇡ v⇡(s)\r
for the approximate value of state s given weight vector w. For example, vˆ might be\r
a linear function in features of the state, with w the vector of feature weights. More\r
generally, vˆ might be the function computed by a multi-layer artificial neural network,\r
with w the vector of connection weights in all the layers. By adjusting the weights, any\r
of a wide range of di↵erent functions can be implemented by the network. Or vˆ might be\r
the function computed by a decision tree, where w is all the numbers defining the split\r
points and leaf values of the tree. Typically, the number of weights (the dimensionality of\r
w) is much less than the number of states (d ⌧ |S|), and changing one weight changes the\r
estimated value of many states. Consequently, when a single state is updated, the change\r
generalizes from that state to a↵ect the values of many other states. Such generalization\r
makes the learning potentially more powerful but also potentially more dicult to manage\r
and understand.\r
Perhaps surprisingly, extending reinforcement learning to function approximation also\r
makes it applicable to partially observable problems, in which the full state is not available\r
to the agent. If the parameterized function form for vˆ does not allow the estimated\r
value to depend on certain aspects of the state, then it is just as if those aspects are\r
unobservable. In fact, all the theoretical results for methods using function approximation\r
presented in this part of the book apply equally well to cases of partial observability.\r
What function approximation can’t do, however, is augment the state representation\r
with memories of past observations. Some such possible further extensions are discussed\r
briefly in Section 17.3."""

[[sections]]
number = "198"
title = "Chapter 9: On-policy Prediction with Approximation"
text = ""

[[sections]]
number = "9.1"
title = "Value-function Approximation"
text = """
All of the prediction methods covered in this book have been described as updates to an\r
estimated value function that shift its value at particular states toward a “backed-up value,”\r
or update target, for that state. Let us refer to an individual update by the notation s 7! u,\r
where s is the state updated and u is the update target that s’s estimated value is shifted\r
toward. For example, the Monte Carlo update for value prediction is St 7! Gt, the TD(0)\r
update is St 7! Rt+1 +vˆ(St+1,wt), and the n-step TD update is St 7! Gt:t+n. In the DP\r
(dynamic programming) policy-evaluation update, s 7! E⇡[Rt+1 + vˆ(St+1,wt) | St =s],\r
an arbitrary state s is updated, whereas in the other cases the state encountered in actual\r
experience, St, is updated.\r
It is natural to interpret each update as specifying an example of the desired input–\r
output behavior of the value function. In a sense, the update s 7! u means that the\r
estimated value for state s should be more like the update target u. Up to now, the\r
actual update has been trivial: the table entry for s’s estimated value has simply been\r
shifted a fraction of the way toward u, and the estimated values of all other states\r
were left unchanged. Now we permit arbitrarily complex and sophisticated methods to\r
implement the update, and updating at s generalizes so that the estimated values of\r
many other states are changed as well. Machine learning methods that learn to mimic\r
input–output examples in this way are called supervised learning methods, and when the\r
outputs are numbers, like u, the process is often called function approximation. Function\r
approximation methods expect to receive examples of the desired input–output behavior\r
of the function they are trying to approximate. We use these methods for value prediction\r
simply by passing to them the s 7! u of each update as a training example. We then\r
interpret the approximate function they produce as an estimated value function.\r
Viewing each update as a conventional training example in this way enables us to use\r
any of a wide range of existing function approximation methods for value prediction. In\r
principle, we can use any method for supervised learning from examples, including artificial\r
neural networks, decision trees, and various kinds of multivariate regression. However,\r
not all function approximation methods are equally well suited for use in reinforcement\r
learning. The most sophisticated artificial neural network and statistical methods all\r
assume a static training set over which multiple passes are made. In reinforcement\r
learning, however, it is important that learning be able to occur online, while the agent\r
interacts with its environment or with a model of its environment. To do this requires\r
methods that are able to learn eciently from incrementally acquired data. In addition,\r
reinforcement learning generally requires function approximation methods able to handle\r
nonstationary target functions (target functions that change over time). For example,\r
in control methods based on GPI (generalized policy iteration) we often seek to learn\r
q⇡ while ⇡ changes. Even if the policy remains the same, the target values of training\r
examples are nonstationary if they are generated by bootstrapping methods (DP and TD\r
learning). Methods that cannot easily handle such nonstationarity are less suitable for\r
reinforcement learning."""

[[sections]]
number = "9.2"
title = "The Prediction Objective (VE) 199"
text = ""

[[sections]]
number = "9.2"
title = "The Prediction Objective (VE)"
text = """
Up to now we have not specified an explicit objective for prediction. In the tabular case\r
a continuous measure of prediction quality was not necessary because the learned value\r
function could come to equal the true value function exactly. Moreover, the learned\r
values at each state were decoupled—an update at one state a↵ected no other. But with\r
genuine approximation, an update at one state a↵ects many others, and it is not possible\r
to get the values of all states exactly correct. By assumption we have far more states\r
than weights, so making one state’s estimate more accurate invariably means making\r
others’ less accurate. We are obligated then to say which states we care most about. We\r
must specify a state distribution µ(s)  0, P\r
s µ(s) = 1, representing how much we care\r
about the error in each state s. By the error in a state s we mean the square of the\r
di↵erence between the approximate value vˆ(s,w) and the true value v⇡(s). Weighting\r
this over the state space by µ, we obtain a natural objective function, the mean square\r
value error, denoted VE:\r
VE(w) .= X\r
s2S\r
µ(s)\r
h\r
v⇡(s)  vˆ(s,w)\r
i2\r
. (9.1)\r
The square root of this measure, the root VE, gives a rough measure of how much the\r
approximate values di↵er from the true values and is often used in plots. Often µ(s) is\r
chosen to be the fraction of time spent in s. Under on-policy training this is called the\r
on-policy distribution; we focus entirely on this case in this chapter. In continuing tasks,\r
the on-policy distribution is the stationary distribution under ⇡.\r
The on-policy distribution in episodic tasks\r
In an episodic task, the on-policy distribution is a little di↵erent in that it depends\r
on how the initial states of episodes are chosen. Let h(s) denote the probability\r
that an episode begins in each state s, and let ⌘(s) denote the number of time\r
steps spent, on average, in state s in a single episode. Time is spent in a state s\r
if episodes start in s, or if transitions are made into s from a preceding state s¯ in\r
which time is spent:\r
⌘(s) = h(s) +X\r
s¯\r
⌘(¯s)\r
X\r
a\r
⇡(a|s¯)p(s|s, a ¯ ), for all s 2 S. (9.2)\r
This system of equations can be solved for the expected number of visits ⌘(s). The\r
on-policy distribution is then the fraction of time spent in each state normalized to\r
sum to one:\r
µ(s) = ⌘(s)\r
P\r
s0 ⌘(s0\r
)\r
, for all s 2 S. (9.3)\r
This is the natural choice without discounting. If there is discounting ( < 1) it\r
should be treated as a form of termination, which can be done simply by including\r
a factor of  in the second term of (9.2)."""

[[sections]]
number = "200"
title = "Chapter 9: On-policy Prediction with Approximation"
text = """
The two cases, continuing and episodic, behave similarly, but with approximation they\r
must be treated separately in formal analyses, as we will see repeatedly in this part of\r
the book. This completes the specification of the learning objective.\r
It is not completely clear that the VE is the right performance objective for rein\u0002forcement learning. Remember that our ultimate purpose—the reason we are learning\r
a value function—is to find a better policy. The best value function for this purpose is\r
not necessarily the best for minimizing VE. Nevertheless, it is not yet clear what a more\r
useful alternative goal for value prediction might be. For now, we will focus on VE.\r
An ideal goal in terms of VE would be to find a global optimum, a weight vector w⇤\r
for which VE(w⇤)  VE(w) for all possible w. Reaching this goal is sometimes possible\r
for simple function approximators such as linear ones, but is rarely possible for complex\r
function approximators such as artificial neural networks and decision trees. Short of\r
this, complex function approximators may seek to converge instead to a local optimum,\r
a weight vector w⇤ for which VE(w⇤)  VE(w) for all w in some neighborhood of w⇤.\r
Although this guarantee is only slightly reassuring, it is typically the best that can be\r
said for nonlinear function approximators, and often it is enough. Still, for many cases of\r
interest in reinforcement learning there is no guarantee of convergence to an optimum, or\r
even to within a bounded distance of an optimum. Some methods may in fact diverge,\r
with their VE approaching infinity in the limit.\r
In the last two sections we outlined a framework for combining a wide range of\r
reinforcement learning methods for value prediction with a wide range of function\r
approximation methods, using the updates of the former to generate training examples\r
for the latter. We also described a VE performance measure which these methods may\r
aspire to minimize. The range of possible function approximation methods is far too\r
large to cover all, and anyway too little is known about most of them to make a reliable\r
evaluation or recommendation. Of necessity, we consider only a few possibilities. In\r
the rest of this chapter we focus on function approximation methods based on gradient\r
principles, and on linear gradient-descent methods in particular. We focus on these\r
methods in part because we consider them to be particularly promising and because they\r
reveal key theoretical issues, but also because they are simple and our space is limited."""

[[sections]]
number = "9.3"
title = "Stochastic-gradient and Semi-gradient Methods"
text = """
We now develop in detail one class of learning methods for function approximation in\r
value prediction, those based on stochastic gradient descent (SGD). SGD methods are\r
among the most widely used of all function approximation methods and are particularly\r
well suited to online reinforcement learning.\r
In gradient-descent methods, the weight vector is a column vector with a fixed number\r
of real valued components, w .= (w1, w2,...,wd)>,\r
1 and the approximate value function\r
vˆ(s,w) is a di↵erentiable function of w for all s 2 S. We will be updating w at each of\r
a series of discrete time steps, t = 0, 1, 2, 3,..., so we will need a notation wt for the\r
1The > denotes transpose, needed here to turn the horizontal row vector in the text into a vertical\r
column vector; in this book vectors are generally taken to be column vectors unless explicitly written out\r
horizontally or transposed."""

[[sections]]
number = "9.3"
title = "Stochastic-gradient and Semi-gradient Methods 201"
text = """
weight vector at each step. For now, let us assume that, on each step, we observe a new\r
example St 7! v⇡(St) consisting of a (possibly randomly selected) state St and its true\r
value under the policy. These states might be successive states from an interaction with\r
the environment, but for now we do not assume so. Even though we are given the exact,\r
correct values, v⇡(St) for each St, there is still a dicult problem because our function\r
approximator has limited resources and thus limited resolution. In particular, there is\r
generally no w that gets all the states, or even all the examples, exactly correct. In\r
addition, we must generalize to all the other states that have not appeared in examples.\r
We assume that states appear in examples with the same distribution, µ, over which\r
we are trying to minimize the VE as given by (9.1). A good strategy in this case is\r
to try to minimize error on the observed examples. Stochastic gradient-descent (SGD)\r
methods do this by adjusting the weight vector after each example by a small amount in\r
the direction that would most reduce the error on that example:\r
wt+1\r
.\r
= wt  1\r
2\r
↵r\r
h\r
v⇡(St)  vˆ(St,wt)\r
i2\r
(9.4)\r
= wt + ↵\r
h\r
v⇡(St)  vˆ(St,wt)\r
i\r
rvˆ(St,wt), (9.5)\r
where ↵ is a positive step-size parameter, and rf(w), for any scalar expression f(w)\r
that is a function of a vector (here w), denotes the column vector of partial derivatives\r
of the expression with respect to the components of the vector:\r
rf(w) .=\r
✓@f(w)\r
@w1\r
,\r
@f(w)\r
@w2\r
,..., @f(w)\r
@wd\r
◆>\r
. (9.6)\r
This derivative vector is the gradient of f with respect to w. SGD methods are “gradient\r
descent” methods because the overall step in wt is proportional to the negative gradient\r
of the example’s squared error (9.4). This is the direction in which the error falls most\r
rapidly. Gradient descent methods are called “stochastic” when the update is done, as\r
here, on only a single example, which might have been selected stochastically. Over many\r
examples, making small steps, the overall e↵ect is to minimize an average performance\r
measure such as the VE.\r
It may not be immediately apparent why SGD takes only a small step in the direction\r
of the gradient. Could we not move all the way in this direction and completely eliminate\r
the error on the example? In many cases this could be done, but usually it is not desirable.\r
Remember that we do not seek or expect to find a value function that has zero error for\r
all states, but only an approximation that balances the errors in di↵erent states. If we\r
completely corrected each example in one step, then we would not find such a balance.\r
In fact, the convergence results for SGD methods assume that ↵ decreases over time. If\r
it decreases in such a way as to satisfy the standard stochastic approximation conditions\r
(2.7), then the SGD method (9.5) is guaranteed to converge to a local optimum.\r
We turn now to the case in which the target output, here denoted Ut 2 R, of the tth\r
training example, St 7! Ut, is not the true value, v⇡(St), but some, possibly random,\r
approximation to it. For example, Ut might be a noise-corrupted version of v⇡(St), or it\r
might be one of the bootstrapping targets using vˆ mentioned in the previous section. In"""

[[sections]]
number = "202"
title = "Chapter 9: On-policy Prediction with Approximation"
text = """
these cases we cannot perform the exact update (9.5) because v⇡(St) is unknown, but\r
we can approximate it by substituting Ut in place of v⇡(St). This yields the following\r
general SGD method for state-value prediction:\r
wt+1\r
.\r
= wt + ↵\r
h\r
Ut  vˆ(St,wt)\r
i\r
rvˆ(St,wt). (9.7)\r
If Ut is an unbiased estimate, that is, if E[Ut|St =s] = v⇡(s), for each t, then wt is\r
guaranteed to converge to a local optimum under the usual stochastic approximation\r
conditions (2.7) for decreasing ↵.\r
For example, suppose the states in the examples are the states generated by interaction\r
(or simulated interaction) with the environment using policy ⇡. Because the true value of\r
a state is the expected value of the return following it, the Monte Carlo target Ut\r
.\r
= Gt is\r
by definition an unbiased estimate of v⇡(St). With this choice, the general SGD method\r
(9.7) converges to a locally optimal approximation to v⇡(St). Thus, the gradient-descent\r
version of Monte Carlo state-value prediction is guaranteed to find a locally optimal\r
solution. Pseudocode for a complete algorithm is shown in the box below.\r
Gradient Monte Carlo Algorithm for Estimating vˆ ⇡ v⇡\r
Input: the policy ⇡ to be evaluated\r
Input: a di↵erentiable function ˆv : S ⇥ Rd ! R\r
Algorithm parameter: step size ↵ > 0\r
Initialize value-function weights w 2 Rd arbitrarily (e.g., w = 0)\r
Loop forever (for each episode):\r
Generate an episode S0, A0, R1, S1, A1,...,RT , ST using ⇡\r
Loop for each step of episode, t = 0, 1,...,T  1:\r
w w + ↵\r
⇥\r
Gt  vˆ(St,w)\r
⇤\r
rvˆ(St,w)\r
One does not obtain the same guarantees if a bootstrapping estimate of v⇡(St) is used\r
as the target Ut in (9.7). Bootstrapping targets such as n-step returns Gt:t+n or the DP\r
target P\r
a,s0,r ⇡(a|St)p(s0\r
, r|St, a)[r + vˆ(s0,wt)] all depend on the current value of the\r
weight vector wt, which implies that they will be biased and that they will not produce a\r
true gradient-descent method. One way to look at this is that the key step from (9.4)\r
to (9.5) relies on the target being independent of wt. This step would not be valid if\r
a bootstrapping estimate were used in place of v⇡(St). Bootstrapping methods are not\r
in fact instances of true gradient descent (Barnard, 1993). They take into account the\r
e↵ect of changing the weight vector wt on the estimate, but ignore its e↵ect on the target.\r
They include only a part of the gradient and, accordingly, we call them semi-gradient\r
methods.\r
Although semi-gradient (bootstrapping) methods do not converge as robustly as\r
gradient methods, they do converge reliably in important cases such as the linear case\r
discussed in the next section. Moreover, they o↵er important advantages that make them\r
often clearly preferred. One reason for this is that they typically enable significantly faster\r
learning, as we have seen in Chapters 6 and 7. Another is that they enable learning to"""

[[sections]]
number = "9.3"
title = "Stochastic-gradient and Semi-gradient Methods 203"
text = """
be continual and online, without waiting for the end of an episode. This enables them to\r
be used on continuing problems and provides computational advantages. A prototypical\r
semi-gradient method is semi-gradient TD(0), which uses Ut\r
.\r
= Rt+1 + vˆ(St+1,w) as its\r
target. Complete pseudocode for this method is given in the box below.\r
Semi-gradient TD(0) for estimating vˆ ⇡ v⇡\r
Input: the policy ⇡ to be evaluated\r
Input: a di↵erentiable function ˆv : S+ ⇥ Rd ! R such that ˆv(terminal,·)=0\r
Algorithm parameter: step size ↵ > 0\r
Initialize value-function weights w 2 Rd arbitrarily (e.g., w = 0)\r
Loop for each episode:\r
Initialize S\r
Loop for each step of episode:\r
Choose A ⇠ ⇡(·|S)\r
Take action A, observe R, S0\r
w w + ↵\r
⇥\r
R + vˆ(S0,w)  vˆ(S,w)\r
⇤\r
rvˆ(S,w)\r
S S0\r
until S is terminal\r
State aggregation is a simple form of generalizing function approximation in which\r
states are grouped together, with one estimated value (one component of the weight\r
vector w) for each group. The value of a state is estimated as its group’s component,\r
and when the state is updated, that component alone is updated. State aggregation\r
is a special case of SGD (9.7) in which the gradient, rvˆ(St,wt), is 1 for St’s group’s\r
component and 0 for the other components.\r
Example 9.1: State Aggregation on the 1000-state Random Walk Consider a\r
1000-state version of the random walk task (Examples 6.2 and 7.1 on pages 125 and\r
144). The states are numbered from 1 to 1000, left to right, and all episodes begin near\r
the center, in state 500. State transitions are from the current state to one of the 100\r
neighboring states to its left, or to one of the 100 neighboring states to its right, all with\r
equal probability. Of course, if the current state is near an edge, then there may be fewer\r
than 100 neighbors on that side of it. In this case, all the probability that would have\r
gone into those missing neighbors goes into the probability of terminating on that side\r
(thus, state 1 has a 0.5 chance of terminating on the left, and state 950 has a 0.25 chance\r
of terminating on the right). As usual, termination on the left produces a reward of\r
1, and termination on the right produces a reward of +1. All other transitions have a\r
reward of zero. We use this task as a running example throughout this section.\r
Figure 9.1 shows the true value function v⇡ for this task. It is nearly a straight line,\r
curving very slightly toward the horizontal for the last 100 states at each end. Also shown\r
is the final approximate value function learned by the gradient Monte-Carlo algorithm\r
with state aggregation after 100,000 episodes with a step size of ↵ = 2 ⇥ 105. For the\r
state aggregation, the 1000 states were partitioned into 10 groups of 100 states each (i.e.,\r
states 1–100 were one group, states 101–200 were another, and so on). The staircase e↵ect"""

[[sections]]
number = "204"
title = "Chapter 9: On-policy Prediction with Approximation"
text = ""

[[sections]]
number = "0"
title = "State"
text = """
Value\r
scale\r
 True \r
value v⇡\r
 Approximate \r
MC value vˆ\r
 State distribution \r
0.0017"""

[[sections]]
number = "0.0137"
title = "Distribution"
text = """
scale\r
1 1000\r
0\r
-1\r
1\r
µ\r
Figure 9.1: Function approximation by state aggregation on the 1000-state random walk task,\r
using the gradient Monte Carlo algorithm (page 202).\r
shown in the figure is typical of state aggregation; within each group, the approximate\r
value is constant, and it changes abruptly from one group to the next. These approximate\r
values are close to the global minimum of the VE (9.1).\r
Some of the details of the approximate values are best appreciated by reference to\r
the state distribution µ for this task, shown in the lower portion of the figure with a\r
right-side scale. State 500, in the center, is the first state of every episode, but is rarely\r
visited again. On average, about 1.37% of the time steps are spent in the start state.\r
The states reachable in one step from the start state are the second most visited, with\r
about 0.17% of the time steps being spent in each of them. From there µ falls o↵ almost\r
linearly, reaching about 0.0147% at the extreme states 1 and 1000. The most visible\r
e↵ect of the distribution is on the leftmost groups, whose values are clearly shifted higher\r
than the unweighted average of the true values of states within the group, and on the\r
rightmost groups, whose values are clearly shifted lower. This is due to the states in\r
these areas having the greatest asymmetry in their weightings by µ. For example, in the\r
leftmost group, state 100 is weighted more than 3 times more strongly than state 1. Thus\r
the estimate for the group is biased toward the true value of state 100, which is higher\r
than the true value of state 1."""

[[sections]]
number = "9.4"
title = "Linear Methods"
text = """
One of the most important special cases of function approximation is that in which the\r
approximate function, vˆ(·,w), is a linear function of the weight vector, w. Corresponding\r
to every state s, there is a real-valued vector x(s) .= (x1(s), x2(s),...,xd(s))>, with the\r
same number of components as w. Linear methods approximate the state-value function"""

[[sections]]
number = "9.4"
title = "Linear Methods 205"
text = """
by the inner product between w and x(s):\r
vˆ(s,w) .= w>x(s) .= X\r
d\r
i=1\r
wixi(s). (9.8)\r
In this case the approximate value function is said to be linear in the weights, or simply\r
linear.\r
The vector x(s) is called a feature vector representing state s. Each component xi(s)\r
of x(s) is the value of a function xi : S ! R. We think of a feature as the entirety of one\r
of these functions, and we call its value for a state s a feature of s. For linear methods,\r
features are basis functions because they form a linear basis for the set of approximate\r
functions. Constructing d-dimensional feature vectors to represent states is the same as\r
selecting a set of d basis functions. Features may be defined in many di↵erent ways; we\r
cover a few possibilities in the next sections.\r
It is natural to use SGD updates with linear function approximation. The gradient of\r
the approximate value function with respect to w in this case is\r
rvˆ(s,w) = x(s).\r
Thus, in the linear case the general SGD update (9.7) reduces to a particularly simple\r
form:\r
wt+1\r
.\r
= wt + ↵\r
h\r
Ut  vˆ(St,wt)\r
i\r
x(St).\r
Because it is so simple, the linear SGD case is one of the most favorable for mathematical\r
analysis. Almost all useful convergence results for learning systems of all kinds are for\r
linear (or simpler) function approximation methods.\r
In particular, in the linear case there is only one optimum (or, in degenerate cases,\r
one set of equally good optima), and thus any method that is guaranteed to converge to\r
or near a local optimum is automatically guaranteed to converge to or near the global\r
optimum. For example, the gradient Monte Carlo algorithm presented in the previous\r
section converges to the global optimum of the VE under linear function approximation\r
if ↵ is reduced over time according to the usual conditions.\r
The semi-gradient TD(0) algorithm presented in the previous section also converges\r
under linear function approximation, but this does not follow from general results on\r
SGD; a separate theorem is necessary. The weight vector converged to is also not the\r
global optimum, but rather a point near the local optimum. It is useful to consider this\r
important case in more detail, specifically for the continuing case. The update at each\r
time t is\r
wt+1\r
.\r
= wt + ↵\r
⇣\r
Rt+1 + w>\r
t xt+1  w>t xt\r
⌘\r
xt (9.9)\r
= wt + ↵\r
⇣\r
Rt+1xt  xt\r
\r
xt  xt+1>wt\r
⌘\r
,\r
where here we have used the notational shorthand xt = x(St). Once the system has\r
reached steady state, for any given wt, the expected next weight vector can be written\r
E[wt+1|wt] = wt + ↵(b  Awt), (9.10)"""

[[sections]]
number = "206"
title = "Chapter 9: On-policy Prediction with Approximation"
text = """
where\r
b .= E[Rt+1xt] 2 Rd and A .= E\r
h\r
xt\r
\r
xt  xt+1>i2 Rd⇥d (9.11)\r
From (9.10) it is clear that, if the system converges, it must converge to the weight vector\r
wTD at which\r
b  AwTD = 0\r
) b = AwTD\r
) wTD\r
.\r
= A1b. (9.12)\r
This quantity is called the TD fixed point. In fact linear semi-gradient TD(0) converges\r
to this point. Some of the theory proving its convergence, and the existence of the inverse\r
above, is given in the box.\r
Proof of Convergence of Linear TD(0)\r
What properties assure convergence of the linear TD(0) algorithm (9.9)? Some\r
insight can be gained by rewriting (9.10) as\r
E[wt+1|wt]=(I  ↵A)wt + ↵b. (9.13)\r
Note that the matrix A multiplies the weight vector wt and not b; only A is\r
important to convergence. To develop intuition, consider the special case in which\r
A is a diagonal matrix. If any of the diagonal elements are negative, then the\r
corresponding diagonal element of I  ↵A will be greater than one, and the\r
corresponding component of wt will be amplified, which will lead to divergence if\r
continued. On the other hand, if the diagonal elements of A are all positive, then\r
↵ can be chosen smaller than one over the largest of them, such that I  ↵A is\r
diagonal with all diagonal elements between 0 and 1. In this case the first term\r
of the update tends to shrink wt, and stability is assured. In general, wt will be\r
reduced toward zero whenever A is positive definite, meaning y>Ay > 0 for any\r
real vector y 6= 0. Positive definiteness also ensures that the inverse A1 exists.\r
For linear TD(0), in the continuing case with  < 1, the A matrix (9.11) can be\r
written\r
A = X\r
s\r
µ(s)\r
X\r
a\r
⇡(a|s)\r
X\r
r,s0\r
p(r, s0|s, a)x(s)\r
\r
x(s)  x(s0)\r
>\r
= X\r
s\r
µ(s)\r
X\r
s0\r
p(s0|s)x(s)\r
\r
x(s)  x(s0)\r
>\r
= X\r
s\r
µ(s)x(s)\r
✓\r
x(s)  \r
X\r
s0\r
p(s0|s)x(s0)\r
◆>\r
= X>D(I  P)X,\r
where µ(s) is the stationary distribution under ⇡, p(s0 |s) is the probability of\r
transition from s to s0 under policy ⇡, P is the |S|⇥|S| matrix of these probabilities,"""

[[sections]]
number = "9.4"
title = "Linear Methods 207"
text = """
D is the |S| ⇥ |S| diagonal matrix with the µ(s) on its diagonal, and X is the |S| ⇥ d\r
matrix with x(s) as its rows. From here it is clear that the inner matrix D(I  P)\r
is key to determining the positive definiteness of A.\r
For a key matrix of this form, positive definiteness is assured if all of its columns\r
sum to a nonnegative number. This was shown by Sutton (1988, p. 27) based\r
on two previously established theorems. One theorem says that any matrix M\r
is positive definite if and only if the symmetric matrix S = M + M> is positive\r
definite (Sutton 1988, appendix). The second theorem says that any symmetric\r
real matrix S is positive definite if all of its diagonal entries are positive and greater\r
than the sum of the absolute values of the corresponding o↵-diagonal entries (Varga\r
1962, p. 23). For our key matrix, D(I  P), the diagonal entries are positive\r
and the o↵-diagonal entries are negative, so all we have to show is that each row\r
sum plus the corresponding column sum is positive. The row sums are all positive\r
because P is a stochastic matrix and  < 1. Thus it only remains to show that\r
the column sums are nonnegative. Note that the row vector of the column sums\r
of any matrix M can be written as 1>M, where 1 is the column vector with all\r
components equal to 1. Let µ denote the |S|-vector of the µ(s), where µ = P>µ by\r
virtue of µ being the stationary distribution. The column sums of our key matrix,\r
then, are:\r
1>D(I  P) = µ>(I  P)\r
= µ>  µ>P\r
= µ>  µ> (because µ is the stationary distribution)\r
= (1  )µ>,\r
all components of which are positive. Thus, the key matrix and its A matrix\r
are positive definite, and on-policy TD(0) is stable. (Additional conditions and a\r
schedule for reducing ↵ over time are needed to prove convergence with probability\r
one.)\r
At the TD fixed point, it has also been proven (in the continuing case) that the VE is\r
within a bounded expansion of the lowest possible error:\r
VE(wTD) \r
1\r
1  \r
min\r
w VE(w). (9.14)\r
That is, the asymptotic error of the TD method is no more than 1\r
1\r
times the smallest\r
possible error, that attained in the limit by the Monte Carlo method. Because  is often\r
near one, this expansion factor can be quite large, so there is substantial potential loss in\r
asymptotic performance with the TD method. On the other hand, recall that the TD\r
methods are often of vastly reduced variance compared to Monte Carlo methods, and\r
thus faster, as we saw in Chapters 6 and 7. Which method will be best depends on the\r
nature of the approximation and problem, and on how long learning continues."""

[[sections]]
number = "208"
title = "Chapter 9: On-policy Prediction with Approximation"
text = """
A bound analogous to (9.14) applies to other on-policy bootstrapping methods as well.\r
For example, linear semi-gradient DP (Eq. 9.7 with Ut\r
.\r
= P\r
a ⇡(a|St)\r
P\r
s0,r p(s0\r
, r|St, a)[r+\r
vˆ(s0,wt)]) with updates according to the on-policy distribution will also converge to\r
the TD fixed point. One-step semi-gradient action-value methods, such as semi-gradient\r
Sarsa(0) covered in the next chapter converge to an analogous fixed point and an analogous\r
bound. For episodic tasks, there is a slightly di↵erent but related bound (see Bertsekas\r
and Tsitsiklis, 1996). There are also a few technical conditions on the rewards, features,\r
and decrease in the step-size parameter, which we have omitted here. The full details\r
can be found in the original paper (Tsitsiklis and Van Roy, 1997).\r
Critical to these convergence results is that states are updated according to the on\u0002policy distribution. For other update distributions, bootstrapping methods using function\r
approximation may actually diverge to infinity. Examples of this and a discussion of\r
possible solution methods are given in Chapter 11.\r
Example 9.2: Bootstrapping on the 1000-state Random Walk State aggregation\r
is a special case of linear function approximation, so let’s return to the 1000-state random\r
walk to illustrate some of the observations made in this chapter. The left panel of\r
Figure 9.2 shows the final value function learned by the semi-gradient TD(0) algorithm\r
(page 203) using the same state aggregation as in Example 9.1. We see that the near\u0002asymptotic TD approximation is indeed farther from the true values than the Monte\r
Carlo approximation shown in Figure 9.1.\r
Nevertheless, TD methods retain large potential advantages in learning rate, and\r
generalize Monte Carlo methods, as we investigated fully with n-step TD methods in\r
Chapter 7. The right panel of Figure 9.2 shows results with an n-step semi-gradient\r
TD method using state aggregation on the 1000-state random walk that are strikingly\r
similar to those we obtained earlier with tabular methods and the 19-state random\r
walk (Figure 7.2). To obtain such quantitatively similar results we switched the state\r
aggregation to 20 groups of 50 states each. The 20 groups were then quantitatively close\r
0.55\r
0.5\r
0.45\r
0.35\r
0.3\r
0.25\r
0.4\r
0 0.2 0.4 0.6 0.8 1\r
↵\r
Average\r
RMS error\r
over 1000 states\r
and first 10 \r
episodes\r
n=1\r
n=2 n=4 n=8\r
n=16\r
n=32\r
n=64\r
128 512256\r
State\r
 True \r
value v⇡\r
 Approximate \r
TD value\r
1\r
0\r
-1\r
1\r
1000\r
vˆ\r
Figure 9.2: Bootstrapping with state aggregation on the 1000-state random walk task. Left:\r
Asymptotic values of semi-gradient TD are worse than the asymptotic Monte Carlo values in\r
Figure 9.1. Right: Performance of n-step methods with state-aggregation are strikingly similar\r
to those with tabular representations (cf. Figure 7.2). These data are averages over 100 runs."""

[[sections]]
number = "9.4"
title = "Linear Methods 209"
text = """
to the 19 states of the tabular problem. In particular, recall that state transitions were\r
up to 100 states to the left or right. A typical transition would then be of 50 states to\r
the right or left, which is quantitatively analogous to the single-state state transitions of\r
the 19-state tabular system. To complete the match, we use here the same performance\r
measure—an unweighted average of the RMS error over all states and over the first\r
10 episodes—rather than a VE objective as is otherwise more appropriate when using\r
function approximation.\r
The semi-gradient n-step TD algorithm used in the example above is the natural\r
extension of the tabular n-step TD algorithm presented in Chapter 7 to semi-gradient\r
function approximation. Pseudocode is given in the box below.\r
n-step semi-gradient TD for estimating vˆ ⇡ v⇡\r
Input: the policy ⇡ to be evaluated\r
Input: a di↵erentiable function ˆv : S+ ⇥ Rd ! R such that ˆv(terminal,·)=0\r
Algorithm parameters: step size ↵ > 0, a positive integer n\r
Initialize value-function weights w arbitrarily (e.g., w = 0)\r
All store and access operations (St and Rt) can take their index mod n + 1\r
Loop for each episode:\r
Initialize and store S0 6= terminal\r
T 1\r
Loop for t = 0, 1, 2,... :\r
| If t<T, then:\r
| Take an action according to ⇡(·|St)\r
| Observe and store the next reward as Rt+1 and the next state as St+1\r
| If St+1 is terminal, then T t + 1\r
| ⌧ t  n +1 (⌧ is the time whose state’s estimate is being updated)\r
| If ⌧  0:\r
| G Pmin(⌧+n,T)\r
i=⌧+1 i⌧1Ri\r
| If ⌧ + n<T, then: G G + nvˆ(S⌧+n,w) (G⌧:⌧+n)\r
| w w + ↵ [G  vˆ(S⌧ ,w)] rvˆ(S⌧ ,w)\r
Until ⌧ = T  1\r
The key equation of this algorithm, analogous to (7.2), is\r
wt+n\r
.\r
= wt+n1 + ↵ [Gt:t+n  vˆ(St,wt+n1)] rvˆ(St,wt+n1), 0  t < T, (9.15)\r
where the n-step return is generalized from (7.1) to\r
Gt:t+n\r
.\r
= Rt+1 +Rt+2 +···+n1Rt+n +nvˆ(St+n,wt+n1), 0  t  T n. (9.16)\r
Exercise 9.1 Show that tabular methods such as presented in Part I of this book are a\r
special case of linear function approximation. What would the feature vectors be? ⇤"""

[[sections]]
number = "210"
title = "Chapter 9: On-policy Prediction with Approximation"
text = ""

[[sections]]
number = "9.5"
title = "Feature Construction for Linear Methods"
text = """
Linear methods are interesting because of their convergence guarantees, but also because\r
in practice they can be very ecient in terms of both data and computation. Whether or\r
not this is so depends critically on how the states are represented in terms of features,\r
which we investigate in this large section. Choosing features appropriate to the task is\r
an important way of adding prior domain knowledge to reinforcement learning systems.\r
Intuitively, the features should correspond to the aspects of the state space along which\r
generalization may be appropriate. If we are valuing geometric objects, for example,\r
we might want to have features for each possible shape, color, size, or function. If we\r
are valuing states of a mobile robot, then we might want to have features for locations,\r
degrees of remaining battery power, recent sonar readings, and so on.\r
A limitation of the linear form is that it cannot take into account any interactions\r
between features, such as the presence of feature i being good only in the absence of\r
feature j. For example, in the pole-balancing task (Example 3.4), high angular velocity\r
can be either good or bad depending on the angle. If the angle is high, then high angular\r
velocity means an imminent danger of falling—a bad state—whereas if the angle is low,\r
then high angular velocity means the pole is righting itself—a good state. A linear value\r
function could not represent this if its features coded separately for the angle and the\r
angular velocity. It needs instead, or in addition, features for combinations of these two\r
underlying state dimensions. In the following subsections we consider a variety of general\r
ways of doing this."""

[[sections]]
number = "9.5.1"
title = "Polynomials"
text = """
The states of many problems are initially expressed as numbers, such as positions and\r
velocities in the pole-balancing task (Example 3.4), the number of cars in each lot in the\r
Jack’s car rental problem (Example 4.2), or the gambler’s capital in the gambler problem\r
(Example 4.3). In these types of problems, function approximation for reinforcement\r
learning has much in common with the familiar tasks of interpolation and regression.\r
Various families of features commonly used for interpolation and regression can also be\r
used in reinforcement learning. Polynomials make up one of the simplest families of\r
features used for interpolation and regression. While the basic polynomial features we\r
discuss here do not work as well as other types of features in reinforcement learning, they\r
serve as a good introduction because they are simple and familiar.\r
As an example, suppose a reinforcement learning problem has states with two numerical\r
dimensions. For a single representative state s, let its two numbers be s1 2 R and s2 2 R.\r
You might choose to represent s simply by its two state dimensions, so that x(s) =\r
(s1, s2)>, but then you would not be able to take into account any interactions between\r
these dimensions. In addition, if both s1 and s2 were zero, then the approximate value\r
would have to also be zero. Both limitations can be overcome by instead representing s by\r
the four-dimensional feature vector x(s) = (1, s1, s2, s1s2)>. The initial 1 feature allows\r
the representation of ane functions in the original state numbers, and the final product\r
feature, s1s2, enables interactions to be taken into account. Or you might choose to use\r
higher-dimensional feature vectors like x(s) = (1, s1, s2, s1s2, s2\r
1, s22, s1s22, s21s2, s21s22)> to"""

[[sections]]
number = "9.5"
title = "Feature Construction for Linear Methods 211"
text = """
take more complex interactions into account. Such feature vectors enable approximations\r
as arbitrary quadratic functions of the state numbers—even though the approximation is\r
still linear in the weights that have to be learned. Generalizing this example from two\r
to k numbers, we can represent highly-complex interactions among a problem’s state\r
dimensions:\r
Suppose each state s corresponds to k numbers, s1, s2, ..., sk, with each si 2 R.\r
For this k-dimensional state space, each order-n polynomial-basis feature xi can be\r
written as\r
xi(s) = ⇧k\r
j=1s\r
ci,j\r
j , (9.17)\r
where each ci,j is an integer in the set {0, 1,...,n} for an integer n  0. These\r
features make up the order-n polynomial basis for dimension k, which contains\r
(n + 1)k di↵erent features.\r
Higher-order polynomial bases allow for more accurate approximations of more compli\u0002cated functions. But because the number of features in an order-n polynomial basis grows\r
exponentially with the dimension k of the natural state space (if n>0), it is generally\r
necessary to select a subset of them for function approximation. This can be done using\r
prior beliefs about the nature of the function to be approximated, and some automated\r
selection methods developed for polynomial regression can be adapted to deal with the\r
incremental and nonstationary nature of reinforcement learning.\r
Exercise 9.2 Why does (9.17) define (n + 1)k distinct features for dimension k? ⇤\r
Exercise 9.3 What n and ci,j produce the feature vectors x(s) = (1, s1, s2, s1s2, s2\r
1, s22,\r
s1s2\r
2, s21s2, s21s22)>? ⇤"""

[[sections]]
number = "9.5.2"
title = "Fourier Basis"
text = """
Another linear function approximation method is based on the time-honored Fourier\r
series, which expresses periodic functions as weighted sums of sine and cosine basis\r
functions (features) of di↵erent frequencies. (A function f is periodic if f(x) = f(x + ⌧ )\r
for all x and some period ⌧ .) The Fourier series and the more general Fourier transform\r
are widely used in applied sciences in part because if a function to be approximated is\r
known, then the basis function weights are given by simple formulae and, further, with\r
enough basis functions essentially any function can be approximated as accurately as\r
desired. In reinforcement learning, where the functions to be approximated are unknown,\r
Fourier basis functions are of interest because they are easy to use and can perform well\r
in a range of reinforcement learning problems.\r
First consider the one-dimensional case. The usual Fourier series representation of a\r
function of one dimension having period ⌧ represents the function as a linear combination\r
of sine and cosine functions that are each periodic with periods that evenly divide ⌧ (in\r
other words, whose frequencies are integer multiples of a fundamental frequency 1/⌧ ).\r
But if you are interested in approximating an aperiodic function defined over a bounded"""

[[sections]]
number = "212"
title = "Chapter 9: On-policy Prediction with Approximation"
text = """
interval, then you can use these Fourier basis features with ⌧ set to the length of the\r
interval. The function of interest is then just one period of the periodic linear combination\r
of the sine and cosine features.\r
Furthermore, if you set ⌧ to twice the length of the interval of interest and restrict\r
attention to the approximation over the half interval [0, ⌧/2], then you can use just the\r
cosine features. This is possible because you can represent any even function, that is,\r
any function that is symmetric about the origin, with just the cosine basis. So any\r
function over the half-period [0, ⌧/2] can be approximated as closely as desired with\r
enough cosine features. (Saying “any function” is not exactly correct because the function\r
has to be mathematically well-behaved, but we skip this technicality here.) Alternatively,\r
it is possible to use just sine features, linear combinations of which are always odd\r
functions, that is functions that are anti-symmetric about the origin. But it is generally\r
better to keep just the cosine features because “half-even” functions tend to be easier to\r
approximate than “half-odd” functions because the latter are often discontinuous at the\r
origin. Of course, this does not rule out using both sine and cosine features to approximate\r
over the interval [0, ⌧/2], which might be advantageous in some circumstances.\r
Following this logic and letting ⌧ = 2 so that the features are defined over the half-⌧\r
interval [0, 1], the one-dimensional order-n Fourier cosine basis consists of the n + 1\r
features\r
xi(s) = cos(i⇡s), s 2 [0, 1],\r
for i = 0,...,n. Figure 9.3 shows one-dimensional Fourier cosine features xi, for i =\r
1, 2, 3, 4; x0 is a constant function.\r
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 −1\r
−0.8\r
−0.6\r
−0.4\r
−0.2\r
0\r
0.2\r
0.4\r
0.6\r
0.8"""

[[sections]]
number = "1"
title = "Univariate Fourier Basis Function k=1"
text = """
1\r
-1 0 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 −1\r
−0.8\r
−0.6\r
−0.4\r
−0.2\r
0\r
0.2\r
0.4\r
0.6\r
0.8"""

[[sections]]
number = "1"
title = "Univariate Fourier Basis Function k=2"
text = """
1\r
-1 0 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 −1\r
−0.8\r
−0.6\r
−0.4\r
−0.2\r
0\r
0.2\r
0.4\r
0.6\r
0.8"""

[[sections]]
number = "1"
title = "Univariate Fourier Basis Function k=3"
text = """
1\r
-1 0 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 −1\r
−0.8\r
−0.6\r
−0.4\r
−0.2\r
0\r
0.2\r
0.4\r
0.6\r
0.8"""

[[sections]]
number = "1"
title = "Univariate Fourier Basis Function k=4"
text = """
1\r
-1 0 1\r
Figure 9.3: One-dimensional Fourier cosine-basis features xi, i = 1, 2, 3, 4, for approximating\r
functions over the interval [0, 1]. After Konidaris et al. (2011).\r
This same reasoning applies to the Fourier cosine series approximation in the multi\u0002dimensional case as described in the box below.\r
Suppose each state s corresponds to a vector of k numbers, s = (s1, s2, ..., sk)>,\r
with each si 2 [0, 1]. The ith feature in the order-n Fourier cosine basis can then\r
be written\r
xi(s) = cos ⇡s>ci\r
\r
, (9.18)\r
where ci = (ci\r
1,...,ci\r
k)>, with ci\r
j 2 {0,...,n} for j = 1,...,k and i = 1,...,(n+1)k.\r
This defines a feature for each of the (n + 1)k possible integer vectors ci. The inner"""

[[sections]]
number = "9.5"
title = "Feature Construction for Linear Methods 213"
text = """
product s>ci has the e↵ect of assigning an integer in {0,...,n} to each dimension\r
of s. As in the one-dimensional case, this integer determines the feature’s frequency\r
along that dimension. The features can of course be shifted and scaled to suit the\r
bounded state space of a particular application.\r
As an example, consider the k = 2 case in which s = (s1, s2)>, where each ci = (ci\r
1, ci2)>.\r
Figure 9.4 shows a selection of six Fourier cosine features, each labeled by the vector ci\r
that defines it (s1 is the horizontal axis and ci is shown as a row vector with the index i\r
omitted). Any zero in c means the feature is constant along that state dimension. So if\r
c = (0, 0)>, the feature is constant over both dimensions; if c = (c1, 0)> the feature is\r
constant over the second dimension and varies over the first with frequency depending\r
on c1; and similarly, for c = (0, c2)>. When c = (c1, c2)> with neither cj = 0, the\r
feature varies along both dimensions and represents an interaction between the two state\r
variables. The values of c1 and c2 determine the frequency along each dimension, and\r
their ratio gives the direction of the interaction.\r
c = (0, 1)\r
1\r
1\r
0\r
0 1\r
1\r
0\r
0\r
c = (0, 1)> c = (1, 0)\r
1\r
1\r
0\r
0 1\r
1\r
0\r
0\r
c = (1, 0)> c = (1, 1)\r
1\r
1\r
0\r
0 1\r
1\r
0\r
0\r
c = (1, 1)>\r
c = (1, 5)\r
1\r
1\r
0\r
0 1\r
1\r
0\r
0\r
c = (0, 5)> c = (2, 5)\r
1\r
1\r
0\r
0 1\r
1\r
0\r
0\r
c = (2, 5)>\r
1\r
1\r
0\r
0\r
c = (5, 2)>\r
Figure 9.4: A selection of six two-dimensional Fourier cosine features, each labeled by the\r
vector ci that defines it (s1 is the horizontal axis, and ci is shown with the index i omitted).\r
After Konidaris et al. (2011).\r
When using Fourier cosine features with a learning algorithm such as (9.7), semi\u0002gradient TD(0), or semi-gradient Sarsa, it may be helpful to use a di↵erent step-size\r
parameter for each feature. If ↵ is the basic step-size parameter, then Konidaris, Osentoski,\r
and Thomas (2011) suggest setting the step-size parameter for feature xi to ↵i =\r
↵/\r
p(ci\r
1)2 + ··· + (ci\r
k)2 (except when each ci\r
j = 0, in which case ↵i = ↵)."""

[[sections]]
number = "214"
title = "Chapter 9: On-policy Prediction with Approximation"
text = """
Fourier cosine features with Sarsa can produce good performance compared to several\r
other collections of basis functions, including polynomial and radial basis functions. Not\r
surprisingly, however, Fourier features have trouble with discontinuities because it is\r
dicult to avoid “ringing” around points of discontinuity unless very high frequency basis\r
functions are included.\r
The number of features in the order-n Fourier basis grows exponentially with the\r
dimension of the state space, but if that dimension is small enough (e.g., k  5), then\r
one can select n so that all of the order-n Fourier features can be used. This makes the\r
selection of features more-or-less automatic. For high dimension state spaces, however, it\r
is necessary to select a subset of these features. This can be done using prior beliefs about\r
the nature of the function to be approximated, and some automated selection methods\r
can be adapted to deal with the incremental and nonstationary nature of reinforcement\r
learning. An advantage of Fourier basis features in this regard is that it is easy to select\r
features by setting the ci vectors to account for suspected interactions among the state\r
variables and by limiting the values in the cj vectors so that the approximation can\r
filter out high frequency components considered to be noise. On the other hand, because\r
Fourier features are non-zero over the entire state space (with the few zeros excepted),\r
they represent global properties of states, which can make it dicult to find good ways\r
to represent local properties.\r
Figure 9.5 shows learning curves comparing the Fourier and polynomial bases on the\r
1000-state random walk example. In general, we do not recommend using polynomials\r
for online learning.2\r
.4\r
.3\r
.2\r
.1\r
0\r
0 5000\r
Episodes\r
Polynomial basis\r
Fourier basis\r
p\r
VE\r
averaged\r
over 30 runs\r
Figure 9.5: Fourier basis vs polynomials on the 1000-state random walk. Shown are learning\r
curves for the gradient Monte Carlo method with Fourier and polynomial bases of order 5, 10,\r
and 20. The step-size parameters were roughly optimized for each case: ↵ = 0.0001 for the\r
polynomial basis and ↵ = 0.00005 for the Fourier basis. The performance measure (y-axis) is\r
the root mean square value error (9.1).\r
2There are families of polynomials more complicated than those we have discussed, for example,\r
di↵erent families of orthogonal polynomials, and these might work better, but at present there is little\r
experience with them in reinforcement learning."""

[[sections]]
number = "9.5"
title = "Feature Construction for Linear Methods 215"
text = ""

[[sections]]
number = "9.5.3"
title = "Coarse Coding"
text = """
s0\r
s\r
Figure 9.6: Coarse coding. Generaliza\u0002tion from state s to state s0 depends on\r
the number of their features whose recep\u0002tive fields (in this case, circles) overlap.\r
These states have one feature in common,\r
so there will be slight generalization be\u0002tween them.\r
Consider a task in which the natural repre\u0002sentation of the state set is a continuous two\u0002dimensional space. One kind of representation for\r
this case is made up of features corresponding to\r
circles in state space, as shown to the right. If\r
the state is inside a circle, then the corresponding\r
feature has the value 1 and is said to be present;\r
otherwise the feature is 0 and is said to be absent.\r
This kind of 1–0-valued feature is called a binary\r
feature. Given a state, which binary features are\r
present indicate within which circles the state lies,\r
and thus coarsely code for its location. Represent\u0002ing a state with features that overlap in this way\r
(although they need not be circles or binary) is\r
known as coarse coding.\r
Assuming linear gradient-descent function ap\u0002proximation, consider the e↵ect of the size and\r
density of the circles. Corresponding to each cir\u0002cle is a single weight (a component of w) that is\r
a↵ected by learning. If we train at one state, a\r
point in the space, then the weights of all circles\r
intersecting that state will be a↵ected. Thus, by (9.8), the approximate value function\r
will be a↵ected at all states within the union of the circles, with a greater e↵ect the more\r
circles a point has “in common” with the state, as shown in Figure 9.6. If the circles are\r
small, then the generalization will be over a short distance, as in Figure 9.7 (left), whereas\r
if they are large, it will be over a large distance, as in Figure 9.7 (middle). Moreover,\r
a) Narrow generalization b) Broad generalization c) Asymmetric generalization\r
Figure 9.7: Generalization in linear function approximation methods is determined by the\r
sizes and shapes of the features’ receptive fields. All three of these cases have roughly the same\r
number and density of features."""

[[sections]]
number = "216"
title = "Chapter 9: On-policy Prediction with Approximation"
text = """
the shape of the features will determine the nature of the generalization. For example, if\r
they are not strictly circular, but are elongated in one direction, then generalization will\r
be similarly a↵ected, as in Figure 9.7 (right).\r
Features with large receptive fields give broad generalization, but might also seem to\r
limit the learned function to a coarse approximation, unable to make discriminations\r
much finer than the width of the receptive fields. Happily, this is not the case. Initial\r
generalization from one point to another is indeed controlled by the size and shape of\r
the receptive fields, but acuity, the finest discrimination ultimately possible, is controlled\r
more by the total number of features.\r
Example 9.3: Coarseness of Coarse Coding This example illustrates the e↵ect on\r
learning of the size of the receptive fields in coarse coding. Linear function approximation\r
based on coarse coding and (9.7) was used to learn a one-dimensional square-wave function\r
(shown at the top of Figure 9.8). The values of this function were used as the targets, Ut.\r
With just one dimension, the receptive fields were intervals rather than circles. Learning\r
was repeated with three di↵erent sizes of the intervals: narrow, medium, and broad, as\r
shown at the bottom of the figure. All three cases had the same density of features,\r
about 50 over the extent of the function being learned. Training examples were generated\r
uniformly at random over this extent. The step-size parameter was ↵ = 0.2\r
n , where n is\r
the number of features that were present at one time. Figure 9.8 shows the functions\r
learned in all three cases over the course of learning. Note that the width of the features\r
had a strong e↵ect early in learning. With broad features, the generalization tended to be\r
broad; with narrow features, only the close neighbors of each trained point were changed,\r
causing the function learned to be more bumpy. However, the final function learned was\r
a↵ected only slightly by the width of the features. Receptive field shape tends to have a\r
strong e↵ect on generalization but little e↵ect on asymptotic solution quality.\r
10\r
40\r
160\r
640\r
2560"""

[[sections]]
number = "10240"
title = "Narrow features"
text = """
desired\r
function\r
Medium features Broad\r
features\r
#Examples approx\u0002imation\r
feature\r
width\r
Figure 9.8: Example of feature width’s strong e↵ect on initial generalization (first row) and\r
weak e↵ect on asymptotic accuracy (last row)."""

[[sections]]
number = "9.5"
title = "Feature Construction for Linear Methods 217"
text = ""

[[sections]]
number = "9.5.4"
title = "Tile Coding"
text = """
Tile coding is a form of coarse coding for multi-dimensional continuous spaces that is\r
flexible and computationally ecient. It may be the most practical feature representation\r
for modern sequential digital computers.\r
In tile coding the receptive fields of the features are grouped into partitions of the state\r
space. Each such partition is called a tiling, and each element of the partition is called a\r
tile. For example, the simplest tiling of a two-dimensional state space is a uniform grid\r
such as that shown on the left side of Figure 9.9. The tiles or receptive field here are\r
squares rather than the circles in Figure 9.6. If just this single tiling were used, then the\r
state indicated by the white spot would be represented by the single feature whose tile\r
it falls within; generalization would be complete to all states within the same tile and\r
nonexistent to states outside it. With just one tiling, we would not have coarse coding\r
but just a case of state aggregation.\r
Point in \r
state space\r
to be\r
represented\r
Tiling 1\r
Tiling 2\r
Tiling 3\r
Tiling 4 Continuous \r
2D state \r
space\r
Four active\r
tiles/features \r
overlap the point\r
and are used to \r
represent it\r
Figure 9.9: Multiple, overlapping grid-tilings on a limited two-dimensional space. These tilings\r
are o↵set from one another by a uniform amount in each dimension.\r
To get the strengths of coarse coding requires overlapping receptive fields, and by\r
definition the tiles of a partition do not overlap. To get true coarse coding with tile coding,\r
multiple tilings are used, each o↵set by a fraction of a tile width. A simple case with\r
four tilings is shown on the right side of Figure 9.9. Every state, such as that indicated\r
by the white spot, falls in exactly one tile in each of the four tilings. These four tiles\r
correspond to four features that become active when the state occurs. Specifically, the\r
feature vector x(s) has one component for each tile in each tiling. In this example there\r
are 4 ⇥ 4 ⇥ 4 = 64 components, all of which will be 0 except for the four corresponding to\r
the tiles that s falls within. Figure 9.10 shows the advantage of multiple o↵set tilings\r
(coarse coding) over a single tiling on the 1000-state random walk example.\r
An immediate practical advantage of tile coding is that, because it works with partitions,\r
the overall number of features that are active at one time is the same for any state.\r
Exactly one feature is present in each tiling, so the total number of features present is\r
always the same as the number of tilings. This allows the step-size parameter, ↵, to"""

[[sections]]
number = "218"
title = "Chapter 9: On-policy Prediction with Approximation"
text = """
.4\r
.3\r
.2\r
.1\r
0\r
averaged\r
over 30 runs\r
0 5000\r
Episodes\r
State aggregation\r
(one tiling)\r
Tile coding (50 tilings)\r
p\r
VE\r
Figure 9.10: Why we use coarse coding. Shown are learning curves on the 1000-state random\r
walk example for the gradient Monte Carlo algorithm with a single tiling and with multiple\r
tilings. The space of 1000 states was treated as a single continuous dimension, covered with tiles\r
each 200 states wide. The multiple tilings were o↵set from each other by 4 states. The step-size\r
parameter was set so that the initial learning rate in the two cases was the same, ↵ = 0.0001 for\r
the single tiling and ↵ = 0.0001/50 for the 50 tilings.\r
be set in an easy, intuitive way. For example, choosing ↵ = 1\r
n , where n is the number\r
of tilings, results in exact one-trial learning. If the example s 7! v is trained on, then\r
whatever the prior estimate, vˆ(s,wt), the new estimate will be vˆ(s,wt+1) = v. Usually\r
one wishes to change more slowly than this, to allow for generalization and stochastic\r
variation in target outputs. For example, one might choose ↵ = 1\r
10n , in which case the\r
estimate for the trained state would move one-tenth of the way to the target in one\r
update, and neighboring states will be moved less, proportional to the number of tiles\r
they have in common.\r
Tile coding also gains computational advantages from its use of binary feature vectors.\r
Because each component is either 0 or 1, the weighted sum making up the approximate\r
value function (9.8) is almost trivial to compute. Rather than performing d multiplications\r
and additions, one simply computes the indices of the n ⌧ d active features and then\r
adds up the n corresponding components of the weight vector.\r
Generalization occurs to states other than the one trained if those states fall within\r
any of the same tiles, proportional to the number of tiles in common. Even the choice of\r
how to o↵set the tilings from each other a↵ects generalization. If they are o↵set uniformly\r
in each dimension, as they were in Figure 9.9, then di↵erent states can generalize in\r
qualitatively di↵erent ways, as shown in the upper half of Figure 9.11. Each of the eight\r
subfigures show the pattern of generalization from a trained state to nearby points. In this\r
example there are eight tilings, thus 64 subregions within a tile that generalize distinctly,\r
but all according to one of these eight patterns. Note how uniform o↵sets result in a\r
strong e↵ect along the diagonal in many patterns. These artifacts can be avoided if the\r
tilings are o↵set asymmetrically, as shown in the lower half of the figure. These lower\r
generalization patterns are better because they are all well centered on the trained state\r
with no obvious asymmetries."""

[[sections]]
number = "9.5"
title = "Feature Construction for Linear Methods 219"
text = """
Possible \r
generalizations \r
for uniformly \r
offset tilings\r
Possible \r
generalizations\r
for asymmetrically \r
offset tilings\r
Figure 9.11: Why tile asymmetrical o↵sets are preferred in tile coding. Shown is the strength\r
of generalization from a trained state, indicated by the small black plus, to nearby states, for the\r
case of eight tilings. If the tilings are uniformly o↵set (above), then there are diagonal artifacts\r
and substantial variations in the generalization, whereas with asymmetrically o↵set tilings the\r
generalization is more spherical and homogeneous.\r
Tilings in all cases are o↵set from each other by a fraction of a tile width in each\r
dimension. If w denotes the tile width and n the number of tilings, then w\r
n is a fundamental\r
unit. Within small squares w\r
n on a side, all states activate the same tiles, have the same\r
feature representation, and the same approximated value. If a state is moved by w\r
n\r
in any cartesian direction, the feature representation changes by one component/tile.\r
Uniformly o↵set tilings are o↵set from each other by exactly this unit distance. For a\r
two-dimensional space, we say that each tiling is o↵set by the displacement vector (1, 1),\r
meaning that it is o↵set from the previous tiling by w\r
n times this vector. In these terms,\r
the asymmetrically o↵set tilings shown in the lower part of Figure 9.11 are o↵set by a\r
displacement vector of (1, 3).\r
Extensive studies have been made of the e↵ect of di↵erent displacement vectors on the\r
generalization of tile coding (Parks and Militzer, 1991; An, 1991; An, Miller and Parks,"""

[[sections]]
number = "220"
title = "Chapter 9: On-policy Prediction with Approximation"
text = """
1991; Miller, An, Glanz and Carter, 1990), assessing their homegeneity and tendency\r
toward diagonal artifacts like those seen for the (1, 1) displacement vectors. Based on this\r
work, Miller and Glanz (1996) recommend using displacement vectors consisting of the\r
first odd integers. In particular, for a continuous space of dimension k, a good choice is\r
to use the first odd integers (1, 3, 5, 7,..., 2k  1), with n (the number of tilings) set to an\r
integer power of 2 greater than or equal to 4k. This is what we have done to produce the\r
tilings in the lower half of Figure 9.11, in which k = 2, n = 23  4k, and the displacement\r
vector is (1, 3). In a three-dimensional case, the first four tilings would be o↵set in total\r
from a base position by (0, 0, 0), (1, 3, 5), (2, 6, 10), and (3, 9, 15). Open-source software\r
that can eciently make tilings like this for any k is readily available.\r
In choosing a tiling strategy, one has to pick the number of the tilings and the shape of\r
the tiles. The number of tilings, along with the size of the tiles, determines the resolution\r
or fineness of the asymptotic approximation, as in general coarse coding and illustrated\r
in Figure 9.8. The shape of the tiles will determine the nature of generalization as in\r
Figure 9.7. Square tiles will generalize roughly equally in each dimension as indicated in\r
Figure 9.11 (lower). Tiles that are elongated along one dimension, such as the stripe tilings\r
in Figure 9.12 (middle), will promote generalization along that dimension. The tilings in\r
Figure 9.12 (middle) are also denser and thinner on the left, promoting discrimination\r
along the horizontal dimension at lower values along that dimension. The diagonal stripe\r
tiling in Figure 9.12 (right) will promote generalization along one diagonal. In higher\r
dimensions, axis-aligned stripes correspond to ignoring some of the dimensions in some\r
of the tilings, that is, to hyperplanar slices. Irregular tilings such as shown in Figure 9.12\r
(left) are also possible, though rare in practice and beyond the standard software.\r
a) Irregular b) Log stripes c) Diagonal stripes\r
Figure 9.12: Tilings need not be grids. They can be arbitrarily shaped and non-uniform, while\r
still in many cases being computationally ecient to compute.\r
In practice, it is often desirable to use di↵erent shaped tiles in di↵erent tilings. For\r
example, one might use some vertical stripe tilings and some horizontal stripe tilings.\r
This would encourage generalization along either dimension. However, with stripe tilings\r
alone it is not possible to learn that a particular conjunction of horizontal and vertical\r
coordinates has a distinctive value (whatever is learned for it will bleed into states with the\r
same horizontal and vertical coordinates). For this one needs the conjunctive rectangular\r
tiles such as originally shown in Figure 9.9. With multiple tilings—some horizontal, some\r
vertical, and some conjunctive—one can get everything: a preference for generalizing\r
along each dimension, yet the ability to learn specific values for conjunctions (see Sutton,"""

[[sections]]
number = "9.5"
title = "Feature Construction for Linear Methods 221"
text = """
1996 for examples). The choice of tilings determines generalization, and until this choice\r
can be e↵ectively automated, it is important that tile coding enables the choice to be\r
made flexibly and in a way that makes sense to people.\r
Another useful trick for reducing memory requirements is hashing—a consistent pseudo\u0002random collapsing of a large tiling into a much smaller set of tiles. Hashing produces\r
tiles consisting of noncontiguous, disjoint regions randomly spread throughout the state\r
one\r
tile\r
space, but that still form an exhaustive partition. For example,\r
one tile might consist of the four subtiles shown to the right.\r
Through hashing, memory requirements are often reduced by\r
large factors with little loss of performance. This is possible\r
because high resolution is needed in only a small fraction of the\r
state space. Hashing frees us from the curse of dimensionality\r
in the sense that memory requirements need not be exponential\r
in the number of dimensions, but need merely match the real\r
demands of the task. Open-source implementations of tile coding\r
commonly include ecient hashing.\r
Exercise 9.4 Suppose we believe that one of two state dimensions is more likely to have\r
an e↵ect on the value function than is the other, that generalization should be primarily\r
across this dimension rather than along it. What kind of tilings could be used to take\r
advantage of this prior knowledge? ⇤"""

[[sections]]
number = "9.5.5"
title = "Radial Basis Functions"
text = """
Radial basis functions (RBFs) are the natural generalization of coarse coding to continuous\u0002valued features. Rather than each feature being either 0 or 1, it can be anything in the\r
interval [0, 1], reflecting various degrees to which the feature is present. A typical RBF\r
feature, xi, has a Gaussian (bell-shaped) response xi(s) dependent only on the distance\r
between the state, s, and the feature’s prototypical or center state, ci, and relative to the\r
feature’s width, i:\r
xi(s) .= exp ✓||s  ci||2\r
22\r
i\r
◆\r
.\r
The norm or distance metric of course can be chosen in whatever way seems most\r
appropriate to the states and task at hand. The figure below shows a one-dimensional\r
example with a Euclidean distance metric.\r
ci\r
!i\r
ci+1 ci-1\r
Figure 9.13: One-dimensional radial basis functions."""

[[sections]]
number = "222"
title = "Chapter 9: On-policy Prediction with Approximation"
text = """
The primary advantage of RBFs over binary features is that they produce approximate\r
functions that vary smoothly and are di↵erentiable. Although this is appealing, in most\r
cases it has no practical significance. Nevertheless, extensive studies have been made of\r
graded response functions such as RBFs in the context of tile coding (An, 1991; Miller et\r
al., 1991; An et al., 1991; Lane, Handelman and Gelfand, 1992). All of these methods\r
require substantial additional computational complexity (over tile coding) and often\r
reduce performance when there are more than two state dimensions. In high dimensions\r
the edges of tiles are much more important, and it has proven dicult to obtain well\r
controlled graded tile activations near the edges.\r
An RBF network is a linear function approximator using RBFs for its features. Learning\r
is defined by equations (9.7) and (9.8), exactly as in other linear function approximators.\r
In addition, some learning methods for RBF networks change the centers and widths of\r
the features as well, bringing them into the realm of nonlinear function approximators.\r
Nonlinear methods may be able to fit target functions much more precisely. The downside\r
to RBF networks, and to nonlinear RBF networks especially, is greater computational\r
complexity and, often, more manual tuning before learning is robust and ecient."""

[[sections]]
number = "9.6"
title = "Selecting Step-Size Parameters Manually"
text = """
Most SGD methods require the designer to select an appropriate step-size parameter ↵.\r
Ideally this selection would be automated, and in some cases it has been, but for most\r
cases it is still common practice to set it manually. To do this, and to better understand\r
the algorithms, it is useful to develop some intuitive sense of the role of the step-size\r
parameter. Can we say in general how it should be set?\r
Theoretical considerations are unfortunately of little help. The theory of stochastic\r
approximation gives us conditions (2.7) on a slowly decreasing step-size sequence that are\r
sucient to guarantee convergence, but these tend to result in learning that is too slow.\r
The classical choice ↵t = 1/t, which produces sample averages in tabular MC methods, is\r
not appropriate for TD methods, for nonstationary problems, or for any method using\r
function approximation. For linear methods, there are recursive least-squares methods\r
that set an optimal matrix step size, and these methods can be extended to temporal\u0002di↵erence learning as in the LSTD method described in Section 9.8, but these require\r
O(d2) step-size parameters, or d times more parameters than we are learning. For this\r
reason we rule them out for use on large problems where function approximation is most\r
needed.\r
To get some intuitive feel for how to set the step-size parameter manually, it is best\r
to go back momentarily to the tabular case. There we can understand that a step size\r
of ↵ = 1 will result in a complete elimination of the sample error after one target (see\r
(2.4) with a step size of one). As discussed on page 201, we usually want to learn slower\r
than this. In the tabular case, a step size of ↵ = 1\r
10 would take about 10 experiences to\r
converge approximately to their mean target, and if we wanted to learn in 100 experiences\r
we would use ↵ = 1"""

[[sections]]
number = "100"
title = "In general, if ↵ = 1⌧ , then the tabular estimate for a state will"
text = """
approach the mean of its targets, with the most recent targets having the greatest e↵ect,\r
after about ⌧ experiences with the state."""

[[sections]]
number = "9.7"
title = "Nonlinear Function Approximation: Artificial Neural Networks 223"
text = """
With general function approximation there is not such a clear notion of number of\r
experiences with a state, as each state may be similar to and dissimilar from all the others\r
to various degrees. However, there is a similar rule that gives similar behavior in the case\r
of linear function approximation. Suppose you wanted to learn in about ⌧ experiences\r
with substantially the same feature vector. A good rule of thumb for setting the step-size\r
parameter of linear SGD methods is then\r
↵ .= \r
⌧E\r
⇥\r
x>x\r
⇤1\r
, (9.19)\r
where x is a random feature vector chosen from the same distribution as input vectors\r
will be in the SGD. This method works best if the feature vectors do not vary greatly in\r
length; ideally x>x is a constant.\r
Exercise 9.5 Suppose you are using tile coding to transform a seven-dimensional continuous\r
state space into binary feature vectors to estimate a state value function vˆ(s,w) ⇡ v⇡(s).\r
You believe that the dimensions do not interact strongly, so you decide to use eight tilings\r
of each dimension separately (stripe tilings), for 7 ⇥ 8 = 56 tilings. In addition, in case\r
there are some pairwise interactions between the dimensions, you also take all 7\r
2\r
\r
= 21\r
pairs of dimensions and tile each pair conjunctively with rectangular tiles. You make\r
two tilings for each pair of dimensions, making a grand total of 21 ⇥ 2 + 56 = 98 tilings.\r
Given these feature vectors, you suspect that you still have to average out some noise,\r
so you decide that you want learning to be gradual, taking about 10 presentations with\r
the same feature vector before learning nears its asymptote. What step-size parameter ↵\r
should you use? Why? ⇤\r
Exercise 9.6 If ⌧ = 1 and x(St)>x(St) = E\r
⇥\r
x>x\r
⇤\r
, prove that (9.19) together with (9.7)\r
and linear function approximation results in the error being reduced to zero in one update."""

[[sections]]
number = "9.7"
title = "Nonlinear Function Approximation:"
text = """
Artificial Neural Networks\r
Artificial neural networks (ANNs) are widely used for nonlinear function approximation.\r
An ANN is a network of interconnected units that have some of the properties of neurons,\r
the main components of nervous systems. ANNs have a long history, with the latest\r
advances in training deeply-layered ANNs (deep learning) being responsible for some\r
of the most impressive abilities of machine learning systems, including reinforcement\r
learning systems. In Chapter 16 we describe several impressive examples of reinforcement\r
learning systems that use ANN function approximation.\r
Figure 9.14 shows a generic feedforward ANN, meaning that there are no loops in the\r
network, that is, there are no paths within the network by which a unit’s output can\r
influence its input. The network in the figure has an output layer consisting of two output\r
units, an input layer with four input units, and two “hidden layers”: layers that are neither\r
input nor output layers. A real-valued weight is associated with each link. A weight\r
roughly corresponds to the ecacy of a synaptic connection in a real neural network (see\r
Section 15.1). If an ANN has at least one loop in its connections, it is a recurrent rather"""

[[sections]]
number = "224"
title = "Chapter 9: On-policy Prediction with Approximation"
text = """
Figure 9.14: A generic feedforward ANN with four input units, two output units, and two\r
hidden layers.\r
than a feedforward ANN. Although both feedforward and recurrent ANNs have been\r
used in reinforcement learning, here we look only at the simpler feedforward case.\r
The units (the circles in Figure 9.14) are typically semi-linear units, meaning that they\r
compute a weighted sum of their input signals and then apply to the result a nonlinear\r
function, called the activation function, to produce the unit’s output, or activation.\r
Di↵erent activation functions are used, but they are typically S-shaped, or sigmoid,\r
functions such as the logistic function f(x)=1/(1 + ex), though sometimes the rectifier\r
nonlinearity f(x) = max(0, x) is used. A step function like f(x) = 1 if x  ✓, and 0\r
otherwise, results in a binary unit with threshold ✓. The units in a network’s input layer\r
are somewhat di↵erent in having their activations set to externally-supplied values that\r
are the inputs to the function the network is approximating.\r
The activation of each output unit of a feedforward ANN is a nonlinear function of the\r
activation patterns over the network’s input units. The functions are parameterized by\r
the network’s connection weights. An ANN with no hidden layers can represent only a\r
very small fraction of the possible input-output functions. However an ANN with a single\r
hidden layer containing a large enough finite number of sigmoid units can approximate\r
any continuous function on a compact region of the network’s input space to any degree\r
of accuracy (Cybenko, 1989). This is also true for other nonlinear activation functions\r
that satisfy mild conditions, but nonlinearity is essential: if all the units in a multi-layer\r
feedforward ANN have linear activation functions, the entire network is equivalent to a\r
network with no hidden layers (because linear functions of linear functions are themselves\r
linear).\r
Despite this “universal approximation” property of one-hidden-layer ANNs, both\r
experience and theory show that approximating the complex functions needed for many\r
artificial intelligence tasks is made easier—indeed may require—abstractions that are\r
hierarchical compositions of many layers of lower-level abstractions, that is, abstractions"""

[[sections]]
number = "9.7"
title = "Nonlinear Function Approximation: Artificial Neural Networks 225"
text = """
produced by deep architectures such as ANNs with many hidden layers. (See Bengio,\r
2009, for a thorough review.) The successive layers of a deep ANN compute increasingly\r
abstract representations of the network’s “raw” input, with each unit providing a feature\r
contributing to a hierarchical representation of the overall input-output function of the\r
network.\r
Training the hidden layers of an ANN is therefore a way to automatically create\r
features appropriate for a given problem so that hierarchical representations can be\r
produced without relying exclusively on hand-crafted features. This has been an enduring\r
challenge for artificial intelligence and explains why learning algorithms for ANNs with\r
hidden layers have received so much attention over the years. ANNs typically learn by a\r
stochastic gradient method (Section 9.3). Each weight is adjusted in a direction aimed at\r
improving the network’s overall performance as measured by an objective function to\r
be either minimized or maximized. In the most common supervised learning case, the\r
objective function is the expected error, or loss, over a set of labeled training examples. In\r
reinforcement learning, ANNs can use TD errors to learn value functions, or they can aim\r
to maximize expected reward as in a gradient bandit (Section 2.8) or a policy-gradient\r
algorithm (Chapter 13). In all of these cases it is necessary to estimate how a change\r
in each connection weight would influence the network’s overall performance, in other\r
words, to estimate the partial derivative of an objective function with respect to each\r
weight, given the current values of all the network’s weights. The gradient is the vector\r
of these partial derivatives.\r
The most successful way to do this for ANNs with hidden layers (provided the units\r
have di↵erentiable activation functions) is the backpropagation algorithm, which consists\r
of alternating forward and backward passes through the network. Each forward pass\r
computes the activation of each unit given the current activations of the network’s input\r
units. After each forward pass, a backward pass eciently computes a partial derivative\r
for each weight. (As in other stochastic gradient learning algorithms, the vector of these\r
partial derivatives is an estimate of the true gradient.) In Section 15.10 we discuss\r
methods for training ANNs with hidden layers that use reinforcement learning principles\r
instead of backpropagation. These methods are less ecient than the backpropagation\r
algorithm, but they may be closer to how real neural networks learn.\r
The backpropagation algorithm can produce good results for shallow networks having\r
1 or 2 hidden layers, but it may not work well for deeper ANNs. In fact, training a\r
network with k + 1 hidden layers can actually result in poorer performance than training\r
a network with k hidden layers, even though the deeper network can represent all the\r
functions that the shallower network can (Bengio, 2009). Explaining results like these\r
is not easy, but several factors are important. First, the large number of weights in\r
a typical deep ANN makes it dicult to avoid the problem of overfitting, that is, the\r
problem of failing to generalize correctly to cases on which the network has not been\r
trained. Second, backpropagation does not work well for deep ANNs because the partial\r
derivatives computed by its backward passes either decay rapidly toward the input side\r
of the network, making learning by deep layers extremely slow, or the partial derivatives\r
grow rapidly toward the input side of the network, making learning unstable. Methods\r
for dealing with these problems are largely responsible for many impressive recent results"""

[[sections]]
number = "226"
title = "Chapter 9: On-policy Prediction with Approximation"
text = """
achieved by systems that use deep ANNs.\r
Overfitting is a problem for any function approximation method that adjusts functions\r
with many degrees of freedom on the basis of limited training data. It is less of a\r
problem for online reinforcement learning that does not rely on limited training sets, but\r
generalizing e↵ectively is still an important issue. Overfitting is a problem for ANNs in\r
general, but especially so for deep ANNs because they tend to have very large numbers\r
of weights. Many methods have been developed for reducing overfitting. These include\r
stopping training when performance begins to decrease on validation data di↵erent\r
from the training data (cross validation), modifying the objective function to discourage\r
complexity of the approximation (regularization), and introducing dependencies among\r
the weights to reduce the number of degrees of freedom (e.g., weight sharing).\r
A particularly e↵ective method for reducing overfitting by deep ANNs is the dropout\r
method introduced by Srivastava, Hinton, Krizhevsky, Sutskever, and Salakhutdinov\r
(2014). During training, units are randomly removed from the network (dropped out)\r
along with their connections. This can be thought of as training a large number of\r
“thinned” networks. Combining the results of these thinned networks at test time is a way\r
to improve generalization performance. The dropout method eciently approximates this\r
combination by multiplying each outgoing weight of a unit by the probability that that\r
unit was retained during training. Srivastava et al. found that this method significantly\r
improves generalization performance. It encourages individual hidden units to learn\r
features that work well with random collections of other features. This increases the\r
versatility of the features formed by the hidden units so that the network does not overly\r
specialize to rarely-occurring cases.\r
Hinton, Osindero, and Teh (2006) took a major step toward solving the problem of\r
training the deep layers of a deep ANN in their work with deep belief networks, layered\r
networks closely related to the deep ANNs discussed here. In their method, the deepest\r
layers are trained one at a time using an unsupervised learning algorithm. Without\r
relying on the overall objective function, unsupervised learning can extract features that\r
capture statistical regularities of the input stream. The deepest layer is trained first, then\r
with input provided by this trained layer, the next deepest layer is trained, and so on,\r
until the weights in all, or many, of the network’s layers are set to values that now act as\r
initial values for supervised learning. The network is then fine-tuned by backpropagation\r
with respect to the overall objective function. Studies show that this approach generally\r
works much better than backpropagation with weights initialized with random values.\r
The better performance of networks trained with weights initialized this way could be\r
due to many factors, but one idea is that this method places the network in a region of\r
weight space from which a gradient-based algorithm can make good progress.\r
Batch normalization (Io↵e and Szegedy, 2015) is another technique that makes it easier\r
to train deep ANNs. It has long been known that ANN learning is easier if the network\r
input is normalized, for example, by adjusting each input variable to have zero mean and\r
unit variance. Batch normalization for training deep ANNs normalizes the output of deep\r
layers before they feed into the following layer. Io↵e and Szegedy (2015) used statistics\r
from subsets, or “mini-batches,” of training examples to normalize these between-layer\r
signals to improve the learning rate of deep ANNs."""

[[sections]]
number = "9.7"
title = "Nonlinear Function Approximation: Artificial Neural Networks 227"
text = """
Another technique useful for training deep ANNs is deep residual learning (He, Zhang,\r
Ren, and Sun, 2016). Sometimes it is easier to learn how a function di↵ers from the\r
identity function than to learn the function itself. Then adding this di↵erence, or residual\r
function, to the input produces the desired function. In deep ANNs, a block of layers\r
can be made to learn a residual function simply by adding shortcut, or skip, connections\r
around the block. These connections add the input to the block to its output, and\r
no additional weights are needed. He et al. (2016) evaluated this method using deep\r
convolutional networks with skip connections around every pair of adjacent layers, finding\r
substantial improvement over networks without the skip connections on benchmark image\r
classification tasks. Both batch normalization and deep residual learning were used in\r
the reinforcement learning application to the game of Go that we describe in Chapter 16.\r
A type of deep ANN that has proven to be very successful in applications, including\r
impressive reinforcement learning applications (Chapter 16), is the deep convolutional\r
network. This type of network is specialized for processing high-dimensional data arranged\r
in spatial arrays, such as images. It was inspired by how early visual processing works in\r
the brain (LeCun, Bottou, Bengio and Ha↵ner, 1998). Because of its special architecture,\r
a deep convolutional network can be trained by backpropagation without resorting to\r
methods like those described above to train the deep layers.\r
Figure 9.15 illustrates the architecture of a deep convolutional network. This instance,\r
from LeCun et al. (1998), was designed to recognize hand-written characters. It consists\r
of alternating convolutional and subsampling layers, followed by several fully connected\r
final layers. Each convolutional layer produces a number of feature maps. A feature\r
map is a pattern of activity over an array of units, where each unit performs the same\r
operation on data in its receptive field, which is the part of the data it “sees” from the\r
preceding layer (or from the external input in the case of the first convolutional layer).\r
The units of a feature map are identical to one another except that their receptive fields,\r
which are all the same size and shape, are shifted to di↵erent locations on the arrays\r
of incoming data. Units in the same feature map share the same weights. This means\r
that a feature map detects the same feature no matter where it is located in the input\r
Figure 9.15: Deep Convolutional Network. Republished with permission of Proceedings of the\r
IEEE, from Gradient-based learning applied to document recognition, LeCun, Bottou, Bengio,\r
and Ha↵ner, volume 86, 1998; permission conveyed through Copyright Clearance Center, Inc."""

[[sections]]
number = "228"
title = "Chapter 9: On-policy Prediction with Approximation"
text = """
array. In the network in Figure 9.15, for example, the first convolutional layer produces\r
6 feature maps, each consisting of 28 ⇥ 28 units. Each unit in each feature map has a\r
5 ⇥ 5 receptive field, and these receptive fields overlap (in this case by four columns and\r
four rows). Consequently, each of the 6 feature maps is specified by just 25 adjustable\r
weights.\r
The subsampling layers of a deep convolutional network reduce the spatial resolution of\r
the feature maps. Each feature map in a subsampling layer consists of units that average\r
over a receptive field of units in the feature maps of the preceding convolutional layer.\r
For example, each unit in each of the 6 feature maps in the first subsampling layer of the\r
network of Figure 9.15 averages over a 2 ⇥ 2 non-overlapping receptive field over one of\r
the feature maps produced by the first convolutional layer, resulting in six 14 ⇥ 14 feature\r
maps. Subsampling layers reduce the network’s sensitivity to the spatial locations of the\r
features detected, that is, they help make the network’s responses spatially invariant.\r
This is useful because a feature detected at one place in an image is likely to be useful at\r
other places as well.\r
Advances in the design and training of ANNs—of which we have only mentioned a\r
few—all contribute to reinforcement learning. Although current reinforcement learning\r
theory is mostly limited to methods using tabular or linear function approximation\r
methods, the impressive performances of notable reinforcement learning applications owe\r
much of their success to nonlinear function approximation by multi-layer ANNs. We\r
discuss several of these applications in Chapter 16."""

[[sections]]
number = "9.8"
title = "Least-Squares TD"
text = """
All the methods we have discussed so far in this chapter have required computation per\r
time step proportional to the number of parameters. With more computation, however,\r
one can do better. In this section we present a method for linear function approximation\r
that is arguably the best that can be done for this case.\r
As we established in Section 9.4 TD(0) with linear function approximation converges\r
asymptotically (for appropriately decreasing step sizes) to the TD fixed point:\r
wTD = A1b,\r
where\r
A .= E\r
⇥\r
xt(xt  xt+1)\r
>⇤ and b .\r
= E[Rt+1xt] .\r
Why, one might ask, must we compute this solution iteratively? This is wasteful of data!\r
Could one not do better by computing estimates of A and b, and then directly computing\r
the TD fixed point? The Least-Squares TD algorithm, commonly known as LSTD, does\r
exactly this. It forms the natural estimates\r
Ab t\r
.\r
= Xt1\r
k=0\r
xk(xk  xk+1)\r
> + "I and bbt\r
.\r
= Xt1\r
k=0\r
Rk+1xk, (9.20)"""

[[sections]]
number = "9.8"
title = "Least-Squares TD 229"
text = """
where I is the identity matrix, and "I, for some small " > 0, ensures that Ab t is always\r
invertible. It might seem that these estimates should both be divided by t, and indeed\r
they should; as defined here, these are really estimates of t times A and t times b.\r
However, the extra t factors cancel out when LSTD uses these estimates to estimate the\r
TD fixed point as\r
wt\r
.\r
= Ab 1\r
t bbt. (9.21)\r
This algorithm is the most data ecient form of linear TD(0), but it is also more\r
expensive computationally. Recall that semi-gradient TD(0) requires memory and per\u0002step computation that is only O(d).\r
How complex is LSTD? As it is written above the complexity seems to increase with\r
t, but the two approximations in (9.20) could be implemented incrementally using the\r
techniques we have covered earlier (e.g., in Chapter 2) so that they can be done in\r
constant time per step. Even so, the update for Ab t would involve an outer product (a\r
column vector times a row vector) and thus would be a matrix update; its computational\r
complexity would be O(d2), and of course the memory required to hold the Ab t matrix\r
would be O(d2).\r
A potentially greater problem is that our final computation (9.21) uses the inverse\r
of Ab t, and the computational complexity of a general inverse computation is O(d3).\r
Fortunately, an inverse of a matrix of our special form—a sum of outer products—can\r
also be updated incrementally with only O(d2) computations, as\r
Ab 1\r
t =\r
⇣\r
Ab t1 + xt1(xt1  xt)\r
>\r
⌘1\r
(from (9.20))\r
= Ab 1\r
t1  Ab 1\r
t1xt1(xt1  xt)>Ab 1t1\r
1+(xt1  xt)>Ab 1\r
t1xt1\r
, (9.22)\r
for t > 0, with Ab 0\r
.\r
= "I. Although the identity (9.22), known as the Sherman-Morrison\r
formula, is superficially complicated, it involves only vector-matrix and vector-vector\r
multiplications and thus is only O(d2). Thus we can store the inverse matrix Ab 1\r
t ,\r
maintain it with (9.22), and then use it in (9.21), all with only O(d2) memory and\r
per-step computation. The complete algorithm is given in the box on the next page.\r
Of course, O(d2) is still significantly more expensive than the O(d) of semi-gradient\r
TD. Whether the greater data eciency of LSTD is worth this computational expense\r
depends on how large d is, how important it is to learn quickly, and the expense of other\r
parts of the system. The fact that LSTD requires no step-size parameter is sometimes\r
also touted, but the advantage of this is probably overstated. LSTD does not require a\r
step size, but it does requires "; if " is chosen too small the sequence of inverses can vary\r
wildly, and if " is chosen too large then learning is slowed. In addition, LSTD’s lack of a\r
step-size parameter means that it never forgets. This is sometimes desirable, but it is\r
problematic if the target policy ⇡ changes as it does in reinforcement learning and GPI.\r
In control applications, LSTD typically has to be combined with some other mechanism\r
to induce forgetting, mooting any initial advantage of not requiring a step-size parameter."""

[[sections]]
number = "230"
title = "Chapter 9: On-policy Prediction with Approximation"
text = """
LSTD for estimating vˆ = w>x(·) ⇡ v⇡ (O(d2) version)\r
Input: feature representation x : S+ ! Rd such that x(terminal) = 0\r
Algorithm parameter: small " > 0\r
A\r
d1 "1I A d ⇥ d matrix\r
bb 0 A d-dimensional vector\r
Loop for each episode:\r
Initialize S; x x(S)\r
Loop for each step of episode:\r
Choose and take action A ⇠ ⇡(·|S), observe R, S0; x0 x(S0)\r
v A\r
d1\r
>\r
(x  x0)\r
A\r
d1 Ad1  \r
A\r
d1x\r
\r
v>/\r
\r
1 + v>x\r
\r
bb bb + Rx\r
w A\r
d1bb\r
S S0; x x0\r
until S0 is terminal"""

[[sections]]
number = "9.9"
title = "Memory-based Function Approximation"
text = """
So far we have discussed the parametric approach to approximating value functions. In\r
this approach, a learning algorithm adjusts the parameters of a functional form intended\r
to approximate the value function over a problem’s entire state space. Each update,\r
s 7! g, is a training example used by the learning algorithm to change the parameters\r
with the aim of reducing the approximation error. After the update, the training example\r
can be discarded (although it might be saved to be used again). When an approximate\r
value of a state (which we will call the query state) is needed, the function is simply\r
evaluated at that state using the latest parameters produced by the learning algorithm.\r
Memory-based function approximation methods are very di↵erent. They simply save\r
training examples in memory as they arrive (or at least save a subset of the examples)\r
without updating any parameters. Then, whenever a query state’s value estimate is\r
needed, a set of examples is retrieved from memory and used to compute a value estimate\r
for the query state. This approach is sometimes called lazy learning because processing\r
training examples is postponed until the system is queried to provide an output.\r
Memory-based function approximation methods are prime examples of nonparametric\r
methods. Unlike parametric methods, the approximating function’s form is not limited\r
to a fixed parameterized class of functions, such as linear functions or polynomials, but is\r
instead determined by the training examples themselves, together with some means for\r
combining them to output estimated values for query states. As more training examples\r
accumulate in memory, one expects nonparametric methods to produce increasingly\r
accurate approximations of any target function."""

[[sections]]
number = "9.9"
title = "Memory-based Function Approximation 231"
text = """
There are many di↵erent memory-based methods depending on how the stored training\r
examples are selected and how they are used to respond to a query. Here, we focus on\r
local-learning methods that approximate a value function only locally in the neighborhood\r
of the current query state. These methods retrieve a set of training examples from memory\r
whose states are judged to be the most relevant to the query state, where relevance\r
usually depends on the distance between states: the closer a training example’s state is\r
to the query state, the more relevant it is considered to be, where distance can be defined\r
in many di↵erent ways. After the query state is given a value, the local approximation is\r
discarded.\r
The simplest example of the memory-based approach is the nearest neighbor method,\r
which simply finds the example in memory whose state is closest to the query state and\r
returns that example’s value as the approximate value of the query state. In other words,\r
if the query state is s, and s0 7! g is the example in memory in which s0 is the closest\r
state to s, then g is returned as the approximate value of s. Slightly more complicated\r
are weighted average methods that retrieve a set of nearest neighbor examples and return\r
a weighted average of their target values, where the weights generally decrease with\r
increasing distance between their states and the query state. Locally weighted regression is\r
similar, but it fits a surface to the values of a set of nearest states by means of a parametric\r
approximation method that minimizes a weighted error measure like (9.1), where the\r
weights depend on distances from the query state. The value returned is the evaluation of\r
the locally-fitted surface at the query state, after which the local approximation surface\r
is discarded.\r
Being nonparametric, memory-based methods have the advantage over parametric\r
methods of not limiting approximations to pre-specified functional forms. This allows\r
accuracy to improve as more data accumulates. Memory-based local approximation\r
methods have other properties that make them well suited for reinforcement learning.\r
Because trajectory sampling is of such importance in reinforcement learning, as discussed\r
in Section 8.6, memory-based local methods can focus function approximation on local\r
neighborhoods of states (or state–action pairs) visited in real or simulated trajectories.\r
There may be no need for global approximation because many areas of the state space will\r
never (or almost never) be reached. In addition, memory-based methods allow an agent’s\r
experience to have a relatively immediate a↵ect on value estimates in the neighborhood\r
of the current state, in contrast with a parametric method’s need to incrementally adjust\r
parameters of a global approximation.\r
Avoiding global approximation is also a way to address the curse of dimensionality.\r
For example, for a state space with k dimensions, a tabular method storing a global\r
approximation requires memory exponential in k. On the other hand, in storing examples\r
for a memory-based method, each example requires memory proportional to k, and the\r
memory required to store, say, n examples is linear in n. Nothing is exponential in k or\r
n. Of course, the critical remaining issue is whether a memory-based method can answer\r
queries quickly enough to be useful to an agent. A related concern is how speed degrades\r
as the size of the memory grows. Finding nearest neighbors in a large database can take\r
too long to be practical in many applications."""

[[sections]]
number = "232"
title = "Chapter 9: On-policy Prediction with Approximation"
text = """
Proponents of memory-based methods have developed ways to accelerate the nearest\r
neighbor search. Using parallel computers or special purpose hardware is one approach;\r
another is the use of special multi-dimensional data structures to store the training data.\r
One data structure studied for this application is the k-d tree (short for k-dimensional\r
tree), which recursively splits a k-dimensional space into regions arranged as nodes of a\r
binary tree. Depending on the amount of data and how it is distributed over the state\r
space, nearest-neighbor search using k-d trees can quickly eliminate large regions of the\r
space in the search for neighbors, making the searches feasible in some problems where\r
naive searches would take too long.\r
Locally weighted regression additionally requires fast ways to do the local regression\r
computations which have to be repeated to answer each query. Researchers have developed\r
many ways to address these problems, including methods for forgetting entries in order to\r
keep the size of the database within bounds. The Bibliographic and Historical Comments\r
section at the end of this chapter points to some of the relevant literature, including a\r
selection of papers describing applications of memory-based learning to reinforcement\r
learning."""

[[sections]]
number = "9.10"
title = "Kernel-based Function Approximation"
text = """
Memory-based methods such as the weighted average and locally weighted regression\r
methods described above depend on assigning weights to examples s0 7! g in the database\r
depending on the distance between s0 and a query states s. The function that assigns\r
these weights is called a kernel function, or simply a kernel. In the weighted average and\r
locally weighted regressions methods, for example, a kernel function k : R ! R assigns\r
weights to distances between states. More generally, weights do not have to depend on\r
distances; they can depend on some other measure of similarity between states. In this\r
case, k : S ⇥ S ! R, so that k(s, s0) is the weight given to data about s0 in its influence\r
on answering queries about s.\r
Viewed slightly di↵erently, k(s, s0) is a measure of the strength of generalization from\r
s0 to s. Kernel functions numerically express how relevant knowledge about any state\r
is to any other state. As an example, the strengths of generalization for tile coding\r
shown in Figure 9.11 correspond to di↵erent kernel functions resulting from uniform and\r
asymmetrical tile o↵sets. Although tile coding does not explicitly use a kernel function\r
in its operation, it generalizes according to one. In fact, as we discuss more below, the\r
strength of generalization resulting from linear parametric function approximation can\r
always be described by a kernel function.\r
Kernel regression is the memory-based method that computes a kernel weighted average\r
of the targets of all examples stored in memory, assigning the result to the query state.\r
If D is the set of stored examples, and g(s0) denotes the target for state s0 in a stored\r
example, then kernel regression approximates the target function, in this case a value\r
function depending on D, as\r
vˆ(s,D) = X\r
s02D\r
k(s, s0)g(s0). (9.23)"""

[[sections]]
number = "9.10"
title = "Kernel-based Function Approximation 233"
text = """
The weighted average method described above is a special case in which k(s, s0) is non-zero\r
only when s and s0 are close to one another so that the sum need not be computed over\r
all of D.\r
A common kernel is the Gaussian radial basis function (RBF) used in RBF function\r
approximation as described in Section 9.5.5. In the method described there, RBFs are\r
features whose centers and widths are either fixed from the start, with centers presumably\r
concentrated in areas where many examples are expected to fall, or are adjusted in some\r
way during learning. Barring methods that adjust centers and widths, this is a linear\r
parametric method whose parameters are the weights of each RBF, which are typically\r
learned by stochastic gradient, or semi-gradient, descent. The form of the approximation\r
is a linear combination of the pre-determined RBFs. Kernel regression with an RBF\r
kernel di↵ers from this in two ways. First, it is memory-based: the RBFs are centered on\r
the states of the stored examples. Second, it is nonparametric: there are no parameters\r
to learn; the response to a query is given by (9.23).\r
Of course, many issues have to be addressed for practical implementation of kernel\r
regression, issues that are beyond the scope of our brief discussion. However, it turns out\r
that any linear parametric regression method like those we described in Section 9.4, with\r
states represented by feature vectors x(s)=(x1(s), x2(s),...,xd(s))>, can be recast as\r
kernel regression where k(s, s0) is the inner product of the feature vector representations\r
of s and s0; that is\r
k(s, s0) = x(s)\r
>x(s0\r
). (9.24)\r
Kernel regression with this kernel function produces the same approximation that a linear\r
parametric method would if it used these feature vectors and learned with the same\r
training data.\r
We skip the mathematical justification for this, which can be found in any modern\r
machine learning text, such as Bishop (2006), and simply point out an important\r
implication. Instead of constructing features for linear parametric function approximators,\r
one can instead construct kernel functions directly without referring at all to feature\r
vectors. Not all kernel functions can be expressed as inner products of feature vectors\r
as in (9.24), but a kernel function that can be expressed like this can o↵er significant\r
advantages over the equivalent parametric method. For many sets of feature vectors,\r
(9.24) has a compact functional form that can be evaluated without any computation\r
taking place in the d-dimensional feature space. In these cases, kernel regression is much\r
less complex than directly using a linear parametric method with states represented by\r
these feature vectors. This is the so-called “kernel trick” that allows e↵ectively working\r
in the high-dimension of an expansive feature space while actually working only with the\r
set of stored training examples. The kernel trick is the basis of many machine learning\r
methods, and researchers have shown how it can sometimes benefit reinforcement learning."""

[[sections]]
number = "234"
title = "Chapter 9: On-policy Prediction with Approximation"
text = ""

[[sections]]
number = "9.11"
title = "Looking Deeper at On-policy Learning:"
text = """
Interest and Emphasis\r
The algorithms we have considered so far in this chapter have treated all the states\r
encountered equally, as if they were all equally important. In some cases, however, we\r
are more interested in some states than others. In discounted episodic problems, for\r
example, we may be more interested in accurately valuing early states in the episode\r
than in later states where discounting may have made the rewards much less important\r
to the value of the start state. Or, if an action-value function is being learned, it may be\r
less important to accurately value poor actions whose value is much less than the greedy\r
action. Function approximation resources are always limited, and if they were used in a\r
more targeted way, then performance could be improved.\r
One reason we have treated all states encountered equally is that then we are updating\r
according to the on-policy distribution, for which stronger theoretical results are available\r
for semi-gradient methods. Recall that the on-policy distribution was defined as the\r
distribution of states encountered in an MDP while following the target policy. Now we\r
will generalize this concept significantly. Rather than having one on-policy distribution\r
for the MDP, we will have many. All of them will have in common that they are a\r
distribution of states encountered in trajectories while following the target policy, but\r
they will vary in how the trajectories are, in a sense, initiated.\r
We now introduce some new concepts. First we introduce a non-negative scalar measure,\r
a random variable It called interest, indicating the degree to which we are interested in\r
accurately valuing the state (or state–action pair) at time t. If we don’t care at all about\r
the state, then the interest should be zero; if we fully care, it might be one, though it is\r
formally allowed to take any non-negative value. The interest can be set in any causal\r
way; for example, it may depend on the trajectory up to time t or the learned parameters\r
at time t. The distribution µ in the VE (9.1) is then defined as the distribution of\r
states encountered while following the target policy, weighted by the interest. Second, we\r
introduce another non-negative scalar random variable, the emphasis Mt. This scalar\r
multiplies the learning update and thus emphasizes or de-emphasizes the learning done\r
at time t. The general n-step learning rule, replacing (9.15), is\r
wt+n\r
.\r
= wt+n1 +↵Mt [Gt:t+n  vˆ(St,wt+n1)] rvˆ(St,wt+n1), 0  t < T, (9.25)\r
with the n-step return given by (9.16) and the emphasis determined recursively from the\r
interest by:\r
Mt = It + nMtn, 0  t < T, (9.26)\r
with Mt\r
.\r
= 0, for all t < 0. These equations are taken to include the Monte Carlo case,\r
for which Gt:t+n = Gt, all the updates are made at end of the episode, n = T  t, and\r
Mt = It."""

[[sections]]
number = "9.11"
title = "Looking Deeper at On-policy Learning: Interest and Emphasis 235"
text = """
Example 9.4 illustrates how interest and emphasis can result in more accurate value\r
estimates.\r
Example 9.4: Interest and Emphasis\r
To see the potential benefits of using interest and emphasis, consider the four-state\r
Markov reward process shown below:\r
+1 +1 +1 +1\r
v⇡ = 4 v⇡ = 3 v⇡ = 2 v⇡ = 1\r
i = 1 i = 0 i = 0 i = 0\r
w1 w1 w2 w2\r
Episodes start in the leftmost state, then transition one state to the right, with a\r
reward of +1, on each step until the terminal state is reached. The true value of\r
the first state is thus 4, of the second state 3, and so on as shown below each state.\r
These are the true values; the estimated values can only approximate these because\r
they are constrained by the parameterization. There are two components to the\r
parameter vector w = (w1, w2)>, and the parameterization is as written inside\r
each state. The estimated values of the first two states are given by w1 alone and\r
thus must be the same even though their true values are di↵erent. Similarly, the\r
estimated values of the third and fourth states are given by w2 alone and must be\r
the same even though their true values are di↵erent. Suppose that we are interested\r
in accurately valuing only the leftmost state; we assign it an interest of 1 while all\r
the other states are assigned an interest of 0, as indicated above the states.\r
First consider applying gradient Monte Carlo algorithms to this problem. The\r
algorithms presented earlier in this chapter that do not take into account interest\r
and emphasis (in (9.7) and the box on page 202) will converge (for decreasing step\r
sizes) to the parameter vector w1 = (3.5, 1.5), which gives the first state—the only\r
one we are interested in—a value of 3.5 (i.e., intermediate between the true values\r
of the first and second states). The methods presented in this section that do use\r
interest and emphasis, on the other hand, will learn the value of the first state\r
exactly correctly; w1 will converge to 4 while w2 will never be updated because the\r
emphasis is zero in all states save the leftmost.\r
Now consider applying two-step semi-gradient TD methods. The methods from\r
earlier in this chapter without interest and emphasis (in (9.15) and (9.16) and\r
the box on page 209) will again converge to w1 = (3.5, 1.5), while the methods\r
with interest and emphasis converge to w1 = (4, 2). The latter produces the\r
exactly correct values for the first state and for the third state (which the first state\r
bootstraps from) while never making any updates corresponding to the second or\r
fourth states."""

[[sections]]
number = "236"
title = "Chapter 9: On-policy Prediction with Approximation"
text = ""

[[sections]]
number = "9.12"
title = "Summary"
text = """
Reinforcement learning systems must be capable of generalization if they are to be\r
applicable to artificial intelligence or to large engineering applications. To achieve this,\r
any of a broad range of existing methods for supervised-learning function approximation\r
can be used simply by treating each update as a training example.\r
Perhaps the most suitable supervised learning methods are those using parameterized\r
function approximation, in which the policy is parameterized by a weight vector w.\r
Although the weight vector has many components, the state space is much larger still,\r
and we must settle for an approximate solution. We defined the mean square value error,\r
VE(w), as a measure of the error in the values v⇡w (s) for a weight vector w under the\r
on-policy distribution, µ. The VE gives us a clear way to rank di↵erent value-function\r
approximations in the on-policy case.\r
To find a good weight vector, the most popular methods are variations of stochastic\r
gradient descent (SGD). In this chapter we have focused on the on-policy case with a fixed\r
policy, also known as policy evaluation or prediction; a natural learning algorithm for this\r
case is n-step semi-gradient TD, which includes gradient Monte Carlo and semi-gradient\r
TD(0) algorithms as the special cases when n=1 and n= 1 respectively. Semi-gradient\r
TD methods are not true gradient methods. In such bootstrapping methods (including\r
DP), the weight vector appears in the update target, yet this is not taken into account in\r
computing the gradient—thus they are semi-gradient methods. As such, they cannot\r
rely on classical SGD results.\r
Nevertheless, good results can be obtained for semi-gradient methods in the special case\r
of linear function approximation, in which the value estimates are sums of features times\r
corresponding weights. The linear case is the most well understood theoretically and\r
works well in practice when provided with appropriate features. Choosing the features\r
is one of the most important ways of adding prior domain knowledge to reinforcement\r
learning systems. They can be chosen as polynomials, but this case generalizes poorly in\r
the online learning setting typically considered in reinforcement learning. Better is to\r
choose features according the Fourier basis, or according to some form of coarse coding\r
with sparse overlapping receptive fields. Tile coding is a form of coarse coding that\r
is particularly computationally ecient and flexible. Radial basis functions are useful\r
for one- or two-dimensional tasks in which a smoothly varying response is important.\r
LSTD is the most data-ecient linear TD prediction method, but requires computation\r
proportional to the square of the number of weights, whereas all the other methods are of\r
complexity linear in the number of weights. Nonlinear methods include artificial neural\r
networks trained by backpropagation and variations of SGD; these methods have become\r
very popular in recent years under the name deep reinforcement learning.\r
Linear semi-gradient n-step TD is guaranteed to converge under standard conditions,\r
for all n, to a VE that is within a bound of the optimal error (achieved asymptotically\r
by Monte Carlo methods). This bound is always tighter for higher n and approaches\r
zero as n ! 1. However, in practice very high n results in very slow learning, and some\r
degree of bootstrapping (n < 1) is usually preferable, just as we saw in comparisons of\r
tabular n-step methods in Chapter 7 and in comparisons of tabular TD and Monte Carlo\r
methods in Chapter 6."""

[[sections]]
number = "9.12"
title = "Summary 237"
text = """
Exercise 9.7 One of the simplest artificial neural networks consists of a single semi-linear\r
unit with a logistic nonlinearity. The need to handle approximate value functions of this\r
form is common in games that end with either a win or a loss, in which case the value of\r
a state can be interpreted as the probability of winning. Derive the learning algorithm\r
for this case, from (9.7), such that no gradient notation appears.\r
⇤\r
Exercise 9.8 Arguably, the squared error used to derive (9.7) is inappropriate for the\r
case treated in the preceding exercise, and the right error measure is the cross-entropy\r
loss (which you can find on Wikipedia). Repeat the derivation in Section 9.3, using the\r
cross-entropy loss instead of the squared error in (9.4), all the way to an explicit form\r
with no gradient or logarithm notation in it. Is your final form more complex, or simpler,\r
than that you obtained in the preceding exercise?\r
Bibliographical and Historical Remarks\r
Generalization and function approximation have always been an integral part of rein\u0002forcement learning. Bertsekas and Tsitsiklis (1996), Bertsekas (2012), and Sugiyama et\r
al. (2013) present the state of the art in function approximation in reinforcement learning.\r
Some of the early work with function approximation in reinforcement learning is discussed\r
at the end of this section."""

[[sections]]
number = "9.3"
title = "Gradient-descent methods for minimizing mean square error in supervised learning"
text = """
are well known. Widrow and Ho↵ (1960) introduced the least-mean-square (LMS)\r
algorithm, which is the prototypical incremental gradient-descent algorithm.\r
Details of this and related algorithms are provided in many texts (e.g., Widrow\r
and Stearns, 1985; Bishop, 1995; Duda and Hart, 1973).\r
Semi-gradient TD(0) was first explored by Sutton (1984, 1988), as part of the\r
linear TD() algorithm that we will treat in Chapter 12. The term “semi-gradient”\r
to describe these bootstrapping methods is new to the second edition of this\r
book.\r
The earliest use of state aggregation in reinforcement learning may have been\r
Michie and Chambers’s BOXES system (1968). The theory of state aggregation\r
in reinforcement learning has been developed by Singh, Jaakkola, and Jordan\r
(1995) and Tsitsiklis and Van Roy (1996). State aggregation has been used in\r
dynamic programming from its earliest days (e.g., Bellman, 1957a)."""

[[sections]]
number = "9.4"
title = "Sutton (1988) proved convergence of linear TD(0) in the mean to the minimal"
text = """
VE solution for the case in which the feature vectors, {x(s) : s 2 S}, are linearly\r
independent. Convergence with probability 1 was proved by several researchers\r
at about the same time (Peng, 1993; Dayan and Sejnowski, 1994; Tsitsiklis,\r
1994; Gurvits, Lin, and Hanson, 1994). In addition, Jaakkola, Jordan, and Singh\r
(1994) proved convergence under online updating. All of these results assumed\r
linearly independent feature vectors, which implies at least as many components\r
to wt as there are states. Convergence for the more important case of general\r
(dependent) feature vectors was first shown by Dayan (1992). A significant"""

[[sections]]
number = "238"
title = "Chapter 9: On-policy Prediction with Approximation"
text = """
generalization and strengthening of Dayan’s result was proved by Tsitsiklis and\r
Van Roy (1997). They proved the main result presented in this section, the\r
bound on the asymptotic error of linear bootstrapping methods.\r
9.5 Our presentation of the range of possibilities for linear function approximation is\r
based on that by Barto (1990)."""

[[sections]]
number = "9.5.2"
title = "Konidaris, Osentoski, and Thomas (2011) introduced the Fourier basis in a"
text = """
simple form suitable for reinforcement learning problems with multi-dimensional\r
continuous state spaces and functions that do not have to be periodic."""

[[sections]]
number = "9.5.3"
title = "The term coarse coding is due to Hinton (1984), and our Figure 9.6 is based on"
text = """
one of his figures. Waltz and Fu (1965) provide an early example of this type of\r
function approximation in a reinforcement learning system.\r
9.5.4 Tile coding, including hashing, was introduced by Albus (1971, 1981). He de\u0002scribed it in terms of his “cerebellar model articulator controller,” or CMAC, as\r
tile coding is sometimes known in the literature. The term “tile coding” was new\r
to the first edition of this book, though the idea of describing CMAC in these\r
terms is taken from Watkins (1989). Tile coding has been used in many rein\u0002forcement learning systems (e.g., Shewchuk and Dean, 1990; Lin and Kim, 1991;\r
Miller, Scalera, and Kim, 1994; Sofge and White, 1992; Tham, 1994; Sutton, 1996;\r
Watkins, 1989) as well as in other types of learning control systems (e.g., Kraft and\r
Campagna, 1990; Kraft, Miller, and Dietz, 1992). This section draws heavily on\r
the work of Miller and Glanz (1996). General software for tile coding is available in\r
several languages (e.g., see http://incompleteideas.net/tiles/tiles3.html)."""

[[sections]]
number = "9.5.5"
title = "Function approximation using radial basis functions has received wide attention"
text = """
ever since being related to ANNs by Broomhead and Lowe (1988). Powell (1987)\r
reviewed earlier uses of RBFs, and Poggio and Girosi (1989, 1990) extensively\r
developed and applied this approach.\r
9.6 Automatic methods for adapting the step-size parameter include RMSprop (Tiele\u0002man and Hinton, 2012), Adam (Kingma and Ba, 2015), stochastic meta-descent\r
methods such as Delta-Bar-Delta (Jacobs, 1988), its incremental generaliza\u0002tion (Sutton, 1992b, c; Mahmood et al., 2012), and nonlinear generalizations\r
(Schraudolph, 1999, 2002). Methods explicitly designed for reinforcement learn\u0002ing include AlphaBound (Dabney and Barto, 2012), SID and NOSID (Dabney,\r
2014), TIDBD (Kearney et al., in preparation) and the application of stochastic\r
meta-descent to policy gradient learning (Schraudolph, Yu, and Aberdeen, 2006)."""

[[sections]]
number = "9.7"
title = "The introduction of the threshold logic unit as an abstract model neuron by"
text = """
McCulloch and Pitts (1943) was the beginning of ANNs. The history of ANNs as\r
learning methods for classification or regression has passed through several stages:\r
roughly, the Perceptron (Rosenblatt, 1962) and ADALINE (ADAptive LINear\r
Element) (Widrow and Ho↵, 1960) stage of learning by single-layer ANNs, the"""

[[sections]]
number = "9.12"
title = "Summary 239"
text = """
error-backpropagation stage (LeCun, 1985; Rumelhart, Hinton, and Williams,\r
1986) of learning by multi-layer ANNs, and the current deep-learning stage with\r
its emphasis on representation learning (e.g., Bengio, Courville, and Vincent,\r
2012; Goodfellow, Bengio, and Courville, 2016). Examples of the many books on\r
ANNs are Haykin (1994), Bishop (1995), and Ripley (2007).\r
ANNs as function approximation for reinforcement learning goes back to the early\r
work of Farley and Clark (1954), who used reinforcement-like learning to modify\r
the weights of linear threshold functions representing policies. Widrow, Gupta,\r
and Maitra (1973) presented a neuron-like linear threshold unit implementing a\r
learning process they called learning with a critic or selective bootstrap adaptation,\r
a reinforcement-learning variant of the ADALINE algorithm. Werbos (1987,\r
1994) developed an approach to prediction and control that uses ANNs trained by\r
error backpropation to learn policies and value functions using TD-like algorithms.\r
Barto, Sutton, and Brouwer (1981) and Barto and Sutton (1981b) extended the\r
idea of an associative memory network (e.g., Kohonen, 1977; Anderson, Silverstein,\r
Ritz, and Jones, 1977) to reinforcement learning. Barto, Anderson, and Sutton\r
(1982) used a two-layer ANN to learn a nonlinear control policy, and emphasized\r
the first layer’s role of learning a suitable representation. Hampson (1983, 1989)\r
was an early proponent of multilayer ANNs for learning value functions. Barto,\r
Sutton, and Anderson (1983) presented an actor–critic algorithm in the form of an\r
ANN learning to balance a simulated pole (see Sections 15.7 and 15.8). Barto and\r
Anandan (1985) introduced a stochastic version of Widrow et al.’s (1973) selective\r
bootstrap algorithm called the associative reward-penalty (ARP ) algorithm.\r
Barto (1985, 1986) and Barto and Jordan (1987) described multi-layer ANNs\r
consisting of ARP units trained with a globally-broadcast reinforcement signal\r
to learn classification rules that are not linearly separable. Barto (1985) discussed\r
this approach to ANNs and how this type of learning rule is related to others in\r
the literature at that time. (See Section 15.10 for additional discussion of this\r
approach to training multi-layer ANNs.) Anderson (1986, 1987, 1989) evaluated\r
numerous methods for training multilayer ANNs and showed that an actor–critic\r
algorithm in which both the actor and critic were implemented by two-layer\r
ANNs trained by error backpropagation outperformed single-layer ANNs in the\r
pole-balancing and tower of Hanoi tasks. Williams (1988) described several ways\r
that backpropagation and reinforcement learning can be combined for training\r
ANNs. Gullapalli (1990) and Williams (1992) devised reinforcement learning\r
algorithms for neuron-like units having continuous, rather than binary, outputs.\r
Barto, Sutton, and Watkins (1990) argued that ANNs can play significant roles\r
for approximating functions required for solving sequential decision problems.\r
Williams (1992) related REINFORCE learning rules (Section 13.3) to the error\r
backpropagation method for training multi-layer ANNs. Tesauro’s TD-Gammon\r
(Tesauro 1992, 1994; Section 16.1) influentially demonstrated the learning abilities\r
of TD() algorithm with function approximation by multi-layer ANNs in learning\r
to play backgammon. The AlphaGo, AlphaGo Zero, and AlphaZero programs\r
of Silver et al. (2016, 2017a, b; Section 16.6) used reinforcement learning with"""

[[sections]]
number = "240"
title = "Chapter 9: On-policy Prediction with Approximation"
text = """
deep convolutional ANNs in achieving impressive results with the game of Go.\r
Schmidhuber (2015) reviews applications of ANNs in reinforcement learning,\r
including applications of recurrent ANNs."""

[[sections]]
number = "9.8"
title = "LSTD is due to Bradtke and Barto (see Bradtke, 1993, 1994; Bradtke and Barto,"
text = """
1996; Bradtke, Ydstie, and Barto, 1994), and was further developed by Boyan\r
(1999, 2002), Nedi´c and Bertsekas (2003), and Yu (2010). The incremental update\r
of the inverse matrix has been known at least since 1949 (Sherman and Morrison,\r
1949). An extension of least-squares methods to control was introduced by\r
Lagoudakis and Parr (2003; Bu¸soniu, Lazaric, Ghavamzadeh, Munos, Babu˘ska,\r
and De Schutter, 2012)."""

[[sections]]
number = "9.9"
title = "Our discussion of memory-based function approximation is largely based on"
text = """
the review of locally weighted learning by Atkeson, Moore, and Schaal (1997).\r
Atkeson (1992) discussed the use of locally weighted regression in memory-based\r
robot learning and supplied an extensive bibliography covering the history of\r
the idea. Stanfill and Waltz (1986) influentially argued for the importance of\r
memory based methods in artificial intelligence, especially in light of parallel\r
architectures then becoming available, such as the Connection Machine. Baird\r
and Klopf (1993) introduced a novel memory-based approach and used it as the\r
function approximation method for Q-learning applied to the pole-balancing task.\r
Schaal and Atkeson (1994) applied locally weighted regression to a robot juggling\r
control problem, where it was used to learn a system model. Peng (1995) used\r
the pole-balancing task to experiment with several nearest-neighbor methods\r
for approximating value functions, policies, and environment models. Tadepalli\r
and Ok (1996) obtained promising results with locally-weighted linear regression\r
to learn a value function for a simulated automatic guided vehicle task. Bottou\r
and Vapnik (1992) demonstrated surprising eciency of several local learning\r
algorithms compared to non-local algorithms in some pattern recognition tasks,\r
discussing the impact of local learning on generalization.\r
Bentley (1975) introduced k-d trees and reported observing average running\r
time of O(log n) for nearest neighbor search over n records. Friedman, Bentley,\r
and Finkel (1977) clarified the algorithm for nearest neighbor search with k-d\r
trees. Omohundro (1987) discussed eciency gains possible with hierarchical\r
data structures such as k-d-trees. Moore, Schneider, and Deng (1997) introduced\r
the use of k-d trees for ecient locally weighted regression.\r
9.10 The origin of kernel regression is the method of potential functions of Aizerman,\r
Braverman, and Rozonoer (1964). They likened the data to point electric charges\r
of various signs and magnitudes distributed over space. The resulting electric\r
potential over space produced by summing the potentials of the point charges\r
corresponded to the interpolated surface. In this analogy, the kernel function is\r
the potential of a point charge, which falls o↵ as the reciprocal of the distance\r
from the charge. Connell and Utgo↵ (1987) applied an actor–critic method\r
to the pole-balancing task in which the critic approximated the value function"""

[[sections]]
number = "9.12"
title = "Summary 241"
text = """
using kernel regression with an inverse-distance weighting. Predating widespread\r
interest in kernel regression in machine learning, these authors did not use the\r
term kernel, but referred to “Shepard’s method” (Shepard, 1968). Other kernel\u0002based approaches to reinforcement learning include those of Ormoneit and Sen\r
(2002), Dietterich and Wang (2002), Xu, Xie, Hu, and Lu (2005), Taylor and Parr\r
(2009), Barreto, Precup, and Pineau (2011), and Bhat, Farias, and Moallemi\r
(2012)."""

[[sections]]
number = "9.11"
title = "For Emphatic-TD methods, see the bibliographical notes to Section 11.8."
text = """
The earliest example we know of in which function approximation methods were\r
used for learning value functions was Samuel’s checkers player (1959, 1967). Samuel\r
followed Shannon’s (1950) suggestion that a value function did not have to be exact to\r
be a useful guide to selecting moves in a game and that it might be approximated by\r
a linear combination of features. In addition to linear function approximation, Samuel\r
experimented with lookup tables and hierarchical lookup tables called signature tables\r
(Grith, 1966, 1974; Page, 1977; Biermann, Fairfield, and Beres, 1982).\r
At about the same time as Samuel’s work, Bellman and Dreyfus (1959) proposed using\r
function approximation methods with DP. (It is tempting to think that Bellman and\r
Samuel had some influence on one another, but we know of no reference to the other in\r
the work of either.) There is now a fairly extensive literature on function approximation\r
methods and DP, such as multigrid methods and methods using splines and orthogonal\r
polynomials (e.g., Bellman and Dreyfus, 1959; Bellman, Kalaba, and Kotkin, 1963; Daniel,\r
1976; Whitt, 1978; Reetz, 1977; Schweitzer and Seidmann, 1985; Chow and Tsitsiklis,\r
1991; Kushner and Dupuis, 1992; Rust, 1996).\r
Holland’s (1986) classifier system used a selective feature-match technique to generalize\r
evaluation information across state–action pairs. Each classifier matched a subset of states\r
having specified values for a subset of features, with the remaining features having arbitrary\r
values (“wild cards”). These subsets were then used in a conventional state-aggregation\r
approach to function approximation. Holland’s idea was to use a genetic algorithm\r
to evolve a set of classifiers that collectively would implement a useful action-value\r
function. Holland’s ideas influenced the early research of the authors on reinforcement\r
learning, but we focused on di↵erent approaches to function approximation. As function\r
approximators, classifiers are limited in several ways. First, they are state-aggregation\r
methods, with concomitant limitations in scaling and in representing smooth functions\r
eciently. In addition, the matching rules of classifiers can implement only aggregation\r
boundaries that are parallel to the feature axes. Perhaps the most important limitation of\r
conventional classifier systems is that the classifiers are learned via the genetic algorithm,\r
an evolutionary method. As we discussed in Chapter 1, there is available during learning\r
much more detailed information about how to learn than can be used by evolutionary\r
methods. This perspective led us to instead adapt supervised learning methods for\r
use in reinforcement learning, specifically gradient-descent and ANN methods. These\r
di↵erences between Holland’s approach and ours are not surprising because Holland’s\r
ideas were developed during a period when ANNs were generally regarded as being too\r
weak in computational power to be useful, whereas our work was at the beginning of"""

[[sections]]
number = "242"
title = "Chapter 9: On-policy Prediction with Approximation"
text = """
the period that saw widespread questioning of that conventional wisdom. There remain\r
many opportunities for combining aspects of these di↵erent approaches.\r
Christensen and Korf (1986) experimented with regression methods for modifying\r
coecients of linear value function approximations in the game of chess. Chapman\r
and Kaelbling (1991) and Tan (1991) adapted decision-tree methods for learning value\r
functions. Explanation-based learning methods have also been adapted for learning\r
value functions, yielding compact representations (Yee, Saxena, Utgo↵, and Barto, 1990;\r
Dietterich and Flann, 1995).

Chapter 10\r
On-policy Control with\r
Approximation\r
In this chapter we return to the control problem, now with parametric approximation of\r
the action-value function qˆ(s, a, w) ⇡ q⇤(s, a), where w 2 Rd is a finite-dimensional weight\r
vector. We continue to restrict attention to the on-policy case, leaving o↵-policy methods\r
to Chapter 11. The present chapter features the semi-gradient Sarsa algorithm, the\r
natural extension of semi-gradient TD(0) (last chapter) to action values and to on-policy\r
control. In the episodic case, the extension is straightforward, but in the continuing case\r
we have to take a few steps backward and re-examine how we have used discounting to\r
define an optimal policy. Surprisingly, once we have genuine function approximation we\r
have to give up discounting and switch to a new “average-reward” formulation of the\r
control problem, with new “di↵erential” value functions.\r
Starting first in the episodic case, we extend the function approximation ideas presented\r
in the last chapter from state values to action values. Then we extend them to control\r
following the general pattern of on-policy GPI, using "-greedy for action selection. We\r
show results for n-step linear Sarsa on the Mountain Car problem. Then we turn to the\r
continuing case and repeat the development of these ideas for the average-reward case\r
with di↵erential values."""

[[sections]]
number = "10.1"
title = "Episodic Semi-gradient Control"
text = """
The extension of the semi-gradient prediction methods of Chapter 9 to action values is\r
straightforward. In this case it is the approximate action-value function, ˆq ⇡ q⇡, that is\r
represented as a parameterized functional form with weight vector w. Whereas before we\r
considered random training examples of the form St 7! Ut, now we consider examples\r
of the form St, At 7! Ut. The update target Ut can be any approximation of q⇡(St, At),\r
including the usual backed-up values such as the full Monte Carlo return (Gt) or any\r
of the n-step Sarsa returns (7.4). The general gradient-descent update for action-value"""

[[sections]]
number = "244"
title = "Chapter 10: On-policy Control with Approximation"
text = """
prediction is\r
wt+1\r
.\r
= wt + ↵\r
h\r
Ut  qˆ(St, At, wt)\r
i\r
rqˆ(St, At, wt). (10.1)\r
For example, the update for the one-step Sarsa method is\r
wt+1\r
.\r
= wt + ↵\r
h\r
Rt+1 + qˆ(St+1, At+1, wt)  qˆ(St, At, wt)\r
i\r
rqˆ(St, At, wt). (10.2)\r
We call this method episodic semi-gradient one-step Sarsa. For a constant policy, this\r
method converges in the same way that TD(0) does, with the same kind of error bound\r
(9.14).\r
To form control methods, we need to couple such action-value prediction methods with\r
techniques for policy improvement and action selection. Suitable techniques applicable to\r
continuous actions, or to actions from large discrete sets, are a topic of ongoing research\r
with as yet no clear resolution. On the other hand, if the action set is discrete and not too\r
large, then we can use the techniques already developed in previous chapters. That is, for\r
each possible action a available in the next state St+1, we can compute qˆ(St+1, a, wt) and\r
then find the greedy action A⇤\r
t+1 = argmaxa qˆ(St+1, a, wt). Policy improvement is then\r
done (in the on-policy case treated in this chapter) by changing the estimation policy to a\r
soft approximation of the greedy policy such as the "-greedy policy. Actions are selected\r
according to this same policy. Pseudocode for the complete algorithm is given in the box.\r
Episodic Semi-gradient Sarsa for Estimating qˆ ⇡ q⇤\r
Input: a di↵erentiable action-value function parameterization ˆq : S ⇥ A ⇥ Rd ! R\r
Algorithm parameters: step size ↵ > 0, small " > 0\r
Initialize value-function weights w 2 Rd arbitrarily (e.g., w = 0)\r
Loop for each episode:\r
S, A initial state and action of episode (e.g., "-greedy)\r
Loop for each step of episode:\r
Take action A, observe R, S0\r
If S0 is terminal:\r
w w + ↵\r
⇥\r
R  qˆ(S, A, w)\r
⇤\r
rqˆ(S, A, w)\r
Go to next episode\r
Choose A0 as a function of ˆq(S0, ·, w) (e.g., "-greedy)\r
w w + ↵\r
⇥\r
R + qˆ(S0, A0, w)  qˆ(S, A, w)\r
⇤\r
rqˆ(S, A, w)\r
S S0\r
A A0\r
Example 10.1: Mountain Car Task Consider the task of driving an underpowered\r
car up a steep mountain road, as suggested by the diagram in the upper left of Figure 10.1.\r
The diculty is that gravity is stronger than the car’s engine, and even at full throttle\r
the car cannot accelerate up the steep slope. The only solution is to first move away from\r
the goal and up the opposite slope on the left. Then, by applying full throttle the car"""

[[sections]]
number = "10.1"
title = "Episodic Semi-gradient Control 245"
text = """
!1.2\r
Position"""

[[sections]]
number = "0.6"
title = "Step 428"
text = """
Goal\r
Position\r
4\r
0\r
!.07 .07\r
VelocityVelocity\r
Velocity\r
Velocity\r
Velocity\r
Velocity\r
Position\r
Position\r
Position\r
0\r
2 7\r
0\r
120\r
0\r
104\r
0\r
4 6\r
Episode 12\r
Episode 104 Episode 1000 Episode 9000\r
MOUNTAIN CAR Goal\r
Figure 10.1: The Mountain Car task (upper left panel) and the cost-to-go function\r
( maxa qˆ(s, a, w)) learned during one run.\r
can build up enough inertia to carry it up the steep slope even though it is slowing down\r
the whole way. This is a simple example of a continuous control task where things have\r
to get worse in a sense (farther from the goal) before they can get better. Many control\r
methodologies have great diculties with tasks of this kind unless explicitly aided by a\r
human designer.\r
The reward in this problem is 1 on all time steps until the car moves past its goal\r
position at the top of the mountain, which ends the episode. There are three possible\r
actions: full throttle forward (+1), full throttle reverse (1), and zero throttle (0). The\r
car moves according to a simplified physics. Its position, xt, and velocity, x˙ t, are updated\r
by\r
xt+1\r
.\r
= bound⇥xt + ˙xt+1⇤\r
x˙ t+1\r
.\r
= bound⇥x˙ t + 0.001At  0.0025 cos(3xt)\r
⇤\r
,\r
where the bound operation enforces 1.2  xt+1  0.5 and 0.07  x˙ t+1  0.07. In\r
addition, when xt+1 reached the left bound, x˙ t+1 was reset to zero. When it reached\r
the right bound, the goal was reached and the episode was terminated. Each episode\r
started from a random position xt 2 [0.6, 0.4) and zero velocity. To convert the two\r
continuous state variables to binary features, we used grid-tilings as in Figure 9.9. We\r
used 8 tilings, with each tile covering 1/8th of the bounded distance in each dimension,"""

[[sections]]
number = "246"
title = "Chapter 10: On-policy Control with Approximation"
text = """
and asymmetrical o↵sets as described in Section 9.5.4.1 The feature vectors x(s, a) created\r
by tile coding were then combined linearly with the parameter vector to approximate the\r
action-value function:\r
qˆ(s, a, w) .= w>x(s, a) = X\r
d\r
i=1\r
wi · xi(s, a), (10.3)\r
for each pair of state, s, and action, a.\r
Figure 10.1 shows what typically happens while learning to solve this task with this\r
form of function approximation.2 Shown is the negative of the value function (the cost\u0002to-go function) learned on a single run. The initial action values were all zero, which was\r
optimistic (all true values are negative in this task), causing extensive exploration to occur\r
even though the exploration parameter, ", was 0. This can be seen in the middle-top panel\r
of the figure, labeled “Step 428”. At this time not even one episode had been completed,\r
but the car has oscillated back and forth in the valley, following circular trajectories in\r
state space. All the states visited frequently are valued worse than unexplored states,\r
because the actual rewards have been worse than what was (unrealistically) expected.\r
This continually drives the agent away from wherever it has been, to explore new states,\r
until a solution is found.\r
Figure 10.2 shows several learning curves for semi-gradient Sarsa on this problem, with\r
various step sizes.\r
100\r
200\r
400\r
1000"""

[[sections]]
number = "0"
title = "Mountain Car"
text = """
Steps per episode\r
log scale\r
averaged over 100 runs\r
Episode\r
500\r
↵= 0.5/8\r
↵= 0.1/8\r
↵= 0.2/8\r
Figure 10.2: Mountain Car learning curves for the semi-gradient Sarsa method with tile-coding\r
function approximation and "-greedy action selection.\r
1In particular, we used the tile-coding software, available at http://incompleteideas.net/tiles/\r
tiles3.html, with iht=IHT(4096) and tiles(iht,8,[8*x/(0.5+1.2),8*xdot/(0.07+0.07)],[A]) to get\r
the indices of the ones in the feature vector for state (x, xdot) and action A.\r
2This data is actually from the “semi-gradient Sarsa()” algorithm that we will not meet until\r
Chapter 12, but semi-gradient Sarsa would behave similarly."""

[[sections]]
number = "10.2"
title = "Semi-gradient n-step Sarsa 247"
text = ""

[[sections]]
number = "10.2"
title = "Semi-gradient n-step Sarsa"
text = """
We can obtain an n-step version of episodic semi-gradient Sarsa by using an n-step return\r
as the update target in the semi-gradient Sarsa update equation (10.1). The n-step return\r
immediately generalizes from its tabular form (7.4) to a function approximation form:\r
Gt:t+n\r
.\r
= Rt+1+Rt+2+···+n1Rt+n+nqˆ(St+n, At+n, wt+n1), t+n < T, (10.4)\r
with Gt:t+n\r
.\r
= Gt if t + n  T, as usual. The n-step update equation is\r
wt+n\r
.\r
= wt+n1 + ↵ [Gt:t+n  qˆ(St, At, wt+n1)] rqˆ(St, At, wt+n1), 0  t < T.\r
(10.5)\r
Complete pseudocode is given in the box below.\r
Episodic semi-gradient n-step Sarsa for estimating qˆ ⇡ q⇤ or q⇡\r
Input: a di↵erentiable action-value function parameterization ˆq : S ⇥ A ⇥ Rd ! R\r
Input: a policy ⇡ (if estimating q⇡)\r
Algorithm parameters: step size ↵ > 0, small " > 0, a positive integer n\r
Initialize value-function weights w 2 Rd arbitrarily (e.g., w = 0)\r
All store and access operations (St, At, and Rt) can take their index mod n + 1\r
Loop for each episode:\r
Initialize and store S0 6= terminal\r
Select and store an action A0 ⇠ ⇡(·|S0) or "-greedy wrt ˆq(S0, ·, w)\r
T 1\r
Loop for t = 0, 1, 2,... :\r
| If t<T, then:\r
| Take action At\r
| Observe and store the next reward as Rt+1 and the next state as St+1\r
| If St+1 is terminal, then:\r
| T t + 1\r
| else:\r
| Select and store At+1 ⇠ ⇡(·|St+1) or "-greedy wrt ˆq(St+1, ·, w)\r
| ⌧ t  n +1 (⌧ is the time whose estimate is being updated)\r
| If ⌧  0:\r
| G Pmin(⌧+n,T)\r
i=⌧+1 i⌧1Ri\r
| If ⌧ + n<T, then G G + nqˆ(S⌧+n, A⌧+n, w) (G⌧:⌧+n)\r
| w w + ↵ [G  qˆ(S⌧ , A⌧ , w)] rqˆ(S⌧ , A⌧ , w)\r
Until ⌧ = T  1\r
As we have seen before, performance is best if an intermediate level of bootstrapping\r
is used, corresponding to an n larger than 1. Figure 10.3 shows how this algorithm tends\r
to learn faster and obtain a better asymptotic performance at n= 8 than at n= 1 on the\r
Mountain Car task. Figure 10.4 shows the results of a more detailed study of the e↵ect\r
of the parameters ↵ and n on the rate of learning on this task."""

[[sections]]
number = "248"
title = "Chapter 10: On-policy Control with Approximation"
text = """
100\r
200\r
400\r
1000"""

[[sections]]
number = "0"
title = "Mountain Car"
text = """
Steps per episode\r
log scale\r
averaged over 100 runs\r
Episode\r
500\r
n=1\r
n=8\r
Figure 10.3: Performance of one-step vs 8-step semi-gradient Sarsa on the Mountain Car task.\r
Good step sizes were used: ↵ = 0.5/8 for n = 1 and ↵ = 0.3/8 for n = 8.\r
220\r
240\r
260\r
300\r
0 0.5 1 1.5\r
Mountain Car\r
Steps per episode\r
averaged over\r
first 50 episodes\r
and 100 runs\r
↵ × number of tilings (8)\r
280\r
n=1\r
n=2\r
n=4\r
n=8\r
n=16\r
n=8\r
n=4\r
n=2\r
n=16\r
n=1\r
Figure 10.4: E↵ect of the ↵ and n on early performance of n-step semi-gradient Sarsa and\r
tile-coding function approximation on the Mountain Car task. As usual, an intermediate level of\r
bootstrapping (n = 4) performed best. These results are for selected ↵ values, on a log scale,\r
and then connected by straight lines. The standard errors ranged from 0.5 (less than the line\r
width) for n = 1 to about 4 for n = 16, so the main e↵ects are all statistically significant.\r
Exercise 10.1 We have not explicitly considered or given pseudocode for any Monte Carlo\r
methods in this chapter. What would they be like? Why is it reasonable not to give\r
pseudocode for them? How would they perform on the Mountain Car task? ⇤\r
Exercise 10.2 Give pseudocode for semi-gradient one-step Expected Sarsa for control. ⇤\r
Exercise 10.3 Why do the results shown in Figure 10.4 have higher standard errors at\r
large n than at small n? ⇤"""

[[sections]]
number = "10.3"
title = "Average Reward: A New Problem Setting for Continuing Tasks 249"
text = ""

[[sections]]
number = "10.3"
title = "Average Reward: A New Problem Setting for"
text = """
Continuing Tasks\r
We now introduce a third classical setting—alongside the episodic and discounted settings—\r
for formulating the goal in Markov decision problems (MDPs). Like the discounted\r
setting, the average reward setting applies to continuing problems, problems for which the\r
interaction between agent and environment goes on and on forever without termination\r
or start states. Unlike that setting, however, there is no discounting—the agent cares just\r
as much about delayed rewards as it does about immediate reward. The average-reward\r
setting is one of the major settings commonly considered in the classical theory of dynamic\r
programming and less-commonly in reinforcement learning. As we discuss in the next\r
section, the discounted setting is problematic with function approximation, and thus the\r
average-reward setting is needed to replace it.\r
In the average-reward setting, the quality of a policy ⇡ is defined as the average rate of\r
reward, or simply average reward, while following that policy, which we denote as r(⇡):\r
r(⇡) .= lim\r
h!1\r
1\r
h\r
X\r
h\r
t=1\r
E[Rt | S0, A0:t1 ⇠⇡] (10.6)\r
= limt!1 E[Rt | S0, A0:t1 ⇠⇡] , (10.7)\r
= X\r
s\r
µ⇡(s)\r
X\r
a\r
⇡(a|s)\r
X\r
s0,r\r
p(s0, r|s, a)r,\r
where the expectations are conditioned on the initial state, S0, and on the subsequent\r
actions, A0, A1,...,At1, being taken according to ⇡. The second and third equations\r
hold if the steady-state distribution, µ⇡(s) .= limt!1 Pr{St =s |A0:t1 ⇠⇡}, exists and\r
is independent of S0, in other words, if the MDP is ergodic. In an ergodic MDP, the\r
starting state and any early decision made by the agent can have only a temporary e↵ect;\r
in the long run the expectation of being in a state depends only on the policy and the\r
MDP transition probabilities. Ergodicity is sucient but not necessary to guarantee the\r
existence of the limit in (10.6).\r
There are subtle distinctions that can be drawn between di↵erent kinds of optimality\r
in the undiscounted continuing case. Nevertheless, for most practical purposes it may\r
be adequate simply to order policies according to their average reward per time step,\r
in other words, according to their r(⇡). This quantity is essentially the average reward\r
under ⇡, as suggested by (10.7), or the reward rate. In particular, we consider all policies\r
that attain the maximal value of r(⇡) to be optimal.\r
Note that the steady state distribution µ⇡ is the special distribution under which, if\r
you select actions according to ⇡, you remain in the same distribution. That is, for which\r
X\r
s\r
µ⇡(s)\r
X\r
a\r
⇡(a|s)p(s0|s, a) = µ⇡(s0). (10.8)"""

[[sections]]
number = "250"
title = "Chapter 10: On-policy Control with Approximation"
text = """
In the average-reward setting, returns are defined in terms of di↵erences between\r
rewards and the average reward:\r
Gt\r
.\r
= Rt+1  r(⇡) + Rt+2  r(⇡) + Rt+3  r(⇡) + ··· . (10.9)\r
This is known as the di↵erential return, and the corresponding value functions are\r
known as di↵erential value functions. Di↵erential value functions are defined in terms\r
of the new return just as conventional value functions were defined in terms of the\r
discounted return; thus we will use the same notation, v⇡(s) .= E⇡[Gt|St = s] and\r
q⇡(s, a) .= E⇡[Gt|St = s, At = a] (similarly for v⇤ and q⇤), for di↵erential value functions.\r
Di↵erential value functions also have Bellman equations, just slightly di↵erent from those\r
we have seen earlier. We simply remove all s and replace all rewards by the di↵erence\r
between the reward and the true average reward:\r
v⇡(s) = X\r
a\r
⇡(a|s)\r
X\r
r,s0\r
p(s0, r|s, a)\r
h\r
r  r(⇡) + v⇡(s0)\r
i\r
,\r
q⇡(s, a) = X\r
r,s0\r
p(s0, r|s, a)\r
h\r
r  r(⇡) +X\r
a0\r
⇡(a0|s0)q⇡(s0, a0)\r
i\r
,\r
v⇤(s) = maxa\r
X\r
r,s0\r
p(s0, r|s, a)\r
h\r
r  max ⇡ r(⇡) + v⇤(s0)\r
i\r
, and\r
q⇤(s, a) = X\r
r,s0\r
p(s0, r|s, a)\r
h\r
r  max ⇡ r(⇡) + max a0 q⇤(s0, a0)\r
i\r
(cf. (3.14), Exercise 3.17, (3.19), and (3.20)).\r
There is also a di↵erential form of the two TD errors:\r
t\r
.\r
= Rt+1R¯t + ˆv(St+1,wt)  vˆ(St,wt), (10.10)\r
and\r
t\r
.\r
= Rt+1R¯t + ˆq(St+1, At+1, wt)  qˆ(St, At, wt), (10.11)\r
where R¯t is an estimate at time t of the average reward r(⇡). With these alternate\r
definitions, most of our algorithms and many theoretical results carry through to the\r
average-reward setting without change.\r
For example, an average reward version of semi-gradient Sarsa could be defined just as\r
in (10.2) except with the di↵erential version of the TD error. That is, by\r
wt+1\r
.\r
= wt + ↵trqˆ(St, At, wt), (10.12)\r
with t given by (10.11). Pseudocode for a complete algorithm is given in the box on the\r
next page. One limitation of this algorithm is that it does not converge to the di↵erential\r
values but to the di↵erential values plus an arbitrary o↵set. Notice that the Bellman\r
equations and TD errors given above are una↵ected if all the values are shifted by the\r
same amount. Thus, the o↵set may not matter in practice. How this algorithm could be\r
changed to eliminate the o↵set is an interesting question for future research.\r
Exercise 10.4 Give pseudocode for a di↵erential version of semi-gradient Q-learning. ⇤\r
Exercise 10.5 What equations are needed (beyond 10.10) to specify the di↵erential\r
version of TD(0)? ⇤"""

[[sections]]
number = "10.3"
title = "Average Reward: A New Problem Setting for Continuing Tasks 251"
text = """
Di↵erential semi-gradient Sarsa for estimating qˆ ⇡ q⇤\r
Input: a di↵erentiable action-value function parameterization ˆq : S ⇥ A ⇥ Rd ! R\r
Algorithm parameters: step sizes ↵,  > 0, small " > 0\r
Initialize value-function weights w 2 Rd arbitrarily (e.g., w = 0)\r
Initialize average reward estimate R¯ 2 R arbitrarily (e.g., R¯ = 0)\r
Initialize state S, and action A\r
Loop for each step:\r
Take action A, observe R, S0\r
Choose A0 as a function of ˆq(S0, ·, w) (e.g., "-greedy)\r
 R  R¯ + ˆq(S0, A0, w)  qˆ(S, A, w)\r
R¯ R¯ + \r
w w + ↵rqˆ(S, A, w)\r
S S0\r
A A0\r
Exercise 10.6 Suppose there is an MDP that under any policy produces the deterministic\r
sequence of rewards +1, 0, +1, 0, +1, 0,... going on forever. Technically, this violates\r
ergodicity; there is no stationary limiting distribution µ⇡ and the limit (10.7) does not\r
exist. Nevertheless, the average reward (10.6) is well defined. What is it? Now consider\r
two states in this MDP. From A, the reward sequence is exactly as described above,\r
starting with a +1, whereas, from B, the reward sequence starts with a 0 and then\r
continues with +1, 0, +1, 0,.... We would like to compute the di↵erential values of A and\r
B. Unfortunately, the di↵erential return (10.9) is not well defined when starting from\r
these states as the implicit limit does not exist. To repair this, one could alternatively\r
define the di↵erential value of a state as\r
v⇡(s) .= lim\r
!1\r
lim\r
h!1\r
X\r
h\r
t=0\r
t\r
⇣\r
E⇡[Rt+1|S0 =s]  r(⇡)\r
⌘\r
. (10.13)\r
Under this definition, what are the di↵erential values of states A and B? ⇤\r
Exercise 10.7 Consider a Markov reward process consisting of a ring of three states A, B,\r
and C, with state transitions going deterministically around the ring. A reward of +1 is\r
received upon arrival in A and otherwise the reward is 0. What are the di↵erential values\r
of the three states, using (10.13)? ⇤\r
Exercise 10.8 The pseudocode in the box on page 251 updates R¯t using t as an error\r
rather than simply Rt+1  R¯t. Both errors work, but using t is better. To see why,\r
consider the ring MRP of three states from Exercise 10.7. The estimate of the average\r
reward should tend towards its true value of 1"""

[[sections]]
number = "3"
title = "Suppose it was already there and was"
text = """
held stuck there. What would the sequence of Rt+1  R¯t errors be? What would the\r
sequence of t errors be (using Equation 10.10)? Which error sequence would produce\r
a more stable estimate of the average reward if the estimate were allowed to change in\r
response to the errors? Why? ⇤"""

[[sections]]
number = "252"
title = "Chapter 10: On-policy Control with Approximation"
text = """
Example 10.2: An Access-Control Queuing Task This is a decision task involving\r
access control to a set of 10 servers. Customers of four di↵erent priorities arrive at a\r
single queue. If given access to a server, the customers pay a reward of 1, 2, 4, or 8 to\r
the server, depending on their priority, with higher priority customers paying more. In\r
each time step, the customer at the head of the queue is either accepted (assigned to one\r
of the servers) or rejected (removed from the queue, with a reward of zero). In either\r
case, on the next time step the next customer in the queue is considered. The queue\r
never empties, and the priorities of the customers in the queue are uniformly randomly\r
distributed. Of course a customer cannot be served if there is no free server; the customer\r
is always rejected in this case. Each busy server becomes free with probability p = 0.06\r
on each time step. Although we have just described them for definiteness, let us assume\r
the statistics of arrivals and departures are unknown. The task is to decide on each step\r
whether to accept or reject the next customer, on the basis of his priority and the number\r
of free servers, so as to maximize long-term reward without discounting.\r
In this example we consider a tabular solution to this problem. Although there is no\r
generalization between states, we can still consider it in the general function approximation\r
setting as this setting generalizes the tabular setting. Thus we have a di↵erential action\u0002value estimate for each pair of state (number of free servers and priority of the customer\r
at the head of the queue) and action (accept or reject). Figure 10.5 shows the solution\r
found by di↵erential semi-gradient Sarsa with parameters ↵ = 0.01,  = 0.01, and " = 0.1.\r
The initial action values and R¯ were zero.\r
-10\r
-5\r
0\r
10"""

[[sections]]
number = "0"
title = "Differential"
text = """
value of \r
best action\r
Number of free servers\r
0 1 2 3 4 5 6 7 8 9 10\r
!15\r
!10\r
!5\r
0\r
5\r
7\r
priority 8\r
priority 4\r
priority 2\r
priority 1\r
Number of free servers\r
4\r
2"""

[[sections]]
number = "8"
title = "ACCEPT"
text = """
REJECT\r
1 2 3 4 5 6 7 8 9 10\r
Number of free servers\r
Priority"""

[[sections]]
number = "1"
title = "POLICY"
text = """
Value of\r
best action\r
VALUE\r
FUNCTION 5\r
1 2 3 4 5 6 7 8 9 10\r
priority 8\r
priority 4\r
priority 2\r
priority 1\r
POLICY\r
VALUE\r
FUNCTION\r
Figure 10.5: The policy and value function found by di↵erential semi-gradient one-step Sarsa\r
on the access-control queuing task after 2 million steps. The drop on the right of the graph\r
is probably due to insucient data; many of these states were never experienced. The value\r
learned for R¯ was about 2.31. (Note that priority 1 here is the lowest priority.)"""

[[sections]]
number = "10.4"
title = "Deprecating the Discounted Setting 253"
text = ""

[[sections]]
number = "10.4"
title = "Deprecating the Discounted Setting"
text = """
The continuing, discounted problem formulation has been very useful in the tabular case,\r
in which the returns from each state can be separately identified and averaged. But in the\r
approximate case it is questionable whether one should ever use this problem formulation.\r
To see why, consider an infinite sequence of returns with no beginning or end, and no\r
clearly identified states. The states might be represented only by feature vectors, which\r
may do little to distinguish the states from each other. As a special case, all of the feature\r
vectors may be the same. Thus one really has only the reward sequence (and the actions),\r
and performance has to be assessed purely from these. How could it be done? One way\r
is by averaging the rewards over a long interval—this is the idea of the average-reward\r
setting. How could discounting be used? Well, for each time step we could measure\r
the discounted return. Some returns would be small and some big, so again we would\r
have to average them over a suciently large time interval. In the continuing setting\r
there are no starts and ends, and no special time steps, so there is nothing else that\r
could be done. However, if you do this, it turns out that the average of the discounted\r
returns is proportional to the average reward. In fact, for policy ⇡, the average of the\r
discounted returns is always r(⇡)/(1  ), that is, it is essentially the average reward,\r
r(⇡). In particular, the ordering of all policies in the average discounted return setting\r
would be exactly the same as in the average-reward setting. The discount rate  thus has\r
no e↵ect on the problem formulation. It could in fact be zero and the ranking would be\r
unchanged.\r
This surprising fact is proven in the box on the next page, but the basic idea can\r
be seen via a symmetry argument. Each time step is exactly the same as every other.\r
With discounting, every reward will appear exactly once in each position in some return.\r
The tth reward will appear undiscounted in the t  1st return, discounted once in the\r
t  2nd return, and discounted 999 times in the t  1000th return. The weight on the\r
tth reward is thus 1 +  + 2 + 3 + ··· = 1/(1  ). Because all states are the same,\r
they are all weighted by this, and thus the average of the returns will be this times the\r
average reward, or r(⇡)/(1  ).\r
This example and the more general argument in the box show that if we optimized\r
discounted value over the on-policy distribution, then the e↵ect would be identical to\r
optimizing undiscounted average reward; the actual value of  would have no e↵ect. This\r
strongly suggests that discounting has no role to play in the definition of the control\r
problem with function approximation. One can nevertheless go ahead and use discounting\r
in solution methods. The discounting parameter  changes from a problem parameter\r
to a solution method parameter! Unfortunately, discounting algorithms with function\r
approximation do not optimize discounted value over the on-policy distribution, and thus\r
are not guaranteed to optimize average reward.\r
The root cause of the diculties with the discounted control setting is that with\r
function approximation we have lost the policy improvement theorem (Section 4.2). It is\r
no longer true that if we change the policy to improve the discounted value of one state\r
then we are guaranteed to have improved the overall policy in any useful sense. That\r
guarantee was key to the theory of our reinforcement learning control methods. With"""

[[sections]]
number = "254"
title = "Chapter 10: On-policy Control with Approximation"
text = """
The Futility of Discounting in Continuing Problems\r
Perhaps discounting can be saved by choosing an objective that sums discounted\r
values over the distribution with which states occur under the policy:\r
J(⇡) = X\r
s\r
µ⇡(s)v\r
⇡(s) (where v⇡ is the discounted value function)\r
= X\r
s\r
µ⇡(s)\r
X\r
a\r
⇡(a|s)\r
X\r
s0\r
X\r
r\r
p(s0, r|s, a) [r + v\r
⇡(s0\r
)] (Bellman Eq.)\r
= r(⇡) +X\r
s\r
µ⇡(s)\r
X\r
a\r
⇡(a|s)\r
X\r
s0\r
X\r
r\r
p(s0, r|s, a)v\r
⇡(s0\r
) (from (10.7))\r
= r(⇡) + \r
X\r
s0\r
v\r
⇡(s0\r
)\r
X\r
s\r
µ⇡(s)\r
X\r
a\r
⇡(a|s)p(s0|s, a) (from (3.4))\r
= r(⇡) + \r
X\r
s0\r
v\r
⇡(s0\r
)µ⇡(s0) (from (10.8))\r
= r(⇡) + J(⇡)\r
= r(⇡) + r(⇡) + 2J(⇡)\r
= r(⇡) + r(⇡) + 2r(⇡) + 3r(⇡) + ···\r
= 1\r
1  \r
r(⇡).\r
The proposed discounted objective orders policies identically to the undiscounted\r
(average reward) objective. The discount rate  does not influence the ordering!\r
function approximation we have lost it!\r
In fact, the lack of a policy improvement theorem is also a theoretical lacuna for the\r
total-episodic and average-reward settings. Once we introduce function approximation\r
we can no longer guarantee improvement for any setting. In Chapter 13 we introduce an\r
alternative class of reinforcement learning algorithms based on parameterized policies,\r
and there we have a theoretical guarantee called the “policy-gradient theorem” which\r
plays a similar role as the policy improvement theorem. But for methods that learn\r
action values we seem to be currently without a local improvement guarantee (possibly\r
the approach taken by Perkins and Precup (2003) may provide a part of the answer). We\r
do know that "-greedification may sometimes result in an inferior policy, as policies may\r
chatter among good policies rather than converge (Gordon, 1996a). This is an area with\r
multiple open theoretical questions."""

[[sections]]
number = "10.5"
title = "Di↵erential Semi-gradient n-step Sarsa 255"
text = ""

[[sections]]
number = "10.5"
title = "Di↵erential Semi-gradient n-step Sarsa"
text = """
In order to generalize to n-step bootstrapping, we need an n-step version of the TD error.\r
We begin by generalizing the n-step return (7.4) to its di↵erential form, with function\r
approximation:\r
Gt:t+n\r
.\r
= Rt+1R¯t+n1 + ··· + Rt+nR¯t+n1 + ˆq(St+n, At+n, wt+n1), (10.14)\r
where R¯ is an estimate of r(⇡), n  1, and t + n<T. If t + n  T, then we define\r
Gt:t+n\r
.\r
= Gt as usual. The n-step TD error is then\r
t\r
.\r
= Gt:t+n  qˆ(St, At, w), (10.15)\r
after which we can apply our usual semi-gradient Sarsa update (10.12). Pseudocode for\r
the complete algorithm is given in the box.\r
Di↵erential semi-gradient n-step Sarsa for estimating qˆ ⇡ q⇡ or q⇤\r
Input: a di↵erentiable function ˆq : S ⇥ A ⇥ Rd ! R, a policy ⇡\r
Initialize value-function weights w 2 Rd arbitrarily (e.g., w = 0)\r
Initialize average-reward estimate R¯ 2 R arbitrarily (e.g., R¯ = 0)\r
Algorithm parameters: step sizes ↵,  > 0, small " > 0, a positive integer n\r
All store and access operations (St, At, and Rt) can take their index mod n + 1\r
Initialize and store S0 and A0\r
Loop for each step, t = 0, 1, 2,... :\r
Take action At\r
Observe and store the next reward as Rt+1 and the next state as St+1\r
Select and store an action At+1 ⇠ ⇡(·|St+1), or "-greedy wrt ˆq(St+1, ·, w)\r
⌧ t  n +1 (⌧ is the time whose estimate is being updated)\r
If ⌧  0:\r
 P⌧+n\r
i=⌧+1(Ri  R¯)+ˆq(S⌧+n, A⌧+n, w)  qˆ(S⌧ , A⌧ , w)\r
R¯ R¯ + \r
w w + ↵rqˆ(S⌧ , A⌧ , w)\r
Exercise 10.9 In the di↵erential semi-gradient n-step Sarsa algorithm, the step-size\r
parameter on the average reward, , needs to be quite small so that R¯ becomes a good\r
long-term estimate of the average reward. Unfortunately, R¯ will then be biased by its\r
initial value for many steps, which may make learning inecient. Alternatively, one could\r
use a sample average of the observed rewards for R¯. That would initially adapt rapidly\r
but in the long run would also adapt slowly. As the policy slowly changed, R¯ would also\r
change; the potential for such long-term nonstationarity makes sample-average methods\r
ill-suited. In fact, the step-size parameter on the average reward is a perfect place to use\r
the unbiased constant-step-size trick from Exercise 2.7. Describe the specific changes\r
needed to the boxed algorithm for di↵erential semi-gradient n-step Sarsa to use this\r
trick. ⇤"""

[[sections]]
number = "256"
title = "Chapter 10: On-policy Control with Approximation"
text = ""

[[sections]]
number = "10.6"
title = "Summary"
text = """
In this chapter we have extended the ideas of parameterized function approximation and\r
semi-gradient descent, introduced in the previous chapter, to control. The extension is\r
immediate for the episodic case, but for the continuing case we have to introduce a whole\r
new problem formulation based on maximizing the average reward setting per time step.\r
Surprisingly, the discounted formulation cannot be carried over to control in the presence\r
of approximations. In the approximate case most policies cannot be represented by a\r
value function. The arbitrary policies that remain need to be ranked, and the scalar\r
average reward r(⇡) provides an e↵ective way to do this.\r
The average reward formulation involves new di↵erential versions of value functions,\r
Bellman equations, and TD errors, but all of these parallel the old ones, and the\r
conceptual changes are small. There is also a new parallel set of di↵erential algorithms\r
for the average-reward case.\r
Bibliographical and Historical Remarks"""

[[sections]]
number = "10.1"
title = "Semi-gradient Sarsa with function approximation was first explored by Rummery"
text = """
and Niranjan (1994). Linear semi-gradient Sarsa with "-greedy action selection\r
does not converge in the usual sense, but does enter a bounded region near\r
the best solution (Gordon, 1996a, 2001). Precup and Perkins (2003) showed\r
convergence in a di↵erentiable action selection setting. See also Perkins and\r
Pendrith (2002) and Melo, Meyn, and Ribeiro (2008). The mountain–car example\r
is based on a similar task studied by Moore (1990), but the exact form used here\r
is from Sutton (1996)."""

[[sections]]
number = "10.2"
title = "Episodic n-step semi-gradient Sarsa is based on the forward Sarsa() algorithm"
text = """
of van Seijen (2016). The empirical results shown here are new to the second\r
edition of this text."""

[[sections]]
number = "10.3"
title = "The average-reward formulation has been described for dynamic programming"
text = """
(e.g., Puterman, 1994) and from the point of view of reinforcement learning\r
(Mahadevan, 1996; Tadepalli and Ok, 1994; Bertsekas and Tsitsiklis, 1996;\r
Tsitsiklis and Van Roy, 1999). The algorithm described here is the on-policy\r
analog of the “R-learning” algorithm introduced by Schwartz (1993). The name\r
R-learning was probably meant to be the alphabetic successor to Q-learning,\r
but we prefer to think of it as a reference to the learning of di↵erential or\r
relative values. The access-control queuing example was suggested by the work\r
of Carlstr¨om and Nordstr¨om (1997).\r
10.4 The recognition of the limitations of discounting as a formulation of the rein\u0002forcement learning problem with function approximation became apparent to\r
the authors shortly after the publication of the first edition of this text. Singh,\r
Jaakkola, and Jordan (1994) may have been the first to observe it in print.

Chapter 11\r
*O↵-policy Methods with\r
Approximation\r
This book has treated on-policy and o↵-policy learning methods since Chapter 5 primarily\r
as two alternative ways of handling the conflict between exploitation and exploration\r
inherent in learning forms of generalized policy iteration. The two chapters preceding this\r
have treated the on-policy case with function approximation, and in this chapter we treat\r
the o↵ -policy case with function approximation. The extension to function approximation\r
turns out to be significantly di↵erent and harder for o↵-policy learning than it is for\r
on-policy learning. The tabular o↵-policy methods developed in Chapters 6 and 7 readily\r
extend to semi-gradient algorithms, but these algorithms do not converge as robustly as\r
they do under on-policy training. In this chapter we explore the convergence problems,\r
take a closer look at the theory of linear function approximation, introduce a notion of\r
learnability, and then discuss new algorithms with stronger convergence guarantees for the\r
o↵-policy case. In the end we will have improved methods, but the theoretical results will\r
not be as strong, nor the empirical results as satisfying, as they are for on-policy learning.\r
Along the way, we will gain a deeper understanding of approximation in reinforcement\r
learning for on-policy learning as well as o↵-policy learning.\r
Recall that in o↵-policy learning we seek to learn a value function for a target policy\r
⇡, given data due to a di↵erent behavior policy b. In the prediction case, both policies\r
are static and given, and we seek to learn either state values vˆ ⇡ v⇡ or action values\r
qˆ ⇡ q⇡. In the control case, action values are learned, and both policies typically change\r
during learning—⇡ being the greedy policy with respect to qˆ, and b being something\r
more exploratory such as the "-greedy policy with respect to ˆq.\r
The challenge of o↵-policy learning can be divided into two parts, one that arises in\r
the tabular case and one that arises only with function approximation. The first part\r
of the challenge has to do with the target of the update (not to be confused with the\r
target policy), and the second part has to do with the distribution of the updates. The\r
techniques related to importance sampling developed in Chapters 5 and 7 deal with\r
the first part; these may increase variance but are needed in all successful algorithms,"""

[[sections]]
number = "258"
title = "Chapter 11: O↵-policy Methods with Approximation"
text = """
tabular and approximate. The extension of these techniques to function approximation\r
are quickly dealt with in the first section of this chapter.\r
Something more is needed for the second part of the challenge of o↵-policy learning\r
with function approximation because the distribution of updates in the o↵-policy case is\r
not according to the on-policy distribution. The on-policy distribution is important to\r
the stability of semi-gradient methods. Two general approaches have been explored to\r
deal with this. One is to use importance sampling methods again, this time to warp the\r
update distribution back to the on-policy distribution, so that semi-gradient methods\r
are guaranteed to converge (in the linear case). The other is to develop true gradient\r
methods that do not rely on any special distribution for stability. We present methods\r
based on both approaches. This is a cutting-edge research area, and it is not clear which\r
of these approaches is most e↵ective in practice."""

[[sections]]
number = "11.1"
title = "Semi-gradient Methods"
text = """
We begin by describing how the methods developed in earlier chapters for the o↵-\r
policy case extend readily to function approximation as semi-gradient methods. These\r
methods address the first part of the challenge of o↵-policy learning (changing the update\r
targets) but not the second part (changing the update distribution). Accordingly, these\r
methods may diverge in some cases, and in that sense are not sound, but still they\r
are often successfully used. Remember that these methods are guaranteed stable and\r
asymptotically unbiased for the tabular case, which corresponds to a special case of\r
function approximation. So it may still be possible to combine them with feature selection\r
methods in such a way that the combined system could be assured stable. In any event,\r
these methods are simple and thus a good place to start.\r
In Chapter 7 we described a variety of tabular o↵-policy algorithms. To convert them\r
to semi-gradient form, we simply replace the update to an array (V or Q) to an update\r
to a weight vector (w), using the approximate value function (vˆ or qˆ) and its gradient.\r
Many of these algorithms use the per-step importance sampling ratio:\r
⇢t\r
.\r
= ⇢t:t = ⇡(At|St)\r
b(At|St)\r
. (11.1)\r
For example, the one-step, state-value algorithm is semi-gradient o↵-policy TD(0), which\r
is just like the corresponding on-policy algorithm (page 203) except for the addition of\r
⇢t:\r
wt+1\r
.\r
= wt + ↵⇢ttrvˆ(St,wt), (11.2)\r
where t is defined appropriately depending on whether the problem is episodic and\r
discounted, or continuing and undiscounted using average reward:\r
t\r
.\r
= Rt+1 + vˆ(St+1,wt)  vˆ(St,wt), or (11.3)\r
t\r
.\r
= Rt+1  R¯t + ˆv(St+1,wt)  vˆ(St,wt). (11.4)"""

[[sections]]
number = "11.1"
title = "Semi-gradient Methods 259"
text = """
For action values, the one-step algorithm is semi-gradient Expected Sarsa:\r
wt+1\r
.\r
= wt + ↵trqˆ(St, At, wt), with (11.5)\r
t\r
.\r
= Rt+1 + \r
X\r
a\r
⇡(a|St+1)ˆq(St+1, a, wt)  qˆ(St, At, wt), or (episodic)\r
t\r
.\r
= Rt+1  R¯t +X\r
a\r
⇡(a|St+1)ˆq(St+1, a, wt)  qˆ(St, At, wt). (continuing)\r
Note that this algorithm does not use importance sampling. In the tabular case it is clear\r
that this is appropriate because the only sample action is At, and in learning its value we\r
do not have to consider any other actions. With function approximation it is less clear\r
because we might want to weight di↵erent state–action pairs di↵erently once they all\r
contribute to the same overall approximation. Proper resolution of this issue awaits a\r
more thorough understanding of the theory of function approximation in reinforcement\r
learning.\r
In the multi-step generalizations of these algorithms, both the state-value and action\u0002value algorithms involve importance sampling. The n-step version of semi-gradient Sarsa\r
is\r
wt+n\r
.\r
= wt+n1+↵⇢t+1 ··· ⇢t+n [Gt:t+n  qˆ(St, At, wt+n1)] rqˆ(St, At, wt+n1) (11.6)\r
with\r
Gt:t+n\r
.\r
= Rt+1 + ··· + n1Rt+n + nqˆ(St+n, At+n, wt+n1), or (episodic)\r
Gt:t+n\r
.\r
= Rt+1  R¯t + ··· + Rt+n  R¯t+n1 + ˆq(St+n, At+n, wt+n1), (continuing)\r
where here we are being slightly informal in our treatment of the ends of episodes. In the\r
first equation, the ⇢ks for k  T (where T is the last time step of the episode) should be\r
taken to be 1, and Gt:t+n should be taken to be Gt if t + n  T.\r
Recall that we also presented in Chapter 7 an o↵-policy algorithm that does not involve\r
importance sampling at all: the n-step tree-backup algorithm. Here is its semi-gradient\r
version:\r
wt+n\r
.\r
= wt+n1 + ↵ [Gt:t+n  qˆ(St, At, wt+n1)] rqˆ(St, At, wt+n1), (11.7)\r
Gt:t+n\r
.\r
= ˆq(St, At, wt+n1) +\r
t+\r
Xn1\r
k=t\r
k\r
Y\r
k\r
i=t+1\r
⇡(Ai|Si), (11.8)\r
with t as defined at the top of this page for Expected Sarsa. We also defined in Chapter 7\r
an algorithm that unifies all action-value algorithms: n-step Q(). We leave the semi\u0002gradient form of that algorithm, and also of the n-step state-value algorithm, as exercises\r
for the reader.\r
Exercise 11.1 Convert the equation of n-step o↵-policy TD (7.9) to semi-gradient form.\r
Give accompanying definitions of the return for both the episodic and continuing cases. ⇤\r
⇤\r
Exercise 11.2 Convert the equations of n-step Q() (7.11 and 7.17) to semi-gradient\r
form. Give definitions that cover both the episodic and continuing cases. ⇤"""

[[sections]]
number = "260"
title = "Chapter 11: O↵-policy Methods with Approximation"
text = ""

[[sections]]
number = "11.2"
title = "Examples of O↵-policy Divergence"
text = """
In this section we begin to discuss the second part of the challenge of o↵-policy learning\r
with function approximation—that the distribution of updates does not match the on\u0002policy distribution. We describe some instructive counterexamples to o↵-policy learning—\r
cases where semi-gradient and other simple algorithms are unstable and diverge.\r
To establish intuitions, it is best to consider first a very simple example. Suppose,\r
perhaps as part of a larger MDP, there are two states whose estimated values are of\r
the functional form w and 2w, where the parameter vector w consists of only a single\r
component w. This occurs under linear function approximation if the feature vectors\r
for the two states are each simple numbers (single-component vectors), in this case 1\r
and 2. In the first state, only one action is available, and it results deterministically in a\r
transition to the second state with a reward of 0:\r
2w\r
0 2w\r
where the expressions inside the two circles indicate the two state’s values.\r
Suppose initially w = 10. The transition will then be from a state of estimated value\r
10 to a state of estimated value 20. It will look like a good transition, and w will be\r
increased to raise the first state’s estimated value. If  is nearly 1, then the TD error will\r
be nearly 10, and, if ↵ = 0.1, then w will be increased to nearly 11 in trying to reduce the\r
TD error. However, the second state’s estimated value will also be increased, to nearly\r
22. If the transition occurs again, then it will be from a state of estimated value ⇡11 to\r
a state of estimated value ⇡22, for a TD error of ⇡11—larger, not smaller, than before.\r
It will look even more like the first state is undervalued, and its value will be increased\r
again, this time to ⇡12.1. This looks bad, and in fact with further updates w will diverge\r
to infinity.\r
To see this definitively we have to look more carefully at the sequence of updates. The\r
TD error on a transition between the two states is\r
t = Rt+1 + vˆ(St+1,wt)  vˆ(St,wt)=0+ 2wt  wt = (2  1)wt,\r
and the o↵-policy semi-gradient TD(0) update (from (11.2)) is\r
wt+1 = wt + ↵⇢ttrvˆ(St,wt) = wt + ↵ · 1 · (2  1)wt · 1 = \r
1 + ↵(2  1)wt.\r
Note that the importance sampling ratio, ⇢t, is 1 on this transition because there is\r
only one action available from the first state, so its probabilities of being taken under\r
the target and behavior policies must both be 1. In the final update above, the new\r
parameter is the old parameter times a scalar constant, 1 + ↵(2  1). If this constant is\r
greater than 1, then the system is unstable and w will go to positive or negative infinity\r
depending on its initial value. Here this constant is greater than 1 whenever  > 0.5.\r
Note that stability does not depend on the specific step size, as long as ↵ > 0. Smaller or\r
larger step sizes would a↵ect the rate at which w goes to infinity, but not whether it goes\r
there or not.\r
Key to this example is that the one transition occurs repeatedly without w being\r
updated on other transitions. This is possible under o↵-policy training because the"""

[[sections]]
number = "11.2"
title = "Examples of O↵-policy Divergence 261"
text = """
behavior policy might select actions on those other transitions which the target policy\r
never would. For these transitions, ⇢t would be zero and no update would be made.\r
Under on-policy training, however, ⇢t is always one. Each time there is a transition from\r
the w state to the 2w state, increasing w, there would also have to be a transition out\r
of the 2w state. That transition would reduce w, unless it were to a state whose value\r
was higher (because  < 1) than 2w, and then that state would have to be followed by a\r
state of even higher value, or else again w would be reduced. Each state can support the\r
one before only by creating a higher expectation. Eventually the piper must be paid. In\r
the on-policy case the promise of future reward must be kept and the system is kept in\r
check. But in the o↵-policy case, a promise can be made and then, after taking an action\r
that the target policy never would, forgotten and forgiven.\r
This simple example communicates much of the reason why o↵-policy training can lead\r
to divergence, but it is not completely convincing because it is not complete—it is just a\r
fragment of a complete MDP. Can there really be a complete system with instability? A\r
simple complete example of divergence is Baird’s counterexample. Consider the episodic\r
seven-state, two-action MDP shown in Figure 11.1. The dashed action takes the system\r
to one of the six upper states with equal probability, whereas the solid action takes the\r
system to the seventh state. The behavior policy b selects the dashed and solid actions\r
with probabilities 6\r
7 and 17 , so that the next-state distribution under it is uniform (the\r
same for all nonterminal states), which is also the starting distribution for each episode.\r
The target policy ⇡ always takes the solid action, and so the on-policy distribution (for ⇡)\r
is concentrated in the seventh state. The reward is zero on all transitions. The discount\r
rate is  = 0.99.\r
Consider estimating the state-value under the linear parameterization indicated by\r
the expression shown in each state circle. For example, the estimated value of the\r
leftmost state is 2w1 +w8, where the subscript corresponds to the component of the\r
2w1+w8 2w2+w8 2w3+w8 2w4+w8 2w5+w8 2w6+w8\r
w7+2w8\r
b(dashed|·)=6/7\r
b(solid|·)=1/7\r
⇡(solid|·)=1\r
 = 0.99\r
Figure 11.1: Baird’s counterexample. The approximate state-value function for this Markov\r
process is of the form shown by the linear expressions inside each state. The solid action usually\r
results in the seventh state, and the dashed action usually results in one of the other six states,\r
each with equal probability. The reward is always zero."""

[[sections]]
number = "262"
title = "Chapter 11: O↵-policy Methods with Approximation"
text = """
overall weight vector w 2 R8; this corresponds to a feature vector for the first state\r
being x(1) = (2, 0, 0, 0, 0, 0, 0, 1)>. The reward is zero on all transitions, so the true value\r
function is v⇡(s) = 0, for all s, which can be exactly approximated if w = 0. In fact,\r
there are many solutions, as there are more components to the weight vector (8) than\r
there are nonterminal states (7). Moreover, the set of feature vectors, {x(s) : s 2 S}, is\r
a linearly independent set. In all these ways this task seems a favorable case for linear\r
function approximation.\r
If we apply semi-gradient TD(0) to this problem (11.2), then the weights diverge\r
to infinity, as shown in Figure 11.2 (left). The instability occurs for any positive step\r
size, no matter how small. In fact, it even occurs if an expected update is done as in\r
dynamic programming (DP), as shown in Figure 11.2 (right). That is, if the weight\r
vector, wk, is updated for all states at the same time in a semi-gradient way, using the\r
DP (expectation-based) target:\r
wk+1\r
.\r
= wk +\r
↵\r
|S|\r
X\r
s\r
⇣\r
E⇡[Rt+1 + vˆ(St+1,wk) | St =s]  vˆ(s,wk)\r
⌘\r
rvˆ(s,wk). (11.9)\r
In this case, there is no randomness and no asynchrony, just as in a classical DP update.\r
The method is conventional except in its use of semi-gradient function approximation.\r
Yet still the system is unstable.\r
If we alter just the distribution of DP updates in Baird’s counterexample, from the\r
uniform distribution to the on-policy distribution (which generally requires asynchronous\r
updating), then convergence is guaranteed to a solution with error bounded by (9.14).\r
This example is striking because the TD and DP methods used are arguably the simplest\r
w8\r
w8\r
300\r
200\r
100\r
10\r
1\r
0 1000 0 1000\r
w1– w6\r
Steps\r
w7\r
Sweeps\r
Semi-gradient Off-policy TD Semi-gradient DP\r
w1– w6\r
w7\r
Figure 11.2: Demonstration of instability on Baird’s counterexample. Shown are the evolution\r
of the components of the parameter vector w of the two semi-gradient algorithms. The step size\r
was ↵ = 0.01, and the initial weights were w = (1, 1, 1, 1, 1, 1, 10, 1)>."""

[[sections]]
number = "11.2"
title = "Examples of O↵-policy Divergence 263"
text = """
and best-understood bootstrapping methods, and the linear, semi-descent method used is\r
arguably the simplest and best-understood kind of function approximation. The example\r
shows that even the simplest combination of bootstrapping and function approximation\r
can be unstable if the updates are not done according to the on-policy distribution.\r
There are also counterexamples similar to Baird’s showing divergence for Q-learning.\r
This is cause for concern because otherwise Q-learning has the best convergence guarantees\r
of all control methods. Considerable e↵ort has gone into trying to find a remedy to\r
this problem or to obtain some weaker, but still workable, guarantee. For example, it\r
may be possible to guarantee convergence of Q-learning as long as the behavior policy is\r
suciently close to the target policy, for example, when it is the "-greedy policy. To the\r
best of our knowledge, Q-learning has never been found to diverge in this case, but there\r
has been no theoretical analysis. In the rest of this section we present several other ideas\r
that have been explored.\r
Suppose that instead of taking just a step toward the expected one-step return on each\r
iteration, as in Baird’s counterexample, we actually change the value function all the way\r
to the best, least-squares approximation. Would this solve the instability problem? Of\r
course it would if the feature vectors, {x(s) : s 2 S}, formed a linearly independent set,\r
as they do in Baird’s counterexample, because then exact approximation is possible on\r
each iteration and the method reduces to standard tabular DP. But of course the point\r
here is to consider the case when an exact solution is not possible. In this case stability\r
is not guaranteed even when forming the best approximation at each iteration, as shown\r
in the example.\r
Example 11.1: Tsitsiklis and Van Roy’s Counterexample This example shows\r
that linear function approximation would not work with DP even if the least-squares\r
1  "\r
"\r
w 2w\r
solution was found at each step. The counterexample is formed\r
by extending the w-to-2w example (from earlier in this section)\r
with a terminal state, as shown to the right. As before, the\r
estimated value of the first state is w, and the estimated value\r
of the second state is 2w. The reward is zero on all transitions,\r
so the true values are zero at both states, which is exactly\r
representable with w = 0. If we set wk+1 at each step so\r
as to minimize the VE between the estimated value and the\r
expected one-step return, then we have\r
wk+1 = argmin w2R\r
X\r
s2S\r
⇣\r
vˆ(s,w)  E⇡\r
⇥\r
Rt+1 + vˆ(St+1,wk)\r
\r
 St = s\r
⇤⌘2\r
= argmin w2R\r
\r
w  2wk\r
2\r
+ 2w  (1  ")2wk\r
2\r
= 6  4"\r
5 wk. (11.10)\r
The sequence {wk} diverges when  > 5\r
64" and w0 6= 0."""

[[sections]]
number = "264"
title = "Chapter 11: O↵-policy Methods with Approximation"
text = """
Another way to try to prevent instability is to use special methods for function\r
approximation. In particular, stability is guaranteed for function approximation methods\r
that do not extrapolate from the observed targets. These methods, called averagers,\r
include nearest neighbor methods and locally weighted regression, but not popular\r
methods such as tile coding and artificial neural networks (ANNs).\r
Exercise 11.3 (programming) Apply one-step semi-gradient Q-learning to Baird’s coun\u0002terexample and show empirically that its weights diverge. ⇤"""

[[sections]]
number = "11.3"
title = "The Deadly Triad"
text = """
Our discussion so far can be summarized by saying that the danger of instability and\r
divergence arises whenever we combine all of the following three elements, making up\r
what we call the deadly triad:\r
Function approximation A powerful, scalable way of generalizing from a state space\r
much larger than the memory and computational resources (e.g., linear function\r
approximation or ANNs).\r
Bootstrapping Update targets that include existing estimates (as in dynamic pro\u0002gramming or TD methods) rather than relying exclusively on actual rewards and\r
complete returns (as in MC methods).\r
O↵-policy training Training on a distribution of transitions other than that produced\r
by the target policy. Sweeping through the state space and updating all states\r
uniformly, as in dynamic programming, does not respect the target policy and is\r
an example of o↵-policy training.\r
In particular, note that the danger is not due to control or to generalized policy iteration.\r
Those cases are more complex to analyze, but the instability arises in the simpler prediction\r
case whenever it includes all three elements of the deadly triad. The danger is also not\r
due to learning or to uncertainties about the environment, because it occurs just as\r
strongly in planning methods, such as dynamic programming, in which the environment\r
is completely known.\r
If any two elements of the deadly triad are present, but not all three, then instability\r
can be avoided. It is natural, then, to go through the three and see if there is any one\r
that can be given up.\r
Of the three, function approximation most clearly cannot be given up. We need\r
methods that scale to large problems and to great expressive power. We need at least\r
linear function approximation with many features and parameters. State aggregation or\r
nonparametric methods whose complexity grows with data are too weak or too expensive.\r
Least-squares methods such as LSTD are of quadratic complexity and are therefore too\r
expensive for large problems.\r
Doing without bootstrapping is possible, at the cost of computational and data eciency.\r
Perhaps most important are the losses in computational eciency. Monte Carlo (non\u0002bootstrapping) methods require memory to save everything that happens between making"""

[[sections]]
number = "11.3"
title = "The Deadly Triad 265"
text = """
each prediction and obtaining the final return, and all their computation is done once the\r
final return is obtained. The cost of these computational issues is not apparent on serial\r
von Neumann computers, but would be on specialized hardware. With bootstrapping and\r
eligibility traces (Chapter 12), data can be dealt with when and where it is generated,\r
then need never be used again. The savings in communication and memory made possible\r
by bootstrapping are great.\r
The losses in data eciency by giving up bootstrapping are also significant. We\r
have seen this repeatedly, such as in Chapters 7 (Figure 7.2) and 9 (Figure 9.2), where\r
some degree of bootstrapping performed much better than Monte Carlo methods on\r
the random-walk prediction task, and in Chapter 10 where the same was seen on the\r
Mountain-Car control task (Figure 10.4). Many other problems show much faster learning\r
with bootstrapping (e.g., see Figure 12.14). Bootstrapping often results in faster learning\r
because it allows learning to take advantage of the state property, the ability to recognize\r
a state upon returning to it. On the other hand, bootstrapping can impair learning on\r
problems where the state representation is poor and causes poor generalization (e.g.,\r
this seems to be the case on Tetris, see S¸im¸sek, Alg´orta, and Kothiyal, 2016). A poor\r
state representation can also result in bias; this is the reason for the poorer bound on\r
the asymptotic approximation quality of bootstrapping methods (Equation 9.14). On\r
balance, the ability to bootstrap has to be considered extremely valuable. One may\r
sometimes choose not to use it by selecting long n-step updates (or a large bootstrapping\r
parameter,  ⇡ 1; see Chapter 12) but often bootstrapping greatly increases eciency. It\r
is an ability that we would very much like to keep in our toolkit.\r
Finally, there is o↵-policy learning; can we give that up? On-policy methods are often\r
adequate. For model-free reinforcement learning, one can simply use Sarsa rather than\r
Q-learning. O↵-policy methods free behavior from the target policy. This could be\r
considered an appealing convenience but not a necessity. However, o↵-policy learning\r
is essential to other anticipated use cases, cases that we have not yet mentioned in this\r
book but may be important to the larger goal of creating a powerful intelligent agent.\r
In these use cases, the agent learns not just a single value function and single policy,\r
but large numbers of them in parallel. There is extensive psychological evidence that\r
people and animals learn to predict many di↵erent sensory events, not just rewards. We\r
can be surprised by unusual events, and correct our predictions about them, even if\r
they are of neutral valence (neither good nor bad). This kind of prediction presumably\r
underlies predictive models of the world such as are used in planning. We predict what\r
we will see after eye movements, how long it will take to walk home, the probability of\r
making a jump shot in basketball, and the satisfaction we will get from taking on a new\r
project. In all these cases, the events we would like to predict depend on our acting in\r
a certain way. To learn them all, in parallel, requires learning from the one stream of\r
experience. There are many target policies, and thus the one behavior policy cannot\r
equal all of them. Yet parallel learning is conceptually possible because the behavior\r
policy may overlap in part with many of the target policies. To take full advantage of\r
this requires o↵-policy learning."""

[[sections]]
number = "266"
title = "Chapter 11: O↵-policy Methods with Approximation"
text = ""

[[sections]]
number = "11.4"
title = "Linear Value-function Geometry"
text = """
To better understand the stability challenge of o↵-policy learning, it is helpful to think\r
about value function approximation more abstractly and independently of how learning\r
is done. We can imagine the space of all possible state-value functions—all functions\r
from states to real numbers v : S ! R. Most of these value functions do not correspond\r
to any policy. More important for our purposes is that most are not representable by the\r
function approximator, which by design has far fewer parameters than there are states.\r
Given an enumeration of the state space S = {s1, s2,...,s|S|}, any value function v\r
corresponds to a vector listing the value of each state in order [v(s1), v(s2),...,v(s|S|)]>.\r
This vector representation of a value function has as many components as there are\r
states. In most cases where we want to use function approximation, this would be far\r
too many components to represent the vector explicitly. Nevertheless, the idea of this\r
vector is conceptually useful. In the following, we treat a value function and its vector\r
representation interchangeably.\r
To develop intuitions, consider the case with three states S = {s1, s2, s3} and two\r
parameters w = (w1, w2)>. We can then view all value functions/vectors as points in\r
a three-dimensional space. The parameters provide an alternative coordinate system\r
over a two-dimensional subspace. Any weight vector w = (w1, w2)> is a point in the\r
two-dimensional subspace and thus also a complete value function vw that assigns values\r
to all three states. With general function approximation the relationship between the\r
full space and the subspace of representable functions could be complex, but in the case\r
of linear value-function approximation the subspace is a simple plane, as suggested by\r
Figure 11.3.\r
Now consider a single fixed policy ⇡. We assume that its true value function, v⇡, is too\r
complex to be represented exactly as an approximation. Thus v⇡ is not in the subspace;\r
in the figure it is depicted as being above the planar subspace of representable functions.\r
If v⇡ cannot be represented exactly, what representable value function is closest to\r
it? This turns out to be a subtle question with multiple answers. To begin, we need\r
a measure of the distance between two value functions. Given two value functions v1\r
and v2, we can talk about the vector di↵erence between them, v = v1  v2. If v is small,\r
then the two value functions are close to each other. But how are we to measure the size\r
of this di↵erence vector? The conventional Euclidean norm is not appropriate because,\r
as discussed in Section 9.2, some states are more important than others because they\r
occur more frequently or because we are more interested in them (Section 9.11). As\r
in Section 9.2, let us use the distribution µ : S ! [0, 1] to specify the degree to which\r
we care about di↵erent states being accurately valued (often taken to be the on-policy\r
distribution). We can then define the distance between value functions using the norm\r
kvk\r
2\r
µ\r
.\r
= X\r
s2S\r
µ(s)v(s)\r
2. (11.11)\r
Note that the VE from Section 9.2 can be written simply using this norm as VE(w) =\r
kvw  v⇡k\r
2\r
µ. For any value function v, the operation of finding its closest value function\r
in the subspace of representable value functions is a projection operation. We define a"""

[[sections]]
number = "11.4"
title = "Linear Value-function Geometry 267"
text = """
from each state:\r
⇡ = arg\r
⇡\r
where\r
v⇡(s) = E⇡\r
⇥\r
Rt+1 + Rt+2 \r
where  2 [0, 1) is known as the dis\r
indicates that the expectation is cond\r
The function v⇡ is called the state-valu\r
A key subproblem underlying almo\r
evaluation, the computation or estima\r
popular DP algorithm known as policy \r
a sequence of policies, each of which is \r
found. In TDL, algorithms such as TD\r
the current policy, for example as part \r
If the state space is finite, then th\r
computer as a large array with one ent\r
form the estimate. Such tabular meth\r
ones, through discretization, state aggr\r
of the state space increases, these me\r
ine↵ective. This is the e↵ect which gav\r
A more general and flexible approa\r
form of fixed size and fixed structure wi\r
are then changed to reshape the approx\r
function. We denote the parameterized \r
v(s) ⇡ \r
where ✓ 2 Rn, with n ⌧ |S|, is the \r
function can have arbitrary form as lo\r
the weights. For example, it could be \r
layer neural network where ✓ is the con\r
refer to ✓ exclusively as the weights, o\r
for things like the discount-rate param\r
An important special case is that i\r
the weights and in features of the state\r
v\r
where the (s) 2 Rn, s 2 S, are fe\r
denotes the inner product of two vecto\r
The subspace of all value functions representable as \r
Bellman error vector (BE)\r
its projected form:\r
v = ⇧B⇡v, \r
Unlike the original Bellman equation, for most function approximato\r
the projected Bellman equation can be solved exactly. If it can’t be so\r
minimize the mean-squared projected Bellman error :\r
PBE(✓) = X\r
sS\r
d(s)\r
⇥\r
(⇧(B⇡v  v))(s)\r
⇤2\r
. \r
The minimum is achieved at the projection fixpoint, at which\r
X\r
sS\r
d(s)\r
⇥\r
(B⇡v)(s)  v(s)\r
⇤\r
rv(s) = 0. \r
p\r
VE pBE pPBE ⇧v⇡ = v⇤\r
VE v⇤PBE v⇤BE"""

[[sections]]
number = "4"
title = "The"
text = """
(1985). Baird (1995, 1999) extended \r
Engel, Mannor, and Meir (2003) extended it to least squares (O(\r
)) \r
Gaussian Process TDL. In the literature, BE minimization is often referred to as Bellman\r
residual minimization."""

[[sections]]
number = "2.3"
title = "Projected Bellman error"
text = """
The third goal for approximation is to approximately solve the projected Bellman equation:\r
v = ⇧B⇡v. (11)\r
Unlike the original Bellman equation, for most function approximators (e.g., linear ones) the\r
projected Bellman equation can be solved exactly. The original TDL methods (Sutton 1988,\r
Dayan 1992) converge to this solution, as does least-squares TDL (Bradke & Barto 1996,\r
Boyan 1999). The goal of achieving (11) exactly is common; less common is to consider\r
approximating it as an objective. The early work on gradient-TD (e.g., Sutton et al. 2009)\r
appears to be first to have explicitly proposed minimizing the d-weighted norm of the error\r
in (11), which we here call the projected Bellman error :\r
PBE(✓) = ||v  ⇧B⇡v||. (12)\r
This objective is best understood by looking at the left side of Figure 1. Starting at v,\r
the Bellman operator takes us outside the subspace, and the projection operator takes us\r
back into it. The distance between where we end up and where we started is the PBE. The\r
distance is minimal (zero) when the trip up and back leaves us in the same place."""

[[sections]]
number = "8"
title = "Value error (VE)"
text = """
w1\r
w2\r
vw\r
vw\r
B⇡vw\r
⇧B⇡vw\r
wTD\r
PBE = 0\r
The 3D space of \r
all value functions \r
over 3 states\r
min BE\r
min TDE\r
(min VE)\r
Figure 11.3: The geometry of linear value-function approximation. Shown is the three\u0002dimensional space of all value functions over three states, while shown as a plane is the subspace of\r
all value functions representable by a linear function approximator with parameter w = (w1, w2)\r
>.\r
The true value function v⇡ is in the larger space and can be projected down (into the subspace,\r
using a projection operator ⇧) to its best approximation in the value error (VE) sense. The\r
best approximators in the Bellman error (BE), projected Bellman error (PBE), and temporal\r
di↵erence error (TDE) senses are all potentially di↵erent and are shown in the lower right. (VE,\r
BE, and PBE are all treated as the corresponding vectors in this figure.) The Bellman operator\r
takes a value function in the plane to one outside, which can then be projected back. If you\r
iteratively applied the Bellman operator outside the space (shown in gray above) you would\r
reach the true value function, as in conventional dynamic programming. If instead you kept\r
projecting back into the subspace at each step, as in the lower step shown in gray, then the fixed\r
point would be the point of vector-zero PBE.\r
projection operator ⇧ that takes an arbitrary value function to the representable function\r
that is closest in our norm:\r
⇧v .= vw where w = argmin w2Rdkv  vwk\r
2\r
µ . (11.12)\r
The representable value function that is closest to the true value function v⇡ is thus its\r
projection, ⇧v⇡, as suggested in Figure 11.3. This is the solution asymptotically found\r
by Monte Carlo methods, albeit often very slowly. The projection operation is discussed\r
more fully in the box on the next page.\r
TD methods find di↵erent solutions. To understand their rationale, recall that the\r
Bellman equation for value function v⇡ is\r
v⇡(s) = X\r
a\r
⇡(a|s)\r
X\r
s0,r\r
p(s0, r|s, a) [r + v⇡(s0)] , for all s 2 S. (11.16)"""

[[sections]]
number = "268"
title = "Chapter 11: O↵-policy Methods with Approximation"
text = """
The projection matrix\r
For a linear function approximator, the projection operation is linear, which implies\r
that it can be represented as an |S| ⇥ |S| matrix:\r
⇧ .= X X>DX1X>D, (11.13)\r
where, as in Section 9.4, D denotes the |S| ⇥ |S| diagonal matrix with the µ(s)\r
on the diagonal, and X denotes the |S| ⇥ d matrix whose rows are the feature\r
vectors x(s)>, one for each state s. If the inverse in (11.13) does not exist, then the\r
pseudoinverse is substituted. Using these matrices, the squared norm of a vector\r
can be written\r
kvk\r
2\r
µ = v>Dv, (11.14)\r
and the approximate linear value function can be written\r
vw = Xw. (11.15)\r
The true value function v⇡ is the only value function that solves (11.16) exactly. If an\r
approximate value function vw were substituted for v⇡, the di↵erence between the right\r
and left sides of the modified equation could be used as a measure of how far o↵ vw is\r
from v⇡. We call this the Bellman error at state s:\r
¯w(s) .\r
=\r
0\r
@X\r
a\r
⇡(a|s)\r
X\r
s0,r\r
p(s0, r|s, a) [r + vw(s0)]"""

[[sections]]
number = "1"
title = "A  vw(s) (11.17)"
text = """
= E⇡\r
⇥\r
Rt+1 + vw(St+1)  vw(St)\r
\r
 St = s, At ⇠ ⇡\r
⇤\r
, (11.18)\r
which shows clearly the relationship of the Bellman error to the TD error (11.3). The\r
Bellman error is the expectation of the TD error.\r
The vector of all the Bellman errors, at all states, ¯w 2 R|S|, is called the Bellman\r
error vector (shown as BE in Figure 11.3). The overall size of this vector, in the norm, is\r
an overall measure of the error in the value function, called the mean square Bellman\r
error :\r
BE(w) = \r
¯w\r
\r
\r
2\r
µ . (11.19)\r
It is not possible in general to reduce the BE to zero (at which point vw = v⇡), but for\r
linear function approximation there is a unique value of w for which the BE is minimized.\r
This point in the representable-function subspace (labeled min BE in Figure 11.3) is\r
di↵erent in general from that which minimizes the VE (shown as ⇧v⇡). Methods that\r
seek to minimize the BE are discussed in the next two sections.\r
The Bellman error vector is shown in Figure 11.3 as the result of applying the Bellman\r
operator B⇡ : R|S| ! R|S| to the approximate value function. The Bellman operator is"""

[[sections]]
number = "11.5"
title = "Gradient Descent in the Bellman Error 269"
text = """
defined by\r
(B⇡v)(s) .= X\r
a\r
⇡(a|s)\r
X\r
s0,r\r
p(s0, r|s, a) [r + v(s0)] , (11.20)\r
for all s 2 S, v : S ! R. The Bellman error vector for vw can be written ¯w = B⇡vw vw.\r
If the Bellman operator is applied to a value function in the representable subspace,\r
then, in general, it will produce a new value function that is outside the subspace, as\r
suggested in the figure. In dynamic programming (without function approximation), this\r
operator is applied repeatedly to the points outside the representable space, as suggested\r
by the gray arrows in the top of Figure 11.3. Eventually that process converges to the\r
true value function v⇡, the only fixed point for the Bellman operator, the only value\r
function for which\r
v⇡ = B⇡v⇡, (11.21)\r
which is just another way of writing the Bellman equation for ⇡ (11.16).\r
With function approximation, however, the intermediate value functions lying outside\r
the subspace cannot be represented. The gray arrows in the upper part of Figure 11.3\r
cannot be followed because after the first update (dark line) the value function must\r
be projected back into something representable. The next iteration then begins within\r
the subspace; the value function is again taken outside of the subspace by the Bellman\r
operator and then mapped back by the projection operator, as suggested by the lower\r
gray arrow and line. Following these arrows is a DP-like process with approximation.\r
In this case we are interested in the projection of the Bellman error vector back into\r
the representable space. This is the projected Bellman error vector ⇧¯w, shown in\r
Figure 11.3 as PBE. The size of this vector, in the norm, is another measure of error in\r
the approximate value function. For any approximate value function vw, we define the\r
mean square Projected Bellman error, denoted PBE, as\r
PBE(w) = \r
⇧¯w\r
\r
\r
2\r
µ . (11.22)\r
With linear function approximation there always exists an approximate value function\r
(within the subspace) with zero PBE; this is the TD fixed point, wTD, introduced in\r
Section 9.4. As we have seen, this point is not always stable under semi-gradient TD\r
methods and o↵-policy training. As shown in the figure, this value function is generally\r
di↵erent from those minimizing VE or BE. Methods that are guaranteed to converge to\r
it are discussed in Sections 11.7 and 11.8."""

[[sections]]
number = "11.5"
title = "Gradient Descent in the Bellman Error"
text = """
Armed with a better understanding of value function approximation and its various\r
objectives, we return now to the challenge of stability in o↵-policy learning. We would\r
like to apply the approach of stochastic gradient descent (SGD, Section 9.3), in which\r
updates are made that in expectation are equal to the negative gradient of an objective"""

[[sections]]
number = "270"
title = "Chapter 11: O↵-policy Methods with Approximation"
text = """
function. These methods always go downhill (in expectation) in the objective and because\r
of this are typically stable with excellent convergence properties. Among the algorithms\r
investigated so far in this book, only the Monte Carlo methods are true SGD methods.\r
These methods converge robustly under both on-policy and o↵-policy training as well\r
as for general nonlinear (di↵erentiable) function approximators, though they are often\r
slower than semi-gradient methods with bootstrapping, which are not SGD methods.\r
Semi-gradient methods may diverge under o↵-policy training, as we have seen earlier in\r
this chapter, and under contrived cases of nonlinear function approximation (Tsitsiklis\r
and Van Roy, 1997). With a true SGD method such divergence would not be possible.\r
The appeal of SGD is so strong that great e↵ort has gone into finding a practical\r
way of harnessing it for reinforcement learning. The starting place of all such e↵orts is\r
the choice of an error or objective function to optimize. In this and the next section\r
we explore the origins and limits of the most popular proposed objective function, that\r
based on the Bellman error introduced in the previous section. Although this has been a\r
popular and influential approach, the conclusion that we reach here is that it is a misstep\r
and yields no good learning algorithms. On the other hand, this approach fails in an\r
interesting way that o↵ers insight into what might constitute a good approach.\r
To begin, let us consider not the Bellman error, but something more immediate\r
and naive. Temporal di↵erence learning is driven by the TD error. Why not take the\r
minimization of the expected square of the TD error as the objective? In the general\r
function-approximation case, the one-step TD error with discounting is\r
t = Rt+1 + vˆ(St+1,wt)  vˆ(St,wt).\r
A possible objective function then is what one might call the mean square TD error :\r
TDE(w) = X\r
s2S\r
µ(s)E\r
⇥\r
2\r
t\r
\r
 St =s, At ⇠⇡\r
⇤\r
= X\r
s2S\r
µ(s)E\r
⇥\r
⇢t2\r
t\r
\r
 St =s, At ⇠b\r
⇤\r
= Eb\r
⇥\r
⇢t2\r
t\r
⇤\r
. (if µ is the distribution encountered under b)\r
The last equation is of the form needed for SGD; it gives the objective as an expectation\r
that can be sampled from experience (remember the experience is due to the behavior\r
policy b). Thus, following the standard SGD approach, one can derive the per-step update\r
based on a sample of this expected value:\r
wt+1 = wt  1\r
2\r
↵r(⇢t2\r
t )\r
= wt  ↵⇢ttrt\r
= wt + ↵⇢tt\r
\r
rvˆ(St,wt)  rvˆ(St+1,wt)\r
\r
, (11.23)\r
which you will recognize as the same as the semi-gradient TD algorithm (11.2) except for\r
the additional final term. This term completes the gradient and makes this a true SGD\r
algorithm with excellent convergence guarantees. Let us call this algorithm the naive"""

[[sections]]
number = "11.5"
title = "Gradient Descent in the Bellman Error 271"
text = """
residual-gradient algorithm (after Baird, 1995). Although the naive residual-gradient\r
algorithm converges robustly, it does not necessarily converge to a desirable place.\r
Example 11.2: A-split example,\r
showing the naivet´e of the naive residual-gradient algorithm\r
A\r
B\r
C\r
0\r
0\r
0"""

[[sections]]
number = "1"
title = "Consider the three-state episodic MRP shown to the right."
text = """
Episodes begin in state A and then ‘split’ stochastically, half\r
the time going to B (and then invariably going on to terminate\r
with a reward of 1) and half the time going to state C (and\r
then invariably terminating with a reward of zero). Reward for\r
the first transition, out of A, is always zero whichever way the\r
episode goes. As this is an episodic problem, we can take  to\r
be 1. We also assume on-policy training, so that ⇢t is always\r
1, and tabular function approximation, so that the learning algorithms are free to\r
give arbitrary, independent values to all three states. Thus, this should be an easy\r
problem.\r
What should the values be? From A, half the time the return is 1, and half the\r
time the return is 0; A should have value 1"""

[[sections]]
number = "2"
title = "From B the return is always 1, so its"
text = """
value should be 1, and similarly from C the return is always 0, so its value should\r
be 0. These are the true values and, as this is a tabular problem, all the methods\r
presented previously converge to them exactly.\r
However, the naive residual-gradient algorithm finds di↵erent values for B and\r
C. It converges with B having a value of 3\r
4 and C having a value of 14 (A converges\r
correctly to 1\r
2 ). These are in fact the values that minimize the TDE.\r
Let us compute the TDE for these values. The first transition of each episode is\r
either up from A’s 1\r
2 to B’s 34 , a change of 14 , or down from A’s 12 to C’s 14 , a change\r
of 1"""

[[sections]]
number = "4"
title = "Because the reward is zero on these transitions, and  = 1, these changes are"
text = """
the TD errors, and thus the squared TD error is always 1\r
16 on the first transition.\r
The second transition is similar; it is either up from B’s 3\r
4 to a reward of 1 (and a\r
terminal state of value 0), or down from C’s 1\r
4 to a reward of 0 (again with a terminal\r
state of value 0). Thus, the TD error is always ±1\r
4 , for a squared error of 116 on the\r
second step. Thus, for this set of values, the TDE on both steps is 1"""

[[sections]]
number = "16"
title = "Now let’s compute the TDE for the true values (B at 1, C at 0, and A at 1"
text = """
2 ). In this\r
case the first transition is either from 1\r
2 up to 1, at B, or from 12 down to 0, at C; in\r
either case the absolute error is 1\r
2 and the squared error is 14 . The second transition\r
has zero error because the starting value, either 1 or 0 depending on whether the\r
transition is from B or C, always exactly matches the immediate reward and return.\r
Thus the squared TD error is 1\r
4 on the first transition and 0 on the second, for a\r
mean reward over the two transitions of 1"""

[[sections]]
number = "8"
title = "As 18 is bigger that 116 , this solution is"
text = """
worse according to the TDE. On this simple problem, the true values do not have\r
the smallest TDE."""

[[sections]]
number = "272"
title = "Chapter 11: O↵-policy Methods with Approximation"
text = """
A tabular representation is used in the A-split example, so the true state values can\r
be exactly represented, yet the naive residual-gradient algorithm finds di↵erent values,\r
and these values have lower TDE than do the true values. Minimizing the TDE is naive;\r
by penalizing all TD errors it achieves something more like temporal smoothing than\r
accurate prediction.\r
A better idea would seem to be minimizing the mean square Bellman error (BE). If\r
the exact values are learned, the Bellman error is zero everywhere. Thus, a Bellman\u0002error-minimizing algorithm should have no trouble with the A-split example. We cannot\r
expect to achieve zero Bellman error in general, as it would involve finding the true value\r
function, which we presume is outside the space of representable value functions. But\r
getting close to this ideal is a natural-seeming goal. As we have seen, the Bellman error\r
is also closely related to the TD error. The Bellman error for a state is the expected TD\r
error in that state. So let’s repeat the derivation above with the expected TD error (all\r
expectations here are implicitly conditional on St):\r
wt+1 = wt  1\r
2\r
↵r(E⇡[t]\r
2\r
)\r
= wt  1\r
2\r
↵r(Eb[⇢tt]\r
2\r
)\r
= wt  ↵Eb[⇢tt] rEb[⇢tt]\r
= wt  ↵Eb\r
⇥\r
⇢t(Rt+1 + vˆ(St+1,w)  vˆ(St,w))⇤Eb[⇢trt]\r
= wt + ↵\r
h\r
Eb\r
⇥\r
⇢t(Rt+1 + vˆ(St+1,w))⇤ vˆ(St,w)\r
ihrvˆ(St,w)  Eb\r
⇥\r
⇢trvˆ(St+1,w)\r
⇤i\r
.\r
This update and various ways of sampling it are referred to as the residual-gradient\r
algorithm. If you simply used the sample values in all the expectations, then the equation\r
above reduces almost exactly to (11.23), the naive residual-gradient algorithm.1 But\r
this is naive, because the equation above involves the next state, St+1, appearing in two\r
expectations that are multiplied together. To get an unbiased sample of the product,\r
two independent samples of the next state are required, but during normal interaction\r
with an external environment only one is obtained. One expectation or the other can be\r
sampled, but not both.\r
There are two ways to make the residual-gradient algorithm work. One is in the case\r
of deterministic environments. If the transition to the next state is deterministic, then\r
the two samples will necessarily be the same, and the naive algorithm is valid. The\r
other way is to obtain two independent samples of the next state, St+1, from St, one for\r
the first expectation and another for the second expectation. In real interaction with\r
an environment, this would not seem possible, but when interacting with a simulated\r
environment, it is. One simply rolls back to the previous state and obtains an alternate\r
next state before proceeding forward from the first next state. In either of these cases the\r
residual-gradient algorithm is guaranteed to converge to a minimum of the BE under the\r
usual conditions on the step-size parameter. As a true SGD method, this convergence is\r
1For state values there remains a small di↵erence in the treatment of the importance sampling ratio\r
⇢t. In the analagous action-value case (which is the most important case for control algorithms), the\r
residual-gradient algorithm would reduce exactly to the naive version."""

[[sections]]
number = "11.5"
title = "Gradient Descent in the Bellman Error 273"
text = """
robust, applying to both linear and nonlinear function approximators. In the linear case,\r
convergence is always to the unique w that minimizes the BE.\r
However, there remain at least three ways in which the convergence of the residual\u0002gradient method is unsatisfactory. The first of these is that empirically it is slow, much\r
slower that semi-gradient methods. Indeed, proponents of this method have proposed\r
increasing its speed by combining it with faster semi-gradient methods initially, then\r
gradually switching over to residual gradient for the convergence guarantee (Baird and\r
Moore, 1999). The second way in which the residual-gradient algorithm is unsatisfactory\r
is that it still seems to converge to the wrong values. It does get the right values in all\r
tabular cases, such as the A-split example, as for those an exact solution to the Bellman\r
Example 11.3: A-presplit example, a counterexample for the BE\r
A1 B\r
C 0 0\r
0 1\r
A2\r
A\r
Consider the three-state episodic MRP shown to the\r
right: Episodes start in either A1 or A2, with equal\r
probability. These two states look exactly the same to\r
the function approximator, like a single state A whose\r
feature representation is distinct from and unrelated to\r
the feature representation of the other two states, B and\r
C, which are also distinct from each other. Specifically,\r
the parameter of the function approximator has three\r
components, one giving the value of state B, one giving the value of state C, and one\r
giving the value of both states A1 and A2. Other than the selection of the initial\r
state, the system is deterministic. If it starts in A1, then it transitions to B with a\r
reward of 0 and then on to termination with a reward of 1. If it starts in A2, then it\r
transitions to C, and then to termination, with both rewards zero.\r
To a learning algorithm, seeing only the features, the system looks identical to\r
the A-split example. The system seems to always start in A, followed by either\r
B or C with equal probability, and then terminating with a 1 or a 0 depending\r
deterministically on the previous state. As in the A-split example, the true values\r
of B and C are 1 and 0, and the best shared value of A1 and A2 is 1\r
2 , by symmetry.\r
Because this problem appears externally identical to the A-split example, we\r
already know what values will be found by the algorithms. Semi-gradient TD\r
converges to the ideal values just mentioned, while the naive residual-gradient\r
algorithm converges to values of 3\r
4 and 14 for B and C respectively. All state\r
transitions are deterministic, so the non-naive residual-gradient algorithm will also\r
converge to these values (it is the same algorithm in this case). It follows then that\r
this ‘naive’ solution must also be the one that minimizes the BE, and so it is. On a\r
deterministic problem, the Bellman errors and TD errors are all the same, so the\r
BE is always the same as the TDE. Optimizing the BE on this example gives rise to\r
the same failure mode as with the naive residual-gradient algorithm on the A-split\r
example."""

[[sections]]
number = "274"
title = "Chapter 11: O↵-policy Methods with Approximation"
text = """
equation is possible. But if we examine examples with genuine function approximation,\r
then the residual-gradient algorithm, and indeed the BE objective, seem to find the\r
wrong value functions. One of the most telling such examples is the variation on the\r
A-split example known as the A-presplit example, shown on the preceding page, in which\r
the residual-gradient algorithm finds the same poor solution as its naive version. This\r
example shows intuitively that minimizing the BE (which the residual-gradient algorithm\r
surely does) may not be a desirable goal.\r
The third way in which the convergence of the residual-gradient algorithm is not\r
satisfactory is explained in the next section. Like the second way, the third way is also\r
a problem with the BE objective itself rather than with any particular algorithm for\r
achieving it."""

[[sections]]
number = "11.6"
title = "The Bellman Error is Not Learnable"
text = """
The concept of learnability that we introduce in this section is di↵erent from that\r
commonly used in machine learning. There, a hypothesis is said to be “learnable” if\r
it is eciently learnable, meaning that it can be learned within a polynomial rather\r
than an exponential number of examples. Here we use the term in a more basic way,\r
to mean learnable at all, with any amount of experience. It turns out many quantities\r
of apparent interest in reinforcement learning cannot be learned even from an infinite\r
amount of experiential data. These quantities are well defined and can be computed\r
given knowledge of the internal structure of the environment, but cannot be computed\r
or estimated from the observed sequence of feature vectors, actions, and rewards.2 We\r
say that they are not learnable. It will turn out that the Bellman error objective (BE)\r
introduced in the last two sections is not learnable in this sense. That the Bellman error\r
objective cannot be learned from the observable data is probably the strongest reason\r
not to seek it.\r
To make the concept of learnability clear, let’s start with some simple examples.\r
Consider the two Markov reward processes3 (MRPs) diagrammed below:\r
0 2 0 2\r
2\r
0\r
w w w\r
Where two edges leave a state, both transitions are assumed to occur with equal probability,\r
and the numbers indicate the reward received. All the states appear the same; they all\r
produce the same single-component feature vector x = 1 and have approximated value\r
w. Thus, the only varying part of the data trajectory is the reward sequence. The left\r
MRP stays in the same state and emits an endless stream of 0s and 2s at random, each\r
with 0.5 probability. The right MRP, on every step, either stays in its current state or\r
2They would of course be estimated if the state sequence were observed rather than only the\r
corresponding feature vectors.\r
3All MRPs can be considered MDPs with a single action in all states; what we conclude about MRPs\r
here applies as well to MDPs."""

[[sections]]
number = "11.6"
title = "The Bellman Error is Not Learnable 275"
text = """
switches to the other, with equal probability. The reward is deterministic in this MRP,\r
always a 0 from one state and always a 2 from the other, but because the each state\r
is equally likely on each step, the observable data is again an endless stream of 0s and\r
2s at random, identical to that produced by the left MRP. (We can assume the right\r
MRP starts in one of two states at random with equal probability.) Thus, even given\r
an infinite amount of data, it would not be possible to tell which of these two MRPs\r
was generating it. In particular, we could not tell if the MRP has one state or two, is\r
stochastic or deterministic. These things are not learnable.\r
This pair of MRPs also illustrates that the VE objective (9.1) is not learnable. If\r
 = 0, then the true values of the three states (in both MRPs), left to right, are 1, 0,\r
and 2. Suppose w = 1. Then the VE is 0 for the left MRP and 1 for the right MRP.\r
Because the VE is di↵erent in the two problems, yet the data generated has the same\r
distribution, the VE cannot be learned. The VE is not a unique function of the data\r
distribution. And if it cannot be learned, then how could the VE possibly be useful as\r
an objective for learning?\r
If an objective cannot be learned, it does indeed draw its utility into question. In\r
the case of the VE, however, there is a way out. Note that the same solution, w = 1,\r
is optimal for both MRPs above (assuming µ is the same for the two indistinguishable\r
states in the right MRP). Is this a coincidence, or could it be generally true that all\r
MDPs with the same data distribution also have the same optimal parameter vector? If\r
this is true—and we will show next that it is—then the VE remains a usable objective.\r
The VE is not learnable, but the parameter that optimizes it is!\r
To understand this, it is useful to bring in another natural objective function, this\r
time one that is clearly learnable. One error that is always observable is that between the\r
value estimate at each time and the return from that time. The mean square return error,\r
denoted RE, is the expectation, under µ, of the square of this error. In the on-policy case\r
the RE can be written\r
RE(w) = E\r
h\r
Gt  vˆ(St,w)\r
2\r
i\r
= VE(w) + E\r
h\r
Gt  v⇡(St)\r
2\r
i\r
. (11.24)\r
Thus, the two objectives are the same except for a variance term that does not depend on\r
the parameter vector. The two objectives must therefore have the same optimal parameter\r
value w⇤. The overall relationships are summarized in the left side of Figure 11.4.\r
⇤\r
Exercise 11.4 Prove (11.24). Hint: Write the RE as an expectation over possible states\r
s of the expectation of the squared error given that St = s. Then add and subtract the\r
true value of state s from the error (before squaring), grouping the subtracted true value\r
with the return and the added true value with the estimated value. Then, if you expand\r
the square, the most complex term will end up being zero, leaving you with (11.24). ⇤\r
Now let us return to the BE. The BE is like the VE in that it can be computed from\r
knowledge of the MDP but is not learnable from data. But it is not like the VE in that its\r
minimum solution is not learnable. The box on the next page presents a counterexample—\r
two MRPs that generate the same data distribution but whose minimizing parameter\r
vector is di↵erent, proving that the optimal parameter vector is not a function of the"""

[[sections]]
number = "276"
title = "Chapter 11: O↵-policy Methods with Approximation"
text = """
Example 11.4: Counterexample to the learnability of the Bellman error\r
To show the full range of possibilities we need a slightly more complex pair of Markov\r
reward processes (MRPs) than those considered earlier. Consider the following two\r
MRPs:\r
A B\r
1\r
0\r
-1\r
A B\r
0\r
-1 B\u0001\r
0\r
1 -1\r
Where two edges leave a state, both transitions are assumed to occur with equal\r
probability, and the numbers indicate the reward received. The MRP on the left has\r
two states that are represented distinctly. The MRP on the right has three states,\r
two of which, B and B0, appear the same and must be given the same approximate\r
value. Specifically, w has two components and the value of state A is given by the first\r
component and the value of B and B0 is given by the second. The second MRP has\r
been designed so that equal time is spent in all three states, so we can take µ(s) = 1\r
3 ,\r
for all s.\r
Note that the observable data distribution is identical for the two MRPs. In both\r
cases the agent will see single occurrences of A followed by a 0, then some number\r
of apparent Bs, each followed by a 1 except the last, which is followed by a 1, then\r
we start all over again with a single A and a 0, etc. All the statistical details are the\r
same as well; in both MRPs, the probability of a string of k Bs is 2k.\r
Now suppose w = 0. In the first MRP, this is an exact solution, and the BE is\r
zero. In the second MRP, this solution produces a squared error in both B and B0 of\r
1, such that BE = µ(B)1 + µ(B0)1 = 2"""

[[sections]]
number = "3"
title = "These two MRPs, which generate the same"
text = """
data distribution, have di↵erent BEs; the BE is not learnable.\r
Moreover (and unlike the earlier example for the VE) the minimizing value of w\r
is di↵erent for the two MRPs. For the first MRP, w = 0 minimizes the BE for any\r
. For the second MRP, the minimizing w is a complicated function of , but in\r
the limit, as  ! 1, it is (1\r
2 , 0)>. Thus the solution that minimizes BE cannot be\r
estimated from data alone; knowledge of the MRP beyond what is revealed in the\r
data is required. In this sense, it is impossible in principle to pursue the BE as an\r
objective for learning.\r
It may be surprising that in the second MRP the BE-minimizing value of A is so far\r
from zero. Recall that A has a dedicated weight and thus its value is unconstrained\r
by function approximation. A is followed by a reward of 0 and transition to a state\r
with a value of nearly 0, which suggests vw(A) should be 0; why is its optimal\r
value substantially negative rather than 0? The answer is that making vw(A) negative\r
reduces the error upon arriving in A from B. The reward on this deterministic transition\r
is 1, which implies that B should have a value 1 more than A. Because B’s value is\r
approximately zero, A’s value is driven toward 1. The BE-minimizing value of ⇡ 1\r
2\r
for A is a compromise between reducing the errors on leaving and on entering A."""

[[sections]]
number = "11.6"
title = "The Bellman Error is Not Learnable 277"
text = """
MDP1 MDP2\r
MSBE1 MSBE2\r
policy together completely determine the probability distribution over da\r
Assume for the moment that the state, action, and reward sets are all \r
for any finite sequence  = 0, a0, r1,...,rk, k, there is a well defined pr\r
sibly zero) of it occuring as the initial portion of a trajectory, which we \r
P() = Pr{(S0) = 0, A0 = a0, R1 = r1,...,Rk = rk, (Sk) = k}. The \r
then is a complete characterization of a source of data trajectories. To know \r
everything about the statistics of the data, but it is still less than knowin\r
particular, the VE and BE objectives are readily computed from the MDP \r
Section 3, but these cannot be determined from P alone.\r
✓1 ✓2 ✓3 ✓4 BE1 BE2 MDP1 MDP2\r
✓\r
1 ✓2 ✓3 ✓4 BE1 BE2 MDP1 MDP2\r
TDE RE VE\r
The problem can be seen in very simple, POMDP-like examples, in which \r
data produced by two di↵erent MDPs is identical in every respect, yet the \r
In such a case the BE is literally not a function of the data, and thus ther\r
estimate it from data. One of the simplest examples is the pair of MDPs sh\r
A B\r
1\r
0\r
-1\r
A B\r
0\r
-1 \r
0"""

[[sections]]
number = "1"
title = "These MDPs have only one action (or, equivalently, no actions), so they are in"
text = """
chains. Where two edges leave a state, both possibilities are assumed to oc\r
probability. The numbers on the edges indicate the reward emitted if that ed\r
The MDP on the left has two states that are represented distinctly; each \r
weight so that they can take on any value. The MDP on the right has th\r
of which, B and B\r
, are represented identically and must be given the sam\r
value. We can imagine that the value of state A is given by the first comp\r
the value of B and B is given by the second. Notice that the observable d\r
for the two MDPs. In both cases the agent will see single occurrences of A \r
0, then some number of Bs each followed by a 1, except the last which i\r
1, then we start all over again with a single A and a 0, etc. All the detail\r
as well; in both MDPs, the probability of a string of k Bs is 2k. Now con\r
function v = 0. In the first MDP, this is an exact solution, and the overall \r
the second MDP, this solution produces an error in both B and B of 1, for \r
of pd(B) + d(B\r
), or p2/3 if the three states are equally weighted by d. T\r
which generate the same data, have di↵erent BEs. Thus, the BE cannot be \r
data alone; knowledge of the MDP beyond what is revealed in the data is r\r
Moreover, the two MDPs have di↵erent minimal-BE value functions.2 For \r
the minimal-BE value function is the exact solution v = 0 for any . For th\r
2. This is a critical observation, as it is possible for an error function to be unobservab\r
perfectly satisfactory for use in learning settings because the value that minimizes it c\r
from data. For example, this is what happens with the VE. The VE is not observable \r
policy together completely determine the probability distributio\r
Assume for the moment that the state, action, and reward se\r
for any finite sequence  = 0, a0, r1,...,rk, k, there is a well \r
sibly zero) of it occuring as the initial portion of a trajectory, \r
P() = Pr{(S0) = 0, A0 = a0, R1 = r1,...,Rk = rk, (Sk) = \r
then is a complete characterization of a source of data trajectorie\r
everything about the statistics of the data, but it is still less tha\r
particular, the VE and BE objectives are readily computed from \r
Section 3, but these cannot be determined from P alone.\r
✓1 ✓2 ✓3 ✓4 BE1 BE2 MDP1 MDP2\r
✓\r
1 ✓2 ✓3 ✓4 BE1 BE2 MDP1 MDP2\r
TDE RE VE\r
The problem can be seen in very simple, POMDP-like example\r
data produced by two di↵erent MDPs is identical in every respec\r
In such a case the BE is literally not a function of the data, and \r
estimate it from data. One of the simplest examples is the pair of \r
A B\r
1\r
0\r
-1\r
A \r
0"""

[[sections]]
number = "1"
title = "These MDPs have only one action (or, equivalently, no actions), so"
text = """
chains. Where two edges leave a state, both possibilities are assu\r
probability. The numbers on the edges indicate the reward emitted \r
The MDP on the left has two states that are represented distin\r
weight so that they can take on any value. The MDP on the ri\r
of which, B and B\r
, are represented identically and must be give\r
value. We can imagine that the value of state A is given by the \r
the value of B and B is given by the second. Notice that the ob\r
for the two MDPs. In both cases the agent will see single occurr\r
0, then some number of Bs each followed by a 1, except the la\r
1, then we start all over again with a single A and a 0, etc. All \r
as well; in both MDPs, the probability of a string of k Bs is 2k\r
function v = 0. In the first MDP, this is an exact solution, and t\r
the second MDP, this solution produces an error in both B and B\r
of pd(B) + d(B\r
), or p2/3 if the three states are equally weighte\r
which generate the same data, have di↵erent BEs. Thus, the BE c\r
data alone; knowledge of the MDP beyond what is revealed in th\r
Moreover, the two MDPs have di↵erent minimal-BE value funct\r
the minimal-BE value function is the exact solution v = 0 for any"""

[[sections]]
number = "2"
title = "This is a critical observation, as it is possible for an error function to be"
text = """
perfectly satisfactory for use in learning settings because the value that m\r
from data. For example, this is what happens with the VE. The VE is not \r
MSPBE\r
policy together completely determine the probability di\r
Assume for the moment that the state, action, and r\r
for any finite sequence  = 0, a0, r1,...,rk, k, there is \r
sibly zero) of it occuring as the initial portion of a tr\r
P() = Pr{(S0) = 0, A0 = a0, R1 = r1,...,Rk = rk, \r
then is a complete characterization of a source of data tr\r
everything about the statistics of the data, but it is stil\r
particular, the VE and BE objectives are readily comput\r
Section 3, but these cannot be determined from P alone.\r
✓1 ✓2 ✓3 ✓4 BE1 BE2 MDP1 \r
✓\r
1 ✓2 ✓3 ✓4 BE1 BE2 MDP1 \r
TDE RE VE\r
The problem can be seen in very simple, POMDP-like \r
data produced by two di↵erent MDPs is identical in ever\r
In such a case the BE is literally not a function of the d\r
estimate it from data. One of the simplest examples is th\r
A B\r
1\r
0\r
-1\r
A \r
These MDPs have only one action (or, equivalently, no act\r
chains. Where two edges leave a state, both possibilities \r
probability. The numbers on the edges indicate the reward \r
The MDP on the left has two states that are represente\r
weight so that they can take on any value. The MDP o\r
of which, B and B\r
, are represented identically and mus\r
value. We can imagine that the value of state A is given \r
the value of B and B is given by the second. Notice tha\r
for the two MDPs. In both cases the agent will see sing\r
0, then some number of Bs each followed by a 1, excep\r
1, then we start all over again with a single A and a 0, \r
as well; in both MDPs, the probability of a string of k B\r
function v = 0. In the first MDP, this is an exact soluti\r
the second MDP, this solution produces an error in both \r
of pd(B) + d(B\r
), or p2/3 if the three states are equally \r
which generate the same data, have di↵erent BEs. Thus, \r
data alone; knowledge of the MDP beyond what is reveal\r
Moreover, the two MDPs have di↵erent minimal-BE va\r
the minimal-BE value function is the exact solution v ="""

[[sections]]
number = "2"
title = "This is a critical observation, as it is possible for an error funct"
text = """
perfectly satisfactory for use in learning settings because the val\r
from data. For example, this is what happens with the VE. The"""

[[sections]]
number = "20"
title = "MSTDE"
text = """
policy together completely determine the proba\r
Assume for the moment that the state, action\r
for any finite sequence  = 0, a0, r1,...,rk, k\r
sibly zero) of it occuring as the initial portion \r
P() = Pr{(S0) = 0, A0 = a0, R1 = r1,...,R\r
then is a complete characterization of a source o\r
everything about the statistics of the data, but \r
particular, the VE and BE objectives are readily \r
Section 3, but these cannot be determined from \r
✓1 ✓2 ✓3 ✓4 BE1 BE2 \r
✓\r
1 ✓2 ✓3 ✓4 BE1 BE2 \r
TDE RE VE\r
The problem can be seen in very simple, POM\r
data produced by two di↵erent MDPs is identica\r
In such a case the BE is literally not a function \r
estimate it from data. One of the simplest exam\r
A B\r
1\r
0\r
-1\r
These MDPs have only one action (or, equivalent\r
chains. Where two edges leave a state, both pos\r
probability. The numbers on the edges indicate t\r
The MDP on the left has two states that are r\r
weight so that they can take on any value. The \r
of which, B and B\r
, are represented identically \r
value. We can imagine that the value of state A \r
the value of B and B is given by the second. N\r
for the two MDPs. In both cases the agent will \r
0, then some number of Bs each followed by a \r
1, then we start all over again with a single A a\r
as well; in both MDPs, the probability of a strin\r
function v = 0. In the first MDP, this is an exa\r
the second MDP, this solution produces an erro\r
of pd(B) + d(B\r
), or p2/3 if the three states ar\r
which generate the same data, have di↵erent BE\r
data alone; knowledge of the MDP beyond what \r
Moreover, the two MDPs have di↵erent minim\r
the minimal-BE value function is the exact solut"""

[[sections]]
number = "2"
title = "This is a critical observation, as it is possible for an"
text = """
perfectly satisfactory for use in learning settings beca\r
from data. For example, this is what happens with th"""

[[sections]]
number = "20"
title = "Data"
text = """
distribution\r
MDP1 MDP2\r
MSVE1 MSVE2\r
MSRE\r
Data\r
distribution \r
mine the probability distribution over data trajectories.\r
e state, action, and reward sets are all finite. Then,\r
0, r1,...,rk, k, there is a well defined probability (pos\u0002\r
initial portion of a trajectory, which we may denoted\r
R1 = r1,...,Rk = rk, (Sk) = k}. The distribution P\r
n of a source of data trajectories. To know P is to know\r
the data, but it is still less than knowing the MDP. In\r
ives are readily computed from the MDP as described in\r
termined from P alone.\r
BE1 BE2 MDP1 MDP2 PBE ✓\r
✓"""

[[sections]]
number = "4"
title = "BE1 BE2 MDP1 MDP2"
text = """
ry simple, POMDP-like examples, in which the observable\r
DPs is identical in every respect, yet the BE is di↵erent.\r
not a function of the data, and thus there is no way to\r
simplest examples is the pair of MDPs shown below:\r
A B\r
0\r
-1 B\u0001\r
0\r
1 -1\r
(or, equivalently, no actions), so they are in e↵ect Markov\r
state, both possibilities are assumed to occur with equal\r
dges indicate the reward emitted if that edge is traversed.\r
ates that are represented distinctly; each has a separate\r
any value. The MDP on the right has three states, two\r
ed identically and must be given the same approximate\r
value of state A is given by the first component of ✓ and\r
the second. Notice that the observable data is identical\r
the agent will see single occurrences of A followed by a\r
followed by a 1, except the last which is followed by a\r
th a single A and a 0, etc. All the details are the same\r
bility of a string of k Bs is 2k. Now consider the value\r
, this is an exact solution, and the overall BE is zero. In\r
oduces an error in both B and B of 1, for an overall BE\r
three states are equally weighted by d. The two MDPs,\r
ve di↵erent BEs. Thus, the BE cannot be estimated from\r
P beyond what is revealed in the data is required.\r
di↵erent minimal-BE value functions.2 For the first MDP,\r
the exact solution v = 0 for any . For the second MDP,\r
iblfftitbbbld t till b\r
w⇤ w⇤\r
1 w⇤2\r
w⇤\r
3 w⇤4\r
VE1 VE2\r
RE\r
BE1 BE2\r
PBE TDE\r
Monte Carlo\r
objectives Bootstrapping\r
objectives\r
Figure 11.4: Causal relationships among the data distribution, MDPs, and various objectives.\r
Left, Monte Carlo objectives: Two di↵erent MDPs can produce the same data distribution\r
yet also produce di↵erent VEs, proving that the VE objective cannot be determined from data\r
and is not learnable. However, all such VEs must have the same optimal parameter vector, w⇤!\r
Moreover, this same w⇤ can be determined from another objective, the RE, which is uniquely\r
determined from the data distribution. Thus w⇤ and the RE are learnable even though the VEs\r
are not. Right, Bootstrapping objectives: Two di↵erent MDPs can produce the same data\r
distribution yet also produce di↵erent BEs and have di↵erent minimizing parameter vectors;\r
these are not learnable from the data distribution. The PBE and TDE objectives and their\r
(di↵erent) minima can be directly determined from data and thus are learnable.\r
data and thus cannot be learned from it. The other bootstrapping objectives that we\r
have considered, the PBE and TDE, can be determined from data (are learnable) and\r
determine optimal solutions that are in general di↵erent from each other and the BE\r
minimums. The general case is summarized in the right side of Figure 11.4.\r
Thus, the BE is not learnable; it cannot be estimated from feature vectors and other\r
observable data. This limits the BE to model-based settings. There can be no algorithm\r
that minimizes the BE without access to the underlying MDP states beyond the feature\r
vectors. The residual-gradient algorithm is only able to minimize BE because it is allowed\r
to double sample from the same state—not a state that has the same feature vector,\r
but one that is guaranteed to be the same underlying state. We can see now that there\r
is no way around this. Minimizing the BE requires some such access to the nominal,\r
underlying MDP. This is an important limitation of the BE beyond that identified in the\r
A-presplit example on page 273. All this directs more attention toward the PBE."""

[[sections]]
number = "278"
title = "Chapter 11: O↵-policy Methods with Approximation"
text = ""

[[sections]]
number = "11.7"
title = "Gradient-TD Methods"
text = """
We now consider SGD methods for minimizing the PBE. As true SGD methods, these\r
Gradient-TD methods have robust convergence properties even under o↵-policy training\r
and nonlinear function approximation. Remember that in the linear case there is always\r
an exact solution, the TD fixed point wTD, at which the PBE is zero. This solution could\r
be found by least-squares methods (Section 9.8), but only by methods of quadratic O(d2)\r
complexity in the number of parameters. We seek instead an SGD method, which should\r
be O(d) and have robust convergence properties. Gradient-TD methods come close to\r
achieving these goals, at the cost of a rough doubling of computational complexity.\r
To derive an SGD method for the PBE (assuming linear function approximation) we\r
begin by expanding and rewriting the objective (11.22) in matrix terms:\r
PBE(w) = \r
⇧¯w\r
\r
\r
2\r
µ\r
= (⇧¯w)\r
>D⇧¯w (from (11.14))\r
= ¯>\r
w⇧>D⇧¯w\r
= ¯>\r
wDX\r
X>DX1X>D¯w (11.25)\r
(using (11.13) and the identity ⇧>D⇧ = DX X>DX1 X>D)\r
= X>D¯w\r
>\r
X>DX1X>D¯w\r
\r
. (11.26)\r
The gradient with respect to w is\r
rPBE(w)=2r⇥\r
X>D¯w\r
⇤> \r
X>DX1X>D¯w\r
\r
.\r
To turn this into an SGD method, we have to sample something on every time step that\r
has this quantity as its expected value. Let us take µ to be the distribution of states\r
visited under the behavior policy. All three of the factors above can then be written in\r
terms of expectations under this distribution. For example, the last factor can be written\r
X>D¯w = X\r
s\r
µ(s)x(s)¯w(s) = E[⇢ttxt] ,\r
which is just the expectation of the semi-gradient TD(0) update (11.2). The first factor\r
is the transpose of the gradient of this update:\r
rE[⇢ttxt]\r
> = E\r
⇥\r
⇢tr>\r
t x>t\r
⇤\r
= E\r
⇥\r
⇢tr(Rt+1 + w>xt+1  w>xt)\r
>x>\r
t\r
⇤ (using episodic t)\r
= E\r
⇥\r
⇢t(xt+1  xt)x>\r
t\r
⇤\r
.\r
Finally, the middle factor is the inverse of the expected outer-product matrix of the\r
feature vectors:\r
X>DX = X\r
s\r
µ(s)x(s)x(s)\r
> = E\r
⇥\r
xtx>\r
t\r
⇤\r
."""

[[sections]]
number = "11.7"
title = "Gradient-TD Methods 279"
text = """
Substituting these expectations for the three factors in our expression for the gradient of\r
the PBE, we get\r
rPBE(w)=2E\r
⇥\r
⇢t(xt+1  xt)x>\r
t\r
⇤\r
E\r
⇥\r
xtx>\r
t\r
⇤1\r
E[⇢ttxt] . (11.27)\r
It might not be obvious that we have made any progress by writing the gradient in this\r
form. It is a product of three expressions and the first and last are not independent.\r
They both depend on the next feature vector xt+1; we cannot simply sample both of\r
these expectations and then multiply the samples. This would give us a biased estimate\r
of the gradient just as in the residual-gradient algorithm.\r
Another idea would be to estimate the three expectations separately and then combine\r
them to produce an unbiased estimate of the gradient. This would work, but would\r
require a lot of computational resources, particularly to store the first two expectations,\r
which are d ⇥ d matrices, and to compute the inverse of the second. This idea can be\r
improved. If two of the three expectations are estimated and stored, then the third could\r
be sampled and used in conjunction with the two stored quantities. For example, you\r
could store estimates of the second two quantities (using the increment inverse-updating\r
techniques in Section 9.8) and then sample the first expression. Unfortunately, the overall\r
algorithm would still be of quadratic complexity (of order O(d2)).\r
The idea of storing some estimates separately and then combining them with samples\r
is a good one and is also used in Gradient-TD methods. Gradient-TD methods estimate\r
and store the product of the second two factors in (11.27). These factors are a d ⇥ d\r
matrix and a d-vector, so their product is just a d-vector, like w itself. We denote this\r
second learned vector as v:\r
v ⇡ E\r
⇥\r
xtx>\r
t\r
⇤1\r
E[⇢ttxt] . (11.28)\r
This form is familiar to students of linear supervised learning. It is the solution to a linear\r
least-squares problem that tries to approximate ⇢tt from the features. The standard\r
SGD method for incrementally finding the vector v that minimizes the expected squared\r
error v>xt  ⇢tt\r
2 is known as the Least Mean Square (LMS) rule (here augmented\r
with an importance sampling ratio):\r
vt+1\r
.\r
= vt + ⇢t\r
\r
t  v>\r
t xt\r
\r
xt,\r
where  > 0 is another step-size parameter. We can use this method to e↵ectively achieve\r
(11.28) with O(d) storage and per-step computation.\r
Given a stored estimate vt approximating (11.28), we can update our main parameter\r
vector wt using SGD methods based on (11.27). The simplest such rule is\r
wt+1 = wt  1\r
2\r
↵rPBE(wt) (the general SGD rule)\r
= wt  1\r
2\r
↵2E\r
⇥\r
⇢t(xt+1  xt)x>\r
t\r
⇤\r
E\r
⇥\r
xtx>\r
t\r
⇤1\r
E[⇢ttxt] (from (11.27))\r
= wt + ↵E\r
⇥\r
⇢t(xt  xt+1)x>\r
t\r
⇤\r
E\r
⇥\r
xtx>\r
t\r
⇤1\r
E[⇢ttxt] (11.29)\r
⇡ wt + ↵E\r
⇥\r
⇢t(xt  xt+1)x>\r
t\r
⇤\r
vt (based on (11.28))\r
⇡ wt + ↵⇢t (xt  xt+1) x>\r
t vt. (sampling)"""

[[sections]]
number = "280"
title = "Chapter 11: O↵-policy Methods with Approximation"
text = """
This algorithm is called GTD2. Note that if the final inner product (x>\r
t vt) is done first,\r
then the entire algorithm is of O(d) complexity.\r
A slightly better algorithm can be derived by doing a few more analytic steps before\r
substituting in vt. Continuing from (11.29):\r
wt+1 = wt + ↵E\r
⇥\r
⇢t(xt  xt+1)x>\r
t\r
⇤\r
E\r
⇥\r
xtx>\r
t\r
⇤1\r
E[⇢ttxt]\r
= wt + ↵ E\r
⇥\r
⇢txtx>\r
t\r
⇤\r
 E\r
⇥\r
⇢txt+1x>\r
t\r
⇤ E⇥\r
xtx>\r
t\r
⇤1\r
E[⇢ttxt]\r
= wt + ↵ E\r
⇥\r
xtx>\r
t\r
⇤\r
 E\r
⇥\r
⇢txt+1x>\r
t\r
⇤ E⇥\r
xtx>\r
t\r
⇤1\r
E[⇢ttxt]\r
= wt + ↵\r
⇣\r
E[⇢ttxt]  E\r
⇥\r
⇢txt+1x>\r
t\r
⇤\r
E\r
⇥\r
xtx>\r
t\r
⇤1\r
E[⇢ttxt]\r
⌘\r
⇡ wt + ↵ E[⇢ttxt]  E\r
⇥\r
⇢txt+1x>\r
t\r
⇤\r
vt\r
 (based on (11.28))\r
⇡ wt + ↵⇢t\r
\r
txt  xt+1x>\r
t vt\r
\r
, (sampling)\r
which again is O(d) if the final product (x>\r
t vt) is done first. This algorithm is known as\r
either TD(0) with gradient correction (TDC) or, alternatively, as GTD(0).\r
Figure 11.5 shows a sample and the expected behavior of TDC on Baird’s counterex\u0002ample. As intended, the PBE falls to zero, but note that the individual components\r
of the parameter vector do not approach zero. In fact, these values are still far from\r
w1– w6\r
w8 w8\r
p\r
VE p\r
VE\r
p\r
PBE p\r
PBE\r
10\r
5\r
2\r
0\r
-2.34\r
0 1000 0 1000 Steps\r
w7\r
w1– w6\r
TDC Expected TDC\r
Sweeps\r
w7\r
Figure 11.5: The behavior of the TDC algorithm on Baird’s counterexample. On the left is\r
shown a typical single run, and on the right is shown the expected behavior of this algorithm if\r
the updates are done synchronously (analogous to (11.9), except for the two TDC parameter\r
vectors). The step sizes were ↵ = 0.005 and  = 0.05."""

[[sections]]
number = "11.8"
title = "Emphatic-TD Methods 281"
text = """
an optimal solution, vˆ(s) = 0, for all s, for which w would have to be proportional to\r
(1, 1, 1, 1, 1, 1, 4, 2)>. After 1000 iterations we are still far from an optimal solution, as\r
we can see from the VE, which remains almost 2. The system is actually converging to\r
an optimal solution, but progress is extremely slow because the PBE is already so close\r
to zero.\r
GTD2 and TDC both involve two learning processes, a primary one for w and a\r
secondary one for v. The logic of the primary learning process relies on the secondary\r
learning process having finished, at least approximately, whereas the secondary learning\r
process proceeds without being influenced by the first. We call this sort of asymmetrical\r
dependence a cascade. In cascades we often assume that the secondary learning process\r
is proceeding faster and thus is always at its asymptotic value, ready and accurate to\r
assist the primary learning process. The convergence proofs for these methods often make\r
this assumption explicitly. These are called two-time-scale proofs. The fast time scale is\r
that of the secondary learning process, and the slower time scale is that of the primary\r
learning process. If ↵ is the step size of the primary learning process, and  is the step\r
size of the secondary learning process, then these convergence proofs will typically require\r
that in the limit  ! 0 and ↵\r
 ! 0.\r
Gradient-TD methods are currently the most well understood and widely used stable\r
o↵-policy methods. There are extensions to action values and control (GQ, Maei et al.,\r
2010), to eligibility traces (GTD() and GQ(), Maei, 2011; Maei and Sutton, 2010), and\r
to nonlinear function approximation (Maei et al., 2009). There have also been proposed\r
hybrid algorithms midway between semi-gradient TD and gradient TD (Hackman, 2012;\r
White and White, 2016). Hybrid-TD algorithms behave like Gradient-TD algorithms in\r
states where the target and behavior policies are very di↵erent, and behave like semi\u0002gradient algorithms in states where the target and behavior policies are the same. Finally,\r
the Gradient-TD idea has been combined with the ideas of proximal methods and control\r
variates to produce more ecient methods (Mahadevan et al., 2014; Du et al., 2017)."""

[[sections]]
number = "11.8"
title = "Emphatic-TD Methods"
text = """
We turn now to the second major strategy that has been extensively explored for obtaining\r
a cheap and ecient o↵-policy learning method with function approximation. Recall\r
that linear semi-gradient TD methods are ecient and stable when trained under the\r
on-policy distribution, and that we showed in Section 9.4 that this has to do with the\r
positive definiteness of the matrix A (9.11)4 and the match between the on-policy state\r
distribution µ⇡ and the state-transition probabilities p(s|s, a) under the target policy. In\r
o↵-policy learning, we reweight the state transitions using importance sampling so that\r
they become appropriate for learning about the target policy, but the state distribution\r
is still that of the behavior policy. There is a mismatch. A natural idea is to somehow\r
reweight the states, emphasizing some and de-emphasizing others, so as to return the\r
distribution of updates to the on-policy distribution. There would then be a match,\r
and stability and convergence would follow from existing results. This is the idea of\r
4In the o↵-policy case, the matrix A is generally defined as Es⇠b\r
⇥\r
x(s)E\r
⇥\r
x(St+1)>\r
 St =s, At ⇠⇡\r
⇤⇤."""

[[sections]]
number = "282"
title = "Chapter 11: O↵-policy Methods with Approximation"
text = """
Emphatic-TD methods, first introduced for on-policy training in Section 9.11.\r
Actually, the notion of “the on-policy distribution” is not quite right, as there are many\r
on-policy distributions, and any one of these is sucient to guarantee stability. Consider\r
an undiscounted episodic problem. The way episodes terminate is fully determined by the\r
transition probabilities, but there may be several di↵erent ways the episodes might begin.\r
However the episodes start, if all state transitions are due to the target policy, then the\r
state distribution that results is an on-policy distribution. You might start close to the\r
terminal state and visit only a few states with high probability before ending the episode.\r
Or you might start far away and pass through many states before terminating. Both are\r
on-policy distributions, and training on both with a linear semi-gradient method would\r
be guaranteed to be stable. However the process starts, an on-policy distribution results\r
as long as all states encountered are updated up until termination.\r
If there is discounting, it can be treated as partial or probabilistic termination for these\r
purposes. If  = 0.9, then we can consider that with probability 0.1 the process terminates\r
on every time step and then immediately restarts in the state that is transitioned to. A\r
discounted problem is one that is continually terminating and restarting with probability\r
1   on every step. This way of thinking about discounting is an example of a more\r
general notion of pseudo termination—termination that does not a↵ect the sequence of\r
state transitions, but does a↵ect the learning process and the quantities being learned.\r
This kind of pseudo termination is important to o↵-policy learning because the restarting\r
is optional—remember we can start any way we want to—and the termination relieves\r
the need to keep including encountered states within the on-policy distribution. That is,\r
if we don’t consider the new states as restarts, then discounting quickly gives us a limited\r
on-policy distribution.\r
The one-step Emphatic-TD algorithm for learning episodic state values is defined by:\r
t = Rt+1 + vˆ(St+1,wt)  vˆ(St,wt),\r
wt+1 = wt + ↵Mt⇢ttrvˆ(St,wt),\r
Mt = ⇢t1Mt1 + It,\r
with It, the interest, being arbitrary and Mt, the emphasis, being initialized to M1 = 0.\r
How does this algorithm perform on Baird’s counterexample? Figure 11.6 shows the\r
trajectory in expectation of the components of the parameter vector (for the case in\r
which It = 1, for all t). There are some oscillations but eventually everything converges\r
and the VE goes to zero. These trajectories are obtained by iteratively computing the\r
expectation of the parameter vector trajectory without any of the variance due to sampling\r
of transitions and rewards. We do not show the results of applying the Emphatic-TD\r
algorithm directly because its variance on Baird’s counterexample is so high that it is\r
nigh impossible to get consistent results in computational experiments. The algorithm\r
converges to the optimal solution in theory on this problem, but in practice it does\r
not. We turn to the topic of reducing the variance of all these algorithms in the next\r
section."""

[[sections]]
number = "11.9"
title = "Reducing Variance 283"
text = """
Sweeps\r
w1– w6\r
w7\r
w8\r
p\r
VE\r
10\r
5\r
2\r
0\r
-5\r
0 1000\r
Figure 11.6: The behavior of the one-step Emphatic-TD algorithm in expectation on Baird’s\r
counterexample. The step size was ↵ = 0.03."""

[[sections]]
number = "11.9"
title = "Reducing Variance"
text = """
O↵-policy learning is inherently of greater variance than on-policy learning. This is not\r
surprising; if you receive data less closely related to a policy, you should expect to learn\r
less about the policy’s values. In the extreme, one may be able to learn nothing. You\r
can’t expect to learn how to drive by cooking dinner, for example. Only if the target and\r
behavior policies are related, if they visit similar states and take similar actions, should\r
one be able to make significant progress in o↵-policy training.\r
On the other hand, any policy has many neighbors, many similar policies with con\u0002siderable overlap in states visited and actions chosen, and yet which are not identical.\r
The raison d’ˆetre of o↵-policy learning is to enable generalization to this vast number\r
of related-but-not-identical policies. The problem remains of how to make the best use\r
of the experience. Now that we have some methods that are stable in expected value\r
(if the step sizes are set right), attention naturally turns to reducing the variance of the\r
estimates. There are many possible ideas, and we can just touch on a few of them in this\r
introductory text.\r
Why is controlling variance especially critical in o↵-policy methods based on importance\r
sampling? As we have seen, importance sampling often involves products of policy ratios.\r
The ratios are always one in expectation (5.13), but their actual values may be very high\r
or as low as zero. Successive ratios are uncorrelated, so their products are also always one\r
in expected value, but they can be of very high variance. Recall that these ratios multiply\r
the step size in SGD methods, so high variance means taking steps that vary greatly in\r
their sizes. This is problematic for SGD because of the occasional very large steps. They\r
must not be so large as to take the parameter to a part of the space with a very di↵erent\r
gradient. SGD methods rely on averaging over multiple steps to get a good sense of\r
the gradient, and if they make large moves from single samples they become unreliable.\r
If the step-size parameter is set small enough to prevent this, then the expected step"""

[[sections]]
number = "284"
title = "Chapter 11: O↵-policy Methods with Approximation"
text = """
can end up being very small, resulting in very slow learning. The notions of momentum\r
(Derthick, 1984), of Polyak-Ruppert averaging (Polyak, 1990; Ruppert, 1988; Polyak and\r
Juditsky, 1992), or further extensions of these ideas may significantly help. Methods for\r
adaptively setting separate step sizes for di↵erent components of the parameter vector\r
are also pertinent (e.g., Jacobs, 1988; Sutton, 1992b, c), as are the “importance weight\r
aware” updates of Karampatziakis and Langford (2010).\r
In Chapter 5 we saw how weighted importance sampling is significantly better behaved,\r
with lower variance updates, than ordinary importance sampling. However, adapting\r
weighted importance sampling to function approximation is challenging and can probably\r
only be done approximately with O(d) complexity (Mahmood and Sutton, 2015).\r
The Tree Backup algorithm (Section 7.5) shows that it is possible to perform some\r
o↵-policy learning without using importance sampling. This idea has been extended to\r
the o↵-policy case to produce stable and more ecient methods by Munos, Stepleton,\r
Harutyunyan, and Bellemare (2016) and by Mahmood, Yu and Sutton (2017).\r
Another, complementary strategy is to allow the target policy to be determined in\r
part by the behavior policy, in such a way that it never can be so di↵erent from it to\r
create large importance sampling ratios. For example, the target policy can be defined by\r
reference to the behavior policy, as in the “recognizers” proposed by Precup et al. (2006)."""

[[sections]]
number = "11.10"
title = "Summary"
text = """
O↵-policy learning is a tempting challenge, testing our ingenuity in designing stable and\r
ecient learning algorithms. Tabular Q-learning makes o↵-policy learning seem easy,\r
and it has natural generalizations to Expected Sarsa and to the Tree Backup algorithm.\r
But as we have seen in this chapter, the extension of these ideas to significant function\r
approximation, even linear function approximation, involves new challenges and forces us\r
to deepen our understanding of reinforcement learning algorithms.\r
Why go to such lengths? One reason to seek o↵-policy algorithms is to give flexibility\r
in dealing with the tradeo↵ between exploration and exploitation. Another is to free\r
behavior from learning, and avoid the tyranny of the target policy. TD learning appears\r
to hold out the possibility of learning about multiple things in parallel, of using one\r
stream of experience to solve many tasks simultaneously. We can certainly do this in\r
special cases, just not in every case that we would like to or as eciently as we would\r
like to.\r
In this chapter we divided the challenge of o↵-policy learning into two parts. The\r
first part, correcting the targets of learning for the behavior policy, is straightforwardly\r
dealt with using the techniques devised earlier for the tabular case, albeit at the cost of\r
increasing the variance of the updates and thereby slowing learning. High variance will\r
probably always remains a challenge for o↵-policy learning.\r
The second part of the challenge of o↵-policy learning emerges as the instability\r
of semi-gradient TD methods that involve bootstrapping. We seek powerful function\r
approximation, o↵-policy learning, and the eciency and flexibility of bootstrapping"""

[[sections]]
number = "11.10"
title = "Summary 285"
text = """
TD methods, but it is challenging to combine all three aspects of this deadly triad in\r
one algorithm without introducing the potential for instability. There have been several\r
attempts. The most popular has been to seek to perform true stochastic gradient descent\r
(SGD) in the Bellman error (a.k.a. the Bellman residual). However, our analysis concludes\r
that this is not an appealing goal in many cases, and that anyway it is impossible to\r
achieve with a learning algorithm—the gradient of the BE is not learnable from experience\r
that reveals only feature vectors and not underlying states. Another approach, Gradient\u0002TD methods, performs SGD in the projected Bellman error. The gradient of the PBE\r
is learnable with O(d) complexity, but at the cost of a second parameter vector with a\r
second step size. The newest family of methods, Emphatic-TD methods, refine an old idea\r
for reweighting updates, emphasizing some and de-emphasizing others. In this way they\r
restore the special properties that make on-policy learning stable with computationally\r
simple semi-gradient methods.\r
The whole area of o↵-policy learning is relatively new and unsettled. Which methods\r
are best or even adequate is not yet clear. Are the complexities of the new methods\r
introduced at the end of this chapter really necessary? Which of them can be combined\r
e↵ectively with variance reduction methods? The potential for o↵-policy learning remains\r
tantalizing, the best way to achieve it still a mystery.\r
Bibliographical and Historical Remarks"""

[[sections]]
number = "11.1"
title = "The first semi-gradient method was linear TD() (Sutton, 1988). The name"
text = """
“semi-gradient” is more recent (Sutton, 2015a). Semi-gradient o↵-policy TD(0)\r
with general importance-sampling ratio may not have been explicitly stated until\r
Sutton, Mahmood, and White (2016), but the action-value forms were introduced\r
by Precup, Sutton, and Singh (2000), who also did eligibility trace forms of these\r
algorithms (see Chapter 12). Their continuing, undiscounted forms have not\r
been significantly explored. The n-step forms given here are new."""

[[sections]]
number = "11.2"
title = "The earliest w-to-2w example was given by Tsitsiklis and Van Roy (1996), who"
text = """
also introduced the specific counterexample in the box on page 263. Baird’s\r
counterexample is due to Baird (1995), though the version we present here is\r
slightly modified. Averaging methods for function approximation were developed\r
by Gordon (1995, 1996b). Other examples of instability with o↵-policy DP\r
methods and more complex methods of function approximation are given by\r
Boyan and Moore (1995). Bradtke (1993) gives an example in which Q-learning\r
using linear function approximation in a linear quadratic regulation problem\r
converges to a destabilizing policy."""

[[sections]]
number = "11.3"
title = "The deadly triad was first identified by Sutton (1995b) and thoroughly analyzed"
text = """
by Tsitsiklis and Van Roy (1997). The name “deadly triad” is due to Sutton\r
(2015a).\r
11.4 This kind of linear analysis was pioneered by Tsitsiklis and Van Roy (1996; 1997),\r
including the dynamic programming operator. Diagrams like Figure 11.3 were"""

[[sections]]
number = "286"
title = "Chapter 11: O↵-policy Methods with Approximation"
text = """
introduced by Lagoudakis and Parr (2003).\r
What we have called the Bellman operator, and denoted B⇡, is more commonly\r
denoted T ⇡ and called a “dynamic programming operator,” while a generalized\r
form, denoted T(), is called the “TD() operator” (Tsitsiklis and Van Roy, 1996,\r
1997)."""

[[sections]]
number = "11.5"
title = "The BE was first proposed as an objective function for dynamic programming by"
text = """
Schweitzer and Seidmann (1985). Baird (1995, 1999) extended it to TD learning\r
based on stochastic gradient descent. In the literature, BE minimization is often\r
referred to as Bellman residual minimization.\r
The earliest A-split example is due to Dayan (1992). The two forms given here\r
were introduced by Sutton et al. (2009a)."""

[[sections]]
number = "11.6"
title = "The contents of this section are new to this text."
text = ""

[[sections]]
number = "11.7"
title = "Gradient-TD methods were introduced by Sutton, Szepesv´ari, and Maei (2009b)."
text = """
The methods highlighted in this section were introduced by Sutton et al. (2009a)\r
and Mahmood et al. (2014). A major extension to proximal TD methods\r
was developed by Mahadevan et al. (2014). The most sensitive empirical\r
investigations to date of Gradient-TD and related methods are given by Geist\r
and Scherrer (2014), Dann, Neumann, and Peters (2014), White (2015), and\r
Ghiassian, Patterson, White, Sutton, and White (2018). Recent developments in\r
the theory of Gradient-TD methods are presented by Yu (2017)."""

[[sections]]
number = "11.8"
title = "Emphatic-TD methods were introduced by Sutton, Mahmood, and White (2016)."
text = """
Full convergence proofs and other theory were later established by Yu (2015;\r
2016; Yu, Mahmood, and Sutton, 2017), Hallak, Tamar, and Mannor (2015), and\r
Hallak, Tamar, Munos, and Mannor (2016).

Chapter 12\r
Eligibility Traces\r
Eligibility traces are one of the basic mechanisms of reinforcement learning. For example,\r
in the popular TD() algorithm, the  refers to the use of an eligibility trace. Almost any\r
temporal-di↵erence (TD) method, such as Q-learning or Sarsa, can be combined with\r
eligibility traces to obtain a more general method that may learn more eciently.\r
Eligibility traces unify and generalize TD and Monte Carlo methods. When TD\r
methods are augmented with eligibility traces, they produce a family of methods spanning\r
a spectrum that has Monte Carlo methods at one end (= 1) and one-step TD methods\r
at the other ( = 0). In between are intermediate methods that are often better than\r
either extreme method. Eligibility traces also provide a way of implementing Monte Carlo\r
methods online and on continuing problems without episodes.\r
Of course, we have already seen one way of unifying TD and Monte Carlo methods: the\r
n-step TD methods of Chapter 7. What eligibility traces o↵er beyond these is an elegant\r
algorithmic mechanism with significant computational advantages. The mechanism is\r
a short-term memory vector, the eligibility trace zt 2 Rd, that parallels the long-term\r
weight vector wt 2 Rd. The rough idea is that when a component of wt participates in\r
producing an estimated value, then the corresponding component of zt is bumped up and\r
then begins to fade away. Learning will then occur in that component of wt if a nonzero\r
TD error occurs before the trace falls back to zero. The trace-decay parameter  2 [0, 1]\r
determines the rate at which the trace falls.\r
The primary computational advantage of eligibility traces over n-step methods is that\r
only a single trace vector is required rather than a store of the last n feature vectors.\r
Learning also occurs continually and uniformly in time rather than being delayed and\r
then catching up at the end of the episode. In addition learning can occur and a↵ect\r
behavior immediately after a state is encountered rather than being delayed n steps.\r
Eligibility traces illustrate that a learning algorithm can sometimes be implemented in\r
a di↵erent way to obtain computational advantages. Many algorithms are most naturally\r
formulated and understood as an update of a state’s value based on events that follow\r
that state over multiple future time steps. For example, Monte Carlo methods (Chapter 5)\r
update a state based on all the future rewards, and n-step TD methods (Chapter 7)"""

[[sections]]
number = "288"
title = "Chapter 12: Eligibility Traces"
text = """
update based on the next n rewards and state n steps in the future. Such formulations,\r
based on looking forward from the updated state, are called forward views. Forward views\r
are always somewhat complex to implement because the update depends on later things\r
that are not available at the time. However, as we show in this chapter it is often possible\r
to achieve nearly the same updates—and sometimes exactly the same updates—with an\r
algorithm that uses the current TD error, looking backward to recently visited states\r
using an eligibility trace. These alternate ways of looking at and implementing learning\r
algorithms are called backward views. Backward views, transformations between forward\r
views and backward views, and equivalences between them, date back to the introduction\r
of temporal di↵erence learning but have become much more powerful and sophisticated\r
since 2014. Here we present the basics of the modern view.\r
As usual, first we fully develop the ideas for state values and prediction, then extend\r
them to action values and control. We develop them first for the on-policy case then\r
extend them to o↵-policy learning. Our treatment pays special attention to the case of\r
linear function approximation, for which the results with eligibility traces are stronger.\r
All these results apply also to the tabular and state aggregation cases because these are\r
special cases of linear function approximation."""

[[sections]]
number = "12.1"
title = "The -return"
text = """
In Chapter 7 we defined an n-step return as the sum of the first n rewards plus the\r
estimated value of the state reached in n steps, each appropriately discounted (7.1). The\r
general form of that equation, for any parameterized function approximator, is\r
Gt:t+n\r
.\r
= Rt+1 +Rt+2 +···+n1Rt+n +nvˆ(St+n,wt+n1), 0  t  T n, (12.1)\r
where vˆ(s,w) is the approximate value of state s given weight vector w (Chapter 9), and\r
T is the time of episode termination, if any. We noted in Chapter 7 that each n-step\r
return, for n  1, is a valid update target for a tabular learning update, just as it is for\r
an approximate SGD learning update such as (9.7).\r
Now we note that a valid update can be done not just toward any n-step return, but\r
toward any average of n-step returns for di↵erent ns. For example, an update can be\r
done toward a target that is half of a two-step return and half of a four-step return:\r
1\r
2Gt:t+2 + 12Gt:t+4. Any set of n-step returns can be averaged in this way, even an infinite\r
set, as long as the weights on the component returns are positive and sum to 1. The\r
composite return possesses an error reduction property similar to that of individual n-step\r
returns (7.3) and thus can be used to construct updates with guaranteed convergence\r
properties. Averaging produces a substantial new range of algorithms. For example, one\r
could average one-step and infinite-step returns to obtain another way of interrelating TD\r
and Monte Carlo methods. In principle, one could even average experience-based updates\r
with DP updates to get a simple combination of experience-based and model-based\r
methods (cf. Chapter 8).\r
An update that averages simpler component updates is called a compound update. The\r
backup diagram for a compound update consists of the backup diagrams for each of the\r
component updates with a horizontal line above them and the weighting fractions below."""

[[sections]]
number = "12.1"
title = "The -return 289"
text = """
1\r
2\r
1"""

[[sections]]
number = "2"
title = "For example, the compound update for the case mentioned at the start of"
text = """
this section, mixing half of a two-step return and half of a four-step return,\r
has the diagram shown to the right. A compound update can only be done\r
when the longest of its component updates is complete. The update at the\r
right, for example, could only be done at time t+ 4 for the estimate formed at\r
time t. In general one would like to limit the length of the longest component\r
update because of the corresponding delay in the updates.\r
The TD() algorithm can be understood as one particular way of averaging\r
n-step updates. This average contains all the n-step updates, each weighted\r
proportionally to n1 (where  2 [0, 1)), and is normalized by a factor of\r
1 to ensure that the weights sum to 1 (Figure 12.1). The resulting update\r
is toward a return, called the -return, defined in its state-based form by\r
G\r
t\r
.\r
= (1  )\r
X1\r
n=1\r
n1Gt:t+n. (12.2)\r
Figure 12.2 further illustrates the weighting on the sequence of n-step returns in the\r
-return. The one-step return is given the largest weight, 1  ; the two-step return is\r
given the next largest weight, (1); the three-step return is given the weight (1)2;\r
and so on. The weight fades by  with each additional step. After a terminal state has\r
been reached, all subsequent n-step returns are equal to the conventional return, Gt. If\r
1  \r
(1  )\r
(1  )2\r
T t1\r
···\r
···\r
St\r
At\r
At+1\r
AT 1\r
St+1 Rt+1\r
ST RT\r
···\r
St+2 Rt+2\r
At+2\r
TD()\r
X = 1\r
Figure 12.1: The backup diagram for TD(). If  = 0, then the overall update reduces to its\r
first component, the one-step TD update, whereas if  = 1, then the overall update reduces to\r
its last component, the Monte Carlo update."""

[[sections]]
number = "290"
title = "Chapter 12: Eligibility Traces"
text = """
1!"\r
weight given to\r
the 3-step return\r
decay by "\r
weight given to\r
actual, final return\r
t T\r
Time\r
Weight\r
total area = 1\r
is (1  )2\r
is T t1\r
Weighting\r
Figure 12.2: Weighting given in the -return to each of the n-step returns.\r
we want, we can separate these post-termination terms from the main sum, yielding\r
G\r
t = (1  )\r
T\r
Xt1\r
n=1\r
n1Gt:t+n + T t1Gt, (12.3)\r
as indicated in the figures. This equation makes it clearer what happens when  = 1. In\r
this case the main sum goes to zero, and the remaining term reduces to the conventional\r
return. Thus, for  = 1, updating according to the -return is a Monte Carlo algorithm.\r
On the other hand, if  = 0, then the -return reduces to Gt:t+1, the one-step return.\r
Thus, for  = 0, updating according to the -return is a one-step TD method.\r
Exercise 12.1 Just as the return can be written recursively in terms of the first reward and\r
itself one-step later (3.9), so can the -return. Derive the analogous recursive relationship\r
from (12.2) and (12.1). ⇤\r
Exercise 12.2 The parameter  characterizes how fast the exponential weighting in\r
Figure 12.2 falls o↵, and thus how far into the future the -return algorithm looks in\r
determining its update. But a rate factor such as  is sometimes an awkward way of\r
characterizing the speed of the decay. For some purposes it is better to specify a time\r
constant, or half-life. What is the equation relating  and the half-life, ⌧, the time by\r
which the weighting sequence will have fallen to half of its initial value? ⇤\r
We are now ready to define our first learning algorithm based on the -return: the\r
o↵-line -return algorithm. As an o↵-line algorithm, it makes no changes to the weight\r
vector during the episode. Then, at the end of the episode, a whole sequence of o↵-line\r
updates are made according to our usual semi-gradient rule, using the -return as the\r
target:\r
wt+1\r
.\r
= wt + ↵\r
h\r
G\r
t  vˆ(St,wt)\r
i\r
rvˆ(St,wt), t = 0,...,T  1. (12.4)"""

[[sections]]
number = "12.1"
title = "The -return 291"
text = """
The -return gives us an alternative way of moving smoothly between Monte Carlo\r
and one-step TD methods that can be compared with the n-step bootstrapping way\r
developed in Chapter 7. There we assessed e↵ectiveness on a 19-state random walk task\r
(Example 7.1, page 144). Figure 12.3 shows the performance of the o↵-line -return\r
algorithm on this task alongside that of the n-step methods (repeated from Figure 7.2).\r
The experiment was just as described earlier except that for the -return algorithm we\r
varied  instead of n. The performance measure used is the estimated root-mean-square\r
error between the correct and estimated values of each state measured at the end of\r
the episode, averaged over the first 10 episodes and the 19 states. Note that overall\r
performance of the o↵-line -return algorithms is comparable to that of the n-step\r
algorithms. In both cases we get best performance with an intermediate value of the\r
bootstrapping parameter, n for n-step methods and  for the o↵-line -return algorithm.\r
n-step TD methods\r
(from Chapter 7)\r
↵\r
Average\r
RMS error\r
over 19 states\r
and first 10 \r
episodes n=1\r
n=2 n=4 n=8\r
n=16\r
n=32\r
n=32 n=64 128 512256 0.55\r
0.5\r
0.45\r
0.35\r
0.3\r
0.25\r
0.4\r
0 0.2 0.4 0.6 0.8 1\r
0.55\r
0.5\r
0.45\r
0.35\r
0.3\r
0.25\r
0.4\r
0 0.2 0.4 0.6 0.8 1\r
Off line λ-return algorithm\r
↵\r
RMS error\r
at the end \r
of the episode\r
over the first\r
10 episodes λ=0\r
λ=.4\r
λ=.8\r
λ=.9\r
λ=.95\r
λ=.975\r
λ=.99 λ=1\r
λ=.95\r
-\r
Figure 12.3: 19-state Random walk results (Example 7.1): Performance of the o↵-line -return\r
algorithm alongside that of the n-step TD methods. In both case, intermediate values of the\r
bootstrapping parameter ( or n) performed best. The results with the o↵-line -return algorithm\r
are slightly better at the best values of ↵ and , and at high ↵.\r
The approach that we have been taking so far is what we call the theoretical, or\r
forward, view of a learning algorithm. For each state visited, we look forward in time to\r
all the future rewards and decide how best to combine them. We might imagine ourselves\r
riding the stream of states, looking forward from each state to determine its update, as\r
suggested by Figure 12.4. After looking forward from and updating one state, we move\r
on to the next and never have to work with the preceding state again. Future states,\r
on the other hand, are viewed and processed repeatedly, once from each vantage point\r
preceding them."""

[[sections]]
number = "292"
title = "Chapter 12: Eligibility Traces"
text = """
Time\r
r\r
t+3\r
rt+2\r
rt+1\r
r\r
T\r
st+1\r
st+2\r
st+3\r
st\r
St+1\r
St\r
St+2\r
St+3 Rt+3\r
Rt+2\r
Rt+1\r
RT\r
Figure 12.4: The forward view. We decide how to update each state by looking forward to\r
future rewards and states."""

[[sections]]
number = "12.2"
title = "TD()"
text = """
TD() is one of the oldest and most widely used algorithms in reinforcement learning.\r
It was the first algorithm for which a formal relationship was shown between a more\r
theoretical forward view and a more computationally-congenial backward view using\r
eligibility traces. Here we will show empirically that it approximates the o↵-line -return\r
algorithm presented in the previous section.\r
TD() improves over the o↵-line -return algorithm in three ways. First it updates\r
the weight vector on every step of an episode rather than only at the end, and thus\r
its estimates may be better sooner. Second, its computations are equally distributed\r
in time rather than all at the end of the episode. And third, it can be applied to\r
continuing problems rather than just to episodic problems. In this section we present the\r
semi-gradient version of TD() with function approximation.\r
With function approximation, the eligibility trace is a vector zt 2 Rd with the same\r
number of components as the weight vector wt. Whereas the weight vector is a long-term\r
memory, accumulating over the lifetime of the system, the eligibility trace is a short-term\r
memory, typically lasting less time than the length of an episode. Eligibility traces assist\r
in the learning process; their only consequence is that they a↵ect the weight vector, and\r
then the weight vector determines the estimated value.\r
In TD(), the eligibility trace vector is initialized to zero at the beginning of the\r
episode, is incremented on each time step by the value gradient, and then fades away by\r
:\r
z1\r
.\r
= 0,\r
zt\r
.\r
= zt1 + rvˆ(St,wt), 0  t  T, (12.5)\r
where  is the discount rate and  is the parameter introduced in the previous section,\r
which we henceforth call the trace-decay parameter. The eligibility trace keeps track\r
of which components of the weight vector have contributed, positively or negatively, to\r
recent state valuations, where “recent” is defined in terms of . (Recall that in linear\r
function approximation, rvˆ(St,wt) is the feature vector, xt, in which case the eligibility\r
trace vector is just a sum of past, fading, input vectors.) The trace is said to indicate\r
the eligibility of each component of the weight vector for undergoing learning changes"""

[[sections]]
number = "12.2"
title = "TD() 293"
text = """
should a reinforcing event occur. The reinforcing events we are concerned with are the\r
moment-by-moment one-step TD errors. The TD error for state-value prediction is\r
t\r
.\r
= Rt+1 + vˆ(St+1,wt)  vˆ(St,wt). (12.6)\r
In TD(), the weight vector is updated on each step proportional to the scalar TD error\r
and the vector eligibility trace:\r
wt+1\r
.\r
= wt + ↵t zt. (12.7)\r
Semi-gradient TD() for estimating vˆ ⇡ v⇡\r
Input: the policy ⇡ to be evaluated\r
Input: a di↵erentiable function ˆv : S+ ⇥ Rd ! R such that ˆv(terminal,·)=0\r
Algorithm parameters: step size ↵ > 0, trace decay rate  2 [0, 1]\r
Initialize value-function weights w arbitrarily (e.g., w = 0)\r
Loop for each episode:\r
Initialize S\r
z 0 (a d-dimensional vector)\r
Loop for each step of episode:\r
| Choose A ⇠ ⇡(·|S)\r
| Take action A, observe R, S0\r
| z z + rvˆ(S,w)\r
|  R + vˆ(S0,w)  vˆ(S,w)\r
| w w + ↵z\r
| S S0\r
until S0 is terminal\r
!t et et\r
et\r
et\r
Time\r
st\r
st+1\r
st-1\r
st-2\r
st-3\r
St\r
St+1\r
St-1\r
St-2\r
St-3\r
zt t zt\r
zt\r
zt\r
Figure 12.5: The backward or mechanistic view of TD(). Each update depends on the current\r
TD error combined with the current eligibility traces of past events."""

[[sections]]
number = "294"
title = "Chapter 12: Eligibility Traces"
text = """
TD() is oriented backward in time. At each moment we look at the current TD error\r
and assign it backward to each prior state according to how much that state contributed\r
to the current eligibility trace at that time. We might imagine ourselves riding along the\r
stream of states, computing TD errors, and shouting them back to the previously visited\r
states, as suggested by Figure 12.5. Where the TD error and traces come together, we\r
get the update given by (12.7), changing the values of those past states for when they\r
occur again in the future.\r
To better understand the backward view of TD(), consider what happens at various\r
values of . If  = 0, then by (12.5) the trace at t is exactly the value gradient\r
corresponding to St. Thus the TD() update (12.7) reduces to the one-step semi-gradient\r
TD update treated in Chapter 9 (and, in the tabular case, to the simple TD rule (6.2)).\r
This is why that algorithm was called TD(0). In terms of Figure 12.5, TD(0) is the\r
case in which only the one state preceding the current one is updated by the TD error\r
(other states may have their value estimates changed by generalization due to function\r
approximation). For larger values of , but still  < 1, more of the preceding states are\r
updated, but each more temporally distant state is updated less because the corresponding\r
eligibility trace is smaller, as suggested by the figure. We say that the earlier states are\r
given less credit for the TD error.\r
If  = 1, then the credit given to earlier states falls only by  per step. This turns out\r
to be just the right thing to do to achieve Monte Carlo behavior. For example, remember\r
that the TD error, t, includes an undiscounted term of Rt+1. In passing this back k\r
steps it needs to be discounted, like any reward in a return, by k, which is just what\r
the falling eligibility trace achieves. If  = 1 and  = 1, then the eligibility traces do not\r
decay at all with time. In this case the method behaves like a Monte Carlo method for\r
an undiscounted, episodic task. If  = 1, the algorithm is also known as TD(1).\r
TD(1) is a way of implementing Monte Carlo algorithms that is more general than those\r
presented earlier and that significantly increases their range of applicability. Whereas\r
the earlier Monte Carlo methods were limited to episodic tasks, TD(1) can be applied to\r
discounted continuing tasks as well. Moreover, TD(1) can be performed incrementally\r
and online. One disadvantage of Monte Carlo methods is that they learn nothing from\r
an episode until it is over. For example, if a Monte Carlo control method takes an action\r
that produces a very poor reward but does not end the episode, then the agent’s tendency\r
to repeat the action will be undiminished during the episode. Online TD(1), on the other\r
hand, learns in an n-step TD way from the incomplete ongoing episode, where the n\r
steps are all the way up to the current step. If something unusually good or bad happens\r
during an episode, control methods based on TD(1) can learn immediately and alter their\r
behavior on that same episode.\r
It is revealing to revisit the 19-state random walk example (Example 7.1) to see how\r
well TD() does in approximating the o↵-line -return algorithm. The results for both\r
algorithms are shown in Figure 12.6. For each  value, if ↵ is selected optimally for it (or\r
smaller), then the two algorithms perform virtually identically. If ↵ is chosen larger than\r
is optimal, however, then the -return algorithm is only a little worse whereas TD()\r
is much worse and may even be unstable. This is not catastrophic for TD() on this\r
problem, as these higher parameter values are not what one would want to use anyway,\r
but for other problems it can be a significant weakness.

12.3. n-step Truncated -return Methods 295\r
Off line λ-return algorithm\r
(from the previous section)\r
↵\r
λ=0\r
λ=.4\r
λ=.8\r
λ=.9\r
λ=.95 .975 .99 1\r
TD(λ)\r
↵\r
λ=.8\r
λ=.9\r
RMS error\r
at the end \r
of the episode\r
over the first\r
10 episodes\r
0 0.2 0.4 0.6 0.8 1\r
λ=0\r
λ=.4\r
λ=.8\r
λ=.9\r
λ=.95\r
λ=.975\r
λ=.99 λ=1\r
λ=.95\r
0.55\r
0.5\r
0.45\r
0.35\r
0.3\r
0.25\r
0.4\r
0 0.2 0.4 0.6 0.8 1\r
-\r
Figure 12.6: 19-state Random walk results (Example 7.1): Performance of TD() alongside\r
that of the o↵-line -return algorithm. The two algorithms performed virtually identically at\r
low (less than optimal) ↵ values, but TD() was worse at high ↵ values.\r
Linear TD() has been proved to converge in the on-policy case if the step-size\r
parameter is reduced over time according to the usual conditions (2.7). Just as discussed\r
in Section 9.4, convergence is not to the minimum-error weight vector, but to a nearby\r
weight vector that depends on . The bound on solution quality presented in that section\r
(9.14) can now be generalized to apply for any . For the continuing discounted case,\r
VE(w1) \r
1  \r
1  \r
min\r
w VE(w). (12.8)\r
That is, the asymptotic error is no more than 1\r
1 times the smallest possible error. As\r
 approaches 1, the bound approaches the minimum error (and it is loosest at  = 0).\r
In practice, however,  = 1 is often the poorest choice, as will be illustrated later in\r
Figure 12.14.\r
Exercise 12.3 Some insight into how TD() can closely approximate the o↵-line -return\r
algorithm can be gained by seeing that the latter’s error term (in brackets in (12.4)) can\r
be written as the sum of TD errors (12.6) for a single fixed w. Show this, following the\r
pattern of (6.6), and using the recursive relationship for the -return you obtained in\r
Exercise 12.1. ⇤\r
Exercise 12.4 Use your result from the preceding exercise to show that, if the weight\r
updates over an episode were computed on each step but not actually used to change the\r
weights (w remained fixed), then the sum of TD()’s weight updates would be the same\r
as the sum of the o↵-line -return algorithm’s updates. ⇤\r
12.3 n-step Truncated -return Methods\r
The o↵-line -return algorithm is an important ideal, but it is of limited utility because\r
it uses the -return (12.2), which is not known until the end of the episode. In the"""

[[sections]]
number = "296"
title = "Chapter 12: Eligibility Traces"
text = """
continuing case, the -return is technically never known, as it depends on n-step returns\r
for arbitrarily large n, and thus on rewards arbitrarily far in the future. However, the\r
dependence becomes weaker for longer-delayed rewards, falling by  for each step of\r
delay. A natural approximation, then, would be to truncate the sequence after some\r
number of steps. Our existing notion of n-step returns provides a natural way to do this\r
in which the missing rewards are replaced with estimated values.\r
In general, we define the truncated -return for time t, given data only up to some\r
later horizon, h, as\r
G\r
t:h\r
.\r
= (1  )\r
h\r
Xt1\r
n=1\r
n1Gt:t+n + ht1Gt:h, 0  t<h  T. (12.9)\r
If you compare this equation with the -return (12.3), it is clear that the horizon h is\r
playing the same role as was previously played by T, the time of termination. Whereas\r
in the -return there is a residual weight given to the conventional return Gt, here it is\r
given to the longest available n-step return, Gt:h (Figure 12.2).\r
The truncated -return immediately gives rise to a family of n-step -return algorithms\r
similar to the n-step methods of Chapter 7. In all of these algorithms, updates are\r
delayed by n steps and only take into account the first n rewards, but now all the k-step\r
returns are included for 1  k  n (whereas the earlier n-step algorithms used only the\r
n-step return), weighted geometrically as in Figure 12.2. In the state-value case, this\r
family of algorithms is known as Truncated TD(), or TTD(). The compound backup\r
diagram, shown in Figure 12.7, is similar to that for TD() (Figure 12.1) except that the\r
longest component update is at most n steps rather than always going all the way to the\r
1  \r
(1  )\r
(1  )2 T t1\r
or,\r
if t + n T ··· ···\r
···\r
n1\r
St\r
At\r
At+1\r
AT 1\r
St+n Rt+n\r
St+1 Rt+1\r
ST RT\r
At+n1\r
n-step truncated TD()\r
Figure 12.7: The backup diagram for Truncated TD()."""

[[sections]]
number = "12.4"
title = "Redoing Updates: Online -return Algorithm 297"
text = """
end of the episode. TTD() is defined by (cf. (9.15)):\r
wt+n\r
.\r
= wt+n1 + ↵ ⇥G\r
t:t+n  vˆ(St,wt+n1)\r
⇤\r
rvˆ(St,wt+n1), 0  t < T.\r
This algorithm can be implemented eciently so that per-step computation does not scale\r
with n (though of course memory must). Much as in n-step TD methods, no updates are\r
made on the first n  1 time steps of each episode, and n  1 additional updates are made\r
upon termination. Ecient implementation relies on the fact that the k-step -return\r
can be written exactly as\r
G\r
t:t+k = ˆv(St,wt1) +\r
t+\r
X\r
k1\r
i=t\r
()\r
it\r
0\r
i, (12.10)\r
where\r
0\r
t\r
.\r
= Rt+1 + vˆ(St+1,wt)  vˆ(St,wt1).\r
Exercise 12.5 Several times in this book (often in exercises) we have established that\r
returns can be written as sums of TD errors if the value function is held constant. Why\r
is (12.10) another instance of this? Prove (12.10). ⇤"""

[[sections]]
number = "12.4"
title = "Redoing Updates: Online -return Algorithm"
text = """
Choosing the truncation parameter n in Truncated TD() involves a tradeo↵. n should\r
be large so that the method closely approximates the o↵-line -return algorithm, but it\r
should also be small so that the updates can be made sooner and can influence behavior\r
sooner. Can we get the best of both? Well, yes, in principle we can, albeit at the cost of\r
computational complexity.\r
The idea is that, on each time step as you gather a new increment of data, you go back\r
and redo all the updates since the beginning of the current episode. The new updates\r
will be better than the ones you previously made because now they can take into account\r
the time step’s new data. That is, the updates are always towards an n-step truncated\r
-return target, but they always use the latest horizon. In each pass over that episode\r
you can use a slightly longer horizon and obtain slightly better results. Recall that the\r
truncated -return is defined in (12.9) as\r
G\r
t:h\r
.\r
= (1  )\r
h\r
Xt1\r
n=1\r
n1Gt:t+n + ht1Gt:h.\r
Let us step through how this target could ideally be used if computational complexity was\r
not an issue. The episode begins with an estimate at time 0 using the weights w0 from\r
the end of the previous episode. Learning begins when the data horizon is extended to\r
time step 1. The target for the estimate at step 0, given the data up to horizon 1, could\r
only be the one-step return G0:1, which includes R1 and bootstraps from the estimate\r
vˆ(S1,w0). Note that this is exactly what G\r
0:1 is, with the sum in the first term of the"""

[[sections]]
number = "298"
title = "Chapter 12: Eligibility Traces"
text = """
equation degenerating to zero. Using this update target, we construct w1. Then, after\r
advancing the data horizon to step 2, what do we do? We have new data in the form of\r
R2 and S2, as well as the new w1, so now we can construct a better update target G\r
0:2\r
for the first update from S0 as well as a better update target G\r
1:2 for the second update\r
from S1. Using these improved targets, we redo the updates at S1 and S2, starting again\r
from w0, to produce w2. Now we advance the horizon to step 3 and repeat, going all the\r
way back to produce three new targets, redoing all updates starting from the original w0\r
to produce w3, and so on. Each time the horizon is advanced, all the updates are redone\r
starting from w0 using the weight vector from the preceding horizon.\r
This conceptual algorithm involves multiple passes over the episode, one at each\r
horizon, each generating a di↵erent sequence of weight vectors. To describe it clearly we\r
have to distinguish between the weight vectors computed at the di↵erent horizons. Let us\r
use wh\r
t to denote the weights used to generate the value at time t in the sequence up to\r
horizon h. The first weight vector wh\r
0 in each sequence is that inherited from the previous\r
episode (so they are the same for all h), and the last weight vector wh\r
h in each sequence\r
defines the ultimate weight-vector sequence of the algorithm. At the final horizon h = T\r
we obtain the final weights wT\r
T which will be passed on to form the initial weights of the\r
next episode. With these conventions, the three first sequences described in the previous\r
paragraph can be given explicitly:\r
h =1: w1\r
1\r
.\r
= w1\r
0 + ↵ ⇥\r
G\r
0:1  vˆ(S0,w10)\r
⇤\r
rvˆ(S0,w1\r
0),\r
h =2: w2\r
1\r
.\r
= w2\r
0 + ↵ ⇥\r
G\r
0:2  vˆ(S0,w20)\r
⇤\r
rvˆ(S0,w2\r
0),\r
w2\r
2\r
.\r
= w2\r
1 + ↵ ⇥\r
G\r
1:2  vˆ(S1,w21)\r
⇤\r
rvˆ(S1,w2\r
1),\r
h =3: w3\r
1\r
.\r
= w3\r
0 + ↵ ⇥\r
G\r
0:3  vˆ(S0,w30)\r
⇤\r
rvˆ(S0,w3\r
0),\r
w3\r
2\r
.\r
= w3\r
1 + ↵ ⇥\r
G\r
1:3  vˆ(S1,w31)\r
⇤\r
rvˆ(S1,w3\r
1),\r
w3\r
3\r
.\r
= w3\r
2 + ↵ ⇥\r
G\r
2:3  vˆ(S2,w32)\r
⇤\r
rvˆ(S2,w3\r
2).\r
The general form for the update is\r
wh\r
t+1\r
.\r
= wh\r
t + ↵ ⇥\r
G\r
t:h  vˆ(St,wht )\r
⇤\r
rvˆ(St,wh\r
t ), 0  t<h  T.\r
This update, together with wt\r
.\r
= wt\r
t defines the online -return algorithm.\r
The online -return algorithm is fully online, determining a new weight vector wt\r
at each step t during an episode, using only information available at time t. Its main\r
drawback is that it is computationally complex, passing over the portion of the episode\r
experienced so far on every step. Note that it is strictly more complex than the o↵-line\r
-return algorithm, which passes through all the steps at the time of termination but does\r
not make any updates during the episode. In return, the online algorithm can be expected\r
to perform better than the o↵-line one, not only during the episode when it makes an\r
update while the o↵-line algorithm makes none, but also at the end of the episode because\r
the weight vector used in bootstrapping (in G\r
t:h) has had a larger number of informative"""

[[sections]]
number = "12.5"
title = "True Online TD() 299"
text = """
updates. This e↵ect can be seen if one looks carefully at Figure 12.8, which compares the\r
two algorithms on the 19-state random walk task.\r
Off line λ-return algorithm\r
(from Section 12.1)\r
↵ ↵\r
RMS error\r
at the end \r
of the episode\r
over the first\r
10 episodes\r
0 0.2 0.4 0.6 0.8 1\r
λ=0\r
λ=.4\r
λ=.8\r
λ=.9\r
λ=.95\r
λ=.975\r
λ=.99 λ=1\r
λ=.95\r
0.55\r
0.5\r
0.45\r
0.35\r
0.3\r
0.25\r
0.4\r
0 0.2 0.4 0.6 0.8 1\r
On-line λ-return algorithm\r
= true online TD(λ)\r
λ=0\r
λ=.4 λ=.8\r
λ=.9\r
λ=.95\r
λ=.975\r
λ=.99\r
λ=1\r
λ=.95\r
-\r
Figure 12.8: 19-state Random walk results (Example 7.1): Performance of online and o↵-line\r
-return algorithms. The performance measure here is the VE at the end of the episode, which\r
should be the best case for the o↵-line algorithm. Nevertheless, the online algorithm performs\r
subtly better. For comparison, the = 0 line is the same for both methods."""

[[sections]]
number = "12.5"
title = "True Online TD()"
text = """
The online -return algorithm just presented is currently the best performing temporal\u0002di↵erence algorithm. It is an ideal which online TD() only approximates. As presented,\r
however, the online -return algorithm is very complex. Is there a way to invert this\r
forward-view algorithm to produce an ecient backward-view algorithm using eligibility\r
traces? It turns out that there is indeed an exact computationally congenial implementa\u0002tion of the online -return algorithm for the case of linear function approximation. This\r
implementation is known as the true online TD() algorithm because it is “truer” to the\r
ideal of the online -return algorithm than the TD() algorithm is.\r
The derivation of true online TD() is a little too complex to present here (see the\r
next section and the appendix to the paper by van Seijen et al., 2016) but its strategy is\r
simple. The sequence of weight vectors produced by the online -return algorithm can\r
be arranged in a triangle:\r
w0\r
0\r
w1\r
0 w11\r
w2\r
0 w21 w22\r
w3\r
0 w31 w32 w33\r
.\r
.\r
. .\r
.\r
. .\r
.\r
. .\r
.\r
. ...\r
wT\r
0 wT1 wT2 wT3 ··· wT\r
T\r
One row of this triangle is produced on each time step. It turns out that the weight vectors\r
on the diagonal, the wt\r
t, are the only ones really needed. The first, w0\r
0, is the initial weight"""

[[sections]]
number = "300"
title = "Chapter 12: Eligibility Traces"
text = """
vector of the episode, the last, wT\r
T , is the final weight vector, and each weight vector\r
along the way, wt\r
t, plays a role in bootstrapping in the n-step returns of the updates.\r
In the final algorithm the diagonal weight vectors are renamed without a superscript,\r
wt\r
.\r
= wt\r
t. The strategy then is to find a compact, ecient way of computing each wtt\r
from the one before. If this is done, for the linear case in which vˆ(s,w) = w>x(s), then\r
we arrive at the true online TD() algorithm:\r
wt+1\r
.\r
= wt + ↵t zt + ↵ w>\r
t xt  w>t1xt\r
\r
(zt  xt),\r
where we have used the shorthand xt\r
.\r
= x(St), t is defined as in TD() (12.6), and zt is\r
defined by\r
zt\r
.\r
= zt1 + 1  ↵z>\r
t1xt\r
\r
xt. (12.11)\r
This algorithm has been proven to produce exactly the same sequence of weight vectors,\r
wt, 0  t  T, as the online -return algorithm (van Seijen et al. 2016). Thus the results\r
on the random walk task on the left of Figure 12.8 are also its results on that task. Now,\r
however, the algorithm is much less expensive. The memory requirements of true online\r
TD() are identical to those of conventional TD(), while the per-step computation is\r
increased by about 50% (there is one more inner product in the eligibility-trace update).\r
Overall, the per-step computational complexity remains of O(d), the same as TD().\r
Pseudocode for the complete algorithm is given in the box.\r
True online TD() for estimating w>x ⇡ v⇡\r
Input: the policy ⇡ to be evaluated\r
Input: a feature function x : S+ ! Rd such that x(terminal, ·) = 0\r
Algorithm parameters: step size ↵ > 0, trace decay rate  2 [0, 1]\r
Initialize value-function weights w 2 Rd (e.g., w = 0)\r
Loop for each episode:\r
Initialize state and obtain initial feature vector x\r
z 0 (a d-dimensional vector)\r
Vold 0 (a temporary scalar variable)\r
Loop for each step of episode:\r
| Choose A ⇠ ⇡\r
| Take action A, observe R, x0 (feature vector of the next state)\r
| V w>x\r
| V 0 w>x0\r
|  R + V 0  V\r
| z z + 1  ↵z>x\r
\r
x\r
| w w + ↵( + V  Vold)z  ↵(V  Vold)x\r
| Vold V 0\r
| x x0\r
until x0 = 0 (signaling arrival at a terminal state)\r
The eligibility trace (12.11) used in true online TD() is called a dutch trace to\r
distinguish it from the trace (12.5) used in TD(), which is called an accumulating trace.

12.6. *Dutch Traces in Monte Carlo Learning 301\r
Earlier work often used a third kind of trace called the replacing trace, defined only for\r
the tabular case or for binary feature vectors such as those produced by tile coding. The\r
replacing trace is defined on a component-by-component basis depending on whether the\r
component of the feature vector was 1 or 0:\r
zi,t\r
.\r
=\r
⇢ 1 if xi,t = 1\r
zi,t1 otherwise. (12.12)\r
Nowadays, we see replacing traces as crude approximations to dutch traces, which largely\r
supersede them. Dutch traces usually perform better than replacing traces and have a\r
clearer theoretical basis. Accumulating traces remain of interest for nonlinear function\r
approximations where dutch traces are not available.\r
12.6 *Dutch Traces in Monte Carlo Learning\r
Although eligibility traces are closely associated historically with TD learning, in fact\r
they have nothing to do with it. In fact, eligibility traces arise even in Monte Carlo\r
learning, as we show in this section. We show that the linear MC algorithm (Chapter 9),\r
taken as a forward view, can be used to derive an equivalent yet computationally cheaper\r
backward-view algorithm using dutch traces. This is the only equivalence of forward- and\r
backward-views that we explicitly demonstrate in this book. It gives some of the flavor\r
of the proof of equivalence of true online TD() and the online -return algorithm, but is\r
much simpler.\r
The linear version of the gradient Monte Carlo prediction algorithm (page 202) makes\r
the following sequence of updates, one for each time step of the episode:\r
wt+1\r
.\r
= wt + ↵ ⇥G  w>\r
t xt\r
⇤\r
xt, 0  t < T. (12.13)\r
To simplify the example, we assume here that the return G is a single reward received at\r
the end of the episode (this is why G is not subscripted by time) and that there is no\r
discounting. In this case the update is also known as the Least Mean Square (LMS) rule.\r
As a Monte Carlo algorithm, all the updates depend on the final reward/return, so none\r
can be made until the end of the episode. The MC algorithm is an o↵-line algorithm and\r
we do not seek to improve this aspect of it. Rather we seek merely an implementation of\r
this algorithm with computational advantages. We will still update the weight vector\r
only at the end of the episode, but we will do some computation during each step of the\r
episode and less at its end. This will give a more equal distribution of computation—O(d)\r
per step—and also remove the need to store the feature vectors at each step for use later\r
at the end of each episode. Instead, we will introduce an additional vector memory, the\r
eligibility trace, keeping in it a summary of all the feature vectors seen so far. This will\r
be sucient to eciently recreate exactly the same overall update as the sequence of MC"""

[[sections]]
number = "302"
title = "Chapter 12: Eligibility Traces"
text = """
updates (12.13), by the end of the episode:\r
wT = wT 1 + ↵ G  w>\r
T 1xT 1\r
\r
xT 1\r
= wT 1 + ↵xT 1\r
\r
x>\r
T 1wT 1\r
\r
+ ↵GxT 1\r
= I  ↵xT 1x>\r
T 1\r
\r
wT 1 + ↵GxT 1\r
= FT 1wT 1 + ↵GxT 1\r
where Ft\r
.\r
= I  ↵xtx>\r
t is a forgetting, or fading, matrix. Now, recursing,\r
= FT 1 (FT 2wT 2 + ↵GxT 2) + ↵GxT 1\r
= FT 1FT 2wT 2 + ↵G (FT 1xT 2 + xT 1)\r
= FT 1FT 2 (FT 3wT 3 + ↵GxT 3) + ↵G (FT 1xT 2 + xT 1)\r
= FT 1FT 2FT 3wT 3 + ↵G (FT 1FT 2xT 3 + FT 1xT 2 + xT 1)\r
.\r
.\r
.\r
= FT 1FT 2 ··· F0w0 | {z } aT 1+ ↵G\r
T\r
X1\r
k=0\r
FT 1FT 2 ··· Fk+1xk\r
| {z } zT 1\r
= aT 1 + ↵GzT 1 , (12.14)\r
where aT 1 and zT 1 are the values at time T  1 of two auxiliary memory vectors that\r
can be updated incrementally without knowledge of G and with O(d) complexity per time\r
step. The zt vector is in fact a dutch-style eligibility trace. It is initialized to z0 = x0\r
and then updated according to\r
zt\r
.\r
= Xt\r
k=0\r
FtFt1 ··· Fk+1xk, 1  t<T\r
= Xt1\r
k=0\r
FtFt1 ··· Fk+1xk + xt\r
= Ft\r
Xt1\r
k=0\r
Ft1Ft2 ··· Fk+1xk + xt\r
= Ftzt1 + xt\r
= I  ↵xtx>\r
t\r
\r
zt1 + xt\r
= zt1  ↵xtx>\r
t zt1 + xt\r
= zt1  ↵ z>\r
t1xt\r
\r
xt + xt\r
= zt1 + 1  ↵z>\r
t1xt\r
\r
xt,\r
which is the dutch trace for the case of = 1 (cf. Eq. 12.11). The at auxiliary vector is\r
initialized to a0 = w0 and then updated according to\r
at\r
.\r
= FtFt1 ··· F0w0 = Ftat1 = at1  ↵xtx>\r
t at1, 1  t < T."""

[[sections]]
number = "12.7"
title = "Sarsa() 303"
text = """
The auxiliary vectors, at and zt, are updated on each time step t<T and then, at time\r
T when G is observed, they are used in (12.14) to compute wT . In this way we achieve\r
exactly the same final result as the MC/LMS algorithm that has poor computational\r
properties (12.13), but now with an incremental algorithm whose time and memory\r
complexity per step is O(d). This is surprising and intriguing because the notion of\r
an eligibility trace (and the dutch trace in particular) has arisen in a setting without\r
temporal-di↵erence (TD) learning (in contrast to van Seijen and Sutton, 2014). It seems\r
eligibility traces are not specific to TD learning at all; they are more fundamental than\r
that. The need for eligibility traces seems to arise whenever one tries to learn long-term\r
predictions in an ecient manner."""

[[sections]]
number = "12.7"
title = "Sarsa()"
text = """
Very few changes in the ideas already presented in this chapter are required in order to\r
extend eligibility-traces to action-value methods. To learn approximate action values,\r
qˆ(s, a, w), rather than approximate state values, vˆ(s,w), we need to use the action-value\r
form of the n-step return, from Chapter 10:\r
Gt:t+n\r
.\r
= Rt+1 + ··· + n1Rt+n + nqˆ(St+n, At+n, wt+n1), t + n < T,\r
with Gt:t+n\r
.\r
= Gt if t + n  T. Using this, we can form the action-value form of the\r
-return, which is otherwise identical to the state-value form (12.3). The action-value\r
form of the o↵-line -return algorithm (12.4) simply uses ˆq rather than ˆv:\r
wt+1\r
.\r
= wt + ↵\r
h\r
G\r
t  qˆ(St, At, wt)\r
i\r
rqˆ(St, At, wt), t = 0,...,T  1, (12.15)\r
where G\r
t\r
.\r
= G\r
t:1. The compound backup diagram for this forward view is shown in\r
Figure 12.9. Notice the similarity to the diagram of the TD() algorithm (Figure 12.1).\r
The first update looks ahead one full step, to the next state–action pair, the second looks\r
ahead two steps, to the second state–action pair, and so on. A final update is based on\r
the complete return. The weighting of each n-step update in the -return is just as in\r
TD() and the -return algorithm (12.3).\r
The temporal-di↵erence method for action values, known as Sarsa(), approximates\r
this forward view. It has the same update rule as given earlier for TD():\r
wt+1\r
.\r
= wt + ↵t zt,\r
except, naturally, using the action-value form of the TD error:\r
t\r
.\r
= Rt+1 + qˆ(St+1, At+1, wt)  qˆ(St, At, wt), (12.16)\r
and the action-value form of the eligibility trace:\r
z1\r
.\r
= 0,\r
zt\r
.\r
= zt1 + rqˆ(St, At, wt), 0  t  T."""

[[sections]]
number = "304"
title = "Chapter 12: Eligibility Traces"
text = """
1  \r
(1  )\r
(1  )2\r
T t1\r
···\r
···\r
St At\r
At+1\r
AT 1\r
St+1 Rt+1\r
ST RT\r
···\r
St+2 Rt+2\r
At+2\r
X = 1\r
Sarsa()\r
Figure 12.9: Sarsa()’s backup diagram. Compare with Figure 12.1.\r
Complete pseudocode for Sarsa() with linear function approximation, binary features,\r
and either accumulating or replacing traces is given in the box on the next page. This\r
pseudocode highlights a few optimizations possible in the special case of binary features\r
(features are either active (=1) or inactive (=0)).\r
Example 12.1: Traces in Gridworld The use of eligibility traces can substantially\r
increase the eciency of control algorithms over one-step methods and even over n-step\r
methods. The reason for this is illustrated by the gridworld example below.\r
Path taken\r
Action values increased\r
by one-step Sarsa\r
Action values increased\r
by Sarsa( ) with =0.9\r
G\r
G\r
G\r
Path taken\r
Action values increased\r
by one-step Sarsa\r
Action values increased\r
by Sarsa( by 10-step Sarsa \u0004 ) with \u0004 =0.9\r
G G G\r
Path taken\r
Action values increased\r
by one-step Sarsa\r
Action values increased\r
by 10-step Sarsa\r
G G G\r
h h\r
The first panel shows the path taken by an agent in a single episode. The initial estimated\r
values were zero, and all rewards were zero except for a positive reward at the goal\r
location marked by G. The arrows in the other panels show, for various algorithms, which\r
action-values would be increased, and by how much, upon reaching the goal. A one-step\r
method would increment only the last action value, whereas an n-step method would\r
equally increment the last n actions’ values (assuming  = 1), and an eligibility trace\r
method would update all the action values up to the beginning of the episode, to di↵erent\r
degrees, fading with recency. The fading strategy is often the best."""

[[sections]]
number = "12.7"
title = "Sarsa() 305"
text = """
Sarsa() with binary features and linear function approximation\r
for estimating w>x ⇡ q⇡ or q⇤\r
Input: a function F(s, a) returning the set of (indices of) active features for s, a\r
Input: a policy ⇡\r
Algorithm parameters: step size ↵ > 0, trace decay rate  2 [0, 1], small " > 0\r
Initialize: w = (w1,...,wd)> 2 Rd (e.g., w = 0), z = (z1,...,zd)> 2 Rd\r
Loop for each episode:\r
Initialize S\r
Choose A ⇠ ⇡(·|S) or "-greedy according to ˆq(S, ·, w)\r
z 0\r
Loop for each step of episode:\r
Take action A, observe R, S0\r
 R\r
Loop for i in F(S, A):\r
   wi\r
zi zi + 1 (accumulating traces)\r
or zi 1 (replacing traces)\r
If S0 is terminal then:\r
w w + ↵z\r
Go to next episode\r
Choose A0 ⇠ ⇡(·|S0) or "-greedy according to ˆq(S0, ·, w)\r
Loop for i in F(S0, A0):   + wi\r
w w + ↵z\r
z z\r
S S0; A A0\r
Exercise 12.6 Modify the pseudocode for Sarsa() to use dutch traces (12.11) without the\r
other distinctive features of a true online algorithm. Assume linear function approximation\r
and binary features. ⇤\r
Example 12.2: Sarsa() on Mountain Car Figure 12.10 (left) on the next page\r
shows results with Sarsa() on the Mountain Car task introduced in Example 10.1. The\r
function approximation, action selection, and environmental details were exactly as in\r
Chapter 10, and thus it is appropriate to numerically compare these results with the\r
Chapter 10 results for n-step Sarsa (right side of the figure). The earlier results varied the\r
update length n whereas here for Sarsa() we vary the trace parameter , which plays\r
a similar role. The fading-trace bootstrapping strategy of Sarsa() appears to result in\r
more ecient learning on this problem.\r
There is also an action-value version of our ideal TD method, the online -return algo\u0002rithm (Section 12.4) and its ecient implementation as true online TD() (Section 12.5).\r
Everything in Section 12.4 goes through without change other than to use the action-value\r
form of the n-step return given at the beginning of the current section. The analyses in\r
Sections 12.5 and 12.6 also carry through for action values, the only change being the use"""

[[sections]]
number = "306"
title = "Chapter 12: Eligibility Traces"
text = """
220\r
240\r
260\r
300\r
0 0.5 1 1.5\r
Mountain Car\r
Steps per episode\r
averaged over\r
first 50 episodes\r
and 100 runs\r
280\r
200\r
180\r
↵ × number of tilings (8)\r
λ=.96\r
λ=.92\r
λ=.99\r
λ=.84\r
λ=.68\r
λ=0\r
220\r
240\r
260\r
300\r
0 0.5 1 1.5\r
280\r
0 0.5 1 1.5\r
n=1 n=2\r
n=4\r
n=8\r
n=16\r
n=8\r
n=4\r
n=2\r
n=16\r
n=1\r
λ=.98\r
↵ × number of tilings (8)\r
Sarsa(λ) with replacing traces n-step Sarsa\r
λ=.92\r
λ=.84\r
Figure 12.10: Early performance on the Mountain Car task of Sarsa() with replacing traces\r
and n-step Sarsa (copied from Figure 10.4) as a function of the step size, ↵.\r
of state–action feature vectors xt = x(St, At) instead of state feature vectors xt = x(St).\r
Pseudocode for the resulting ecient algorithm, called true online Sarsa() is given in\r
the box on the next page. The figure below compares the performance of various versions\r
of Sarsa() on the Mountain Car example.\r
True Online TD()\r
1.5 0 0.5 1 1.5 0\r
0.2\r
0.4\r
0.6\r
0.8\r
1\r
step−size\r
RMS \r
error \r
TD(λ), replacing traces − Task 1\r
λ = 0\r
λ = 1\r
0 0.5 1 1.5 0\r
0.2\r
0.4\r
0.6\r
0.8\r
1\r
step−size\r
RMS \r
error \r
true online TD(λ) − Task 1\r
λ = 0\r
λ = 1\r
1.5 \r
0 0.5 1 1.5 0\r
0.2\r
0.4\r
0.6\r
0.8\r
1\r
step−size\r
RMS \r
error \r
TD(λ), replacing traces − Task 2\r
0 0.5 1 1.5 0\r
0.2\r
0.4\r
0.6\r
0.8\r
1\r
step−size\r
RMS \r
error \r
true online TD(λ) − Task 2\r
λ = 1\r
λ = 0\r
he end of each episode, averaged over the first 10 episodes, as well as 100 independent runs, for\r
() with the traditional\r
dard mountain car task\r
gs of each 10⇥10 tiles.\r
↵ = ↵0/10, for ↵0 from\r
ring/no clearing refers\r
selected actions are set\r
), in case of replacing\r
true online principle is\r
nline version of the for\u0002\r
ical and intuitive foun\u0002\r
addition, we have pre\u0002\r
with the same compu\u0002\r
l algorithm, which we\r
that true online TD()\r
ew exactly, in contrast\r
only approximates its\r
monstrated empirically\r
conventional TD() on\r
ms, by adhering more\r
matching an intuitively\r
ne case—that we have\r
0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2 −550\r
−500\r
−450\r
−400\r
−350\r
−300\r
−250\r
−200\r
−150\r
α0\r
return\r
Sarsa(λ), replacing, clearing\r
Sarsa(λ), replacing, no clearing\r
Sarsa(λ), accumulating\r
Figure 4. Average return over first 20 episodes on mountain car\r
task for  = 0.9 and different ↵0. Results are averaged over 100\r
independent runs.\r
Acknowledgements\r
The authors thank Hado van Hasselt and Rupam Mahmood\r
Mountain Car\r
Reward per episode\r
averaged over\r
first 20 episodes\r
and 100 runs\r
↵ × number of tilings (8)\r
7UXH\u0003RQOLQH\u00036DUVD\u000Bλ\f\r
Sarsa(λ) with replacing traces\r
Sarsa(λ) with replacing traces\r
and clearing the traces of other actions\r
Sarsa(λ) with accumulating traces\r
Figure 12.11: Summary comparison of Sarsa() algorithms on the Mountain Car task. True\r
online Sarsa() performed better than regular Sarsa() with both accumulating and replacing\r
traces. Also included is a version of Sarsa() with replacing traces in which, on each time step,\r
the traces for the state and the actions not selected were set to zero."""

[[sections]]
number = "12.8"
title = "Variable  and  307"
text = """
True online Sarsa() for estimating w>x ⇡ q⇡ or q⇤\r
Input: a feature function x : S+ ⇥ A ! Rd such that x(terminal, ·) = 0\r
Input: a policy ⇡ (if estimating q⇡)\r
Algorithm parameters: step size ↵ > 0, trace decay rate  2 [0, 1], small " > 0\r
Initialize: w 2 Rd (e.g., w = 0)\r
Loop for each episode:\r
Initialize S\r
Choose A ⇠ ⇡(·|S) or "-greedy according to ˆq(S, ·, w)\r
x x(S, A)\r
z 0\r
Qold 0\r
Loop for each step of episode:\r
| Take action A, observe R, S0\r
| Choose A0 ⇠ ⇡(·|S0) or "-greedy according to ˆq(S0, ·, w)\r
| x0 x(S0, A0)\r
| Q w>x\r
| Q0 w>x0\r
|  R + Q0  Q\r
| z z + 1  ↵z>x\r
\r
x\r
| w w + ↵( + Q  Qold)z  ↵(Q  Qold)x\r
| Qold Q0\r
| x x0\r
| A A0\r
until S0 is terminal\r
Finally, there is also a truncated version of Sarsa(), called forward Sarsa() (van\r
Seijen, 2016), which appears to be a particularly e↵ective model-free control method for\r
use in conjunction with multi-layer artificial neural networks."""

[[sections]]
number = "12.8"
title = "Variable  and"
text = """
We are starting now to reach the end of our development of fundamental TD learning\r
algorithms. To present the final algorithms in their most general forms, it is useful to\r
generalize the degree of bootstrapping and discounting beyond constant parameters to\r
functions potentially dependent on the state and action. That is, each time step will have\r
a di↵erent  and , denoted t and t. We change notation now so that  : S⇥A ! [0, 1]\r
is now a function from states and actions to the unit interval such that t\r
.\r
= (St, At), and\r
similarly,  : S ! [0, 1] is a function from states to the unit interval such that t\r
.\r
= (St).\r
Introducing the function , the termination function, is particularly significant because\r
it changes the return, the fundamental random variable whose expectation we seek to"""

[[sections]]
number = "308"
title = "Chapter 12: Eligibility Traces"
text = """
estimate. Now the return is defined more generally as\r
Gt\r
.\r
= Rt+1 + t+1Gt+1\r
= Rt+1 + t+1Rt+2 + t+1t+2Rt+3 + t+1t+2t+3Rt+4 + ···\r
= X1\r
k=t\r
 Y\r
k\r
i=t+1\r
i\r
!\r
Rk+1, (12.17)\r
where, to assure the sums are finite, we require that Q1\r
k=t k = 0 with probability one for\r
all t. One convenient aspect of this definition is that it enables the episodic setting and\r
its algorithms to be presented in terms of a single stream of experience, without special\r
terminal states, start distributions, or termination times. An erstwhile terminal state\r
becomes a state at which (s) = 0 and which transitions to the start distribution. In that\r
way (and by choosing (·) as a constant in all other states) we can recover the classical\r
episodic setting as a special case. State dependent termination includes other prediction\r
cases such as pseudo termination, in which we seek to predict a quantity without altering\r
the flow of the Markov process. Discounted returns can be thought of as such a quantity,\r
in which case state-dependent termination unifies the episodic and discounted-continuing\r
cases. (The undiscounted-continuing case still needs some special treatment.)\r
The generalization to variable bootstrapping is not a change in the problem, like\r
discounting, but a change in the solution strategy. The generalization a↵ects the -\r
returns for states and actions. The new state-based -return can be written recursively\r
as\r
Gs\r
t\r
.\r
= Rt+1 + t+1 (1  t+1)ˆv(St+1,wt) + t+1Gs\r
t+1\r
, (12.18)\r
where now we have added the “s” to the superscript  to remind us that this is a return\r
that bootstraps from state values, distinguishing it from returns that bootstrap from\r
action values, which we present below with “a” in the superscript. This equation says\r
that the -return is the first reward, undiscounted and una↵ected by bootstrapping, plus\r
possibly a second term to the extent that we are not discounting at the next state (that\r
is, according to t+1; recall that this is zero if the next state is terminal). To the extent\r
that we aren’t terminating at the next state, we have a second term which is itself divided\r
into two cases depending on the degree of bootstrapping in the state. To the extent we\r
are bootstrapping, this term is the estimated value at the state, whereas, to the extent\r
that we are not bootstrapping, the term is the -return for the next time step. The\r
action-based -return is either the Sarsa form\r
Ga\r
t\r
.\r
= Rt+1 + t+1⇣(1  t+1)ˆq(St+1, At+1, wt) + t+1Ga\r
t+1⌘\r
, (12.19)\r
or the Expected Sarsa form,\r
Ga\r
t\r
.\r
= Rt+1 + t+1⇣(1  t+1)V¯t(St+1) + t+1Ga\r
t+1⌘\r
, (12.20)\r
where (7.8) is generalized to function approximation as\r
V¯t(s) .= X\r
a\r
⇡(a|s)ˆq(s, a, wt). (12.21)"""

[[sections]]
number = "12.9"
title = "O↵-policy Traces with Control Variates 309"
text = """
Exercise 12.7 Generalize the three recursive equations above to their truncated versions,\r
defining Gs\r
t:h and Gat:h. ⇤"""

[[sections]]
number = "12.9"
title = "O↵-policy Traces with Control Variates"
text = """
The final step is to incorporate importance sampling. For methods using non-truncated\r
-returns, there is not a practical option in which the importance-sampling weighting is\r
applied to the target return (as there is for n-step methods as explained in Section 7.3).\r
Instead, we move directly to the bootstrapping generalization of per-decision importance\r
sampling with control variates (Section 7.4).\r
In the state case, our final definition of the -return generalizes (12.18), after the model\r
of (7.13), to\r
Gs\r
t\r
.\r
= ⇢t\r
⇣\r
Rt+1 +t+1(1t+1)ˆv(St+1,wt)+t+1Gs\r
t+1⌘\r
+ (1⇢t)ˆv(St,wt), (12.22)\r
where ⇢t = ⇡(At|St)\r
b(At|St) is the usual single-step importance sampling ratio. Much like the\r
other returns we have seen in this book, this final -return can be approximated simply\r
in terms of sums of the state-based TD error,\r
s\r
t\r
.\r
= Rt+1 + t+1vˆ(St+1,wt)  vˆ(St,wt), (12.23)\r
as\r
Gs\r
t ⇡ vˆ(St,wt) + ⇢t\r
X1\r
k=t\r
s\r
k\r
Y\r
k\r
i=t+1\r
ii⇢i, (12.24)\r
with the approximation becoming exact if the approximate value function does not change.\r
Exercise 12.8 Prove that (12.24) becomes exact if the value function does not change.\r
To save writing, consider the case of t = 0, and use the notation Vk\r
.\r
= ˆv(Sk,w). ⇤\r
Exercise 12.9 The truncated version of the general o↵-policy return is denoted Gs\r
t:h.\r
Guess the correct equation, based on (12.24). ⇤\r
The above form of the -return (12.24) is convenient to use in a forward-view update,\r
wt+1 = wt + ↵ Gs\r
t  vˆ(St,wt)\r
\r
rvˆ(St,wt)\r
⇡ wt + ↵⇢t\r
 X1\r
k=t\r
s\r
k\r
Y\r
k\r
i=t+1\r
ii⇢i\r
!\r
rvˆ(St,wt),\r
which to the experienced eye looks like an eligibility-based TD update—the product is\r
like an eligibility trace and it is multiplied by TD errors. But this is just one time step of\r
a forward view. The relationship that we are looking for is that the forward-view update,\r
summed over time, is approximately equal to a backward-view update, summed over\r
time (this relationship is only approximate because again we ignore changes in the value"""

[[sections]]
number = "310"
title = "Chapter 12: Eligibility Traces"
text = """
function). The sum of the forward-view update over time is\r
X1\r
t=0\r
(wt+1  wt) ⇡ X1\r
t=0\r
X1\r
k=t\r
↵⇢ts\r
krvˆ(St,wt) Y\r
k\r
i=t+1\r
ii⇢i\r
= X1\r
k=0\r
X\r
k\r
t=0\r
↵⇢trvˆ(St,wt)s\r
k\r
Y\r
k\r
i=t+1\r
ii⇢i\r
(using the summation rule: Py\r
t=x\r
Py\r
k=t = Pyk=x\r
Pk\r
t=x)\r
= X1\r
k=0\r
↵s\r
k\r
X\r
k\r
t=0\r
⇢trvˆ(St,wt) Y\r
k\r
i=t+1\r
ii⇢i,\r
which would be in the form of the sum of a backward-view TD update if the entire\r
expression from the second sum on could be written and updated incrementally as an\r
eligibility trace, which we now show can be done. That is, we show that if this expression\r
was the trace at time k, then we could update it from its value at time k  1 by:\r
zk = X\r
k\r
t=0\r
⇢trvˆ(St,wt) Y\r
k\r
i=t+1\r
ii⇢i\r
=\r
k\r
X1\r
t=0\r
⇢trvˆ(St,wt) Y\r
k\r
i=t+1\r
ii⇢i + ⇢krvˆ(Sk,wk)\r
= kk⇢k\r
k\r
X1\r
t=0\r
⇢trvˆ(St,wt)\r
k\r
Y1\r
i=t+1\r
ii⇢i\r
| {z } zk1\r
+ ⇢krvˆ(Sk,wk)\r
= ⇢k\r
\r
kkzk1 + rvˆ(Sk,wk)\r
\r
,\r
which, changing the index from k to t, is the general accumulating trace update for state\r
values:\r
zt\r
.\r
= ⇢t\r
\r
ttzt1 + rvˆ(St,wt)\r
\r
, (12.25)\r
This eligibility trace, together with the usual semi-gradient parameter-update rule for\r
TD() (12.7), forms a general TD() algorithm that can be applied to either on-policy or\r
o↵-policy data. In the on-policy case, the algorithm is exactly TD() because ⇢t is alway\r
1 and (12.25) becomes the usual accumulating trace (12.5) (extended to variable  and\r
). In the o↵-policy case, the algorithm often works well but, as a semi-gradient method,\r
is not guaranteed to be stable. In the next few sections we will consider extensions of it\r
that do guarantee stability.\r
A very similar series of steps can be followed to derive the o↵-policy eligibility traces\r
for action-value methods and corresponding general Sarsa() algorithms. One could start\r
with either recursive form for the general action-based -return, (12.19) or (12.20), but\r
the latter (the Expected Sarsa form) works out to be simpler. We extend (12.20) to the"""

[[sections]]
number = "12.9"
title = "O↵-policy Traces with Control Variates 311"
text = """
o↵-policy case after the model of (7.14) to produce\r
Ga\r
t\r
.\r
= Rt+1+t+1⇣(1t+1)V¯t(St+1)+t+1⇥⇢t+1Ga\r
t+1+V¯t(St+1)⇢t+1qˆ(St+1, At+1, wt)\r
⇤\r
⌘\r
= Rt+1 + t+1⇣V¯t(St+1) + t+1⇢t+1 ⇥Ga\r
t+1  qˆ(St+1, At+1, wt)\r
⇤⌘\r
(12.26)\r
where V¯t(St+1) is as given by (12.21). Again the -return can be written approximately\r
as the sum of TD errors,\r
Ga\r
t ⇡ qˆ(St, At, wt) +X1\r
k=t\r
a\r
k\r
Y\r
k\r
i=t+1\r
ii⇢i, (12.27)\r
using the expectation form of the action-based TD error:\r
a\r
t = Rt+1 + t+1V¯t(St+1)  qˆ(St, At, wt). (12.28)\r
As before, the approximation becomes exact if the approximate value function does not\r
change.\r
Exercise 12.10 Prove that (12.27) becomes exact if the value function does not change.\r
To save writing, consider the case of t = 0, and use the notation Qk = qˆ(Sk, Ak, w). Hint:\r
Start by writing out a\r
0 and Ga0 , then Ga0  Q0. ⇤\r
Exercise 12.11 The truncated version of the general o↵-policy return is denoted Ga\r
t:h.\r
Guess the correct equation for it, based on (12.27). ⇤\r
Using steps entirely analogous to those for the state case, one can write a forward-view\r
update based on (12.27), transform the sum of the updates using the summation rule,\r
and finally derive the following form for the eligibility trace for action values:\r
zt\r
.\r
= tt⇢tzt1 + rqˆ(St, At, wt). (12.29)\r
This eligibility trace, together with the expectation-based TD error (12.28) and the usual\r
semi-gradient parameter-update rule (12.7), forms an elegant, ecient Expected Sarsa()\r
algorithm that can be applied to either on-policy or o↵-policy data. It is probably the\r
best algorithm of this type at the current time (though of course it is not guaranteed to\r
be stable until combined in some way with one of the methods presented in the following\r
sections). In the on-policy case with constant  and , and the usual state–action TD\r
error (12.16), the algorithm would be identical to the Sarsa() algorithm presented in\r
Section 12.7.\r
Exercise 12.12 Show in detail the steps outlined above for deriving (12.29) from (12.27).\r
Start with the update (12.15), substitute Ga\r
t from (12.26) for Gt , then follow similar\r
steps as led to (12.25). ⇤\r
At  = 1, these algorithms become closely related to corresponding Monte Carlo\r
algorithms. One might expect that an exact equivalence would hold for episodic problems\r
and o↵-line updating, but in fact the relationship is subtler and slightly weaker than that.\r
Under these most favorable conditions still there is not an episode by episode equiva\u0002lence of updates, only of their expectations. This should not be surprising as these methods"""

[[sections]]
number = "312"
title = "Chapter 12: Eligibility Traces"
text = """
make irrevocable updates as a trajectory unfolds, whereas true Monte Carlo methods\r
would make no update for a trajectory if any action within it has zero probability under\r
the target policy. In particular, all of these methods, even at  = 1, still bootstrap in\r
the sense that their targets depend on the current value estimates—it’s just that the\r
dependence cancels out in expected value. Whether this is a good or bad property in\r
practice is another question. Recently, methods have been proposed that do achieve an\r
exact equivalence (Sutton, Mahmood, Precup and van Hasselt, 2014). These methods\r
require an additional vector of “provisional weights” that keep track of updates which\r
have been made but may need to be retracted (or emphasized) depending on the actions\r
taken later. The state and state–action versions of these methods are called PTD() and\r
PQ() respectively, where the ‘P’ stands for Provisional.\r
The practical consequences of all these new o↵-policy methods have not yet been\r
established. Undoubtedly, issues of high variance will arise as they do in all o↵-policy\r
methods using importance sampling (Section 11.9).\r
If  < 1, then all these o↵-policy algorithms involve bootstrapping and the deadly\r
triad applies (Section 11.3), meaning that they can be guaranteed stable only for the\r
tabular case, for state aggregation, and for other limited forms of function approximation.\r
For linear and more-general forms of function approximation the parameter vector may\r
diverge to infinity as in the examples in Chapter 11. As we discussed there, the challenge\r
of o↵-policy learning has two parts. O↵-policy eligibility traces deal e↵ectively with the\r
first part of the challenge, correcting for the expected value of the targets, but not at\r
all with the second part of the challenge, having to do with the distribution of updates.\r
Algorithmic strategies for meeting the second part of the challenge of o↵-policy learning\r
with eligibility traces are summarized in Section 12.11.\r
Exercise 12.13 What are the dutch-trace and replacing-trace versions of o↵-policy\r
eligibility traces for state-value and action-value methods? ⇤"""

[[sections]]
number = "12.10"
title = "Watkins’s Q() to Tree-Backup()"
text = """
Several methods have been proposed over the years to extend Q-learning to eligibility\r
traces. The original is Watkins’s Q(), which decays its eligibility traces in the usual way\r
as long as a greedy action was taken, then cuts the traces to zero after the first non-greedy\r
action. The backup diagram for Watkins’s Q() is shown in Figure 12.12. In Chapter 6,\r
we unified Q-learning and Expected Sarsa in the o↵-policy version of the latter, which\r
includes Q-learning as a special case, and generalizes it to arbitrary target policies, and\r
in the previous section of this chapter we completed our treatment of Expected Sarsa by\r
generalizing it to o↵-policy eligibility traces. In Chapter 7, however, we distinguished\r
n-step Expected Sarsa from n-step Tree Backup, where the latter retained the property\r
of not using importance sampling. It remains then to present the eligibility trace version\r
of Tree Backup, which we will call Tree-Backup(), or TB() for short. This is arguably\r
the true successor to Q-learning because it retains its appealing absence of importance\r
sampling even though it can be applied to o↵-policy data."""

[[sections]]
number = "12.10"
title = "Watkins’s Q() to Tree-Backup() 313"
text = """
1  \r
(1  )\r
(1  )2\r
T t1\r
···\r
St At\r
At+1\r
St+1 Rt+1\r
ST RT\r
···\r
St+2 Rt+2\r
At+2\r
X = 1\r
OR\r
···\r
···\r
St+n Rt+n\r
First non-greedy action\r
n1\r
Watkins’s Q()\r
Figure 12.12: The backup diagram for Watkins’s Q(). The series of component updates ends\r
either with the end of the episode or with the first nongreedy action, whichever comes first.\r
The concept of TB() is straightforward. As shown in its backup diagram in Fig\u0002ure 12.13, the tree-backup updates of each length (from Section 7.5) are weighted in the\r
usual way dependent on the bootstrapping parameter . To get the detailed equations,\r
with the right indices on the general bootstrapping and discounting parameters, it is\r
best to start with a recursive form (12.20) for the -return using action values, and then\r
expand the bootstrapping case of the target after the model of (7.16):\r
Ga\r
t\r
.\r
= Rt+1+t+1✓(1t+1)V¯t(St+1)+t+1hX\r
a6=At+1\r
⇡(a|St+1)ˆq(St+1, a, wt)+⇡(At+1|St+1)Ga\r
t+1i◆\r
= Rt+1 + t+1✓V¯t(St+1) + t+1⇡(At+1|St+1)\r
⇣\r
Ga\r
t+1  qˆ(St+1, At+1, wt)\r
⌘◆\r
As per the usual pattern, it can also be written approximately (ignoring changes in the\r
approximate value function) as a sum of TD errors,\r
Ga\r
t ⇡ qˆ(St, At, wt) +X1\r
k=t\r
a\r
k\r
Y\r
k\r
i=t+1\r
ii⇡(Ai|Si),\r
using the expectation form of the action-based TD error (12.28).\r
Following the same steps as in the previous section, we arrive at a special eligibility\r
trace update involving the target-policy probabilities of the selected actions,\r
zt\r
.\r
= tt⇡(At|St)zt1 + rqˆ(St, At, wt)."""

[[sections]]
number = "314"
title = "Chapter 12: Eligibility Traces"
text = """
1  \r
(1  )\r
(1  )2\r
T t1\r
···\r
St At\r
At+1\r
AT 1\r
St+1 Rt+1\r
ST RT\r
···\r
St+2 Rt+2\r
At+2\r
X = 1\r
···\r
Tree Backup()\r
ST 1\r
Figure 12.13: The backup diagram for the  version of the Tree Backup algorithm.\r
This, together with the usual parameter-update rule (12.7), defines the TB() algorithm.\r
Like all semi-gradient algorithms, TB() is not guaranteed to be stable when used with\r
o↵-policy data and with a powerful function approximator. To obtain those assurances,\r
TB() would have to be combined with one of the methods presented in the next section.\r
⇤\r
Exercise 12.14 How might Double Expected Sarsa be extended to eligibility traces? ⇤"""

[[sections]]
number = "12.11"
title = "Stable O↵-policy Methods with Traces"
text = """
Several methods using eligibility traces have been proposed that achieve guarantees\r
of stability under o↵-policy training, and here we present four of the most important\r
using this book’s standard notation, including general bootstrapping and discounting\r
functions. All are based on either the Gradient-TD or the Emphatic-TD ideas presented\r
in Sections 11.7 and 11.8. All the algorithms assume linear function approximation,\r
though extensions to nonlinear function approximation can also be found in the literature.\r
GTD() is the eligibility-trace algorithm analogous to TDC, the better of the two\r
state-value Gradient-TD prediction algorithms discussed in Section 11.7. Its goal is to\r
learn a parameter wt such that vˆ(s,w) .= w>\r
t x(s) ⇡ v⇡(s), even from data that is due to\r
following another policy b. Its update is\r
wt+1\r
.\r
= wt + ↵s\r
t zt  ↵t+1(1  t+1)\r
\r
z>\r
t vt\r
\r
xt+1,"""

[[sections]]
number = "12.11"
title = "Stable O↵-policy Methods with Traces 315"
text = """
with s\r
t , zt, and ⇢t defined in the usual ways for state values (12.23) (12.25) (11.1), and\r
vt+1\r
.\r
= vt + s\r
t zt  \r
\r
v>\r
t xt\r
\r
xt, (12.30)\r
where, as in Section 11.7, v 2 Rd is a vector of the same dimension as w, initialized to\r
v0 = 0, and  > 0 is a second step-size parameter.\r
GQ() is the Gradient-TD algorithm for action values with eligibility traces. Its goal\r
is to learn a parameter wt such that qˆ(s, a, wt) .= w>\r
t x(s, a) ⇡ q⇡(s, a) from o↵-policy\r
data. If the target policy is "-greedy, or otherwise biased toward the greedy policy for qˆ,\r
then GQ() can be used as a control algorithm. Its update is\r
wt+1\r
.\r
= wt + ↵a\r
t zt  ↵t+1(1  t+1)\r
\r
z>\r
t vt\r
\r
x¯t+1,\r
where x¯t is the average feature vector for St under the target policy,\r
x¯t\r
.\r
= X\r
a\r
⇡(a|St)x(St, a),\r
a\r
t is the expectation form of the TD error, which can be written\r
a\r
t\r
.\r
= Rt+1 + t+1w>\r
t x¯t+1  w>t xt,\r
zt is defined in the usual way for action values (12.29), and the rest is as in GTD(),\r
including the update for vt (12.30).\r
HTD() is a hybrid state-value algorithm combining aspects of GTD() and TD(). Its\r
most appealing feature is that it is a strict generalization of TD() to o↵-policy learning,\r
meaning that if the behavior policy happens to be the same as the target policy, then\r
HTD() becomes the same as TD(), which is not true for GTD(). This is appealing\r
because TD() is often faster than GTD() when both algorithms converge, and TD()\r
requires setting only a single step size. HTD() is defined by\r
wt+1\r
.\r
= wt + ↵s\r
t zt + ↵ \r
(zt  zb\r
t )\r
>vt\r
\r
(xt  t+1xt+1),\r
vt+1\r
.\r
= vt + s\r
t zt  \r
⇣\r
zb\r
t\r
>vt\r
⌘\r
(xt  t+1xt+1), with v0\r
.\r
= 0,\r
zt\r
.\r
= ⇢t\r
\r
ttzt1 + xt\r
\r
, with z1\r
.\r
= 0,\r
zb\r
t\r
.\r
= ttzb\r
t1 + xt, with zb1\r
.\r
= 0,\r
where  > 0 again is a second step-size parameter. In addition to the second set of\r
weights, vt, HTD() also has a second set of eligibility traces, zb\r
t . These are conventional\r
accumulating eligibility traces for the behavior policy and become equal to zt if all the ⇢t\r
are 1, which causes the last term in the wt update to be zero and the overall update to\r
reduce to TD().\r
Emphatic TD() is the extension of the one-step Emphatic-TD algorithm (Sections\r
9.11 and 11.8) to eligibility traces. The resultant algorithm retains strong o↵-policy\r
convergence guarantees while enabling any degree of bootstrapping, albeit at the cost of"""

[[sections]]
number = "316"
title = "Chapter 12: Eligibility Traces"
text = """
high variance and potentially slow convergence. Emphatic TD() is defined by\r
wt+1\r
.\r
= wt + ↵tzt\r
t\r
.\r
= Rt+1 + t+1w>\r
t xt+1  w>t xt\r
zt\r
.\r
= ⇢t\r
\r
ttzt1 + Mtxt\r
\r
, with z1\r
.\r
= 0,\r
Mt\r
.\r
= t It + (1  t)Ft\r
Ft\r
.\r
= ⇢t1tFt1 + It, with F0\r
.\r
= i(S0),\r
where Mt  0 is the general form of emphasis, Ft  0 is termed the followon trace, and\r
It  0 is the interest, as described in Section 11.8. Note that Mt, like t, is not really an\r
additional memory variable. It can be removed from the algorithm by substituting its\r
definition into the eligibility-trace equation. Pseudocode and software for the true online\r
version of Emphatic-TD() are available on the web (Sutton, 2015b).\r
In the on-policy case (⇢t = 1, for all t), Emphatic-TD() is similar to conventional\r
TD(), but still significantly di↵erent. In fact, whereas Emphatic-TD() is guaranteed\r
to converge for all state-dependent  functions, TD() is not. TD() is guaranteed\r
convergent only for all constant . See Yu’s counterexample (Ghiassian, Rafiee, and\r
Sutton, 2016)."""

[[sections]]
number = "12.12"
title = "Implementation Issues"
text = """
It might at first appear that tabular methods using eligibility traces are much more\r
complex than one-step methods. A naive implementation would require every state (or\r
state–action pair) to update both its value estimate and its eligibility trace on every time\r
step. This would not be a problem for implementations on single-instruction, multiple\u0002data, parallel computers or in plausible artificial neural network (ANN) implementations,\r
but it is a problem for implementations on conventional serial computers. Fortunately,\r
for typical values of  and  the eligibility traces of almost all states are almost always\r
nearly zero; only those states that have recently been visited will have traces significantly\r
greater than zero and only these few states need to be updated to closely approximate\r
these algorithms.\r
In practice, then, implementations on conventional computers may keep track of and\r
update only the few traces that are significantly greater than zero. Using this trick, the\r
computational expense of using traces in tabular methods is typically just a few times\r
that of a one-step method. The exact multiple of course depends on  and  and on the\r
expense of the other computations. Note that the tabular case is in some sense the worst\r
case for the computational complexity of eligibility traces. When function approximation\r
is used, the computational advantages of not using traces generally decrease. For example,\r
if ANNs and backpropagation are used, then eligibility traces generally cause only a\r
doubling of the required memory and computation per step. Truncated -return methods\r
(Section 12.3) can be computationally ecient on conventional computers though they\r
always require some additional memory."""

[[sections]]
number = "12.13"
title = "Conclusions 317"
text = ""

[[sections]]
number = "12.13"
title = "Conclusions"
text = """
Eligibility traces in conjunction with TD errors provide an ecient, incremental way of\r
shifting and choosing between Monte Carlo and TD methods. The n-step methods of\r
Chapter 7 also enabled this, but eligibility trace methods are more general, often faster to\r
learn, and o↵er di↵erent computational complexity tradeo↵s. This chapter has o↵ered an\r
introduction to the elegant, emerging theoretical understanding of eligibility traces for on\u0002and o↵-policy learning and for variable bootstrapping and discounting. One aspect of this\r
elegant theory is true online methods, which exactly reproduce the behavior of expensive\r
ideal methods while retaining the computational congeniality of conventional TD methods.\r
Another aspect is the possibility of derivations that automatically convert from intuitive\r
forward-view methods to more ecient incremental backward-view algorithms. We\r
illustrated this general idea in a derivation that started with a classical, expensive Monte\r
Carlo algorithm and ended with a cheap incremental non-TD implementation using the\r
same novel eligibility trace used in true online TD methods.\r
As we mentioned in Chapter 5, Monte Carlo methods may have advantages in non\u0002Markov tasks because they do not bootstrap. Because eligibility traces make TD methods\r
more like Monte Carlo methods, they also can have advantages in these cases. If one\r
wants to use TD methods because of their other advantages, but the task is at least\r
partially non-Markov, then the use of an eligibility trace method is indicated. Eligibility\r
traces are the first line of defense against both long-delayed rewards and non-Markov\r
tasks.\r
By adjusting , we can place eligibility trace methods anywhere along a continuum\r
from Monte Carlo to one-step TD methods. Where shall we place them? We do not yet\r
have a good theoretical answer to this question, but a clear empirical answer appears to\r
be emerging. On tasks with many steps per episode, or many steps within the half-life of\r
discounting, it appears significantly better to use eligibility traces than not to (e.g., see\r
Figure 12.14). On the other hand, if the traces are so long as to produce a pure Monte\r
Carlo method, or nearly so, then performance degrades sharply. An intermediate mixture\r
appears to be the best choice. Eligibility traces should be used to bring us toward Monte\r
Carlo methods, but not all the way there. In the future it may be possible to more finely\r
vary the trade-o↵ between TD and Monte Carlo methods by using variable , but at\r
present it is not clear how this can be done reliably and usefully.\r
Methods using eligibility traces require more computation than one-step methods, but\r
in return they o↵er significantly faster learning, particularly when rewards are delayed by\r
many steps. Thus it often makes sense to use eligibility traces when data are scarce and\r
cannot be repeatedly processed, as is often the case in online applications. On the other\r
hand, in o↵-line applications in which data can be generated cheaply, perhaps from an\r
inexpensive simulation, then it often does not pay to use eligibility traces. In these cases\r
the objective is not to get more out of a limited amount of data, but simply to process as\r
much data as possible as quickly as possible. In these cases the speedup per datum due to\r
traces is typically not worth their computational cost, and one-step methods are favored."""

[[sections]]
number = "318"
title = "Chapter 12: Eligibility Traces"
text = """
accumulating\r
traces\r
0.2\r
0.3\r
0.4\r
0.5\r
0 0.2 0.4 0.6 0.8 1\r
!\r
RANDOM WALK\r
50\r
100\r
150\r
200\r
250"""

[[sections]]
number = "300"
title = "Failures per"
text = """
100,000 steps\r
0 0.2 0.4 0.6 0.8 1\r
!\r
CART AND POLE\r
400\r
450\r
500\r
550\r
600\r
650"""

[[sections]]
number = "700"
title = "Steps per"
text = """
episode\r
0 0.2 0.4 0.6 0.8 1\r
!\r
MOUNTAIN CAR\r
replacing\r
traces\r
150\r
160\r
170\r
180\r
190\r
200\r
210\r
220\r
230"""

[[sections]]
number = "240"
title = "Cost per"
text = """
episode\r
0 0.2 0.4 0.6 0.8 1\r
!\r
PUDDLE WORLD\r
replacing\r
traces\r
accumulating\r
traces \r
replacing\r
traces\r
accumulating\r
traces\r
RMS error\r
Figure 12.14: The e↵ect of  on reinforcement learning performance in four di↵erent test\r
problems. In all cases, performance is generally best (a lower number in the graph) at an\r
intermediate value of . The two left panels are applications to simple continuous-state control\r
tasks using the Sarsa() algorithm and tile coding, with either replacing or accumulating traces\r
(Sutton, 1996). The upper-right panel is for policy evaluation on a random walk task using TD()\r
(Singh and Sutton, 1996). The lower right panel is unpublished data for the pole-balancing task\r
(Example 3.4) from an earlier study (Sutton, 1984)."""

[[sections]]
number = "12.13"
title = "Conclusions 319"
text = """
Bibliographical and Historical Remarks\r
Eligibility traces came into reinforcement learning via the fecund ideas of Klopf (1972).\r
Our use of eligibility traces is based on Klopf’s work (Sutton, 1978a, 1978b, 1978c; Barto\r
and Sutton, 1981a, 1981b; Sutton and Barto, 1981a; Barto, Sutton, and Anderson, 1983;\r
Sutton, 1984). We may have been the first to use the term “eligibility trace” (Sutton\r
and Barto, 1981a). The idea that stimuli produce after e↵ects in the nervous system\r
that are important for learning is very old (see Chapter 14). Some of the earliest uses of\r
eligibility traces were in the actor–critic methods discussed in Chapter 13 (Barto, Sutton,\r
and Anderson, 1983; Sutton, 1984)."""

[[sections]]
number = "12.1"
title = "Compound updates were called “complex backups” in the first edition of this"
text = """
book.\r
The -return and its error-reduction properties were introduced by Watkins (1989)\r
and further developed by Jaakkola, Jordan, and Singh (1994). The random walk\r
results in this and subsequent sections are new to this text, as are the terms\r
“forward view” and “backward view.” The notion of a -return algorithm was\r
introduced in the first edition of this text. The more refined treatment presented\r
here was developed in conjunction with Harm van Seijen (e.g., van Seijen and\r
Sutton, 2014).\r
12.2 TD() with accumulating traces was introduced by Sutton (1988, 1984). Con\u0002vergence in the mean was proved by Dayan (1992), and with probability 1 by\r
many researchers, including Peng (1993), Dayan and Sejnowski (1994), Tsitsiklis\r
(1994), and Gurvits, Lin, and Hanson (1994). The bound on the error of the\r
asymptotic -dependent solution of linear TD() is due to Tsitsiklis and Van\r
Roy (1997)."""

[[sections]]
number = "12.3"
title = "Truncated TD methods were developed by Cichosz (1995) and van Seijen (2016)."
text = ""

[[sections]]
number = "12.4"
title = "The idea of redoing updates was extensively developed by van Seijen, originally"
text = """
under the name “best-match learning” (van Seijen, 2011; van Seijen, Whiteson,\r
van Hasselt, and Weiring, 2011)."""

[[sections]]
number = "12.5"
title = "True online TD() is primarily due to Harm van Seijen (van Seijen and Sutton,"
text = """
2014; van Seijen et al., 2016) though some of its key ideas were discovered\r
independently by Hado van Hasselt (personal communication). The name “dutch\r
traces” is in recognition of the contributions of both scientists. Replacing traces\r
are due to Singh and Sutton (1996)."""

[[sections]]
number = "12.6"
title = "The material in this section is from van Hasselt and Sutton (2015)."
text = ""

[[sections]]
number = "12.7"
title = "Sarsa() with accumulating traces was first explored as a control method by"
text = "Rummery and Niranjan (1994; Rummery, 1995). True Online Sarsa() was"

[[sections]]
number = "320"
title = "Chapter 12: Eligibility Traces"
text = """
introduced by van Seijen and Sutton (2014). The algorithm on page 307 was\r
adapted from van Seijen et al. (2016). The Mountain Car results were made for\r
this text, except for Figure 12.11 which is adapted from van Seijen and Sutton\r
(2014)."""

[[sections]]
number = "12.8"
title = "Perhaps the first published discussion of variable  was by Watkins (1989), who"
text = """
pointed out that the cutting o↵ of the update sequence (Figure 12.12) in his\r
Q() when a nongreedy action was selected could be implemented by temporarily\r
setting  to 0.\r
Variable  was introduced in the first edition of this text. The roots of variable \r
are in the work on options (Sutton, Precup, and Singh, 1999) and its precursors\r
(Sutton, 1995a), becoming explicit in the GQ() paper (Maei and Sutton, 2010),\r
which also introduced some of these recursive forms for the -returns.\r
A di↵erent notion of variable  has been developed by Yu (2012)."""

[[sections]]
number = "12.9"
title = "O↵-policy eligibility traces were introduced by Precup et al. (2000, 2001), then"
text = """
further developed by Bertsekas and Yu (2009), Maei (2011; Maei and Sutton,\r
2010), Yu (2012), and by Sutton, Mahmood, Precup, and van Hasselt (2014).\r
The last reference in particular gives a powerful forward view for o↵-policy TD\r
methods with general state-dependent  and . The presentation here seems to\r
be new.\r
This section ends with an elegant Expected Sarsa() algorithm. Although it is\r
a natural algorithm, to our knowledge it has not previously been described or\r
tested in the literature."""

[[sections]]
number = "12.10"
title = "Watkins’s Q() is due to Watkins (1989). The tabular, episodic, o↵-line version"
text = """
has been proven convergent by Munos, Stepleton, Harutyunyan, and Bellemare\r
(2016). Alternative Q() algorithms were proposed by Peng and Williams (1994,\r
1996) and by Sutton, Mahmood, Precup, and van Hasselt (2014). Tree Backup()\r
is due to Precup, Sutton, and Singh (2000)."""

[[sections]]
number = "12.11"
title = "GTD() is due to Maei (2011). GQ() is due to Maei and Sutton (2010)."
text = """
HTD() is due to White and White (2016) based on the one-step HTD algorithm\r
introduced by Hackman (2012). The latest developments in the theory of\r
Gradient-TD methods are by Yu (2017). Emphatic TD() was introduced by\r
Sutton, Mahmood, and White (2016), who proved its stability. Yu (2015, 2016)\r
proved its convergence, and the algorithm was developed further by Hallak et\r
al. (2015, 2016).

Chapter 13\r
Policy Gradient Methods\r
In this chapter we consider something new. So far in this book almost all the methods\r
have been action-value methods; they learned the values of actions and then selected\r
actions based on their estimated action values1; their policies would not even exist without\r
the action-value estimates. In this chapter we consider methods that instead learn a\r
parameterized policy that can select actions without consulting a value function. A value\r
function may still be used to learn the policy parameter, but is not required for action\r
selection. We use the notation ✓ 2 Rd0for the policy’s parameter vector. Thus we write\r
⇡(a|s, ✓) = Pr{At =a | St =s, ✓t =✓} for the probability that action a is taken at time t\r
given that the environment is in state s at time t with parameter ✓. If a method uses a\r
learned value function as well, then the value function’s weight vector is denoted w 2 Rd\r
as usual, as in ˆv(s,w).\r
In this chapter we consider methods for learning the policy parameter based on the\r
gradient of some scalar performance measure J(✓) with respect to the policy parameter.\r
These methods seek to maximize performance, so their updates approximate gradient\r
ascent in J:\r
✓t+1 = ✓t + ↵r\\J(✓t), (13.1)\r
where r\\J(✓t) 2 Rd0is a stochastic estimate whose expectation approximates the gradient\r
of the performance measure with respect to its argument ✓t. All methods that follow\r
this general schema we call policy gradient methods, whether or not they also learn an\r
approximate value function. Methods that learn approximations to both policy and value\r
functions are often called actor–critic methods, where ‘actor’ is a reference to the learned\r
policy, and ‘critic’ refers to the learned value function, usually a state-value function.\r
First we treat the episodic case, in which performance is defined as the value of the start\r
state under the parameterized policy, before going on to consider the continuing case, in\r
1The lone exception is the gradient bandit algorithms of Section 2.8. In fact, that section goes through\r
many of the same steps, in the single-state bandit case, as we go through here for full MDPs. Reviewing\r
that section would be good preparation for fully understanding this chapter."""

[[sections]]
number = "322"
title = "Chapter 13: Policy Gradient Methods"
text = """
which performance is defined as the average reward rate, as in Section 10.3. In the end,\r
we are able to express the algorithms for both cases in very similar terms."""

[[sections]]
number = "13.1"
title = "Policy Approximation and its Advantages"
text = """
In policy gradient methods, the policy can be parameterized in any way, as long as\r
⇡(a|s, ✓) is di↵erentiable with respect to its parameters, that is, as long as r⇡(a|s, ✓) (the\r
column vector of partial derivatives of ⇡(a|s, ✓) with respect to the components of ✓) exists\r
and is finite for all s 2 S, a 2 A(s), and ✓ 2 Rd0. In practice, to ensure exploration we\r
generally require that the policy never becomes deterministic (i.e., that ⇡(a|s, ✓) 2 (0, 1),\r
for all s, a, ✓). In this section we introduce the most common parameterization for\r
discrete action spaces and point out the advantages it o↵ers over action-value methods.\r
Policy-based methods also o↵er useful ways of dealing with continuous action spaces, as\r
we describe later in Section 13.7.\r
If the action space is discrete and not too large, then a natural and common kind of\r
parameterization is to form parameterized numerical preferences h(s, a, ✓) 2 R for each\r
state–action pair. The actions with the highest preferences in each state are given the\r
highest probabilities of being selected, for example, according to an exponential soft-max\r
distribution:\r
⇡(a|s, ✓) .= eh(s,a,✓)\r
P\r
b eh(s,b,✓) , (13.2)\r
where e ⇡ 2.71828 is the base of the natural logarithm. Note that the denominator here\r
is just what is required so that the action probabilities in each state sum to one. We call\r
this kind of policy parameterization soft-max in action preferences.\r
The action preferences themselves can be parameterized arbitrarily. For example, they\r
might be computed by a deep artificial neural network (ANN), where ✓ is the vector\r
of all the connection weights of the network (as in the AlphaGo system described in\r
Section 16.6). Or the preferences could simply be linear in features,\r
h(s, a, ✓) = ✓>x(s, a), (13.3)\r
using feature vectors x(s, a) 2 Rd0constructed by any of the methods described in\r
Section 9.5.\r
One advantage of parameterizing policies according to the soft-max in action preferences\r
is that the approximate policy can approach a deterministic policy, whereas with "-greedy\r
action selection over action values there is always an " probability of selecting a random\r
action. Of course, one could select according to a soft-max distribution based on action\r
values, but this alone would not allow the policy to approach a deterministic policy.\r
Instead, the action-value estimates would converge to their corresponding true values,\r
which would di↵er by a finite amount, translating to specific probabilities other than 0 and\r
1. If the soft-max distribution included a temperature parameter, then the temperature\r
could be reduced over time to approach determinism, but in practice it would be dicult\r
to choose the reduction schedule, or even the initial temperature, without more prior\r
knowledge of the true action values than we would like to assume. Action preferences"""

[[sections]]
number = "13.1"
title = "Policy Approximation and its Advantages 323"
text = """
are di↵erent because they do not approach specific values; instead they are driven to\r
produce the optimal stochastic policy. If the optimal policy is deterministic, then the\r
preferences of the optimal actions will be driven infinitely higher than all suboptimal\r
actions (if permitted by the parameterization).\r
A second advantage of parameterizing policies according to the soft-max in action\r
preferences is that it enables the selection of actions with arbitrary probabilities. In\r
problems with significant function approximation, the best approximate policy may be\r
stochastic. For example, in card games with imperfect information the optimal play is\r
often to do two di↵erent things with specific probabilities, such as when blung in Poker.\r
Action-value methods have no natural way of finding stochastic optimal policies, whereas\r
policy approximating methods can, as shown in Example 13.1.\r
Example 13.1 Short corridor with switched actions\r
Consider the small corridor gridworld shown inset in the graph below. The reward\r
is 1 per step, as usual. In each of the three nonterminal states there are only\r
two actions, right and left. These actions have their usual consequences in the\r
first and third states (left causes no movement in the first state), but in the\r
second state they are reversed, so that right moves to the left and left moves to\r
the right. The problem is dicult because all the states appear identical under\r
the function approximation. In particular, we define x(s,right) = [1, 0]> and\r
x(s, left) = [0, 1]>, for all s. An action-value method with "-greedy action selection\r
is forced to choose between just two policies: choosing right with high probability\r
1  "/2 on all steps or choosing left with the same high probability on all time\r
steps. If " = 0.1, then these two policies achieve a value (at the start state)\r
of less than 44 and 82, respectively, as shown in the graph. A method can\r
do significantly better if it can learn a specific probability with which to select\r
right. The best probability is about 0.59, which achieves a value of about 11.6.\r
probability of right action\r
-11.6\r
0.1 0.2\r
-20\r
-40\r
-60\r
-80\r
-100\r
0 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1\r
-greedy left \r
-greedy right \r
optimal\r
stochastic\r
policy \r
J(✓) = v⇡✓ (S)\r
S G"""

[[sections]]
number = "324"
title = "Chapter 13: Policy Gradient Methods"
text = """
Perhaps the simplest advantage that policy parameterization may have over action\u0002value parameterization is that the policy may be a simpler function to approximate.\r
Problems vary in the complexity of their policies and action-value functions. For some,\r
the action-value function is simpler and thus easier to approximate. For others, the policy\r
is simpler. In the latter case a policy-based method will typically learn faster and yield a\r
superior asymptotic policy (as in Tetris; see S¸im¸sek, Alg´orta, and Kothiyal, 2016).\r
Finally, we note that the choice of policy parameterization is sometimes a good way\r
of injecting prior knowledge about the desired form of the policy into the reinforcement\r
learning system. This is often the most important reason for using a policy-based learning\r
method.\r
Exercise 13.1 Use your knowledge of the gridworld and its dynamics to determine an\r
exact symbolic expression for the optimal probability of selecting the right action in\r
Example 13.1. ⇤"""

[[sections]]
number = "13.2"
title = "The Policy Gradient Theorem"
text = """
In addition to the practical advantages of policy parameterization over "-greedy action\r
selection, there is also an important theoretical advantage. With continuous policy\r
parameterization the action probabilities change smoothly as a function of the learned\r
parameter, whereas in "-greedy selection the action probabilities may change dramatically\r
for an arbitrarily small change in the estimated action values, if that change results in a\r
di↵erent action having the maximal value. Largely because of this, stronger convergence\r
guarantees are available for policy-gradient methods than for action-value methods. In\r
particular, it is the continuity of the policy dependence on the parameters that enables\r
policy-gradient methods to approximate gradient ascent (13.1).\r
The episodic and continuing cases define the performance measure, J(✓), di↵erently\r
and thus have to be treated separately to some extent. Nevertheless, we will try to\r
present both cases uniformly, and we develop a notation so that the major theoretical\r
results can be described with a single set of equations.\r
In this section we treat the episodic case, for which we define the performance measure\r
as the value of the start state of the episode. We can simplify the notation without\r
losing any meaningful generality by assuming that every episode starts in some particular\r
(non-random) state s0. Then, in the episodic case we define performance as\r
J(✓) .= v⇡✓ (s0), (13.4)\r
where v⇡✓ is the true value function for ⇡✓, the policy determined by ✓. From here on in\r
our discussion we will assume no discounting ( = 1) for the episodic case, although for\r
completeness we do include the possibility of discounting in the boxed algorithms.\r
With function approximation it may seem challenging to change the policy parameter\r
in a way that ensures improvement. The problem is that performance depends on both\r
the action selections and the distribution of states in which those selections are made,\r
and that both of these are a↵ected by the policy parameter. Given a state, the e↵ect of\r
the policy parameter on the actions, and thus on reward, can be computed in a relatively\r
straightforward way from knowledge of the parameterization. But the e↵ect of the policy"""

[[sections]]
number = "13.2"
title = "The Policy Gradient Theorem 325"
text = """
Proof of the Policy Gradient Theorem (episodic case)\r
With just elementary calculus and re-arranging of terms, we can prove the policy\r
gradient theorem from first principles. To keep the notation simple, we leave it\r
implicit in all cases that ⇡ is a function of ✓, and all gradients are also implicitly\r
with respect to ✓. First note that the gradient of the state-value function can be\r
written in terms of the action-value function as\r
rv⇡(s) = r\r
"\r
X\r
a\r
⇡(a|s)q⇡(s, a)\r
#\r
, for all s 2 S (Exercise 3.18)\r
= X\r
a\r
h\r
r⇡(a|s)q⇡(s, a) + ⇡(a|s)rq⇡(s, a)\r
i\r
(product rule of calculus)\r
= X\r
a\r
h\r
r⇡(a|s)q⇡(s, a) + ⇡(a|s)rX\r
s0,r\r
p(s0, r|s, a)\r
\r
r + v⇡(s0)\r
i\r
(Exercise 3.19 and Equation 3.2)\r
= X\r
a\r
h\r
r⇡(a|s)q⇡(s, a) + ⇡(a|s)\r
X\r
s0\r
p(s0|s, a)rv⇡(s0)\r
i\r
(Eq. 3.4)\r
= X\r
a\r
h\r
r⇡(a|s)q⇡(s, a) + ⇡(a|s)\r
X\r
s0\r
p(s0|s, a) (unrolling)\r
X\r
a0\r
⇥\r
r⇡(a0|s0)q⇡(s0, a0) + ⇡(a0|s0)\r
X\r
s00\r
p(s00 |s0, a0)rv⇡(s00)\r
⇤i\r
= X\r
x2S\r
X1\r
k=0\r
Pr(s!x, k, ⇡)\r
X\r
a\r
r⇡(a|x)q⇡(x, a),\r
after repeated unrolling, where Pr(s!x, k, ⇡) is the probability of transitioning\r
from state s to state x in k steps under policy ⇡. It is then immediate that\r
rJ(✓) = rv⇡(s0)\r
= X\r
s\r
 X1\r
k=0\r
Pr(s0!s, k, ⇡)\r
!X\r
a\r
r⇡(a|s)q⇡(s, a)\r
= X\r
s\r
⌘(s)\r
X\r
a\r
r⇡(a|s)q⇡(s, a) (box page 199)\r
= X\r
s0\r
⌘(s0)\r
X\r
s\r
⌘(s)\r
P\r
s0 ⌘(s0\r
)\r
X\r
a\r
r⇡(a|s)q⇡(s, a)\r
= X\r
s0\r
⌘(s0)\r
X\r
s\r
µ(s)\r
X\r
a\r
r⇡(a|s)q⇡(s, a) (Eq. 9.3)\r
/ X\r
s\r
µ(s)\r
X\r
a\r
r⇡(a|s)q⇡(s, a) (Q.E.D.)"""

[[sections]]
number = "326"
title = "Chapter 13: Policy Gradient Methods"
text = """
on the state distribution is a function of the environment and is typically unknown. How\r
can we estimate the performance gradient with respect to the policy parameter when the\r
gradient depends on the unknown e↵ect of policy changes on the state distribution?\r
Fortunately, there is an excellent theoretical answer to this challenge in the form of\r
the policy gradient theorem, which provides an analytic expression for the gradient of\r
performance with respect to the policy parameter (which is what we need to approximate\r
for gradient ascent (13.1)) that does not involve the derivative of the state distribution.\r
The policy gradient theorem for the episodic case establishes that\r
rJ(✓) / X\r
s\r
µ(s)\r
X\r
a\r
q⇡(s, a)r⇡(a|s, ✓), (13.5)\r
where the gradients are column vectors of partial derivatives with respect to the compo\u0002nents of ✓, and ⇡ denotes the policy corresponding to parameter vector ✓. The symbol /\r
here means “proportional to”. In the episodic case, the constant of proportionality is the\r
average length of an episode, and in the continuing case it is 1, so that the relationship is\r
actually an equality. The distribution µ here (as in Chapters 9 and 10) is the on-policy\r
distribution under ⇡ (see page 199). The policy gradient theorem is proved for the\r
episodic case in the box on the previous page."""

[[sections]]
number = "13.3"
title = "REINFORCE: Monte Carlo Policy Gradient"
text = """
We are now ready to derive our first policy-gradient learning algorithm. Recall our overall\r
strategy of stochastic gradient ascent (13.1), which requires a way to obtain samples such\r
that the expectation of the sample gradient is proportional to the actual gradient of the\r
performance measure as a function of the parameter. The sample gradients need only be\r
proportional to the gradient because any constant of proportionality can be absorbed\r
into the step size ↵, which is otherwise arbitrary. The policy gradient theorem gives an\r
exact expression proportional to the gradient; all that is needed is some way of sampling\r
whose expectation equals or approximates this expression. Notice that the right-hand\r
side of the policy gradient theorem is a sum over states weighted by how often the states\r
occur under the target policy ⇡; if ⇡ is followed, then states will be encountered in these\r
proportions. Thus\r
rJ(✓) / X\r
s\r
µ(s)\r
X\r
a\r
q⇡(s, a)r⇡(a|s, ✓)\r
= E⇡\r
"\r
X\r
a\r
q⇡(St, a)r⇡(a|St, ✓)\r
#\r
. (13.6)\r
We could stop here and instantiate our stochastic gradient-ascent algorithm (13.1) as\r
✓t+1\r
.\r
= ✓t + ↵\r
X\r
a\r
qˆ(St, a, w)r⇡(a|St, ✓), (13.7)\r
where qˆ is some learned approximation to q⇡. This algorithm, which has been called\r
an all-actions method because its update involves all of the actions, is promising and"""

[[sections]]
number = "13.3"
title = "REINFORCE: Monte Carlo Policy Gradient 327"
text = """
deserving of further study, but our current interest is the classical REINFORCE algorithm\r
(Willams, 1992) whose update at time t involves just At, the one action actually taken at\r
time t.\r
We continue our derivation of REINFORCE by introducing At in the same way as we\r
introduced St in (13.6)—by replacing a sum over the random variable’s possible values\r
by an expectation under ⇡, and then sampling the expectation. Equation (13.6) involves\r
an appropriate sum over actions, but each term is not weighted by ⇡(a|St, ✓) as is needed\r
for an expectation under ⇡. So we introduce such a weighting, without changing the\r
equality, by multiplying and then dividing the summed terms by ⇡(a|St, ✓). Continuing\r
from (13.6), we have\r
rJ(✓) / E⇡\r
"\r
X\r
a\r
⇡(a|St, ✓)q⇡(St, a)\r
r⇡(a|St, ✓)\r
⇡(a|St, ✓)\r
#\r
= E⇡\r
\r
q⇡(St, At)\r
r⇡(At|St, ✓)\r
⇡(At|St, ✓)\r
\r
(replacing a by the sample At ⇠ ⇡)\r
= E⇡\r
\r
Gt\r
r⇡(At|St, ✓)\r
⇡(At|St, ✓)\r
\r
, (because E⇡[Gt|St, At] = q⇡(St, At))\r
where Gt is the return as usual. The final expression in brackets is exactly what is needed,\r
a quantity that can be sampled on each time step whose expectation is proportional\r
to the gradient. Using this sample to instantiate our generic stochastic gradient ascent\r
algorithm (13.1) yields the REINFORCE update:\r
✓t+1\r
.\r
= ✓t + ↵Gt\r
r⇡(At|St, ✓t)\r
⇡(At|St, ✓t) . (13.8)\r
This update has an intuitive appeal. Each increment is proportional to the product of a\r
return Gt and a vector, the gradient of the probability of taking the action actually taken\r
divided by the probability of taking that action. The vector is the direction in parameter\r
space that most increases the probability of repeating the action At on future visits\r
to state St. The update increases the parameter vector in this direction proportional\r
to the return, and inversely proportional to the action probability. The former makes\r
sense because it causes the parameter to move most in the directions that favor actions\r
that yield the highest return. The latter makes sense because otherwise actions that are\r
selected frequently are at an advantage (the updates will be more often in their direction)\r
and might win out even if they do not yield the highest return.\r
Note that REINFORCE uses the complete return from time t, which includes all\r
future rewards up until the end of the episode. In this sense REINFORCE is a Monte\r
Carlo algorithm and is well defined only for the episodic case with all updates made in\r
retrospect after the episode is completed (like the Monte Carlo algorithms in Chapter 5).\r
This is shown explicitly in the boxed algorithm on the next page.\r
Notice that the update in the last line of pseudocode appears rather di↵erent from\r
the REINFORCE update rule (13.8). One di↵erence is that the pseudocode uses the\r
compact expression r ln ⇡(At|St, ✓t) for the fractional vector r⇡(At|St,✓t)\r
⇡(At|St,✓t) in (13.8). That\r
these two expressions for the vector are equivalent follows from the identity r ln x = rx\r
x ."""

[[sections]]
number = "328"
title = "Chapter 13: Policy Gradient Methods"
text = """
This vector has been given several names and notations in the literature; we will refer\r
to it simply as the eligibility vector. Note that it is the only place that the policy\r
parameterization appears in the algorithm.\r
REINFORCE: Monte-Carlo Policy-Gradient Control (episodic) for ⇡⇤\r
Input: a di↵erentiable policy parameterization ⇡(a|s, ✓)\r
Algorithm parameter: step size ↵ > 0\r
Initialize policy parameter ✓ 2 Rd0(e.g., to 0)\r
Loop forever (for each episode):\r
Generate an episode S0, A0, R1,...,ST 1, AT 1, RT , following ⇡(·|·, ✓)\r
Loop for each step of the episode t = 0, 1,...,T  1:\r
G PT\r
k=t+1 kt1Rk (Gt)\r
✓ ✓ + ↵tGr ln ⇡(At|St, ✓)\r
The second di↵erence between the pseudocode update and the REINFORCE update\r
equation (13.8) is that the former includes a factor of t. This is because, as mentioned\r
earlier, in the text we are treating the non-discounted case ( = 1) while in the boxed\r
algorithms we are giving the algorithms for the general discounted case. All of the ideas\r
go through in the discounted case with appropriate adjustments (including to the box on\r
page 199) but involve additional complexity that distracts from the main ideas.\r
⇤\r
Exercise 13.2 Generalize the box on page 199, the policy gradient theorem (13.5), the\r
proof of the policy gradient theorem (page 325), and the steps leading to the REINFORCE\r
update equation (13.8), so that (13.8) ends up with a factor of t and thus aligns with\r
the general algorithm given in the pseudocode. ⇤\r
Figure 13.1 shows the performance of REINFORCE on the short-corridor gridworld\r
from Example 13.1.\r
↵ = 213\r
↵ = 212\r
Episode\r
1 200 400 600 800 1000\r
-80\r
-90\r
-60\r
-40\r
-20\r
-10\r
Total reward\r
on episode\r
averaged over 100 runs\r
G0\r
v⇤(s0)\r
↵ = 214\r
Figure 13.1: REINFORCE on the short-corridor gridworld (Example 13.1). With a good step\r
size, the total reward per episode approaches the optimal value of the start state."""

[[sections]]
number = "13.4"
title = "REINFORCE with Baseline 329"
text = """
As a stochastic gradient method, REINFORCE has good theoretical convergence\r
properties. By construction, the expected update over an episode is in the same direction\r
as the performance gradient. This assures an improvement in expected performance for\r
suciently small ↵, and convergence to a local optimum under standard stochastic approx\u0002imation conditions for decreasing ↵. However, as a Monte Carlo method REINFORCE\r
may be of high variance and thus produce slow learning.\r
Exercise 13.3 In Section 13.1 we considered policy parameterizations using the soft-max in\r
action preferences (13.2) with linear action preferences (13.3). For this parameterization,\r
prove that the eligibility vector is\r
r ln ⇡(a|s, ✓) = x(s, a) X\r
b\r
⇡(b|s, ✓)x(s, b), (13.9)\r
using the definitions and elementary calculus. ⇤"""

[[sections]]
number = "13.4"
title = "REINFORCE with Baseline"
text = """
The policy gradient theorem (13.5) can be generalized to include a comparison of the\r
action value to an arbitrary baseline b(s):\r
rJ(✓) / X\r
s\r
µ(s)\r
X\r
a\r
⇣\r
q⇡(s, a)  b(s)\r
⌘\r
r⇡(a|s, ✓). (13.10)\r
The baseline can be any function, even a random variable, as long as it does not vary\r
with a; the equation remains valid because the subtracted quantity is zero:\r
X\r
a\r
b(s)r⇡(a|s, ✓) = b(s)rX\r
a\r
⇡(a|s, ✓) = b(s)r1=0.\r
The policy gradient theorem with baseline (13.10) can be used to derive an update\r
rule using similar steps as in the previous section. The update rule that we end up with\r
is a new version of REINFORCE that includes a general baseline:\r
✓t+1\r
.\r
= ✓t + ↵\r
⇣\r
Gt  b(St)\r
⌘r⇡(At|St, ✓t)\r
⇡(At|St, ✓t) . (13.11)\r
Because the baseline could be uniformly zero, this update is a strict generalization of\r
REINFORCE. In general, the baseline leaves the expected value of the update unchanged,\r
but it can have a large e↵ect on its variance. For example, we saw in Section 2.8 that an\r
analogous baseline can significantly reduce the variance (and thus speed the learning) of\r
gradient bandit algorithms. In the bandit algorithms the baseline was just a number (the\r
average of the rewards seen so far), but for MDPs the baseline should vary with state.\r
In some states all actions have high values and we need a high baseline to di↵erentiate\r
the higher valued actions from the less highly valued ones; in other states all actions will\r
have low values and a low baseline is appropriate.\r
One natural choice for the baseline is an estimate of the state value, vˆ(St,w), where\r
w 2 Rd is a weight vector learned by one of the methods presented in previous chapters."""

[[sections]]
number = "330"
title = "Chapter 13: Policy Gradient Methods"
text = """
Because REINFORCE is a Monte Carlo method for learning the policy parameter, ✓,\r
it seems natural to also use a Monte Carlo method to learn the state-value weights, w.\r
A complete pseudocode algorithm for REINFORCE with baseline using such a learned\r
state-value function as the baseline is given in the box below.\r
REINFORCE with Baseline (episodic), for estimating ⇡✓ ⇡ ⇡⇤\r
Input: a di↵erentiable policy parameterization ⇡(a|s, ✓)\r
Input: a di↵erentiable state-value function parameterization ˆv(s,w)\r
Algorithm parameters: step sizes ↵✓ > 0, ↵w > 0\r
Initialize policy parameter ✓ 2 Rd0and state-value weights w 2 Rd (e.g., to 0)\r
Loop forever (for each episode):\r
Generate an episode S0, A0, R1,...,ST 1, AT 1, RT , following ⇡(·|·, ✓)\r
Loop for each step of the episode t = 0, 1,...,T  1:\r
G PT\r
k=t+1 kt1Rk (Gt)\r
 G  vˆ(St,w)\r
w w + ↵w rvˆ(St,w)\r
✓ ✓ + ↵✓ t r ln ⇡(At|St, ✓)\r
This algorithm has two step sizes, denoted ↵✓ and ↵w (where ↵✓ is the ↵ in (13.11)).\r
Choosing the step size for values (here ↵w) is relatively easy; in the linear case we have\r
rules of thumb for setting it, such as ↵w = 0.1/E\r
⇥\r
krvˆ(St,w)k\r
2\r
µ\r
⇤\r
(see Section 9.6). It is\r
much less clear how to set the step size for the policy parameters, ↵✓, whose best value\r
depends on the range of variation of the rewards and on the policy parameterization.\r
↵ = 213\r
Episode\r
1 200 400 600 800 1000\r
-80\r
-90\r
-60\r
-40\r
-20\r
-10 v⇤(s0)\r
REINFORCE\r
REINFORCE with baseline\r
↵ = 29\r
↵✓ = 29, ↵w = 26\r
Total reward\r
on episode\r
averaged over 100 runs\r
G0\r
Figure 13.2: Adding a baseline to REINFORCE can make it learn much faster, as illus\u0002trated here on the short-corridor gridworld (Example 13.1). The step size used here for plain\r
REINFORCE is that at which it performs best (to the nearest power of two; see Figure 13.1)."""

[[sections]]
number = "13.5"
title = "Actor–Critic Methods 331"
text = """
Figure 13.2 compares the behavior of REINFORCE with and without a baseline on\r
the short-corridor gridword (Example 13.1). Here the approximate state-value function\r
used in the baseline is ˆv(s,w) = w. That is, w is a single component, w."""

[[sections]]
number = "13.5"
title = "Actor–Critic Methods"
text = """
In REINFORCE with baseline, the learned state-value function estimates the value of\r
the first state of each state transition. This estimate sets a baseline for the subsequent\r
return, but is made prior to the transition’s action and thus cannot be used to assess that\r
action. In actor–critic methods, on the other hand, the state-value function is applied\r
also to the second state of the transition. The estimated value of the second state, when\r
discounted and added to the reward, constitutes the one-step return, Gt:t+1, which is a\r
useful estimate of the actual return and thus is a way of assessing the action. As we have\r
seen in the TD learning of value functions throughout this book, the one-step return is\r
often superior to the actual return in terms of its variance and computational congeniality,\r
even though it introduces bias. We also know how we can flexibly modulate the extent\r
of the bias with n-step returns and eligibility traces (Chapters 7 and 12). When the\r
state-value function is used to assess actions in this way it is called a critic, and the\r
overall policy-gradient method is termed an actor–critic method. Note that the bias in\r
the gradient estimate is not due to bootstrapping as such; the actor would be biased even\r
if the critic was learned by a Monte Carlo method.\r
First consider one-step actor–critic methods, the analog of the TD methods introduced\r
in Chapter 6 such as TD(0), Sarsa(0), and Q-learning. The main appeal of one-step\r
methods is that they are fully online and incremental, yet avoid the complexities of\r
eligibility traces. They are a special case of the eligibility trace methods, but easier\r
to understand. One-step actor–critic methods replace the full return of REINFORCE\r
(13.11) with the one-step return (and use a learned state-value function as the baseline)\r
as follows:\r
✓t+1\r
.\r
= ✓t + ↵\r
⇣\r
Gt:t+1  vˆ(St,w)\r
⌘r⇡(At|St, ✓t)\r
⇡(At|St, ✓t) (13.12)\r
= ✓t + ↵\r
⇣\r
Rt+1 + vˆ(St+1,w)  vˆ(St,w)\r
⌘r⇡(At|St, ✓t)\r
⇡(At|St, ✓t) (13.13)\r
= ✓t + ↵t\r
r⇡(At|St, ✓t)\r
⇡(At|St, ✓t) . (13.14)\r
The natural state-value-function learning method to pair with this is semi-gradient TD(0).\r
Pseudocode for the complete algorithm is given in the box at the top of the next page.\r
Note that it is now a fully online, incremental algorithm, with states, actions, and rewards\r
processed as they occur and then never revisited."""

[[sections]]
number = "332"
title = "Chapter 13: Policy Gradient Methods"
text = """
One-step Actor–Critic (episodic), for estimating ⇡✓ ⇡ ⇡⇤\r
Input: a di↵erentiable policy parameterization ⇡(a|s, ✓)\r
Input: a di↵erentiable state-value function parameterization ˆv(s,w)\r
Parameters: step sizes ↵✓ > 0, ↵w > 0\r
Initialize policy parameter ✓ 2 Rd0and state-value weights w 2 Rd (e.g., to 0)\r
Loop forever (for each episode):\r
Initialize S (first state of episode)\r
I 1\r
Loop while S is not terminal (for each time step):\r
A ⇠ ⇡(·|S, ✓)\r
Take action A, observe S0, R\r
 R +  vˆ(S0,w)  vˆ(S,w) (if S0 is terminal, then ˆv(S0,w) .= 0)\r
w w + ↵w rvˆ(S,w)\r
✓ ✓ + ↵✓ I r ln ⇡(A|S, ✓)\r
I I\r
S S0\r
The generalizations to the forward view of n-step methods and then to a -return\r
algorithm are straightforward. The one-step return in (13.12) is merely replaced by Gt:t+n\r
or G\r
t respectively. The backward view of the -return algorithm is also straightforward,\r
using separate eligibility traces for the actor and critic, each after the patterns in\r
Chapter 12. Pseudocode for the complete algorithm is given in the box below.\r
Actor–Critic with Eligibility Traces (episodic), for estimating ⇡✓ ⇡ ⇡⇤\r
Input: a di↵erentiable policy parameterization ⇡(a|s, ✓)\r
Input: a di↵erentiable state-value function parameterization ˆv(s,w)\r
Parameters: trace-decay rates ✓ 2 [0, 1], w 2 [0, 1]; step sizes ↵✓ > 0, ↵w > 0\r
Initialize policy parameter ✓ 2 Rd0and state-value weights w 2 Rd (e.g., to 0)\r
Loop forever (for each episode):\r
Initialize S (first state of episode)\r
z✓ 0 (d0-component eligibility trace vector)\r
zw 0 (d-component eligibility trace vector)\r
I 1\r
Loop while S is not terminal (for each time step):\r
A ⇠ ⇡(·|S, ✓)\r
Take action A, observe S0, R\r
 R +  vˆ(S0,w)  vˆ(S,w) (if S0 is terminal, then ˆv(S0,w) .= 0)\r
zw wzw + rvˆ(S,w)\r
z✓ ✓z✓ + I r ln ⇡(A|S, ✓)\r
w w + ↵w zw\r
✓ ✓ + ↵✓ z✓\r
I I\r
S S0"""

[[sections]]
number = "13.6"
title = "Policy Gradient for Continuing Problems 333"
text = ""

[[sections]]
number = "13.6"
title = "Policy Gradient for Continuing Problems"
text = """
As discussed in Section 10.3, for continuing problems without episode boundaries we need\r
to define performance in terms of the average rate of reward per time step:\r
J(✓) .= r(⇡) .= lim\r
h!1\r
1\r
h\r
X\r
h\r
t=1\r
E[Rt | S0, A0:t1 ⇠⇡] (13.15)\r
= limt!1 E[Rt | S0, A0:t1 ⇠⇡]\r
= X\r
s\r
µ(s)\r
X\r
a\r
⇡(a|s)\r
X\r
s0,r\r
p(s0, r|s, a)r,\r
where µ is the steady-state distribution under ⇡, µ(s) .= limt!1 Pr{St =s|A0:t ⇠⇡},\r
which is assumed to exist and to be independent of S0 (an ergodicity assumption).\r
Remember that this is the special distribution under which, if you select actions according\r
to ⇡, you remain in the same distribution:\r
X\r
s\r
µ(s)\r
X\r
a\r
⇡(a|s, ✓)p(s0|s, a) = µ(s0), for all s0 2 S. (13.16)\r
Complete pseudocode for the actor–critic algorithm in the continuing case (backward\r
view) is given in the box below.\r
Actor–Critic with Eligibility Traces (continuing), for estimating ⇡✓ ⇡ ⇡⇤\r
Input: a di↵erentiable policy parameterization ⇡(a|s, ✓)\r
Input: a di↵erentiable state-value function parameterization ˆv(s,w)\r
Algorithm parameters: w 2 [0, 1], ✓ 2 [0, 1], ↵w > 0, ↵✓ > 0, ↵R¯ > 0\r
Initialize R¯ 2 R (e.g., to 0)\r
Initialize state-value weights w 2 Rd and policy parameter ✓ 2 Rd0(e.g., to 0)\r
Initialize S 2 S (e.g., to s0)\r
zw 0 (d-component eligibility trace vector)\r
z✓ 0 (d0-component eligibility trace vector)\r
Loop forever (for each time step):\r
A ⇠ ⇡(·|S, ✓)\r
Take action A, observe S0, R\r
 R  R¯ + ˆv(S0,w)  vˆ(S,w)\r
R¯ R¯ + ↵R¯ \r
zw wzw + rvˆ(S,w)\r
z✓ ✓z✓ + r ln ⇡(A|S, ✓)\r
w w + ↵w zw\r
✓ ✓ + ↵✓ z✓\r
S S0"""

[[sections]]
number = "334"
title = "Chapter 13: Policy Gradient Methods"
text = """
Naturally, in the continuing case, we define values, v⇡(s) .= E⇡[Gt|St =s] and q⇡(s, a) .=\r
E⇡[Gt|St =s, At =a], with respect to the di↵erential return:\r
Gt\r
.\r
= Rt+1  r(⇡) + Rt+2  r(⇡) + Rt+3  r(⇡) + ··· . (13.17)\r
With these alternate definitions, the policy gradient theorem as given for the episodic\r
case (13.5) remains true for the continuing case. A proof is given in the box on the next\r
page. The forward and backward view equations also remain the same.\r
Proof of the Policy Gradient Theorem (continuing case)\r
The proof of the policy gradient theorem for the continuing case begins similarly\r
to the episodic case. Again we leave it implicit in all cases that ⇡ is a function\r
of ✓ and that the gradients are with respect to ✓. Recall that in the continuing\r
case J(✓) = r(⇡) (13.15) and that v⇡ and q⇡ denote values with respect to the\r
di↵erential return (13.17). The gradient of the state-value function can be written,\r
for any s 2 S, as\r
rv⇡(s) = r\r
"\r
X\r
a\r
⇡(a|s)q⇡(s, a)\r
#\r
, for all s 2 S (Exercise 3.18)\r
= X\r
a\r
h\r
r⇡(a|s)q⇡(s, a) + ⇡(a|s)rq⇡(s, a)\r
i\r
(product rule of calculus)\r
= X\r
a\r
h\r
r⇡(a|s)q⇡(s, a) + ⇡(a|s)rX\r
s0,r\r
p(s0, r|s, a)\r
\r
r  r(✓) + v⇡(s0)\r
i\r
= X\r
a\r
h\r
r⇡(a|s)q⇡(s, a) + ⇡(a|s)\r
⇥\r
rr(✓) +X\r
s0\r
p(s0|s, a)rv⇡(s0)\r
⇤i\r
.\r
After re-arranging terms, we obtain\r
rr(✓) = X\r
a\r
h\r
r⇡(a|s)q⇡(s, a) + ⇡(a|s)\r
X\r
s0\r
p(s0|s, a)rv⇡(s0)\r
i\r
 rv⇡(s).\r
Notice that the left-hand side can be written rJ(✓), and that it does not depend\r
on s. Thus the right-hand side does not depend on s either, and we can safely sum\r
it over all s 2 S, weighted by µ(s), without changing it (because P\r
s µ(s) = 1):\r
rJ(✓) = X\r
s\r
µ(s)\r
 X\r
a\r
h\r
r⇡(a|s)q⇡(s, a) + ⇡(a|s)\r
X\r
s0\r
p(s0|s, a)rv⇡(s0)\r
i\r
 rv⇡(s)\r
!\r
= X\r
s\r
µ(s)\r
X\r
a\r
r⇡(a|s)q⇡(s, a)\r
+X\r
s\r
µ(s)\r
X\r
a\r
⇡(a|s)\r
X\r
s0\r
p(s0|s, a)rv⇡(s0) X\r
s\r
µ(s)rv⇡(s)"""

[[sections]]
number = "13.7"
title = "Policy Parameterization for Continuous Actions 335"
text = """
= X\r
s\r
µ(s)\r
X\r
a\r
r⇡(a|s)q⇡(s, a)\r
+X\r
s0\r
X\r
s\r
µ(s)\r
X\r
a\r
⇡(a|s)p(s0|s, a)\r
| {z } µ(s0) (13.16)\r
rv⇡(s0) X\r
s\r
µ(s)rv⇡(s)\r
= X\r
s\r
µ(s)\r
X\r
a\r
r⇡(a|s)q⇡(s, a) +X\r
s0\r
µ(s0)rv⇡(s0) X\r
s\r
µ(s)rv⇡(s)\r
= X\r
s\r
µ(s)\r
X\r
a\r
r⇡(a|s)q⇡(s, a). Q.E.D."""

[[sections]]
number = "13.7"
title = "Policy Parameterization for Continuous Actions"
text = """
Policy-based methods o↵er practical ways of dealing with large action spaces, even\r
continuous spaces with an infinite number of actions. Instead of computing learned\r
probabilities for each of the many actions, we instead learn statistics of the probability\r
distribution. For example, the action set might be the real numbers, with actions chosen\r
from a normal (Gaussian) distribution.\r
The probability density function for the normal distribution is conventionally written\r
p(x) .= 1\r
\r
p2⇡ exp✓\r
(x  µ)2\r
22\r
◆\r
, (13.18)\r
p(x) .= 1\r
\r
p2⇡ exp ✓\r
(x  µ)2\r
22\r
◆\r
φµ,σ 2 ( 0.8\r
0.6\r
0.4\r
0.2\r
0.0\r
−5 −3 1 3 5\r
x\r
1.0\r
−4 −2 −1 0 2 4\r
x)\r
µ= 0,\r
µ= 0,\r
µ= 0,\r
µ= −2,\r
2 σ = 0.2, 2 σ = 1.0,\r
2 σ = 5.0,\r
2 σ = 0.5,\r
p(x) .= 1\r
\r
p2⇡ exp ✓\r
(x  µ)2\r
22\r
◆\r
where µ and  here are the mean and stan\u0002dard deviation of the normal distribution,\r
and of course ⇡ here is just the number\r
⇡ ⇡ 3.14159. The probability density func\u0002tions for several di↵erent means and stan\u0002dard deviations are shown to the right. The\r
value p(x) is the density of the probability\r
at x, not the probability. It can be greater\r
than 1; it is the total area under p(x) that\r
must sum to 1. In general, one can take\r
the integral under p(x) for any range of x\r
values to get the probability of x falling\r
within that range.\r
To produce a policy parameterization, the policy can be defined as the normal proba\u0002bility density over a real-valued scalar action, with mean and standard deviation given\r
by parametric function approximators that depend on the state. That is,\r
⇡(a|s, ✓) .= 1\r
(s, ✓)\r
p2⇡ exp✓\r
(a  µ(s, ✓))2\r
2(s, ✓)2\r
◆\r
, (13.19)\r
where µ : S⇥Rd0! R and  : S⇥Rd0! R+ are two parameterized function approximators."""

[[sections]]
number = "336"
title = "Chapter 13: Policy Gradient Methods"
text = """
To complete the example we need only give a form for these approximators. For this we\r
divide the policy’s parameter vector into two parts, ✓ = [✓µ, ✓]\r
>, one part to be used\r
for the approximation of the mean and one part for the approximation of the standard\r
deviation. The mean can be approximated as a linear function. The standard deviation\r
must always be positive and is better approximated as the exponential of a linear function.\r
Thus\r
µ(s, ✓) .= ✓µ\r
>xµ(s) and (s, ✓) .\r
= exp⇣✓\r
>x(s)\r
⌘\r
, (13.20)\r
where xµ(s) and x(s) are state feature vectors perhaps constructed by one of the methods\r
described in Section 9.5. With these definitions, all the algorithms described in the rest\r
of this chapter can be applied to learn to select real-valued actions.\r
Exercise 13.4 Show that for the Gaussian policy parameterization (Equations 13.19 and\r
13.20) the eligibility vector has the following two parts:\r
r ln ⇡(a|s, ✓µ) = r⇡(a|s, ✓µ)\r
⇡(a|s, ✓) = 1(s, ✓)2\r
\r
a  µ(s, ✓)\r
\r
xµ(s), and\r
r ln ⇡(a|s, ✓) = r⇡(a|s, ✓)\r
⇡(a|s, ✓) =\r
 \r
a  µ(s, ✓)\r
2\r
(s, ✓)2  1\r
!\r
x(s). ⇤\r
Exercise 13.5 A Bernoulli-logistic unit is a stochastic neuron-like unit used in some ANNs\r
(Section 9.7). Its input at time t is a feature vector x(St); its output, At, is a random\r
variable having two values, 0 and 1, with Pr{At = 1} = Pt and Pr{At = 0} = 1  Pt (the\r
Bernoulli distribution). Let h(s, 0, ✓) and h(s, 1, ✓) be the preferences in state s for the\r
unit’s two actions given policy parameter ✓. Assume that the di↵erence between the\r
action preferences is given by a weighted sum of the unit’s input vector, that is, assume\r
that h(s, 1, ✓)  h(s, 0, ✓) = ✓>x(s), where ✓ is the unit’s weight vector.\r
(a) Show that if the exponential soft-max distribution (13.2) is used to convert action\r
preferences to policies, then Pt = ⇡(1|St, ✓t)=1/(1 + exp(✓>\r
t x(St))) (the logistic\r
function).\r
(b) What is the Monte-Carlo REINFORCE update of ✓t to ✓t+1 upon receipt of return\r
Gt?\r
(c) Express the eligibility r ln ⇡(a|s, ✓) for a Bernoulli-logistic unit, in terms of a, x(s),\r
and ⇡(a|s, ✓) by calculating the gradient.\r
Hint for part (c): Define P = ⇡(1|s, ✓) and compute the derivative of the logarithm, for\r
each action, using the chain rule on P. Combine the two results into one expression that\r
depends on a and P, and then use the chain rule again, this time on ✓>x(s), noting that\r
the derivative of the logistic function f(x)=1/(1 + ex) is f(x)(1  f(x)). ⇤"""

[[sections]]
number = "13.8"
title = "Summary 337"
text = ""

[[sections]]
number = "13.8"
title = "Summary"
text = """
Prior to this chapter, this book focused on action-value methods—meaning methods that\r
learn action values and then use them to determine action selections. In this chapter, on\r
the other hand, we considered methods that learn a parameterized policy that enables\r
actions to be taken without consulting action-value estimates. In particular, we have\r
considered policy-gradient methods—meaning methods that update the policy parameter\r
on each step in the direction of an estimate of the gradient of performance with respect\r
to the policy parameter.\r
Methods that learn and store a policy parameter have many advantages. They can\r
learn specific probabilities for taking the actions. They can learn appropriate levels\r
of exploration and approach deterministic policies asymptotically. They can naturally\r
handle continuous action spaces. All these things are easy for policy-based methods but\r
awkward or impossible for "-greedy methods and for action-value methods in general. In\r
addition, on some problems the policy is just simpler to represent parametrically than\r
the value function; these problems are more suited to parameterized policy methods.\r
Parameterized policy methods also have an important theoretical advantage over\r
action-value methods in the form of the policy gradient theorem, which gives an exact\r
formula for how performance is a↵ected by the policy parameter that does not involve\r
derivatives of the state distribution. This theorem provides a theoretical foundation for\r
all policy gradient methods.\r
The REINFORCE method follows directly from the policy gradient theorem. Adding\r
a state-value function as a baseline reduces REINFORCE’s variance without introducing\r
bias. If the state-value function is also used to assess—or criticize—the policy’s action\r
selections, then the value function is called a critic and the policy is called an actor ;\r
the overall method is called an actor–critic method. The critic introduces bias into the\r
actor’s gradient estimates, but is often desirable for the same reason that bootstrapping\r
TD methods are often superior to Monte Carlo methods (substantially reduced variance).\r
Overall, policy-gradient methods provide a significantly di↵erent set of strengths and\r
weaknesses than action-value methods. Today they are less well understood in some\r
respects, but a subject of excitement and ongoing research.\r
Bibliographical and Historical Remarks\r
Methods that we now see as related to policy gradients were actually some of the earliest\r
to be studied in reinforcement learning (Witten, 1977; Barto, Sutton, and Anderson,\r
1983; Sutton, 1984; Williams, 1987, 1992) and in predecessor fields (see Phansalkar\r
and Thathachar, 1995). They were largely supplanted in the 1990s by the action-value\r
methods that are the focus of the other chapters of this book. In recent years, however,\r
attention has returned to actor–critic methods and to policy-gradient methods in general.\r
Among the further developments beyond what we cover here are natural-gradient methods\r
(Amari, 1998; Kakade, 2002, Peters, Vijayakumar and Schaal, 2005; Peters and Schaal,\r
2008; Park, Kim and Kang, 2005; Bhatnagar, Sutton, Ghavamzadeh and Lee, 2009; see\r
Grondman, Busoniu, Lopes and Babuska, 2012), deterministic policy-gradient methods"""

[[sections]]
number = "338"
title = "Chapter 13: Policy Gradient Methods"
text = """
(Silver et al., 2014), o↵-policy policy-gradient methods (Degris, White, and Sutton, 2012;\r
Maei, 2018), and entropy regularization (see Schulman, Chen, and Abbeel, 2017). Major\r
applications include acrobatic helicopter autopilots and AlphaGo (Section 16.6).\r
Our presentation in this chapter is based primarily on that by Sutton, McAllester,\r
Singh, and Mansour (2000), who introduced the term “policy gradient methods.” A\r
useful overview is provided by Bhatnagar et al. (2009). One of the earliest related works\r
is by Aleksandrov, Sysoyev, and Shemeneva (1968). Thomas (2014) first realized that\r
the factor of t, as specified in the boxed algorithms of this chapter, was needed in the\r
case of discounted episodic problems."""

[[sections]]
number = "13.1"
title = "Example 13.1 and the results with it in this chapter were developed with Eric"
text = "Graves."

[[sections]]
number = "13.2"
title = "The policy gradient theorem here and on page 334 was first obtained by Marbach"
text = """
and Tsitsiklis (1998, 2001) and then independently by Sutton et al. (2000). A\r
similar expression was obtained by Cao and Chen (1997). Other early results\r
are due to Konda and Tsitsiklis (2000, 2003), Baxter and Bartlett (2001), and\r
Baxter, Bartlett, and Weaver (2001). Some additional results are developed by\r
Sutton, Singh, and McAllester (2000)."""

[[sections]]
number = "13.3"
title = "REINFORCE is due to Williams (1987, 1992). Phansalkar and Thathachar"
text = """
(1995) proved both local and global convergence theorems for modified versions\r
of REINFORCE algorithms.\r
The all-actions algorithm was first presented in an unpublished but widely\r
circulated incomplete paper (Sutton, Singh, and McAllester, 2000) and then\r
developed further by Ciosek and Whiteson (2017, 2018), who termed it “expected\r
policy gradients,” and by Asadi, Allen, Roderick, Mohamed, Konidaris, and\r
Littman (2017), who called it “mean actor critic.”\r
13.4 The baseline was introduced in Williams’s (1987, 1992) original work. Greensmith,\r
Bartlett, and Baxter (2004) analyzed an arguably better baseline (see Dick, 2015).\r
Thomas and Brunskill (2017) argue that an action-dependent baseline can be\r
used without incurring bias.\r
13.5–6 Actor–critic methods were among the earliest to be investigated in reinforcement\r
learning (Witten, 1977; Barto, Sutton, and Anderson, 1983; Sutton, 1984). The\r
algorithms presented here are based on the work of Degris, White, and Sutton\r
(2012). Actor–critic methods are sometimes referred to as advantage actor–critic\r
(“A2C”) methods in the literature."""

[[sections]]
number = "13.7"
title = "The first to show how continuous actions could be handled this way appears"
text = """
to have been Williams (1987, 1992). The figure on page 335 is adapted from\r
Wikipedia.

Part III: Looking Deeper\r
In this last part of the book we look beyond the standard reinforcement learning ideas\r
presented in the first two parts of the book to briefly survey their relationships with\r
psychology and neuroscience, a sampling of reinforcement learning applications, and some\r
of the active frontiers for future reinforcement learning research.

Chapter 14\r
Psychology\r
In previous chapters we developed ideas for algorithms based on computational con\u0002siderations alone. In this chapter we look at some of these algorithms from another\r
perspective: the perspective of psychology and its study of how animals learn. The goals\r
of this chapter are, first, to discuss ways that reinforcement learning ideas and algorithms\r
correspond to what psychologists have discovered about animal learning, and second, to\r
explain the influence reinforcement learning is having on the study of animal learning.\r
The clear formalism provided by reinforcement learning that systemizes tasks, returns,\r
and algorithms is proving to be enormously useful in making sense of experimental data,\r
in suggesting new kinds of experiments, and in pointing to factors that may be critical to\r
manipulate and to measure. The idea of optimizing return over the long term that is\r
at the core of reinforcement learning is contributing to our understanding of otherwise\r
puzzling features of animal learning and behavior.\r
Some of the correspondences between reinforcement learning and psychological theories\r
are not surprising because the development of reinforcement learning drew inspiration\r
from psychological learning theories. However, as developed in this book, reinforcement\r
learning explores idealized situations from the perspective of an artificial intelligence\r
researcher or engineer, with the goal of solving computational problems with ecient\r
algorithms, rather than to replicate or explain in detail how animals learn. As a result,\r
some of the correspondences we describe connect ideas that arose independently in their\r
respective fields. We believe these points of contact are specially meaningful because they\r
expose computational principles important to learning, whether it is learning by artificial\r
or by natural systems.\r
For the most part, we describe correspondences between reinforcement learning and\r
learning theories developed to explain how animals like rats, pigeons, and rabbits learn\r
in controlled laboratory experiments. Thousands of these experiments were conducted\r
throughout the 20th century, and many are still being conducted today. Although\r
sometimes dismissed as irrelevant to wider issues in psychology, these experiments probe\r
subtle properties of animal learning, often motivated by precise theoretical questions.\r
As psychology shifted its focus to more cognitive aspects of behavior, that is, to mental"""

[[sections]]
number = "342"
title = "Chapter 14: Psychology"
text = """
processes such as thought and reasoning, animal learning experiments came to play\r
less of a role in psychology than they once did. But this experimentation led to the\r
discovery of learning principles that are elemental and widespread throughout the animal\r
kingdom, principles that should not be neglected in designing artificial learning systems.\r
In addition, as we shall see, some aspects of cognitive processing connect naturally to the\r
computational perspective provided by reinforcement learning.\r
This chapter’s final section includes references relevant to the connections we discuss as\r
well as to connections we neglect. We hope this chapter encourages readers to probe all\r
of these connections more deeply. Also included in this final section is a discussion of how\r
the terminology used in reinforcement learning relates to that of psychology. Many of\r
the terms and phrases used in reinforcement learning are borrowed from animal learning\r
theories, but the computational/engineering meanings of these terms and phrases do not\r
always coincide with their meanings in psychology."""

[[sections]]
number = "14.1"
title = "Prediction and Control"
text = """
The algorithms we describe in this book fall into two broad categories: algorithms\r
for prediction and algorithms for control."""

[[sections]]
number = "1"
title = "These categories arise naturally in solution"
text = """
methods for the reinforcement learning problem presented in Chapter 3. In many ways\r
these categories respectively correspond to categories of learning extensively studied\r
by psychologists: classical, or Pavlovian, conditioning and instrumental, or operant,\r
conditioning. These correspondences are not completely accidental because of psychology’s\r
influence on reinforcement learning, but they are nevertheless striking because they connect\r
ideas arising from di↵erent objectives.\r
The prediction algorithms presented in this book estimate quantities that depend\r
on how features of an agent’s environment are expected to unfold over the future. We\r
specifically focus on estimating the amount of reward an agent can expect to receive over\r
the future while it interacts with its environment. In this role, prediction algorithms are\r
policy evaluation algorithms, which are integral components of algorithms for improving\r
policies. But prediction algorithms are not limited to predicting future reward; they can\r
predict any feature of the environment (see, for example, Modayil, White, and Sutton,\r
2014). The correspondence between prediction algorithms and classical conditioning rests\r
on their common property of predicting upcoming stimuli, whether or not those stimuli\r
are rewarding (or punishing).\r
The situation in an instrumental, or operant, conditioning experiment is di↵erent.\r
Here, the experimental apparatus is set up so that an animal is given something it likes\r
(a reward) or something it dislikes (a penalty) depending on what the animal did. The\r
animal learns to increase its tendency to produce rewarded behavior and to decrease its\r
tendency to produce penalized behavior. The reinforcing stimulus is said to be contingent\r
on the animal’s behavior, whereas in classical conditioning it is not (although it is dicult\r
to remove all behavior contingencies in a classical conditioning experiment). Instrumental\r
1What control means for us is di↵erent from what it typically means in animal learning theories; there\r
the environment controls the agent instead of the other way around. See our comments on terminology\r
at the end of this chapter."""

[[sections]]
number = "14.2"
title = "Classical Conditioning 343"
text = """
conditioning experiments are like those that inspired Thorndike’s Law of E↵ect that\r
we briefly discuss in Chapter 1. Control is at the core of this form of learning, which\r
corresponds to the operation of reinforcement learning’s policy-improvement algorithms.\r
Thinking of classical conditioning in terms of prediction, and instrumental conditioning\r
in terms of control, is a starting point for connecting our computational view of rein\u0002forcement learning to animal learning, but in reality, the situation is more complicated\r
than this. There is more to classical conditioning than prediction; it also involves action,\r
and so is a mode of control, sometimes called Pavlovian control. Further, classical and\r
instrumental conditioning interact in interesting ways, with both sorts of learning likely\r
being engaged in most experimental situations. Despite these complications, aligning the\r
classical/instrumental distinction with the prediction/control distinction is a convenient\r
first approximation in connecting reinforcement learning to animal learning.\r
In psychology, the term reinforcement is used to describe learning in both classical and\r
instrumental conditioning. Originally referring only to the strengthening of a pattern of\r
behavior, it is frequently also used for the weakening of a pattern of behavior. A stimulus\r
considered to be the cause of the change in behavior is called a reinforcer, whether or\r
not it is contingent on the animal’s previous behavior. At the end of this chapter we\r
discuss this terminology in more detail and how it relates to terminology used in machine\r
learning."""

[[sections]]
number = "14.2"
title = "Classical Conditioning"
text = """
While studying the activity of the digestive system, the celebrated Russian physiologist\r
Ivan Pavlov found that an animal’s innate responses to certain triggering stimuli can\r
come to be triggered by other stimuli that are quite unrelated to the inborn triggers. His\r
experimental subjects were dogs that had undergone minor surgery to allow the intensity\r
of their salivary reflex to be accurately measured. In one case he describes, the dog did\r
not salivate under most circumstances, but about 5 seconds after being presented with\r
food it produced about six drops of saliva over the next several seconds. After several\r
repetitions of presenting another stimulus, one not related to food, in this case the sound\r
of a metronome, shortly before the introduction of food, the dog salivated in response to\r
the sound of the metronome in the same way it did to the food. “The activity of the\r
salivary gland has thus been called into play by impulses of sound—a stimulus quite\r
alien to food” (Pavlov, 1927, p. 22). Summarizing the significance of this finding, Pavlov\r
wrote:\r
It is pretty evident that under natural conditions the normal animal must\r
respond not only to stimuli which themselves bring immediate benefit or\r
harm, but also to other physical or chemical agencies—waves of sound, light,\r
and the like—which in themselves only signal the approach of these stimuli;\r
though it is not the sight and sound of the beast of prey which is in itself\r
harmful to the smaller animal, but its teeth and claws. (Pavlov, 1927, p. 14)\r
Connecting new stimuli to innate reflexes in this way is now called classical, or Pavlovian,\r
conditioning. Pavlov (or more exactly, his translators) called inborn responses (e.g.,"""

[[sections]]
number = "344"
title = "Chapter 14: Psychology"
text = """
salivation in his demonstration described above) “unconditioned responses” (URs), their\r
natural triggering stimuli (e.g., food) “unconditioned stimuli” (USs), and new responses\r
triggered by predictive stimuli (e.g., here also salivation) “conditioned responses” (CRs).\r
A stimulus that is initially neutral, meaning that it does not normally elicit strong\r
responses (e.g., the metronome sound), becomes a “conditioned stimulus” (CS) as the\r
animal learns that it predicts the US and so comes to produce a CR in response to the CS.\r
These terms are still used in describing classical conditioning experiments (though better\r
translations would have been “conditional” and “unconditional” instead of conditioned\r
and unconditioned). The US is called a reinforcer because it reinforces producing a CR\r
in response to the CS.\r
t\r
Trace Conditioning\r
Delay Conditioning\r
CS\r
US\r
CS\r
US\r
ISI\r
The arrangement of stimuli in two\r
common types of classical condition\u0002ing experiments is shown to the right.\r
In delay conditioning, the CS extends\r
throughout the interstimulus interval, or\r
ISI, which is the time interval between\r
the CS onset and the US onset (with\r
the CS ending when the US ends in a\r
common version shown here). In trace\r
conditioning, the US begins after the CS\r
ends, and the time interval between CS\r
o↵set and US onset is called the trace\r
interval.\r
The salivation of Pavlov’s dogs to the\r
sound of a metronome is just one exam\u0002ple of classical conditioning, which has\r
been intensively studied across many response systems of many species of animals. URs\r
are often preparatory in some way, like the salivation of Pavlov’s dog, or protective in\r
some way, like an eye blink in response to something irritating to the eye, or freezing\r
in response to seeing a predator. Experiencing the CS-US predictive relationship over\r
a series of trials causes the animal to learn that the CS predicts the US so that the\r
animal can respond to the CS with a CR that prepares the animal for, or protects it\r
from, the predicted US. Some CRs are similar to the UR but begin earlier and di↵er in\r
ways that increase their e↵ectiveness. In one intensively studied type of experiment, for\r
example, a tone CS reliably predicts a pu↵ of air (the US) to a rabbit’s eye, triggering a\r
UR consisting of the closure of a protective inner eyelid called the nictitating membrane.\r
After one or more trials, the tone comes to trigger a CR consisting of membrane closure\r
that begins before the air pu↵ and eventually becomes timed so that peak closure occurs\r
just when the air pu↵ is likely to occur. This CR, being initiated in anticipation of the\r
air pu↵ and appropriately timed, o↵ers better protection than simply initiating closure\r
as a reaction to the irritating US. The ability to act in anticipation of important events\r
by learning about predictive relationships among stimuli is so beneficial that it is widely\r
present across the animal kingdom."""

[[sections]]
number = "14.2"
title = "Classical Conditioning 345"
text = ""

[[sections]]
number = "14.2.1"
title = "Blocking and Higher-order Conditioning"
text = """
Many interesting properties of classical conditioning have been observed in experiments.\r
Beyond the anticipatory nature of CRs, two widely observed properties figured prominently\r
in the development of classical conditioning models: blocking and higher-order conditioning.\r
Blocking occurs when an animal fails to learn a CR when a potential CS is presented along\r
with another CS that had been used previously to condition the animal to produce that\r
CR. For example, in the first stage of a blocking experiment involving rabbit nictitating\r
membrane conditioning, a rabbit is first conditioned with a tone CS and an air pu↵ US\r
to produce the CR of closing its nictitating membrane in anticipation of the air pu↵. The\r
experiment’s second stage consists of additional trials in which a second stimulus, say\r
a light, is added to the tone to form a compound tone/light CS followed by the same\r
air pu↵ US. In the experiment’s third phase, the second stimulus alone—the light—is\r
presented to the rabbit to see if the rabbit has learned to respond to it with a CR. It\r
turns out that the rabbit produces very few, or no, CRs in response to the light: learning\r
to the light had been blocked by the previous learning to the tone.2 Blocking results like\r
this challenged the idea that conditioning depends only on simple temporal contiguity,\r
that is, that a necessary and sucient condition for conditioning is that a US frequently\r
follows a CS closely in time. In the next section we describe the Rescorla–Wagner model\r
(Rescorla and Wagner, 1972) that o↵ered an influential explanation for blocking.\r
Higher-order conditioning occurs when a previously-conditioned CS acts as a US\r
in conditioning another initially neutral stimulus. Pavlov described an experiment in\r
which his assistant first conditioned a dog to salivate to the sound of a metronome that\r
predicted a food US, as described above. After this stage of conditioning, a number of\r
trials were conducted in which a black square, to which the dog was initially indi↵erent,\r
was placed in the dog’s line of vision followed by the sound of the metronome—and\r
this was not followed by food. In just ten trials, the dog began to salivate merely upon\r
seeing the black square, despite the fact that the sight of it had never been followed by\r
food. The sound of the metronome itself acted as a US in conditioning a salivation CR\r
to the black square CS. This was second-order conditioning. If the black square had\r
been used as a US to establish salivation CRs to another otherwise neutral CS, it would\r
have been third-order conditioning, and so on. Higher-order conditioning is dicult to\r
demonstrate, especially above the second order, in part because a higher-order reinforcer\r
loses its reinforcing value due to not being repeatedly followed by the original US during\r
higher-order conditioning trials. But under the right conditions, such as intermixing\r
first-order trials with higher-order trials or by providing a general energizing stimulus,\r
higher-order conditioning beyond the second order can be demonstrated. As we describe\r
below, the TD model of classical conditioning uses the bootstrapping idea that is central\r
to our approach to extend the Rescorla–Wagner model’s account of blocking to include\r
both the anticipatory nature of CRs and higher-order conditioning.\r
2Comparison with a control group is necessary to show that the previous conditioning to the tone is\r
responsible for blocking learning to the light. This is done by trials with the tone/light CS but with no\r
prior conditioning to the tone. Learning to the light in this case is unimpaired. Moore and Schmajuk\r
(2008) give a full account of this procedure."""

[[sections]]
number = "346"
title = "Chapter 14: Psychology"
text = """
Higher-order instrumental conditioning occurs as well. In this case, a stimulus that\r
consistently predicts primary reinforcement becomes a reinforcer itself, where reinforce\u0002ment is primary if its rewarding or penalizing quality has been built into the animal by\r
evolution. The predicting stimulus becomes a secondary reinforcer, or more generally, a\r
higher-order or conditioned reinforcer—the latter being a better term when the predicted\r
reinforcing stimulus is itself a secondary, or an even higher-order, reinforcer. A condi\u0002tioned reinforcer delivers conditioned reinforcement: conditioned reward or conditioned\r
penalty. Conditioned reinforcement acts like primary reinforcement in increasing an\r
animal’s tendency to produce behavior that leads to conditioned reward, and to decrease\r
an animal’s tendency to produce behavior that leads to conditioned penalty. (See our\r
comments at the end of this chapter that explain how our terminology sometimes di↵ers,\r
as it does here, from terminology used in psychology.)\r
Conditioned reinforcement is a key phenomenon that explains, for instance, why we\r
work for the conditioned reinforcer money, whose worth derives solely from what is\r
predicted by having it. In actor–critic methods described in Section 13.5 (and discussed\r
in the context of neuroscience in Sections 15.7 and 15.8), the critic uses a TD method\r
to evaluate the actor’s policy, and its value estimates provide conditioned reinforcement\r
to the actor, allowing the actor to improve its policy. This analog of higher-order\r
instrumental conditioning helps address the credit-assignment problem mentioned in\r
Section 1.7 because the critic gives moment-by-moment reinforcement to the actor when\r
the primary reward signal is delayed. We discuss this more below in Section 14.4."""

[[sections]]
number = "14.2.2"
title = "The Rescorla–Wagner Model"
text = """
Rescorla and Wagner created their model mainly to account for blocking. The core\r
idea of the Rescorla–Wagner model is that an animal only learns when events violate\r
its expectations, in other words, only when the animal is surprised (although without\r
necessarily implying any conscious expectation or emotion). We first present Rescorla and\r
Wagner’s model using their terminology and notation before shifting to the terminology\r
and notation we use to describe the TD model.\r
Here is how Rescorla and Wagner described their model. The model adjusts the\r
“associative strength” of each component stimulus of a compound CS, which is a number\r
representing how strongly or reliably that component is predictive of a US. When\r
a compound CS consisting of several component stimuli is presented in a classical\r
conditioning trial, the associative strength of each component stimulus changes in a way\r
that depends on an associative strength associated with the entire stimulus compound,\r
called the “aggregate associative strength,” and not just on the associative strength of\r
each component itself.\r
Rescorla and Wagner considered a compound CS AX, consisting of component stimuli\r
A and X, where the animal may have already experienced stimulus A, and stimulus X\r
might be new to the animal. Let VA, VX, and VAX respectively denote the associative\r
strengths of stimuli A, X, and the compound AX. Suppose that on a trial the compound\r
CS AX is followed by a US, which we label stimulus Y. Then the associative strengths of"""

[[sections]]
number = "14.2"
title = "Classical Conditioning 347"
text = """
the stimulus components change according to these expressions:\r
VA = ↵AY(RY  VAX)\r
VX = ↵XY(RY  VAX),\r
where ↵AY and ↵XY are the step-size parameters, which depend on the identities of\r
the CS components and the US, and RY is the asymptotic level of associative strength\r
that the US Y can support. (Rescorla and Wagner used  here instead of R, but we\r
use R to avoid confusion with our use of  and because we usually think of this as the\r
magnitude of a reward signal, with the caveat that the US in classical conditioning is not\r
necessarily rewarding or penalizing.) A key assumption of the model is that the aggregate\r
associative strength VAX is equal to VA + VX. The associative strengths as changed by\r
these s become the associative strengths at the beginning of the next trial.\r
To be complete, the model needs a response-generation mechanism, which is a way\r
of mapping values of V s to CRs. Because this mapping would depend on details of\r
the experimental situation, Rescorla and Wagner did not specify a mapping but simply\r
assumed that larger V s would produce stronger or more likely CRs, and that negative\r
V s would mean that there would be no CRs.\r
The Rescorla–Wagner model accounts for the acquisition of CRs in a way that explains\r
blocking. As long as the aggregate associative strength, VAX, of the stimulus compound\r
is below the asymptotic level of associative strength, RY, that the US Y can support, the\r
prediction error RY VAX is positive. This means that over successive trials the associative\r
strengths VA and VX of the component stimuli increase until the aggregate associative\r
strength VAX equals RY, at which point the associative strengths stop changing (unless the\r
US changes). When a new component is added to a compound CS to which the animal has\r
already been conditioned, further conditioning with the augmented compound produces\r
little or no increase in the associative strength of the added CS component because the\r
error has already been reduced to zero, or to a low value. The occurrence of the US is\r
already predicted nearly perfectly, so little or no error—or surprise—is introduced by the\r
new CS component. Prior learning blocks learning to the new component.\r
To transition from Rescorla and Wagner’s model to the TD model of classical condi\u0002tioning (which we just call the TD model), we first recast their model in terms of the\r
concepts that we are using throughout this book. Specifically, we match the notation\r
we use for learning with linear function approximation (Section 9.4), and we think of\r
the conditioning process as one of learning to predict the “magnitude of the US” on a\r
trial on the basis of the compound CS presented on that trial, where the magnitude of\r
a US Y is the RY of the Rescorla–Wagner model as given above. We also introduce\r
states. Because the Rescorla–Wagner model is a trial-level model, meaning that it deals\r
with how associative strengths change from trial to trial without considering any details\r
about what happens within and between trials, we do not have to consider how states\r
change during a trial until we present the full TD model in the following section. Instead,\r
here we simply think of a state as a way of labeling a trial in terms of the collection of\r
component CSs that are present on the trial.\r
Therefore, assume that trial-type, or state, s is described by a real-valued vector of\r
features x(s)=(x1(s), x2(s),...,xd(s))> where xi(s) = 1 if CSi, the i\r
th component of a"""

[[sections]]
number = "348"
title = "Chapter 14: Psychology"
text = """
compound CS, is present on the trial and 0 otherwise. Then if the d-dimensional vector\r
of associative strengths is w, the aggregate associative strength for trial-type s is\r
vˆ(s,w) = w>x(s). (14.1)\r
This corresponds to a value estimate in reinforcement learning, and we think of it as the\r
US prediction.\r
Now temporally let t denote the number of a complete trial and not its usual meaning\r
as a time step (we revert to t’s usual meaning when we extend this to the TD model\r
below), and assume that St is the state corresponding to trial t. Conditioning trial t\r
updates the associative strength vector wt to wt+1 as follows:\r
wt+1 = wt + ↵tx(St), (14.2)\r
where ↵ is the step-size parameter, and—because here we are describing the Rescorla–\r
Wagner model—t is the prediction error\r
t = Rt  vˆ(St,wt). (14.3)\r
Rt is the target of the prediction on trial t, that is, the magnitude of the US, or in Rescorla\r
and Wagner’s terms, the associative strength that the US on the trial can support. Note\r
that because of the factor x(St) in (14.2), only the associative strengths of CS components\r
present on a trial are adjusted as a result of that trial. You can think of the prediction\r
error as a measure of surprise, and the aggregate associative strength as the animal’s\r
expectation that is violated when it does not match the target US magnitude.\r
From the perspective of machine learning, the Rescorla–Wagner model is an error\u0002correction supervised learning rule. It is essentially the same as the Least Mean Square\r
(LMS), or Widrow-Ho↵, learning rule (Widrow and Ho↵, 1960) that finds the weights—\r
here the associative strengths—that make the average of the squares of all the errors as\r
close to zero as possible. It is a “curve-fitting,” or regression, algorithm that is widely\r
used in engineering and scientific applications (see Section 9.4).3\r
The Rescorla–Wagner model was very influential in the history of animal learning\r
theory because it showed that a “mechanistic” theory could account for the main facts\r
about blocking without resorting to more complex cognitive theories involving, for\r
example, an animal’s explicit recognition that another stimulus component had been\r
added and then scanning its short-term memory backward to reassess the predictive\r
relationships involving the US. The Rescorla–Wagner model showed how traditional\r
contiguity theories of conditioning—that temporal contiguity of stimuli was a necessary\r
and sucient condition for learning—could be adjusted in a simple way to account for\r
blocking (Moore and Schmajuk, 2008).\r
The Rescorla–Wagner model provides a simple account of blocking and some other\r
features of classical conditioning, but it is not a complete or perfect model of classical\r
3The only di↵erences between the LMS rule and the Rescorla–Wagner model are that for LMS the\r
input vectors xt can have any real numbers as components, and—at least in the simplest version of the\r
LMS rule—the step-size parameter ↵ does not depend on the input vector or the identity of the stimulus\r
setting the prediction target."""

[[sections]]
number = "14.2"
title = "Classical Conditioning 349"
text = """
conditioning. Di↵erent ideas account for a variety of other observed e↵ects, and progress\r
is still being made toward understanding the many subtleties of classical conditioning.\r
The TD model, which we describe next, though also not a complete or perfect model\r
model of classical conditioning, extends the Rescorla–Wagner model to address how\r
within-trial and between-trial timing relationships among stimuli can influence learning\r
and how higher-order conditioning might arise."""

[[sections]]
number = "14.2.3"
title = "The TD Model"
text = """
The TD model is a real-time model, as opposed to a trial-level model like the Rescorla–\r
Wagner model. A single step t in the Rescorla–Wagner model represents an entire\r
conditioning trial. The model does not apply to details about what happens during the\r
time a trial is taking place, or what might happen between trials. Within each trial an\r
animal might experience various stimuli whose onsets occur at particular times and that\r
have particular durations. These timing relationships strongly influence learning. The\r
Rescorla–Wagner model also does not include a mechanism for higher-order conditioning,\r
whereas for the TD model, higher-order conditioning is a natural consequence of the\r
bootstrapping idea that is at the base of TD algorithms.\r
To describe the TD model we begin with the formulation of the Rescorla–Wagner\r
model above, but t now labels time steps within or between trials instead of complete\r
trials. Think of the time between t and t + 1 as a small time interval, say .01 second, and\r
think of a trial as a sequences of states, one associated with each time step, where the\r
state at step t now represents details of how stimuli are represented at t instead of just\r
a label for the CS components present on a trial. In fact, we can completely abandon\r
the idea of trials. From the point of view of the animal, a trial is just a fragment of its\r
continuing experience interacting with its world. Following our usual view of an agent\r
interacting with its environment, imagine that the animal is experiencing an endless\r
sequence of states s, each represented by a feature vector x(s). That said, it is still often\r
convenient to refer to trials as fragments of time during which patterns of stimuli repeat\r
in an experiment.\r
State features are not restricted to describing the external stimuli that an animal\r
experiences; they can describe neural activity patterns that external stimuli produce in\r
an animal’s brain, and these patterns can be history-dependent, meaning that they can\r
be persistent patterns produced by sequences of external stimuli. Of course, we do not\r
know exactly what these neural activity patterns are, but a real-time model like the TD\r
model allows one to explore the consequences on learning of di↵erent hypotheses about\r
the internal representations of external stimuli. For these reasons, the TD model does\r
not commit to any particular state representation. In addition, because the TD model\r
includes discounting and eligibility traces that span time intervals between stimuli, the\r
model also makes it possible to explore how discounting and eligibility traces interact with\r
stimulus representations in making predictions about the results of classical conditioning\r
experiments.\r
Below we describe some of the state representations that have been used with the\r
TD model and some of their implications, but for the moment we stay agnostic about"""

[[sections]]
number = "350"
title = "Chapter 14: Psychology"
text = """
the representation and just assume that each state s is represented by a feature vector\r
x(s)=(x1(s), x2(s),...,xn(s))>. Then the aggregate associative strength corresponding\r
to a state s is given by (14.1), the same as for the Rescorla-Wagner model, but the TD\r
model updates the associative strength vector, w, di↵erently. With t now labeling a time\r
step instead of a complete trial, the TD model governs learning according to this update:\r
wt+1 = wt + ↵t zt, (14.4)\r
which replaces xt(St) in the Rescorla–Wagner update (14.2) with zt, a vector of eligibility\r
traces, and instead of the t of (14.3), here t is a TD error:\r
t = Rt+1 + vˆ(St+1,wt)  vˆ(St,wt), (14.5)\r
where  is a discount factor (between 0 and 1), Rt is the prediction target at time t, and\r
vˆ(St+1,wt) and vˆ(St,wt) are aggregate associative strengths at t + 1 and t as defined by\r
(14.1).\r
Each component i of the eligibility-trace vector zt increments or decrements according\r
to the component xi(St) of the feature vector x(St), and otherwise decays with a rate\r
determined by :\r
zt = zt1 + x(St). (14.6)\r
Here  is the usual eligibility trace decay parameter.\r
Note that if  = 0, the TD model reduces to the Rescorla–Wagner model with the\r
exceptions that: the meaning of t is di↵erent in each case (a trial number for the\r
Rescorla–Wagner model and a time step for the TD model), and in the TD model there\r
is a one-time-step lead in the prediction target R. The TD model is equivalent to the\r
backward view of the semi-gradient TD() algorithm with linear function approximation\r
(Chapter 12), except that Rt in the model does not have to be a reward signal as it does\r
when the TD algorithm is used to learn a value function for policy-improvement."""

[[sections]]
number = "14.2.4"
title = "TD Model Simulations"
text = """
Real-time conditioning models like the TD model are interesting primarily because they\r
make predictions for a wide range of situations that cannot be represented by trial-level\r
models. These situations involve the timing and durations of conditionable stimuli, the\r
timing of these stimuli in relation to the timing of the US, and the timing and shapes\r
of CRs. For example, the US generally must begin after the onset of a neutral stimulus\r
for conditioning to occur, with the rate and e↵ectiveness of learning depending on the\r
inter-stimulus interval, or ISI, the interval between the onsets of the CS and the US. When\r
CRs appear, they generally begin before the appearance of the US and their temporal\r
profiles change during learning. In conditioning with compound CSs, the component\r
stimuli of the compound CSs may not all begin and end at the same time, sometimes\r
forming what is called a serial compound in which the component stimuli occur in a\r
sequence over time. Timing considerations like these make it important to consider how"""

[[sections]]
number = "14.2"
title = "Classical Conditioning 351"
text = """
Presence Complete Serial \r
Compound\r
Stimulus \r
Representation\r
Microstimuli\r
CS\r
US\r
Figure 14.1: Three stimulus representations (in columns) sometimes used with the TD model.\r
Each row represents one element of the stimulus representation. The three representations vary\r
along a temporal generalization gradient, with no generalization between nearby time points in\r
the complete serial compound (left column) and complete generalization between nearby time\r
points in the presence representation (right column). The microstimulus representation occupies\r
a middle ground. The degree of temporal generalization determines the temporal granularity\r
with which US predictions are learned. Adapted with minor changes from Learning & Behavior,\r
Evaluating the TD Model of Classical Conditioning, volume 40, 2012, p. 311, E. A. Ludvig, R. S.\r
Sutton, E. J. Kehoe. With permission of Springer.\r
stimuli are represented, how these representations unfold over time during and between\r
trials, and how they interact with discounting and eligibility traces.\r
Figure 14.1 shows three of the stimulus representations that have been used in exploring\r
the behavior of the TD model: the complete serial compound (CSC), the microstimulus\r
(MS), and the presence representations (Ludvig, Sutton, and Kehoe, 2012). These\r
representations di↵er in the degree to which they force generalization among nearby time\r
points during which a stimulus is present.\r
The simplest of the representations shown in Figure 14.1 is the presence representation\r
in the figure’s right column. This representation has a single feature for each component\r
CS present on a trial, where the feature has value 1 whenever that component is present,\r
and 0 otherwise.4 The presence representation is not a realistic hypothesis about how\r
stimuli are represented in an animal’s brain, but as we describe below, the TD model\r
with this representation can produce many of the timing phenomena seen in classical\r
conditioning.\r
4In our formalism, there is a di↵erent state, St, for each time step t during a trial, and for a trial\r
in which a compound CS consists of n component CSs of various durations occurring at various times\r
throughout the trial, there is a feature, xi, for each component CSi, i = 1,...,n, where xi(St) = 1 for\r
all times t when the CSi is present, and equals zero otherwise."""

[[sections]]
number = "352"
title = "Chapter 14: Psychology"
text = """
For the CSC representation (left column of Figure 14.1), the onset of each external\r
stimulus initiates a sequence of precisely-timed short-duration internal signals that\r
continues until the external stimulus ends.5 This is like assuming the animal’s nervous\r
system has a clock that keeps precise track of time during stimulus presentations; it is\r
what engineers call a “tapped delay line.” Like the presence representation, the CSC\r
representation is unrealistic as a hypothesis about how the brain internally represents\r
stimuli, but Ludvig et al. (2012) call it a “useful fiction” because it can reveal details of\r
how the TD model works when relatively unconstrained by the stimulus representation.\r
The CSC representation is also used in most TD models of dopamine-producing neurons\r
in the brain, a topic we take up in Chapter 15. The CSC representation is often viewed\r
as an essential part of the TD model, although this view is mistaken.\r
The MS representation (center column of Figure 14.1) is like the CSC representation\r
in that each external stimulus initiates a cascade of internal stimuli, but in this case the\r
internal stimuli—the microstimuli—are not of such limited and non-overlapping form;\r
they are extended over time and overlap. As time elapses from stimulus onset, di↵erent\r
sets of microstimuli become more or less active, and each subsequent microstimulus\r
becomes progressively wider in time and reaches a lower maximal level. Of course, there\r
are many MS representations depending on the nature of the microstimuli, and a number\r
of examples of MS representations have been studied in the literature, in some cases along\r
with proposals for how an animal’s brain might generate them (see the Bibliographic and\r
Historical Comments at the end of this chapter). MS representations are more realistic\r
than the presence or CSC representations as hypotheses about neural representations of\r
stimuli, and they allow the behavior of the TD model to be related to a broader collection\r
of phenomena observed in animal experiments. In particular, by assuming that cascades\r
of microstimuli are initiated by USs as well as by CSs, and by studying the significant\r
e↵ects on learning of interactions between microstimuli, eligibility traces, and discounting,\r
the TD model is helping to frame hypotheses to account for many of the subtle phenomena\r
of classical conditioning and how an animal’s brain might produce them. We say more\r
about this below, particularly in Chapter 15 where we discuss reinforcement learning and\r
neuroscience.\r
Even with the simple presence representation, however, the TD model produces all the\r
basic properties of classical conditioning that are accounted for by the Rescorla–Wagner\r
model, plus features of conditioning that are beyond the scope of trial-level models. For\r
example, as we have already mentioned, a conspicuous feature of classical conditioning is\r
that the US generally must begin after the onset of a neutral stimulus for conditioning\r
to occur, and that after conditioning, the CR begins before the appearance of the US.\r
In other words, conditioning generally requires a positive ISI, and the CR generally\r
anticipates the US. How the strength of conditioning (e.g., the percentage of CRs elicited\r
by a CS) depends on the ISI varies substantially across species and response systems, but\r
it typically has the following properties: it is negligible for a zero or negative ISI, i.e., when\r
5In our formalism, for each CS component CSi present on a trial, and for each time step t during a\r
trial, there is a separate feature xt\r
i , where xti (St0 ) = 1 if t = t0 for any t0 at which CSi is present, and\r
equals 0 otherwise. This is di↵erent from the CSC representation in Sutton and Barto (1990) in which\r
there are the same distinct features for each time step but no reference to external stimuli; hence the\r
name complete serial compound."""

[[sections]]
number = "14.2"
title = "Classical Conditioning 353"
text = """
the US onset occurs simultaneously with, or earlier than, the CS onset (although research\r
has found that associative strengths sometimes increase slightly or become negative with\r
negative ISIs); it increases to a maximum at a positive ISI where conditioning is most\r
e↵ective; and it then decreases to zero after an interval that varies widely with response\r
systems. The precise shape of this dependency for the TD model depends on the values\r
of its parameters and details of the stimulus representation, but these basic features of\r
ISI-dependency are core properties of the TD model.\r
Facilitation of remote associations in the TD model\r
One of the theoretical issues arising\r
with serial-compound conditioning, that\r
is, conditioning with a compound CS\r
whose components occur in a sequence,\r
concerns the facilitation of remote asso\u0002ciations. It has been found that if the\r
empty trace interval between a first CS\r
(CSA) and the US is filled with a second\r
CS (CSB) to form a serial-compound\r
stimulus, then conditioning to CSA is\r
facilitated. Shown to the right is the\r
behavior of the TD model with the pres\u0002ence representation in a simulation of\r
such an experiment whose timing details\r
are shown above. Consistent with the\r
experimental results (Kehoe, 1982), the\r
model shows facilitation of both the rate\r
of conditioning and the asymptotic level\r
of conditioning of the first CS due to the\r
presence of the second CS.\r
wCSB\r
The Egger-Miller e↵ect in the TD model\r
A well-known demonstration of the\r
e↵ects on conditioning of temporal re\u0002lationships among stimuli within a trial\r
is an experiment by Egger and Miller\r
(1962) that involved two overlapping\r
CSs in a delay configuration as shown\r
to the right (top). Although CSB was\r
in a better temporal relationship with\r
the US, the presence of CSA substan\u0002tially reduced conditioning to CSB as\r
compared to controls in which CSA was\r
absent. Directly to the right is shown\r
the same result being generated by the\r
TD model in a simulation of this exper\u0002iment with the presence representation.\r
The TD model accounts for blocking\r
because it is an error-correcting learning"""

[[sections]]
number = "354"
title = "Chapter 14: Psychology"
text = """
rule like the Rescorla–Wagner model. Beyond accounting for basic blocking results,\r
however, the TD model predicts (with the presence representation and more complex\r
representations as well) that blocking is reversed if the blocked stimulus is moved earlier\r
wCSB\r
Figure 14.2: Temporal primacy overriding\r
blocking in the TD model.\r
in time (like CSA in the diagram to the\r
right) so that its onset occurs before the\r
onset of the blocking stimulus. This feature\r
of the TD model’s behavior deserves atten\u0002tion because it had not been observed at\r
the time of the model’s introduction. Recall\r
that in blocking, if an animal has already\r
learned that one CS predicts a US, then\r
learning that a newly-added second CS also\r
predicts the US is much reduced, i.e., is\r
blocked. But if the newly-added second CS\r
begins earlier than the pretrained CS, then—\r
according to the TD model—learning to the\r
newly-added CS is not blocked. In fact, as\r
training continues and the newly-added CS\r
gains associative strength, the pretrained\r
CS loses associative strength. The behavior\r
of the TD model under these conditions\r
is shown in the lower part of Figure 14.2.\r
This simulation experiment di↵ered from the Egger-Miller experiment (bottom of the\r
preceding page) in that the shorter CS with the later onset was given prior training\r
until it was fully associated with the US. This surprising prediction led Kehoe, Schreurs,\r
and Graham (1987) to conduct the experiment using the well-studied rabbit nictitating\r
membrane preparation. Their results confirmed the model’s prediction, and they noted\r
that non-TD models have considerable diculty explaining their data.\r
With the TD model, an earlier predictive stimulus takes precedence over a later\r
predictive stimulus because, like all the prediction methods described in this book, the\r
TD model is based on the backing-up or bootstrapping idea: updates to associative\r
strengths shift the strengths at a particular state toward the strength at later states.\r
Another consequence of bootstrapping is that the TD model provides an account of higher\u0002order conditioning, a feature of classical conditioning that is beyond the scope of the\r
Rescorla-Wagner and similar models. As we described above, higher-order conditioning\r
is the phenomenon in which a previously-conditioned CS can act as a US in conditioning\r
another initially neutral stimulus. Figure 14.3 shows the behavior of the TD model (again\r
with the presence representation) in a higher-order conditioning experiment—in this case\r
it is second-order conditioning. In the first phase (not shown in the figure), CSB is trained\r
to predict a US so that its associative strength increases, here to 1.65. In the second\r
phase, CSA is paired with CSB in the absence of the US, in the sequential arrangement\r
shown at the top of the figure. CSA acquires associative strength even though it is never\r
paired with the US. With continued training, CSA’s associative strength reaches a peak\r
and then decreases because the associative strength of CSB, the secondary reinforcer,\r
decreases so that it loses its ability to provide secondary reinforcement. CSB’s associative"""

[[sections]]
number = "14.2"
title = "Classical Conditioning 355"
text = """
ww\r
Figure 14.3: Second-order conditioning with\r
the TD model.\r
strength decreases because the US does not\r
occur in these higher-order conditioning tri\u0002als. These are extinction trials for CSB\r
because its predictive relationship to the\r
US is disrupted so that its ability to act as\r
a reinforcer decreases. This same pattern\r
is seen in animal experiments. This extinc\u0002tion of conditioned reinforcement in higher\u0002order conditioning trials makes it dicult\r
to demonstrate higher-order conditioning\r
unless the original predictive relationships\r
are periodically refreshed by occasionally\r
inserting first-order trials.\r
The TD model produces an analog of\r
second- and higher-order conditioning be\u0002cause vˆ(St+1,wt)  vˆ(St,wt) appears in\r
the TD error t (14.5). Due to the first\r
phase of learning, vˆ(St+1,wt) may di↵er\r
from vˆ(St,wt), making t non-zero (a temporal di↵erence). This di↵erence has the same\r
status as Rt+1 in (14.5), implying that as far as learning is concerned there is no di↵erence\r
between a temporal di↵erence and the occurrence of a US. In fact, this feature of the\r
TD algorithm is one of the major reasons for its development, which we now understand\r
through its connection to dynamic programming as described in Chapter 6. Bootstrapping\r
values is intimately related to second-order, and higher-order, conditioning.\r
In the examples of the TD model’s behavior described above, we examined only the\r
changes in the associative strengths of the CS components; we did not look at what\r
the model predicts about properties of an animal’s conditioned responses (CRs): their\r
timing, shape, and how they develop over conditioning trials. These properties depend\r
on the species, the response system being observed, and parameters of the conditioning\r
trials, but in many experiments with di↵erent animals and di↵erent response systems, the\r
magnitude of the CR, or the probability of a CR, increases as the expected time of the\r
US approaches. For example, in classical conditioning of a rabbit’s nictitating membrane\r
response that we mentioned above, over conditioning trials the delay from CS onset to\r
when the nictitating membrane begins to move across the eye decreases over trials, and\r
the amplitude of this anticipatory closure gradually increases over the interval between\r
the CS and the US until the membrane reaches maximal closure at the expected time of\r
the US. The timing and shape of this CR is critical to its adaptive significance—covering\r
the eye too early reduces vision (even though the nictitating membrane is translucent),\r
while covering it too late is of little protective value. Capturing CR features like these is\r
challenging for models of classical conditioning.\r
The TD model does not include as part of its definition any mechanism for translat\u0002ing the time course of the US prediction, vˆ(St,wt), into a profile that can be compared"""

[[sections]]
number = "356"
title = "Chapter 14: Psychology"
text = """
with the properties of an animal’s CR. The simplest choice is to let the time course of\r
a simulated CR equal the time course of the US prediction. In this case, features of\r
simulated CRs and how they change over trials depend only on the stimulus representation\r
chosen and the values of the model’s parameters ↵, , and .\r
Figure 14.4 shows the time courses of US predictions at di↵erent points during learning\r
with the three representations shown in Figure 14.1. For these simulations the US\r
occurred 25 time steps after the onset of the CS, and ↵ = .05,  = .95 and  = .97.\r
With the CSC representation (Figure 14.4 left), the curve of the US prediction formed\r
by the TD model increases exponentially throughout the interval between the CS and\r
the US until it reaches a maximum exactly when the US occurs (at time step 25). This\r
exponential increase is the result of discounting in the TD model learning rule. With the\r
presence representation (Figure 14.4 middle), the US prediction is nearly constant while\r
the stimulus is present because there is only one weight, or associative strength, to be\r
learned for each stimulus. Consequently, the TD model with the presence representation\r
cannot recreate many features of CR timing. With an MS representation (Figure 14.4\r
right), the development of the TD model’s US prediction is more complicated. After 200\r
trials the prediction’s profile is a reasonable approximation of the US prediction curve\r
produced with the CSC representation.\r
ˆv\r
ˆv\r
ˆv\r
Figure 14.4: Time course of US prediction over the course of acquisition for the TD model\r
with three di↵erent stimulus representations. Left: With the complete serial compound (CSC),\r
the US prediction increases exponentially through the interval, peaking at the time of the US.\r
At asymptote (trial 200), the US prediction peaks at the US intensity (1 in these simulations).\r
Middle: With the presence representation, the US prediction converges to an almost constant\r
level. This constant level is determined by the US intensity and the length of the CS–US interval.\r
Right: With the microstimulus representation, at asymptote, the TD model approximates the\r
exponentially increasing time course depicted with the CSC through a linear combination of the\r
di↵erent microstimuli. Adapted with minor changes from Learning & Behavior, Evaluating the\r
TD Model of Classical Conditioning, volume 40, 2012, E. A. Ludvig, R. S. Sutton, E. J. Kehoe.\r
With permission of Springer.\r
The US prediction curves shown in Figure 14.4 were not intended to precisely match\r
profiles of CRs as they develop during conditioning in any particular animal experiment,\r
but they illustrate the strong influence that the stimulus representation has on predictions\r
derived from the TD model. Further, although we can only mention it here, how the"""

[[sections]]
number = "14.3"
title = "Instrumental Conditioning 357"
text = """
stimulus representation interacts with discounting and eligibility traces is important\r
in determining properties of the US prediction profiles produced by the TD model.\r
Another dimension beyond what we can discuss here is the influence of di↵erent response\u0002generation mechanisms that translate US predictions into CR profiles; the profiles shown\r
in Figure 14.4 are “raw” US prediction profiles. Even without any special assumption\r
about how an animal’s brain might produce overt responses from US predictions, however,\r
the profiles in Figure 14.4 for the CSC and MS representations increase as the time of the\r
US approaches and reach a maximum at the time of the US, as is seen in many animal\r
conditioning experiments.\r
The TD model, when combined with particular stimulus representations and response\u0002generation mechanisms, is able to account for a surprisingly wide range of phenomena\r
observed in animal classical conditioning experiments, but it is far from being a perfect\r
model. To generate other details of classical conditioning the model needs to be extended,\r
perhaps by adding model-based elements and mechanisms for adaptively altering some of\r
its parameters. Other approaches to modeling classical conditioning depart significantly\r
from the Rescorla–Wagner-style error-correction process. Bayesian models, for example,\r
work within a probabilistic framework in which experience revises probability estimates.\r
All of these models usefully contribute to our understanding of classical conditioning.\r
Perhaps the most notable feature of the TD model is that it is based on a theory—the\r
theory we have described in this book—that suggests an account of what an animal’s\r
nervous system is trying to do while undergoing conditioning: it is trying to form accurate\r
long-term predictions, consistent with the limitations imposed by the way stimuli are\r
represented and how the nervous system works. In other words, it suggests a normative\r
account of classical conditioning in which long-term, instead of immediate, prediction is a\r
key feature.\r
The development of the TD model of classical conditioning is one instance in which the\r
explicit goal was to model some of the details of animal learning behavior. In addition to\r
its standing as an algorithm, then, TD learning is also the basis of this model of aspects\r
of biological learning. As we discuss in Chapter 15, TD learning has also turned out\r
to underlie an influential model of the activity of neurons that produce dopamine, a\r
chemical in the brain of mammals that is deeply involved in reward processing. These\r
are instances in which reinforcement learning theory makes detailed contact with animal\r
behavioral and neural data.\r
We now turn to considering correspondences between reinforcement learning and animal\r
behavior in instrumental conditioning experiments, the other major type of laboratory\r
experiment studied by animal learning psychologists."""

[[sections]]
number = "14.3"
title = "Instrumental Conditioning"
text = """
In instrumental conditioning experiments learning depends on the consequences of be\u0002havior: the delivery of a reinforcing stimulus is contingent on what the animal does.\r
In classical conditioning experiments, in contrast, the reinforcing stimulus—the US—is\r
delivered independently of the animal’s behavior. Instrumental conditioning is usually\r
considered to be the same as operant conditioning, the term B. F. Skinner (1938, 1963)"""

[[sections]]
number = "358"
title = "Chapter 14: Psychology"
text = """
introduced for experiments with behavior-contingent reinforcement, though the experi\u0002ments and theories of those who use these two terms di↵er in a number of ways, some of\r
which we touch on below. We will exclusively use the term instrumental conditioning for\r
experiments in which reinforcement is contingent upon behavior. The roots of instrumen\u0002tal conditioning go back to experiments performed by the American psychologist Edward\r
Thorndike one hundred years before publication of the first edition of this book.\r
One of Thorndike’s puzzle boxes.\r
Reprinted from Thorndike, Animal Intelligence: An\r
Experimental Study of the Associative Processes in\r
Animals, The Psychological Review, Series of Mono\u0002graph Supplements II(4), Macmillan, New York, 1898.\r
Thorndike observed the behavior of cats\r
when they were placed in “puzzle boxes,”\r
such as the one at the right, from which\r
they could escape by appropriate actions.\r
For example, a cat could open the door\r
of one box by performing a sequence of\r
three separate actions: depressing a plat\u0002form at the back of the box, pulling a string\r
by clawing at it, and pushing a bar up or\r
down. When first placed in a puzzle box,\r
with food visible outside, all but a few of\r
Thorndike’s cats displayed “evident signs\r
of discomfort” and extraordinarily vigorous\r
activity “to strive instinctively to escape\r
from confinement” (Thorndike, 1898).\r
In experiments with di↵erent cats and\r
boxes with di↵erent escape mechanisms, Thorndike recorded the amounts of time each\r
cat took to escape over multiple experiences in each box. He observed that the time\r
almost invariably decreased with successive experiences, for example, from 300 seconds\r
to 6 or 7 seconds. He described cats’ behavior in a puzzle box like this:\r
The cat that is clawing all over the box in her impulsive struggle will probably\r
claw the string or loop or button so as to open the door. And gradually all the\r
other non-successful impulses will be stamped out and the particular impulse\r
leading to the successful act will be stamped in by the resulting pleasure,\r
until, after many trials, the cat will, when put in the box, immediately claw\r
the button or loop in a definite way. (Thorndike 1898, p. 13)\r
These and other experiments (some with dogs, chicks, monkeys, and even fish) led\r
Thorndike to formulate a number of “laws” of learning, the most influential being the\r
Law of E↵ect. This law describes what is generally known as learning by trial and\r
error. As we mentioned in Chapter 1, many aspects of the Law of E↵ect have generated\r
controversy, and its details have been modified over the years. Still the law—in one form\r
or another—expresses an enduring principle of learning.\r
Essential features of reinforcement learning algorithms correspond to features of animal\r
learning described by the Law of E↵ect. First, reinforcement learning algorithms are\r
selectional, meaning that they try alternatives and select among them by comparing their\r
consequences. Second, reinforcement learning algorithms are associative, meaning that\r
the alternatives found by selection are associated with particular situations, or states,\r
to form the agent’s policy. Like learning described by the Law of E↵ect, reinforcement"""

[[sections]]
number = "14.3"
title = "Instrumental Conditioning 359"
text = """
learning is not just the process of finding actions that produce a lot of reward, but also\r
of connecting these actions to situations or states. Thorndike used the phrase learning\r
by “selecting and connecting” (Hilgard, 1956). Natural selection in evolution is a prime\r
example of a selectional process, but it is not associative (at least as it is commonly\r
understood); supervised learning is associative, but it is not selectional because it relies\r
on instructions that directly tell the agent how to change its behavior.\r
In computational terms, the Law of E↵ect describes an elementary way of combining\r
search and memory: search in the form of trying and selecting among many actions\r
in each situation, and memory in the form of associations linking situations with the\r
actions found—so far—to work best in those situations. Search and memory are essential\r
components of all reinforcement learning algorithms, whether memory takes the form of\r
an agent’s policy, value function, or environment model.\r
A reinforcement learning algorithm’s need to search means that it has to explore in\r
some way. Animals clearly explore as well, and early animal learning researchers disagreed\r
about the degree of guidance an animal uses in selecting its actions in situations like\r
Thorndike’s puzzle boxes. Are actions the result of “absolutely random, blind groping”\r
(Woodworth, 1938, p. 777), or is there some degree of guidance, either from prior learning,\r
reasoning, or other means? Although some thinkers, including Thorndike, seem to have\r
taken the former position, others favored more deliberate exploration. Reinforcement\r
learning algorithms allow wide latitude for how much guidance an agent can employ in\r
selecting actions. The forms of exploration we have used in the algorithms presented\r
in this book, such as "-greedy and upper-confidence-bound action selection, are merely\r
among the simplest. More sophisticated methods are possible, with the only stipulation\r
being that there has to be some form of exploration for the algorithms to work e↵ectively.\r
The feature of our treatment of reinforcement learning allowing the set of actions\r
available at any time to depend on the environment’s current state echoes something\r
Thorndike observed in his cats’ puzzle-box behaviors. The cats selected actions from\r
those that they instinctively perform in their current situation, which Thorndike called\r
their “instinctual impulses.” First placed in a puzzle box, a cat instinctively scratches,\r
claws, and bites with great energy: a cat’s instinctual responses to finding itself in a\r
confined space. Successful actions are selected from these and not from every possible\r
action or activity. This is like the feature of our formalism where the action selected\r
from a state s belongs to a set of admissible actions, A(s). Specifying these sets is an\r
important aspect of reinforcement learning because it can radically simplify learning.\r
They are like an animal’s instinctual impulses. On the other hand, Thorndike’s cats might\r
have been exploring according to an instinctual context-specific ordering over actions\r
rather than by just selecting from a set of instinctual impulses. This is another way to\r
make reinforcement learning easier.\r
Among the most prominent animal learning researchers influenced by the Law of E↵ect\r
were Clark Hull (e.g., Hull, 1943) and B. F. Skinner (e.g., Skinner, 1938). At the center\r
of their research was the idea of selecting behavior on the basis of its consequences.\r
Reinforcement learning has features in common with Hull’s theory, which included\r
eligibility-like mechanisms and secondary reinforcement to account for the ability to learn\r
when there is a significant time interval between an action and the consequent reinforcing"""

[[sections]]
number = "360"
title = "Chapter 14: Psychology"
text = """
stimulus (see Section 14.4). Randomness also played a role in Hull’s theory through what\r
he called “behavioral oscillation” to introduce exploratory behavior.\r
Skinner did not fully subscribe to the memory aspect of the Law of E↵ect. Being averse\r
to the idea of associative linkages, he instead emphasized selection from spontaneously\u0002emitted behavior. He introduced the term “operant” to emphasize the key role of an\r
action’s e↵ects on an animal’s environment. Unlike the experiments of Thorndike and\r
others, which consisted of sequences of separate trials, Skinner’s operant conditioning\r
experiments allowed animal subjects to behave for extended periods of time without\r
interruption. He invented the operant conditioning chamber, now called a “Skinner box,”\r
the most basic version of which contains a lever or key that an animal can press to obtain\r
a reward, such as food or water, which would be delivered according to a well-defined rule,\r
called a reinforcement schedule. By recording the cumulative number of lever presses\r
as a function of time, Skinner and his followers could investigate the e↵ect of di↵erent\r
reinforcement schedules on the animal’s rate of lever-pressing. Modeling results from\r
experiments likes these using the reinforcement learning principles we present in this\r
book is not well developed, but we mention some exceptions in the Bibliographic and\r
Historical Remarks section at the end of this chapter.\r
Another of Skinner’s contributions resulted from his recognition of the e↵ectiveness of\r
training an animal by reinforcing successive approximations of the desired behavior, a\r
process he called shaping. Although this technique had been used by others, including\r
Skinner himself, its significance was impressed upon him when he and colleagues were\r
attempting to train a pigeon to bowl by swiping a wooden ball with its beak. After\r
waiting for a long time without seeing any swipe that they could reinforce, they\r
... decided to reinforce any response that had the slightest resemblance to\r
a swipe—perhaps, at first, merely the behavior of looking at the ball—and\r
then to select responses which more closely approximated the final form. The\r
result amazed us. In a few minutes, the ball was caroming o↵ the walls of\r
the box as if the pigeon had been a champion squash player. (Skinner, 1958,\r
p. 94)\r
Not only did the pigeon learn a behavior that is unusual for pigeons, it learned quickly\r
through an interactive process in which its behavior and the reinforcement contingencies\r
changed in response to each other. Skinner compared the process of altering reinforcement\r
contingencies to the work of a sculptor shaping clay into a desired form. Shaping is a\r
powerful technique for computational reinforcement learning systems as well. When it is\r
dicult for an agent to receive any non-zero reward signal at all, either due to sparseness\r
of rewarding situations or their inaccessibility given initial behavior, starting with an\r
easier problem and incrementally increasing its diculty as the agent learns can be an\r
e↵ective, and sometimes indispensable, strategy.\r
A concept from psychology that is especially relevant in the context of instrumental\r
conditioning is motivation, which refers to processes that influence the direction and\r
strength, or vigor, of behavior. Thorndike’s cats, for example, were motivated to escape\r
from puzzle boxes because they wanted the food that was sitting just outside. Obtaining\r
this goal was rewarding to them and reinforced the actions allowing them to escape. It\r
is dicult to link the concept of motivation, which has many dimensions, in a precise"""

[[sections]]
number = "14.4"
title = "Delayed Reinforcement 361"
text = """
way to reinforcement learning’s computational perspective, but there are clear links with\r
some of its dimensions.\r
In one sense, a reinforcement learning agent’s reward signal is at the base of its\r
motivation: the agent is motivated to maximize the total reward it receives over the long\r
run. A key facet of motivation, then, is what makes an agent’s experience rewarding. In\r
reinforcement learning, reward signals depend on the state of the reinforcement learning\r
agent’s environment and the agent’s actions. Further, as pointed out in Chapter 1, the\r
state of the agent’s environment not only includes information about what is external to\r
the machine, like an organism or a robot, that houses the agent, but also what is internal\r
to this machine. Some internal state components correspond to what psychologists call\r
an animal’s motivational state, which influences what is rewarding to the animal. For\r
example, an animal will be more rewarded by eating when it is hungry than when it has\r
just finished a satisfying meal. The concept of state dependence is broad enough to allow\r
for many types of modulating influences on the generation of reward signals.\r
Value functions provide a further link to psychologists’ concept of motivation. If the\r
most basic motive for selecting an action is to obtain as much reward as possible, for a\r
reinforcement learning agent that selects actions using a value function, a more proximal\r
motive is to ascend the gradient of its value function, that is, to select actions expected\r
to lead to the most highly-valued next states (or what is essentially the same thing, to\r
select actions with the greatest action-values). For these agents, value functions are the\r
main driving force determining the direction of their behavior.\r
Another dimension of motivation is that an animal’s motivational state not only\r
influences learning, but also influences the strength, or vigor, of the animal’s behavior\r
after learning. For example, after learning to find food in the goal box of a maze, a hungry\r
rat will run faster to the goal box than one that is not hungry. This aspect of motivation\r
does not link so cleanly to the reinforcement learning framework we present here, but\r
in the Bibliographical and Historical Remarks section at the end of this chapter we cite\r
several publications that propose theories of behavioral vigor based on reinforcement\r
learning.\r
We turn now to the subject of learning when reinforcing stimuli occur well after the\r
events they reinforce. The mechanisms used by reinforcement learning algorithms to\r
enable learning with delayed reinforcement—eligibility traces and TD learning—closely\r
correspond to psychologists’ hypotheses about how animals can learn under these condi\u0002tions."""

[[sections]]
number = "14.4"
title = "Delayed Reinforcement"
text = """
The Law of E↵ect requires a backward e↵ect on connections, and some early critics of the\r
law could not conceive of how the present could a↵ect something that was in the past. This\r
concern was amplified by the fact that learning can even occur when there is a considerable\r
delay between an action and the consequent reward or penalty. Similarly, in classical\r
conditioning, learning can occur when US onset follows CS o↵set by a non-negligible time\r
interval. We call this the problem of delayed reinforcement, which is related to what\r
Minsky (1961) called the “credit-assignment problem for learning systems”: how do you"""

[[sections]]
number = "362"
title = "Chapter 14: Psychology"
text = """
distribute credit for success among the many decisions that may have been involved in\r
producing it? The reinforcement learning algorithms presented in this book include two\r
basic mechanisms for addressing this problem. The first is the use of eligibility traces,\r
and the second is the use of TD methods to learn value functions that provide nearly\r
immediate evaluations of actions (in tasks like instrumental conditioning experiments) or\r
that provide immediate prediction targets (in tasks like classical conditioning experiments).\r
Both of these methods correspond to similar mechanisms proposed in theories of animal\r
learning.\r
Pavlov (1927) pointed out that every stimulus must leave a trace in the nervous system\r
that persists for some time after the stimulus ends, and he proposed that stimulus traces\r
make learning possible when there is a temporal gap between the CS o↵set and the\r
US onset. To this day, conditioning under these conditions is called trace conditioning\r
(page 344). Assuming a trace of the CS remains when the US arrives, learning occurs\r
through the simultaneous presence of the trace and the US. We discuss some proposals\r
for trace mechanisms in the nervous system in Chapter 15.\r
Stimulus traces were also proposed as a means for bridging the time interval between\r
actions and consequent rewards or penalties in instrumental conditioning. In Hull’s\r
influential learning theory, for example, “molar stimulus traces” accounted for what\r
he called an animal’s goal gradient, a description of how the maximum strength of an\r
instrumentally-conditioned response decreases with increasing delay of reinforcement\r
(Hull, 1932, 1943). Hull hypothesized that an animal’s actions leave internal stimuli whose\r
traces decay exponentially as functions of time since an action was taken. Looking at the\r
animal learning data available at the time, he hypothesized that the traces e↵ectively\r
reach zero after 30 to 40 seconds.\r
The eligibility traces used in the algorithms described in this book are like Hull’s\r
traces: they are decaying traces of past state visitations, or of past state–action pairs.\r
Eligibility traces were introduced by Klopf (1972) in his neuronal theory in which they\r
are temporally-extended traces of past activity at synapses, the connections between\r
neurons. Klopf’s traces are more complex than the exponentially-decaying traces our\r
algorithms use, and we discuss this more when we take up his theory in Section 15.9.\r
To account for goal gradients that extend over longer time periods than spanned\r
by stimulus traces, Hull (1943) proposed that longer gradients result from conditioned\r
reinforcement passing backwards from the goal, a process acting in conjunction with\r
his molar stimulus traces. Animal experiments showed that if conditions favor the\r
development of conditioned reinforcement during a delay period, learning does not\r
decrease with increased delay as much as it does under conditions that obstruct secondary\r
reinforcement. Conditioned reinforcement is favored if there are stimuli that regularly\r
occur during the delay interval. Then it is as if reward is not actually delayed because\r
there is more immediate conditioned reinforcement. Hull therefore envisioned that there\r
is a primary gradient based on the delay of the primary reinforcement mediated by\r
stimulus traces, and that this is progressively modified, and lengthened, by conditioned\r
reinforcement.\r
Algorithms presented in this book that use both eligibility traces and value functions\r
to enable learning with delayed reinforcement correspond to Hull’s hypothesis about how\r
animals are able to learn under these conditions. The actor–critic architecture discussed"""

[[sections]]
number = "14.5"
title = "Cognitive Maps 363"
text = """
in Sections 13.5, 15.7, and 15.8 illustrates this correspondence most clearly. The critic uses\r
a TD algorithm to learn a value function associated with the system’s current behavior,\r
that is, to predict the current policy’s return. The actor updates the current policy based\r
on the critic’s predictions, or more exactly, on changes in the critic’s predictions. The\r
TD error produced by the critic acts as a conditioned reinforcement signal for the actor,\r
providing an immediate evaluation of performance even when the primary reward signal\r
itself is considerably delayed. Algorithms that estimate action-value functions, such as\r
Q-learning and Sarsa, similarly use TD learning principles to enable learning with delayed\r
reinforcement by means of conditioned reinforcement. The close parallel between TD\r
learning and the activity of dopamine producing neurons that we discuss in Chapter 15\r
lends additional support to links between reinforcement learning algorithms and this\r
aspect of Hull’s learning theory."""

[[sections]]
number = "14.5"
title = "Cognitive Maps"
text = """
Model-based reinforcement learning algorithms use environment models that have elements\r
in common with what psychologists call cognitive maps. Recall from our discussion of\r
planning and learning in Chapter 8 that by an environment model we mean anything\r
an agent can use to predict how its environment will respond to its actions in terms of\r
state transitions and rewards, and by planning we mean any process that computes a\r
policy from such a model. Environment models consist of two parts: the state-transition\r
part encodes knowledge about the e↵ect of actions on state changes, and the reward\u0002model part encodes knowledge about the reward signals expected for each state or each\r
state–action pair. A model-based algorithm selects actions by using a model to predict\r
the consequences of possible courses of action in terms of future states and the reward\r
signals expected to arise from those states. The simplest kind of planning is to compare\r
the predicted consequences of collections of “imagined” sequences of decisions.\r
Questions about whether or not animals use environment models, and if so, what are the\r
models like and how are they learned, have played influential roles in the history of animal\r
learning research. Some researchers challenged the then-prevailing stimulus-response\r
(S–R) view of learning and behavior, which corresponds to the simplest model-free way\r
of learning policies, by demonstrating latent learning. In the earliest latent learning\r
experiment, two groups of rats were run in a maze. For the experimental group, there\r
was no reward during the first stage of the experiment, but food was suddenly introduced\r
into the goal box of the maze at the start of the second stage. For the control group, food\r
was in the goal box throughout both stages. The question was whether or not rats in the\r
experimental group would have learned anything during the first stage in the absence\r
of food reward. Although the experimental rats did not appear to learn much during\r
the first, unrewarded, stage, as soon as they discovered the food that was introduced\r
in the second stage, they rapidly caught up with the rats in the control group. It was\r
concluded that “during the non-reward period, the rats [in the experimental group] were\r
developing a latent learning of the maze which they were able to utilize as soon as reward\r
was introduced” (Blodgett, 1929)."""

[[sections]]
number = "364"
title = "Chapter 14: Psychology"
text = """
Latent learning is most closely associated with the psychologist Edward Tolman, who\r
interpreted this result, and others like it, as showing that animals could learn a “cognitive\r
map of the environment” in the absence of rewards or penalties, and that they could use\r
the map later when they were motivated to reach a goal (Tolman, 1948). A cognitive map\r
could also allow a rat to plan a route to the goal that was di↵erent from the route the rat\r
had used in its initial exploration. Explanations of results like these led to the enduring\r
controversy lying at the heart of the behaviorist/cognitive dichotomy in psychology. In\r
modern terms, cognitive maps are not restricted to models of spatial layouts but are\r
more generally environment models, or models of an animal’s “task space” (e.g., Wilson,\r
Takahashi, Schoenbaum, and Niv, 2014). The cognitive map explanation of latent learning\r
experiments is analogous to the claim that animals use model-based algorithms, and that\r
environment models can be learned even without explicit rewards or penalties. Models\r
are then used for planning when the animal is motivated by the appearance of rewards or\r
penalties.\r
Tolman’s account of how animals learn cognitive maps was that they learn stimulus\u0002stimulus, or S–S, associations by experiencing successions of stimuli as they explore an\r
environment. In psychology this is called expectancy theory: given S–S associations, the\r
occurrence of a stimulus generates an expectation about the stimulus to come next. This\r
is much like what control engineers call system identification, in which a model of a\r
system with unknown dynamics is learned from labeled training examples. In the simplest\r
discrete-time versions, training examples are S–S0 pairs, where S is a state and S0, the\r
subsequent state, is the label. When S is observed, the model creates the “expectation”\r
that S0 will be observed next. Models more useful for planning involve actions as well,\r
so that examples look like SA–S0, where S0 is expected when action A is executed in\r
state S. It is also useful to learn how the environment generates rewards. In this case,\r
examples are of the form S–R or SA–R, where R is a reward signal associated with S or\r
the SA pair. These are all forms of supervised learning by which an agent can acquire\r
cognitive-like maps whether or not it receives any non-zero reward signals while exploring\r
its environment."""

[[sections]]
number = "14.6"
title = "Habitual and Goal-directed Behavior"
text = """
The distinction between model-free and model-based reinforcement learning algorithms\r
corresponds to the distinction psychologists make between habitual and goal-directed\r
control of learned behavioral patterns. Habits are behavior patterns triggered by appro\u0002priate stimuli and then performed more-or-less automatically. Goal-directed behavior,\r
according to how psychologists use the phrase, is purposeful in the sense that it is con\u0002trolled by knowledge of the value of goals and the relationship between actions and their\r
consequences. Habits are sometimes said to be controlled by antecedent stimuli, whereas\r
goal-directed behavior is said to be controlled by its consequences (Dickinson, 1980,\r
1985). Goal-directed control has the advantage that it can rapidly change an animal’s\r
behavior when the environment changes its way of reacting to the animal’s actions. While\r
habitual behavior responds quickly to input from an accustomed environment, it is unable"""

[[sections]]
number = "14.6"
title = "Habitual and Goal-directed Behavior 365"
text = """
to quickly adjust to changes in the environment. The development of goal-directed\r
behavioral control was likely a major advance in the evolution of animal intelligence.\r
Figure 14.5 illustrates the di↵erence between model-free and model-based decision\r
strategies in a hypothetical task in which a rat has to navigate a maze that has distinctive\r
goal boxes, each delivering an associated reward of the magnitude shown (Figure 14.5\r
top). Starting at S1, the rat has to first select left (L) or right (R) and then has to select\r
L or R again at S2 or S3 to reach one of the goal boxes. The goal boxes are the terminal\r
states of each episode of the rat’s episodic task. A model-free strategy (Figure 14.5 lower\r
left) relies on stored values for state–action pairs. These action values are estimates of\r
the highest return the rat can expect for each action taken from each (nonterminal) state.\r
They are obtained over many trials of running the maze from start to finish. When the\r
action values have become good enough estimates of the optimal returns, the rat just has\r
S1, L\r
S2, L\r
S2\r
, R\r
S3\r
, L\r
S3\r
, R\r
S1\r
, R\r
4\r
3\r
0\r
4\r
3"""

[[sections]]
number = "2"
title = "Model-Free"
text = """
= 4\r
= 0\r
= 2\r
= 3\r
Reward\r
Model-Based\r
S1\r
L\r
R\r
S2\r
S3\r
L\r
R\r
L\r
R\r
(4)\r
S2 S3\r
S1\r
0 4 2 3\r
Figure 14.5: Model-based and model-free strategies to solve a hypothetical sequential action\u0002selection problem. Top: a rat navigates a maze with distinctive goal boxes, each associated\r
with a reward having the value shown. Lower left: a model-free strategy relies on stored action\r
values for all the state–action pairs obtained over many learning trials. To make decisions the\r
rat just has to select at each state the action with the largest action value for that state. Lower\r
right: in a model-based strategy, the rat learns an environment model, consisting of knowledge\r
of state–action-next-state transitions and a reward model consisting of knowledge of the reward\r
associated with each distinctive goal box. The rat can decide which way to turn at each state\r
by using the model to simulate sequences of action choices to find a path yielding the highest\r
return. Adapted from Trends in Cognitive Science, volume 10, number 8, Y. Niv, D. Joel, and P.\r
Dayan, A Normative Perspective on Motivation, p. 376, 2006, with permission from Elsevier."""

[[sections]]
number = "366"
title = "Chapter 14: Psychology"
text = """
to select at each state the action with the largest action value in order to make optimal\r
decisions. In this case, when the action-value estimates become accurate enough, the\r
rat selects L from S1 and R from S2 to obtain the maximum return of 4. A di↵erent\r
model-free strategy might simply rely on a cached policy instead of action values, making\r
direct links from S1 to L and from S2 to R. In neither of these strategies do decisions\r
rely on an environment model. There is no need to consult a state-transition model, and\r
no connection is required between the features of the goal boxes and the rewards they\r
deliver.\r
Figure 14.5 (lower right) illustrates a model-based strategy. It uses an environment\r
model consisting of a state-transition model and a reward model. The state-transition\r
model is shown as a decision tree, and the reward model associates the distinctive features\r
of the goal boxes with the rewards to be found in each. (The rewards associated with\r
states S1, S2, and S3 are also part of the reward model, but here they are zero and are\r
not shown.) A model-based agent can decide which way to turn at each state by using\r
the model to simulate sequences of action choices to find a path yielding the highest\r
return. In this case the return is the reward obtained from the outcome at the end of\r
the path. Here, with a suciently accurate model, the rat would select L and then R\r
to obtain a return of 4. Comparing the predicted returns of simulated paths is a simple\r
form of planning, which can be done in a variety of ways as discussed in Chapter 8.\r
When the environment of a model-free agent changes the way it reacts to the agent’s\r
actions, the agent has to acquire new experience in the changed environment during\r
which it can update its policy and/or value function. In the model-free strategy shown\r
in Figure 14.5 (lower left), for example, if one of the goal boxes were to somehow shift\r
to delivering a di↵erent reward, the rat would have to traverse the maze, possibly many\r
times, to experience the new reward upon reaching that goal box, all the while updating\r
either its policy or its action-value function (or both) based on this experience. The key\r
point is that for a model-free agent to change the action its policy specifies for a state, or\r
to change an action value associated with a state, it has to move to that state, act from\r
it, possibly many times, and experience the consequences of its actions.\r
A model-based agent can accommodate changes in its environment without this kind\r
of ‘personal experience’ with the states and actions a↵ected by the change. A change in\r
its model automatically (through planning) changes its policy. Planning can determine\r
the consequences of changes in the environment that have never been linked together in\r
the agent’s own experience. For example, again referring to the maze task of Figure 14.5,\r
imagine that a rat with a previously learned transition and reward model is placed directly\r
in the goal box to the right of S2 to find that the reward available there now has value 1\r
instead of 4. The rat’s reward model will change even though the action choices required\r
to find that goal box in the maze were not involved. The planning process will bring\r
knowledge of the new reward to bear on maze running without the need for additional\r
experience in the maze; in this case changing the policy to right turns at both S1 and S3\r
to obtain a return of 3.\r
Exactly this logic is the basis of outcome-devaluation experiments with animals. Results\r
from these experiments provide insight into whether an animal has learned a habit or if\r
its behavior is under goal-directed control. Outcome-devaluation experiments are like\r
latent-learning experiments in that the reward changes from one stage to the next. After"""

[[sections]]
number = "14.6"
title = "Habitual and Goal-directed Behavior 367"
text = """
an initial rewarded stage of learning, the reward value of an outcome is changed, including\r
being shifted to zero or even to a negative value.\r
An early important experiment of this type was conducted by Adams and Dickinson\r
(1981). They trained rats via instrumental conditioning until the rats energetically pressed\r
a lever for sucrose pellets in a training chamber. The rats were then placed in the same\r
chamber with the lever retracted and allowed non-contingent food, meaning that pellets\r
were made available to them independently of their actions. After 15-minutes of this\r
free-access to the pellets, rats in one group were injected with the nausea-inducing poison\r
lithium chloride. This was repeated for three sessions, in the last of which none of the\r
injected rats consumed any of the non-contingent pellets, indicating that the reward\r
value of the pellets had been decreased—the pellets had been devalued. In the next stage\r
taking place a day later, the rats were again placed in the chamber and given a session of\r
extinction training, meaning that the response lever was back in place but disconnected\r
from the pellet dispenser so that pressing it did not release pellets. The question was\r
whether the rats that had the reward value of the pellets decreased would lever-press\r
less than rats that did not have the reward value of the pellets decreased, even without\r
experiencing the devalued reward as a result of lever-pressing. It turned out that the\r
injected rats had significantly lower response rates than the non-injected rats right from\r
the start of the extinction trials.\r
Adams and Dickinson concluded that the injected rats associated lever pressing with\r
consequent nausea by means of a cognitive map linking lever pressing to pellets, and\r
pellets to nausea. Hence, in the extinction trials, the rats “knew” that the consequences\r
of pressing the lever would be something they did not want, and so they reduced their\r
lever-pressing right from the start. The important point is that they reduced lever-pressing\r
without ever having experienced lever-pressing directly followed by being sick: no lever\r
was present when they were made sick. They seemed able to combine knowledge of the\r
outcome of a behavioral choice (pressing the lever will be followed by getting a pellet)\r
with the reward value of the outcome (pellets are to be avoided) and hence could alter\r
their behavior accordingly. Not every psychologist agrees with this “cognitive” account\r
of this kind of experiment, and it is not the only possible way to explain these results,\r
but the model-based planning explanation is widely accepted.\r
Nothing prevents an agent from using both model-free and model-based algorithms, and\r
there are good reasons for using both. We know from our own experience that with enough\r
repetition, goal-directed behavior tends to turn into habitual behavior. Experiments show\r
that this happens for rats too. Adams (1982) conducted an experiment to see if extended\r
training would convert goal-directed behavior into habitual behavior. He did this by\r
comparing the e↵ect of outcome devaluation on rats that experienced di↵erent amounts\r
of training. If extended training made the rats less sensitive to devaluation compared to\r
rats that received less training, this would be evidence that extended training made the\r
behavior more habitual. Adams’ experiment closely followed the Adams and Dickinson\r
(1981) experiment just described. Simplifying a bit, rats in one group were trained until\r
they made 100 rewarded lever-presses, and rats in the other group—the overtrained\r
group—were trained until they made 500 rewarded lever-presses. After this training,\r
the reward value of the pellets was decreased (using lithium chloride injections) for rats"""

[[sections]]
number = "368"
title = "Chapter 14: Psychology"
text = """
in both groups. Then both groups of rats were given a session of extinction training.\r
Adams’ question was whether devaluation would e↵ect the rate of lever-pressing for the\r
overtrained rats less than it would for the non-overtrained rats, which would be evidence\r
that extended training reduces sensitivity to outcome devaluation. It turned out that\r
devaluation strongly decreased the lever-pressing rate of the non-overtrained rats. For\r
the overtrained rats, in contrast, devaluation had little e↵ect on their lever-pressing; in\r
fact, if anything, it made it more vigorous. (The full experiment included control groups\r
showing that the di↵erent amounts of training did not by themselves significantly e↵ect\r
lever-pressing rates after learning.) This result suggested that while the non-overtrained\r
rats were acting in a goal-directed manner sensitive to their knowledge of the outcome of\r
their actions, the overtrained rats had developed a lever-pressing habit.\r
Viewing this and other results like it from a computational perspective provides insight\r
as to why one might expect animals to behave habitually in some circumstances but in a\r
goal-directed way in others, and why they shift from one mode of control to another as\r
they continue to learn. While animals undoubtedly use algorithms that do not exactly\r
match those we have presented in this book, one can gain insight into animal behavior by\r
considering the tradeo↵s that various reinforcement learning algorithms imply. An idea\r
developed by computational neuroscientists Daw, Niv, and Dayan (2005) is that animals\r
use both model-free and model-based processes. Each process proposes an action, and\r
the action chosen for execution is the one proposed by the process judged to be the more\r
trustworthy of the two as determined by measures of confidence that are maintained\r
throughout learning. Early in learning the planning process of a model-based system is\r
more trustworthy because it chains together short-term predictions which can become\r
accurate with less experience than long-term predictions of the model-free process. But\r
with continued experience, the model-free process becomes more trustworthy because\r
planning is prone to making mistakes due to model inaccuracies and short-cuts necessary\r
to make planning feasible, such as various forms of “tree-pruning”: the removal of\r
unpromising search tree branches. According to this idea one would expect a shift from\r
goal-directed behavior to habitual behavior as more experience accumulates. Other ideas\r
have been proposed for how animals arbitrate between goal-directed and habitual control,\r
and both behavioral and neuroscience research continues to examine this and related\r
questions.\r
The distinction between model-free and model-based algorithms is proving to be useful\r
for this research. One can examine the computational implications of these types of\r
algorithms in abstract settings that expose basic advantages and limitations of each\r
type. This serves both to suggest and to sharpen questions that guide the design\r
of experiments necessary for increasing psychologists’ understanding of habitual and\r
goal-directed behavioral control."""

[[sections]]
number = "14.7"
title = "Summary"
text = """
Our goal in this chapter has been to discuss correspondences between reinforcement\r
learning and the experimental study of animal learning in psychology. We emphasized\r
at the outset that reinforcement learning as described in this book is not intended"""

[[sections]]
number = "14.7"
title = "Summary 369"
text = """
to model details of animal behavior. It is an abstract computational framework that\r
explores idealized situations from the perspective of artificial intelligence and engineering.\r
But many of the basic reinforcement learning algorithms were inspired by psychological\r
theories, and in some cases, these algorithms have contributed to the development of\r
new animal learning models. This chapter described the most conspicuous of these\r
correspondences.\r
The distinction in reinforcement learning between algorithms for prediction and al\u0002gorithms for control parallels animal learning theory’s distinction between classical, or\r
Pavlovian, conditioning and instrumental conditioning. The key di↵erence between\r
instrumental and classical conditioning experiments is that in the former the reinforcing\r
stimulus is contingent upon the animal’s behavior, whereas in the latter it is not. Learning\r
to predict via a TD algorithm corresponds to classical conditioning, and we described\r
the TD model of classical conditioning as one instance in which reinforcement learning\r
principles account for some details of animal learning behavior. This model generalizes\r
the influential Rescorla–Wagner model by including the temporal dimension where events\r
within individual trials influence learning, and it provides an account of second-order\r
conditioning, where predictors of reinforcing stimuli become reinforcing themselves. It\r
also is the basis of an influential view of the activity of dopamine neurons in the brain,\r
something we take up in Chapter 15.\r
Learning by trial and error is at the base of the control aspect of reinforcement learning.\r
We presented some details about Thorndike’s experiments with cats and other animals that\r
led to his Law of E↵ect, which we discussed here and in Chapter 1 (page 15). We pointed\r
out that in reinforcement learning, exploration does not have to be limited to “blind\r
groping”; trials can be generated by sophisticated methods using innate and previously\r
learned knowledge as long as there is some exploration. We discussed the training method\r
B. F. Skinner called shaping in which reward contingencies are progressively altered to\r
train an animal to successively approximate a desired behavior. Shaping is not only\r
indispensable for animal training, it is also an e↵ective tool for training reinforcement\r
learning agents. There is also a connection to the idea of an animal’s motivational state,\r
which influences what an animal will approach or avoid and what events are rewarding\r
or punishing for the animal.\r
The reinforcement learning algorithms presented in this book include two basic mecha\u0002nisms for addressing the problem of delayed reinforcement: eligibility traces and value\r
functions learned via TD algorithms. Both mechanisms have antecedents in theories of\r
animal learning. Eligibility traces are similar to stimulus traces of early theories, and\r
value functions correspond to the role of secondary reinforcement in providing nearly\r
immediate evaluative feedback.\r
The next correspondence the chapter addressed is that between reinforcement learning’s\r
environment models and what psychologists call cognitive maps. Experiments conducted\r
in the mid 20th century purported to demonstrate the ability of animals to learn cognitive\r
maps as alternatives to, or as additions to, state–action associations, and later use them\r
to guide behavior, especially when the environment changes unexpectedly. Environment\r
models in reinforcement learning are like cognitive maps in that they can be learned by\r
supervised learning methods without relying on reward signals, and then they can be\r
used later to plan behavior."""

[[sections]]
number = "370"
title = "Chapter 14: Psychology"
text = """
Reinforcement learning’s distinction between model-free and model-based algorithms\r
corresponds to the distinction in psychology between habitual and goal-directed behavior.\r
Model-free algorithms make decisions by accessing information that has been stored in a\r
policy or an action-value function, whereas model-based methods select actions as the\r
result of planning ahead using a model of the agent’s environment. Outcome-devaluation\r
experiments provide information about whether an animal’s behavior is habitual or under\r
goal-directed control. Reinforcement learning theory has helped clarify thinking about\r
these issues.\r
Animal learning clearly informs reinforcement learning, but as a type of machine\r
learning, reinforcement learning is directed toward designing and understanding e↵ective\r
learning algorithms, not toward replicating or explaining details of animal behavior.\r
We focused on aspects of animal learning that relate in clear ways to methods for\r
solving prediction and control problems, highlighting the fruitful two-way flow of ideas\r
between reinforcement learning and psychology without venturing deeply into many of the\r
behavioral details and controversies that have occupied the attention of animal learning\r
researchers. Future development of reinforcement learning theory and algorithms will\r
likely exploit links to many other features of animal learning as the computational utility\r
of these features becomes better appreciated. We expect that a flow of ideas between\r
reinforcement learning and psychology will continue to bear fruit for both disciplines.\r
Many connections between reinforcement learning and areas of psychology and other\r
behavioral sciences are beyond the scope of this chapter. We largely omit discussing\r
links to the psychology of decision making, which focuses on how actions are selected,\r
or how decisions are made, after learning has taken place. We also do not discuss links\r
to ecological and evolutionary aspects of behavior studied by ethologists and behavioral\r
ecologists: how animals relate to one another and to their physical surroundings, and how\r
their behavior contributes to evolutionary fitness. Optimization, MDPs, and dynamic\r
programming figure prominently in these fields, and our emphasis on agent interaction\r
with dynamic environments connects to the study of agent behavior in complex “ecologies.”\r
Multi-agent reinforcement learning, omitted in this book, has connections to social aspects\r
of behavior. Despite the lack of treatment here, reinforcement learning should by no means\r
be interpreted as dismissing evolutionary perspectives. Nothing about reinforcement\r
learning implies a tabula rasa view of learning and behavior. Indeed, experience with\r
engineering applications has highlighted the importance of building into reinforcement\r
learning systems knowledge that is analogous to what evolution provides to animals."""

[[sections]]
number = "14.7"
title = "Summary 371"
text = """
Bibliographical and Historical Remarks\r
Ludvig, Bellemare, and Pearson (2011) and Shah (2012) review reinforcement learning in\r
the contexts of psychology and neuroscience. These publications are useful companions\r
to this chapter and the following chapter on reinforcement learning and neuroscience.\r
14.1 Dayan, Niv, Seymour, and Daw (2006) focused on interactions between clas\u0002sical and instrumental conditioning, particularly situations where classically\u0002conditioned and instrumental responses are in conflict. They proposed a Q\u0002learning framework for modeling aspects of this interaction. Modayil and Sutton\r
(2014) used a mobile robot to demonstrate the e↵ectiveness of a control method\r
combining a fixed response with online prediction learning. Calling this Pavlo\u0002vian control, they emphasized that it di↵ers from the usual control methods of\r
reinforcement learning, being based on predictively executing fixed responses\r
and not on reward maximization. The electro-mechanical machine of Ross (1933)\r
and especially the learning version of Walter’s turtle (Walter, 1951) were very\r
early illustrations of Pavlovian control."""

[[sections]]
number = "14.2.1"
title = "Kamin (1968) first reported blocking, now commonly known as Kamin blocking,"
text = """
in classical conditioning. Moore and Schmajuk (2008) provide an excellent\r
summary of the blocking phenomenon, the research it stimulated, and its lasting\r
influence on animal learning theory. Gibbs, Cool, Land, Kehoe, and Gormezano\r
(1991) describe second-order conditioning of the rabbit’s nictitating membrane\r
response and its relationship to conditioning with serial-compound stimuli. Finch\r
and Culler (1934) reported obtaining fifth-order conditioning of a dog’s foreleg\r
withdrawal “when the motivation of the animal is maintained through the various\r
orders.”"""

[[sections]]
number = "14.2.2"
title = "The idea built into the Rescorla–Wagner model that learning occurs when animals"
text = """
are surprised is derived from Kamin (1969). Models of classical conditioning\r
other than Rescorla and Wagner’s include the models of Klopf (1988), Grossberg\r
(1975), Mackintosh (1975), Moore and Stickney (1980), Pearce and Hall (1980),\r
and Courville, Daw, and Touretzky (2006). Schmajuk (2008) reviews models of\r
classical conditioning. Wagner (2008) provides a modern psychological perspective\r
on the Rescorla-Wagner model and similar elemental theories of learning.\r
14.2.3 An early version of the TD model of classical conditioning appeared in Sutton and\r
Barto (1981a), which also included the early model’s prediction that temporal\r
primacy overrides blocking, later shown by Kehoe, Schreurs, and Graham (1987)\r
to occur in the rabbit nictitating membrane preparation. Sutton and Barto\r
(1981a) contains the earliest recognition of the near identity between the Rescorla–\r
Wagner model and the Least-Mean-Square (LMS), or Widrow-Ho↵, learning\r
rule (Widrow and Ho↵, 1960). This early model was revised following Sutton’s\r
development of the TD algorithm (Sutton, 1984, 1988) and was first presented as\r
the TD model in Sutton and Barto (1987) and more completely in Sutton and\r
Barto (1990), upon which this section is largely based. Additional exploration"""

[[sections]]
number = "372"
title = "Chapter 14: Psychology"
text = """
of the TD model and its possible neural implementation was conducted by\r
Moore and colleagues (Moore, Desmond, Berthier, Blazis, Sutton, and Barto,\r
1986; Moore and Blazis, 1989; Moore, Choi, and Brunzell, 1998; Moore, Marks,\r
Castagna, and Polewan, 2001). Klopf’s (1988) drive-reinforcement theory of\r
classical conditioning extends the TD model to address additional experimental\r
details, such as the S-shape of acquisition curves. In some of these publications\r
TD is taken to mean Time Derivative instead of Temporal Di↵erence."""

[[sections]]
number = "14.2.4"
title = "Ludvig, Sutton, and Kehoe (2012) evaluated the performance of the TD model"
text = """
in previously unexplored tasks involving classical conditioning and examined\r
the influence of various stimulus representations, including the microstimulus\r
representation that they introduced earlier (Ludvig, Sutton, and Kehoe, 2008).\r
Earlier investigations of the influence of various stimulus representations and their\r
possible neural implementations on response timing and topography in the context\r
of the TD model are those of Moore and colleagues cited above. Although not in\r
the context of the TD model, representations like the microstimulus representation\r
of Ludvig et al. (2012) have been proposed and studied by Grossberg and\r
Schmajuk (1989), Brown, Bullock, and Grossberg (1999), Buhusi and Schmajuk\r
(1999), and Machado (1997). The figures on pages 353–355 are adapted from\r
Sutton and Barto (1990)."""

[[sections]]
number = "14.3"
title = "Section 1.7 includes comments on the history of trial-and-error learning and"
text = """
the Law of E↵ect. The idea that Thorndike’s cats might have been exploring\r
according to an instinctual context-specific ordering over actions rather than by\r
just selecting from a set of instinctual impulses was suggested by Peter Dayan\r
(personal communication). Selfridge, Sutton, and Barto (1985) illustrated the\r
e↵ectiveness of shaping in a pole-balancing reinforcement learning task. Other\r
examples of shaping in reinforcement learning are Gullapalli and Barto (1992),\r
Mahadevan and Connell (1992), Mataric (1994), Dorigo and Colombette (1994),\r
Saksida, Raymond, and Touretzky (1997), and Randløv and Alstrøm (1998). Ng\r
(2003) and Ng, Harada, and Russell (1999) used the term shaping in a sense\r
somewhat di↵erent from Skinner’s, focusing on the problem of how to alter the\r
reward signal without altering the set of optimal policies.\r
Dickinson and Balleine (2002) discuss the complexity of the interaction between\r
learning and motivation. Wise (2004) provides an overview of reinforcement\r
learning and its relation to motivation. Daw and Shohamy (2008) link motivation\r
and learning to aspects of reinforcement learning theory. See also McClure,\r
Daw, and Montague (2003), Niv, Joel, and Dayan (2006), Rangel, Camerer, and\r
Montague (2008), and Dayan and Berridge (2014). McClure et al. (2003), Niv,\r
Daw, and Dayan (2006), and Niv, Daw, Joel, and Dayan (2007) present theories\r
of behavioral vigor related to the reinforcement learning framework.\r
14.4 Spence, Hull’s student and collaborator at Yale, elaborated the role of higher\u0002order reinforcement in addressing the problem of delayed reinforcement (Spence,\r
1947). Learning over very long delays, as in taste-aversion conditioning with"""

[[sections]]
number = "14.7"
title = "Summary 373"
text = """
delays up to several hours, led to interference theories as alternatives to decaying\u0002trace theories (e.g., Revusky and Garcia, 1970; Boakes and Costa, 2014). Other\r
views of learning under delayed reinforcement invoke roles for awareness and\r
working memory (e.g., Clark and Squire, 1998; Seo, Barraclough, and Lee, 2007).\r
14.5 Thistlethwaite (1951) provides an extensive review of latent learning experiments\r
up to the time of its publication. Ljung (1998) provides an overview of model\r
learning, or system identification, techniques in engineering. Gopnik, Glymour,\r
Sobel, Schulz, Kushnir, and Danks (2004) present a Bayesian theory about how\r
children learn models."""

[[sections]]
number = "14.6"
title = "Connections between habitual and goal-directed behavior and model-free and"
text = """
model-based reinforcement learning were first proposed by Daw, Niv, and Dayan\r
(2005). The hypothetical maze task used to explain habitual and goal-directed\r
behavioral control is based on the explanation of Niv, Joel, and Dayan (2006).\r
Dolan and Dayan (2013) review four generations of experimental research related\r
to this issue and discuss how it can move forward on the basis of reinforcement\r
learning’s model-free/model-based distinction. Dickinson (1980, 1985) and Dick\u0002inson and Balleine (2002) discuss experimental evidence related to this distinction.\r
Donahoe and Burgos (2000) alternatively argue that model-free processes can\r
account for the results of outcome-devaluation experiments. Dayan and Berridge\r
(2014) argue that classical conditioning involves model-based processes. Rangel,\r
Camerer, and Montague (2008) review many of the outstanding issues involving\r
habitual, goal-directed, and Pavlovian modes of control.\r
Comments on Terminology— The traditional meaning of reinforcement in psychol\u0002ogy is the strengthening of a pattern of behavior (by increasing either its intensity or\r
frequency) as a result of an animal receiving a stimulus (or experiencing the omission\r
of a stimulus) in an appropriate temporal relationship with another stimulus or with a\r
response. Reinforcement produces changes that remain in future behavior. Sometimes in\r
psychology reinforcement refers to the process of producing lasting changes in behavior,\r
whether the changes strengthen or weaken a behavior pattern (Mackintosh, 1983). Letting\r
reinforcement refer to weakening in addition to strengthening is at odds with the everyday\r
meaning of reinforce, and its traditional use in psychology, but it is a useful extension\r
that we have adopted here. In either case, a stimulus considered to be the cause of the\r
behavioral change is called a reinforcer.\r
Psychologists do not generally use the specific phrase reinforcement learning as we\r
do. Animal learning pioneers probably regarded reinforcement and learning as being\r
synonymous, so it would be redundant to use both words. Our use of the phrase follows\r
its use in computational and engineering research, influenced mostly by Minsky (1961).\r
But the phrase is lately gaining currency in psychology and neuroscience, likely because\r
strong parallels have surfaced between reinforcement learning algorithms and animal\r
learning—parallels described in this chapter and the next.\r
According to common usage, a reward is an object or event that an animal will\r
approach and work for. A reward may be given to an animal in recognition of its ‘good’"""

[[sections]]
number = "374"
title = "Chapter 14: Psychology"
text = """
behavior, or given in order to make the animal’s behavior ‘better.’ Similarly, a penalty is\r
an object or event that the animal usually avoids and that is given as a consequence of\r
‘bad’ behavior, usually in order to change that behavior. Primary reward is reward due\r
to machinery built into an animal’s nervous system by evolution to improve its chances\r
of survival and reproduction, for example, reward produced by the taste of nourishing\r
food, sexual contact, successful escape, and many other stimuli and events that predicted\r
reproductive success over the animal’s ancestral history. As explained in Section 14.2.1,\r
higher-order reward is reward delivered by stimuli that predict primary reward, either\r
directly or indirectly by predicting other stimuli that predict primary reward. Reward is\r
secondary if its rewarding quality is the result of directly predicting primary reward.\r
In this book we call Rt the ‘reward signal at time t,’ or sometimes just the ‘reward\r
at time t,’ but we do not think of it as an object or event in the agent’s environment.\r
Because Rt is a number—not an object or an event—it is more like a reward signal in\r
neuroscience, which is a signal internal to the brain, like the activity of neurons, that\r
influences decision making and learning. This signal might be triggered when the animal\r
perceives an attractive (or an aversive) object, but it can also be triggered by things that\r
do not physically exist in the animal’s external environment, such as memories, ideas, or\r
hallucinations. Because our Rt can be positive, negative, or zero, it might be better to\r
call a negative Rt a penalty, and an Rt equal to zero a neutral signal, but for simplicity\r
we generally avoid these terms.\r
In reinforcement learning, the process that generates all the Rts defines the problem\r
the agent is trying to solve. The agent’s objective is to keep the magnitude of Rt as large\r
as possible over time. In this respect, Rt is like primary reward for an animal if we think\r
of the problem the animal faces as the problem of obtaining as much primary reward as\r
possible over its lifetime (and thereby, through the prospective “wisdom” of evolution,\r
improve its chances of solving its real problem, which is to pass its genes on to future\r
generations). However, as we suggest in Chapter 15, it is unlikely that there is a single\r
“master” reward signal like Rt in an animal’s brain.\r
Not all reinforcers are rewards or penalties. Sometimes reinforcement is not the result\r
of an animal receiving a stimulus that evaluates its behavior by labeling the behavior\r
good or bad. A behavior pattern can be reinforced by a stimulus that arrives to an animal\r
no matter how the animal behaved. As described in Section 14.1, whether the delivery of\r
reinforcement depends, or does not depend, on preceding behavior is the defining di↵erence\r
between instrumental, or operant, conditioning experiments and classical, or Pavlovian,\r
conditioning experiments. Reinforcement is at work in both types of experiments, but\r
only in the former is it feedback that evaluates past behavior. (Though it has often been\r
pointed out that even when the reinforcing US in a classical conditioning experiment is\r
not contingent on the subject’s preceding behavior, its reinforcing value can be influenced\r
by this behavior, an example being that a closed eye makes an air pu↵ to the eye less\r
aversive.)\r
The distinction between reward signals and reinforcement signals is a crucial point\r
when we discuss neural correlates of these signals in the next chapter. Like a reward signal,\r
for us, the reinforcement signal at any specific time is a positive or negative number, or\r
zero. A reinforcement signal is the major factor directing changes a learning algorithm"""

[[sections]]
number = "14.7"
title = "Summary 375"
text = """
makes in an agent’s policy, value estimates, or environment models. The definition that\r
makes the most sense to us is that a reinforcement signal at any time is a number that\r
multiplies (possibly along with some constants) a vector to determine parameter updates\r
in some learning algorithm.\r
For some algorithms, the reward signal alone is the critical multiplier in the parameter\u0002update equation. For these algorithms the reinforcement signal is the same as the\r
reward signal. But for most of the algorithms we discuss in this book, reinforcement\r
signals include terms in addition to the reward signal, an example being a TD error\r
t = Rt+1 + V (St+1)  V (St), which is the reinforcement signal for TD state-value\r
learning (and analogous TD errors for action-value learning). In this reinforcement signal,\r
Rt+1 is the primary reinforcement contribution, and the temporal di↵erence in predicted\r
values, V (St+1)  V (St) (or an analogous temporal di↵erence for action values), is\r
the conditioned reinforcement contribution. Thus, whenever V (St+1)  V (St) = 0, t\r
signals ‘pure’ primary reinforcement; and whenever Rt+1 = 0, it signals ‘pure’ conditioned\r
reinforcement, but it often signals a mixture of these. Note as we mentioned in Section 6.1,\r
this t is not available until time t + 1. We therefore think of t as the reinforcement\r
signal at time t + 1, which is fitting because it reinforces predictions and/or actions made\r
earlier at step t.\r
A possible source of confusion is the terminology used by the famous psychologist\r
B. F. Skinner and his followers. For Skinner, positive reinforcement occurs when the\r
consequences of an animal’s behavior increase the frequency of that behavior; punishment\r
occurs when the behavior’s consequences decrease that behavior’s frequency. Negative\r
reinforcement occurs when behavior leads to the removal of an aversive stimulus (that is,\r
a stimulus the animal does not like), thereby increasing the frequency of that behavior.\r
Negative punishment, on the other hand, occurs when behavior leads to the removal of an\r
appetitive stimulus (that is, a stimulus the animal likes), thereby decreasing the frequency\r
of that behavior. We find no critical need for these distinctions because our approach\r
is more abstract than this, with both reward and reinforcement signals allowed to take\r
on both positive and negative values. (But note especially that when our reinforcement\r
signal is negative, it is not the same as Skinner’s negative reinforcement.)\r
On the other hand, it has often been pointed out that using a single number as a\r
reward or a penalty signal, depending only on its sign, is at odds with the fact that\r
animals’ appetitive and aversive systems have qualitatively di↵erent properties and\r
involve di↵erent brain mechanisms. This points to a direction in which the reinforcement\r
learning framework might be developed in the future to exploit computational advantages\r
of separate appetitive and aversive systems, but for now we are passing over these\r
possibilities.\r
Another discrepancy in terminology is how we use the word action. To many cognitive\r
scientists, an action is purposeful in the sense of being the result of an animal’s knowledge\r
about the relationship between the behavior in question and the consequences of that\r
behavior. An action is goal-directed and the result of a decision, whereas a response\r
is triggered by a stimulus and is the result of a reflex or a habit. We use the word\r
action without di↵erentiating among what others call actions, decisions, and responses.\r
These are important distinctions, but for us they are encompassed by di↵erences between"""

[[sections]]
number = "376"
title = "Chapter 14: Psychology"
text = """
model-free and model-based reinforcement learning algorithms, which we discussed above\r
in relation to habitual and goal-directed behavior in Section 14.6. Dickinson (1985)\r
discusses the distinction between responses and actions.\r
A term used a lot in this book is control. What we mean by control is entirely di↵erent\r
from what it means to animal learning psychologists. By control we mean that an agent\r
influences its environment to bring about states or events that the agent prefers: the agent\r
exerts control over its environment. This is the sense of control used by control engineers.\r
In psychology, on the other hand, control typically means that an animal’s behavior is\r
influenced by—is controlled by—the stimuli the animal receives (stimulus control) or\r
the reinforcement schedule it experiences. Here the environment is controlling the agent.\r
Control in this sense is the basis of behavior modification therapy. Of course, both of\r
these directions of control are at play when an agent interacts with its environment,\r
but our focus is on the agent as controller, not the environment as controller. A view\r
equivalent to ours, and perhaps more illuminating, is that the agent is actually controlling\r
the input it receives from its environment (Powers, 1973). This is not what psychologists\r
mean by stimulus control.\r
Sometimes reinforcement learning is understood to refer solely to learning policies\r
directly from rewards (and penalties) without the involvement of value functions or\r
environment models. This is what psychologists call stimulus-response, or S-R, learning.\r
But for us, along with most of today’s psychologists, reinforcement learning is much\r
broader than this, including in addition to S-R learning, methods involving value functions,\r
environment models, planning, and other processes that are commonly thought to belong\r
to the more cognitive side of mental functioning.

Chapter 15\r
Neuroscience\r
Neuroscience is the multidisciplinary study of nervous systems: how they regulate bodily\r
functions; control behavior; change over time as a result of development, learning, and\r
aging; and how cellular and molecular mechanisms make these functions possible. One\r
of the most exciting aspects of reinforcement learning is the mounting evidence from\r
neuroscience that the nervous systems of humans and many other animals implement\r
algorithms that correspond in striking ways to reinforcement learning algorithms. The\r
main objective of this chapter is to explain these parallels and what they suggest about\r
the neural basis of reward-related learning in animals.\r
The most remarkable point of contact between reinforcement learning and neuroscience\r
involves dopamine, a chemical deeply involved in reward processing in the brains of mam\u0002mals. Dopamine appears to convey temporal-di↵erence (TD) errors to brain structures\r
where learning and decision making take place. This parallel is expressed by the reward\r
prediction error hypothesis of dopamine neuron activity, a hypothesis that resulted from\r
the convergence of computational reinforcement learning and results of neuroscience\r
experiments. In this chapter we discuss this hypothesis, the neuroscience findings that\r
led to it, and why it is a significant contribution to understanding brain reward systems.\r
We also discuss parallels between reinforcement learning and neuroscience that are less\r
striking than this dopamine/TD-error parallel but that provide useful conceptual tools\r
for thinking about reward-based learning in animals. Other elements of reinforcement\r
learning have the potential to impact the study of nervous systems, but their connections\r
to neuroscience are still relatively undeveloped. We discuss several of these evolving\r
connections that we think will grow in importance over time.\r
As we outlined in the history section of this book’s introductory chapter (Section 1.7),\r
many aspects of reinforcement learning were influenced by neuroscience. A second\r
objective of this chapter is to acquaint readers with ideas about brain function that have\r
contributed to our approach to reinforcement learning. Some elements of reinforcement\r
learning are easier to understand when seen in light of theories of brain function. This\r
is particularly true for the idea of the eligibility trace, one of the basic mechanisms\r
of reinforcement learning, that originated as a conjectured property of synapses, the\r
structures by which nerve cells—neurons—communicate with one another."""

[[sections]]
number = "378"
title = "Chapter 15: Neuroscience"
text = """
In this chapter we do not delve very deeply into the enormous complexity of the neural\r
systems underlying reward-based learning in animals: this chapter is too short, and we are\r
not neuroscientists. We do not try to describe—or even to name—the very many brain\r
structures and pathways, or any of the molecular mechanisms, believed to be involved in\r
these processes. We also do not do justice to hypotheses and models that are alternatives\r
to those that align so well with reinforcement learning. It should not be surprising that\r
there are di↵ering views among experts in the field. We can only provide a glimpse into\r
this fascinating and developing story. We hope, though, that this chapter convinces\r
you that a very fruitful channel has emerged connecting reinforcement learning and its\r
theoretical underpinnings to the neuroscience of reward-based learning in animals.\r
Many excellent publications cover links between reinforcement learning and neuro\u0002science, some of which we cite in this chapter’s final section. Our treatment di↵ers from\r
most of these because we assume familiarity with reinforcement learning as presented\r
in the earlier chapters of this book, but we do not assume knowledge of neuroscience.\r
We begin with a brief introduction to the neuroscience concepts needed for a basic\r
understanding of what is to follow."""

[[sections]]
number = "15.1"
title = "Neuroscience Basics"
text = """
Some basic information about nervous systems is helpful for following what we cover in\r
this chapter. Terms that we refer to later are italicized. Skipping this section will not be\r
a problem if you already have an elementary knowledge of neuroscience.\r
Neurons, the main components of nervous systems, are cells specialized for processing\r
and transmitting information using electrical and chemical signals. They come in many\r
forms, but a neuron typically has a cell body, dendrites, and a single axon. Dendrites\r
are structures that branch from the cell body to receive input from other neurons (or to\r
also receive external signals in the case of sensory neurons). A neuron’s axon is a fiber\r
that carries the neuron’s output to other neurons (or to muscles or glands). A neuron’s\r
output consists of sequences of electrical pulses called action potentials that travel along\r
the axon. Action potentials are also called spikes, and a neuron is said to fire when it\r
generates a spike. In models of neural networks it is common to use real numbers to\r
represent a neuron’s firing rate, the average number of spikes per some unit of time.\r
A neuron’s axon can branch widely so that the neuron’s action potentials reach\r
many targets. The branching structure of a neuron’s axon is called the neuron’s axonal\r
arbor. Because the conduction of an action potential is an active process, not unlike the\r
burning of a fuse, when an action potential reaches an axonal branch point it “lights\r
up” action potentials on all of the outgoing branches (although propagation to a branch\r
can sometimes fail). As a result, the activity of a neuron with a large axonal arbor can\r
influence many target sites."""

[[sections]]
number = "15.1"
title = "Neuroscience Basics 379"
text = """
A synapse is a structure generally at the termination of an axon branch that mediates\r
the communication of one neuron to another. A synapse transmits information from\r
the presynaptic neuron’s axon to a dendrite or cell body of the postsynaptic neuron.\r
With a few exceptions, synapses release a chemical neurotransmitter upon the arrival\r
of an action potential from the presynaptic neuron. (The exceptions are cases of direct\r
electric coupling between neurons, but these will not concern us here.) Neurotransmitter\r
molecules released from the presynaptic side of the synapse di↵use across the synaptic\r
cleft, the very small space between the presynaptic ending and the postsynaptic neuron,\r
and then bind to receptors on the surface of the postsynaptic neuron to excite or inhibit\r
its spike-generating activity, or to modulate its behavior in other ways. A particular\r
neurotransmitter may bind to several di↵erent types of receptors, with each producing a\r
di↵erent e↵ect on the postsynaptic neuron. For example, there are at least five di↵erent\r
receptor types by which the neurotransmitter dopamine can a↵ect a postsynaptic neuron.\r
Many di↵erent chemicals have been identified as neurotransmitters in animal nervous\r
systems.\r
A neuron’s background activity is its level of activity, usually its firing rate, when the\r
neuron does not appear to be driven by synaptic input related to the task of interest\r
to the experimenter, for example, when the neuron’s activity is not correlated with a\r
stimulus delivered to a subject as part of an experiment. Background activity can be\r
irregular due to input from the wider network, or due to noise within the neuron or its\r
synapses. Sometimes background activity is the result of dynamic processes intrinsic to\r
the neuron. A neuron’s phasic activity, in contrast to its background activity, consists of\r
bursts of spiking activity usually caused by synaptic input. Activity that varies slowly\r
and often in a graded manner, whether as background activity or not, is called a neuron’s\r
tonic activity.\r
The strength or e↵ectiveness by which the neurotransmitter released at a synapse\r
influences the postsynaptic neuron is the synapse’s ecacy. One way a nervous system\r
can change through experience is through changes in synaptic ecacies as a result of\r
combinations of the activities of the presynaptic and postsynaptic neurons, and sometimes\r
by the presence of a neuromodulator, which is a neurotransmitter having e↵ects other\r
than, or in addition to, direct fast excitation or inhibition.\r
Brains contain several di↵erent neuromodulation systems consisting of clusters of\r
neurons with widely branching axonal arbors, with each system using a di↵erent neuro\u0002transmitter. Neuromodulation can alter the function of neural circuits, mediate motivation,\r
arousal, attention, memory, mood, emotion, sleep, and body temperature. Important\r
here is that a neuromodulatory system can distribute something like a scalar signal, such\r
as a reinforcement signal, to alter the operation of synapses in widely distributed sites\r
critical for learning.\r
The ability of synaptic ecacies to change is called synaptic plasticity. It is one of the\r
primary mechanisms responsible for learning. The parameters, or weights, adjusted by\r
learning algorithms correspond to synaptic ecacies. As we detail below, modulation of\r
synaptic plasticity via the neuromodulator dopamine is a plausible mechanism for how\r
the brain might implement learning algorithms like many of those described in this book."""

[[sections]]
number = "380"
title = "Chapter 15: Neuroscience"
text = ""

[[sections]]
number = "15.2"
title = "Reward Signals, Reinforcement Signals, Values,"
text = """
and Prediction Errors\r
Links between neuroscience and computational reinforcement learning begin as parallels\r
between signals in the brain and signals playing prominent roles in reinforcement learning\r
theory and algorithms. In Chapter 3 we said that any problem of learning goal-directed\r
behavior can be reduced to the three signals representing actions, states, and rewards.\r
However, to explain links that have been made between neuroscience and reinforcement\r
learning, we have to be less abstract than this and consider other reinforcement learning\r
signals that correspond, in certain ways, to signals in the brain. In addition to reward\r
signals, these include reinforcement signals (which we argue are di↵erent from reward\r
signals), value signals, and signals conveying prediction errors. When we label a signal by\r
its function in this way, we are doing it in the context of reinforcement learning theory\r
in which the signal corresponds to a term in an equation or an algorithm. On the other\r
hand, when we refer to a signal in the brain, we mean a physiological event such as\r
a burst of action potentials or the secretion of a neurotransmitter. Labeling a neural\r
signal by its function, for example calling the phasic activity of a dopamine neuron a\r
reinforcement signal, means that the neural signal behaves like, and is conjectured to\r
function like, the corresponding theoretical signal.\r
Uncovering evidence for these correspondences involves many challenges. Neural\r
activity related to reward processing can be found in nearly every part of the brain,\r
and it is dicult to interpret results unambiguously because representations of di↵erent\r
reward-related signals tend to be highly correlated with one another. Experiments need to\r
be carefully designed to allow one type of reward-related signal to be distinguished with\r
any degree of certainty from others—or from an abundance of other signals not related to\r
reward processing. Despite these diculties, many experiments have been conducted with\r
the aim of reconciling aspects of reinforcement learning theory and algorithms with neural\r
signals, and some compelling links have been established. To prepare for examining these\r
links, in the rest of this section we remind the reader of what various reward-related\r
signals mean according to reinforcement learning theory.\r
In our Comments on Terminology at the end of the previous chapter, we said that\r
Rt is like a reward signal in an animal’s brain and not an object or event in the\r
animal’s environment. In reinforcement learning, the reward signal (along with an agent’s\r
environment) defines the problem a reinforcement learning agent is trying to solve. In\r
this respect, Rt is like a signal in an animal’s brain that distributes primary reward to\r
sites throughout the brain. But it is unlikely that a unitary master reward signal like Rt\r
exists in an animal’s brain. It is best to think of Rt as an abstraction summarizing the\r
overall e↵ect of a multitude of neural signals generated by many systems in the brain\r
that assess the rewarding or punishing qualities of sensations and states.\r
Reinforcement signals in reinforcement learning are di↵erent from reward signals. The\r
function of a reinforcement signal is to direct the changes a learning algorithm makes in\r
an agent’s policy, value estimates, or environment models. For a TD method, for instance,\r
the reinforcement signal at time t is the TD error t1 = Rt + V (St)  V (St1).1 The\r
1As we mentioned in Section 6.1, t in our notation is defined to be Rt+1 + V (St+1)  V (St), so t"""

[[sections]]
number = "15.3"
title = "The Reward Prediction Error Hypothesis 381"
text = """
reinforcement signal for some algorithms could be just the reward signal, but for most\r
of the algorithms we consider the reinforcement signal is the reward signal adjusted by\r
other information, such as the value estimates in TD errors.\r
Estimates of state values or of action values, that is, V or Q, specify what is good or\r
bad for the agent over the long run. They are predictions of the total reward an agent can\r
expect to accumulate over the future. Agents make good decisions by selecting actions\r
leading to states with the largest estimated state values, or by selecting actions with the\r
largest estimated action values.\r
Prediction errors measure discrepancies between expected and actual signals or sensa\u0002tions. Reward prediction errors (RPEs) specifically measure discrepancies between the\r
expected and the received reward signal, being positive when the reward signal is greater\r
than expected, and negative otherwise. TD errors like (6.5) are special kinds of RPEs\r
that signal discrepancies between current and earlier expectations of reward over the\r
long-term. When neuroscientists refer to RPEs they generally (though not always) mean\r
TD RPEs, which we simply call TD errors throughout this chapter. Also in this chapter,\r
a TD error is generally one that does not depend on actions, as opposed to TD errors\r
used in learning action-values by algorithms like Sarsa and Q-learning. This is because\r
the most well-known links to neuroscience are stated in terms of action-free TD errors,\r
but we do not mean to rule out possible similar links involving action-dependent TD\r
errors. (TD errors for predicting signals other than rewards are useful too, but that case\r
will not concern us here. See, for example, Modayil, White, and Sutton, 2014.)\r
One can ask many questions about links between neuroscience data and these theoretically\u0002defined signals. Is an observed signal more like a reward signal, a value signal, a prediction\r
error, a reinforcement signal, or something altogether di↵erent? And if it is an error\r
signal, is it an RPE, a TD error, or a simpler error like the Rescorla–Wagner error (14.3)?\r
And if it is a TD error, does it depend on actions like the TD error of Q-learning or\r
Sarsa? As indicated above, probing the brain to answer questions like these is extremely\r
dicult. But experimental evidence suggests that one neurotransmitter, specifically\r
the neurotransmitter dopamine, signals RPEs, and further, that the phasic activity of\r
dopamine-producing neurons in fact conveys TD errors (see Section 15.1 for a definition of\r
phasic activity). This evidence led to the reward prediction error hypothesis of dopamine\r
neuron activity, which we describe next."""

[[sections]]
number = "15.3"
title = "The Reward Prediction Error Hypothesis"
text = """
The reward prediction error hypothesis of dopamine neuron activity proposes that one\r
of the functions of the phasic activity of dopamine-producing neurons in mammals is to\r
deliver an error between an old and a new estimate of expected future reward to target\r
areas throughout the brain. This hypothesis (though not in these exact words) was\r
first explicitly stated by Montague, Dayan, and Sejnowski (1996), who showed how the\r
TD error concept from reinforcement learning accounts for many features of the phasic\r
is not available until time t + 1. The TD error available at t is actually t1 = Rt + V (St)  V (St1).\r
Because we are thinking of time steps as very small, or even infinitesimal, time intervals, one should not\r
attribute undue importance to this one-step time shift."""

[[sections]]
number = "382"
title = "Chapter 15: Neuroscience"
text = """
activity of dopamine neurons in mammals. The experiments that led to this hypothesis\r
were performed in the 1980s and early 1990s in the laboratory of neuroscientist Wolfram\r
Schultz. Section 15.5 describes these influential experiments, Section 15.6 explains how the\r
results of these experiments align with TD errors, and the Bibliographical and Historical\r
Remarks section at the end of this chapter includes a guide to the literature surrounding\r
the development of this influential hypothesis.\r
Montague et al. (1996) compared the TD errors of the TD model of classical conditioning\r
with the phasic activity of dopamine-producing neurons during classical conditioning\r
experiments. Recall from Section 14.2 that the TD model of classical conditioning is\r
basically the semi-gradient-descent TD() algorithm with linear function approximation.\r
Montague et al. made several assumptions to set up this comparison. First, because a\r
TD error can be negative but neurons cannot have a negative firing rate, they assumed\r
that the quantity corresponding to dopamine neuron activity is t1 + bt, where bt is the\r
background firing rate of the neuron. A negative TD error corresponds to a drop in a\r
dopamine neuron’s firing rate below its background rate.2\r
A second assumption was needed about the states visited in each classical conditioning\r
trial and how they are represented as inputs to the learning algorithm. This is the same\r
issue we discussed in Section 14.2.4 for the TD model. Montague et al. chose a complete\r
serial compound (CSC) representation as shown in the left column of Figure 14.1, but\r
where the sequence of short-duration internal signals continues until the onset of the US,\r
which here is the arrival of a non-zero reward signal. This representation allows the TD\r
error to mimic the fact that dopamine neuron activity not only predicts a future reward,\r
but that it is also sensitive to when after a predictive cue that reward is expected to\r
arrive. There has to be some way to keep track of the time between sensory cues and\r
the arrival of reward. If a stimulus initiates a sequence of internal signals that continues\r
after the stimulus ends, and if there is a di↵erent signal for each time step following the\r
stimulus, then each time step after the stimulus is represented by a distinct state. Thus,\r
the TD error, being state-dependent, can be sensitive to the timing of events within a\r
trial.\r
In simulated trials with these assumptions about background firing rate and input\r
representation, TD errors of the TD model are remarkably similar to dopamine neuron\r
phasic activity. Previewing our description of details about these similarities in Section 15.5\r
below, the TD errors parallel the following features of dopamine neuron activity: (1) the\r
phasic response of a dopamine neuron only occurs when a rewarding event is unpredicted;\r
(2) early in learning, neutral cues that precede a reward do not cause substantial phasic\r
dopamine responses, but with continued learning these cues gain predictive value and\r
come to elicit phasic dopamine responses; (3) if an even earlier cue reliably precedes a\r
cue that has already acquired predictive value, the phasic dopamine response shifts to\r
the earlier cue, ceasing for the later cue; and (4) if after learning, the predicted rewarding\r
event is omitted, a dopamine neuron’s response decreases below its baseline level shortly\r
after the expected time of the rewarding event.\r
2In the literature relating TD errors to the activity of dopamine neurons, their t is the same as our\r
t1 = Rt + V (St)  V (St1)."""

[[sections]]
number = "15.4"
title = "Dopamine 383"
text = """
Although not every dopamine neuron monitored in the experiments of Schultz and\r
colleagues behaved in all of these ways, the striking correspondence between the ac\u0002tivities of most of the monitored neurons and TD errors lends strong support to the\r
reward prediction error hypothesis. There are situations, however, in which predictions\r
based on the hypothesis do not match what is observed in experiments. The choice\r
of input representation is critical to how closely TD errors match some of the details\r
of dopamine neuron activity, particularly details about the timing of dopamine neuron\r
responses. Di↵erent ideas, some of which we discuss below, have been proposed about\r
input representations and other features of TD learning to make the TD errors fit the data\r
better, though the main parallels appear with the CSC representation that Montague et\r
al. used. Overall, the reward prediction error hypothesis has received wide acceptance\r
among neuroscientists studying reward-based learning, and it has proven to be remarkably\r
resilient in the face of accumulating results from neuroscience experiments.\r
To prepare for our description of the neuroscience experiments supporting the reward\r
prediction error hypothesis, and to provide some context so that the significance of the\r
hypothesis can be appreciated, we next present some of what is known about dopamine,\r
the brain structures it influences, and how it is involved in reward-based learning."""

[[sections]]
number = "15.4"
title = "Dopamine"
text = """
Dopamine is produced as a neurotransmitter by neurons whose cell bodies lie mainly\r
in two clusters of neurons in the midbrain of mammals: the substantia nigra pars\r
compacta (SNpc) and the ventral tegmental area (VTA). Dopamine plays essential roles\r
in many processes in the mammalian brain. Prominent among these are motivation,\r
learning, action-selection, most forms of addiction, and the disorders schizophrenia and\r
Parkinson’s disease. Dopamine is called a neuromodulator because it performs many\r
functions other than direct fast excitation or inhibition of targeted neurons. Although\r
much remains unknown about dopamine’s functions and details of its cellular e↵ects, it is\r
clear that it is fundamental to reward processing in the mammalian brain. Dopamine\r
is not the only neuromodulator involved in reward processing, and its role in aversive\r
situations—punishment—remains controversial. Dopamine also can function di↵erently in\r
non-mammals. But no one doubts that dopamine is essential for reward-related processes\r
in mammals, including humans.\r
An early, traditional view is that dopamine neurons broadcast a reward signal to\r
multiple brain regions implicated in learning and motivation. This view followed from a\r
famous 1954 paper by James Olds and Peter Milner that described the e↵ects of electrical\r
stimulation on certain areas of a rat’s brain. They found that electrical stimulation to\r
particular regions acted as a very powerful reward in controlling the rat’s behavior: “...the\r
control exercised over the animal’s behavior by means of this reward is extreme, possibly\r
exceeding that exercised by any other reward previously used in animal experimentation”\r
(Olds and Milner, 1954). Later research revealed that the sites at which stimulation\r
was most e↵ective in producing this rewarding e↵ect excited dopamine pathways, either\r
directly or indirectly, that ordinarily are excited by natural rewarding stimuli. E↵ects\r
similar to these were also observed with human subjects. These observations strongly\r
suggested that dopamine neuron activity signals reward."""

[[sections]]
number = "384"
title = "Chapter 15: Neuroscience"
text = """
But if the reward prediction error hypothesis is correct—even if it accounts for only\r
some features of a dopamine neuron’s activity—this traditional view of dopamine neuron\r
activity is not entirely correct: phasic responses of dopamine neurons signal reward\r
prediction errors, not reward itself. In reinforcement learning’s terms, a dopamine\r
neuron’s phasic response at a time t corresponds to t1 = Rt + V (St)  V (St1), not\r
to Rt.\r
Reinforcement learning theory and algorithms help reconcile the reward-prediction\u0002error view with the conventional notion that dopamine signals reward. In many of the\r
algorithms we discuss in this book,  functions as a reinforcement signal, meaning that it\r
is the main driver of learning. For example,  is the critical factor in the TD model of\r
classical conditioning, and  is the reinforcement signal for learning both a value function\r
and a policy in an actor–critic architecture (Sections 13.5 and 15.7). Action-dependent\r
forms of  are reinforcement signals for Q-learning and Sarsa. The reward signal Rt is\r
a crucial component of t1, but it is not the complete determinant of its reinforcing\r
e↵ect in these algorithms. The additional term V (St)  V (St1) is the higher-order\r
reinforcement part of t1, and even if reward occurs (Rt 6= 0), the TD error can be silent\r
if the reward is fully predicted (which is fully explained in Section 15.6 below).\r
A closer look at Olds’ and Milner’s 1954 paper, in fact, reveals that it is mainly\r
about the reinforcing e↵ect of electrical stimulation in an instrumental conditioning task.\r
Electrical stimulation not only energized the rats’ behavior—through dopamine’s e↵ect on\r
motivation—it also led to the rats quickly learning to stimulate themselves by pressing a\r
lever, which they would do frequently for long periods of time. The activity of dopamine\r
neurons triggered by electrical stimulation reinforced the rats’ lever pressing.\r
More recent experiments using optogenetic methods clinch the role of phasic responses\r
of dopamine neurons as reinforcement signals. These methods allow neuroscientists to\r
precisely control the activity of selected neuron types at a millisecond timescale in awake\r
behaving animals. Optogenetic methods introduce light-sensitive proteins into selected\r
neuron types so that these neurons can be activated or silenced by means of flashes of\r
laser light. The first experiment using optogenetic methods to study dopamine neurons\r
showed that optogenetic stimulation producing phasic activation of dopamine neurons\r
in mice was enough to condition the mice to prefer the side of a chamber where they\r
received this stimulation as compared to the chamber’s other side where they received\r
no, or lower-frequency, stimulation (Tsai et al. 2009). In another example, Steinberg\r
et al. (2013) used optogenetic activation of dopamine neurons to create artificial bursts\r
of dopamine neuron activity in rats at the times when rewarding stimuli were expected\r
but omitted—times when dopamine neuron activity normally pauses. With these pauses\r
replaced by artificial bursts, responding was sustained when it would ordinarily decrease\r
due to lack of reinforcement (in extinction trials), and learning was enabled when it would\r
ordinarily be blocked due to the reward being already predicted (the blocking paradigm;\r
Section 14.2.1).\r
Additional evidence for the reinforcing function of dopamine comes from optogenetic\r
experiments with fruit flies, except in these animals dopamine’s e↵ect is the opposite of\r
its e↵ect in mammals: optically triggered bursts of dopamine neuron activity act just\r
like electric foot shock in reinforcing avoidance behavior, at least for the population"""

[[sections]]
number = "15.4"
title = "Dopamine 385"
text = """
of dopamine neurons activated (Claridge-Chang et al. 2009). Although none of these\r
optogenetic experiments showed that phasic dopamine neuron activity is specifically\r
like a TD error, they convincingly demonstrated that phasic dopamine neuron activity\r
acts just like  acts (or perhaps like minus  acts in fruit flies) as the reinforcement\r
signal in algorithms for both prediction (classical conditioning) and control (instrumental\r
conditioning).\r
Axonal arbor of a single neuron producing\r
dopamine as a neurotransmitter. These\r
axons make synaptic contacts with a huge\r
number of dendrites of neurons in targeted\r
brain areas.\r
Adapted from The Journal of Neuroscience,\r
Matsuda, Furuta, Nakamura, Hioki, Fujiyama,\r
Arai, and Kaneko, volume 29, 2009, page 451.\r
Dopamine neurons are particularly well suited\r
to broadcasting a reinforcement signal to many\r
areas of the brain. These neurons have huge\r
axonal arbors, each releasing dopamine at 100 to\r
1,000 times more synaptic sites than reached by\r
the axons of typical neurons. Shown to the right\r
is the axonal arbor of a single dopamine neuron\r
whose cell body is in the SNpc of a rat’s brain.\r
Each axon of a SNpc or VTA dopamine neuron\r
makes roughly 500,000 synaptic contacts on the\r
dendrites of neurons in targeted brain areas.\r
If dopamine neurons broadcast a reinforce\u0002ment signal like reinforcement learning’s , then\r
because this is a scalar signal, i.e., a single num\u0002ber, all dopamine neurons in both the SNpc\r
and VTA would be expected to activate more\u0002or-less identically so that they would act in near\r
synchrony to send the same signal to all of the\r
sites their axons target. Although it has been\r
a common belief that dopamine neurons do act\r
together like this, modern evidence is pointing\r
to the more complicated picture that di↵erent\r
subpopulations of dopamine neurons respond to\r
input di↵erently depending on the structures to\r
which they send their signals and the di↵erent\r
ways these signals act on their target structures. Dopamine has functions other than\r
signaling RPEs, and even for dopamine neurons that do signal RPEs, it can make sense\r
to send di↵erent RPEs to di↵erent structures depending on the roles these structures\r
play in producing reinforced behavior. This is beyond what we treat in any detail in this\r
book, but vector-valued RPE signals make sense from the perspective of reinforcement\r
learning when decisions can be decomposed into separate sub-decisions, or more generally,\r
as a way to address the structural version of the credit assignment problem: How do\r
you distribute credit for success (or blame for failure) of a decision among the many\r
component structures that could have been involved in producing it? We say a bit more\r
about this in Section 15.10 below.\r
The axons of most dopamine neurons make synaptic contact with neurons in the frontal\r
cortex and the basal ganglia, areas of the brain involved in voluntary movement, decision\r
making, learning, and cognitive functions such as planning. Because most ideas relating"""

[[sections]]
number = "386"
title = "Chapter 15: Neuroscience"
text = """
dopamine to reinforcement learning focus on the basal ganglia, and the connections from\r
dopamine neurons are particularly dense there, we focus on the basal ganglia here. The\r
basal ganglia are a collection of neuron groups, or nuclei, lying at the base of the forebrain.\r
The main input structure of the basal ganglia is called the striatum. Essentially all of the\r
cerebral cortex, among other structures, provides input to the striatum. The activity of\r
cortical neurons conveys a wealth of information about sensory input, internal states, and\r
motor activity. The axons of cortical neurons make synaptic contacts on the dendrites of\r
the main input/output neurons of the striatum, called medium spiny neurons. Output\r
from the striatum loops back via other basal ganglia nuclei and the thalamus to frontal\r
areas of cortex, and to motor areas, making it possible for the striatum to influence\r
movement, abstract decision processes, and reward processing. Two main subdivisions\r
of the striatum are important for reinforcement learning: the dorsal striatum, primarily\r
implicated in influencing action selection, and the ventral striatum, thought to be critical\r
for di↵erent aspects of reward processing, including the assignment of a↵ective value to\r
sensations.\r
The dendrites of medium spiny neurons are covered with spines on whose tips the\r
axons of neurons in the cortex make synaptic contact. Also making synaptic contact with\r
these spines—in this case contacting the spine stems—are axons of dopamine neurons\r
(Figure 15.1). This arrangement brings together presynaptic activity of cortical neurons,\r
Figure 15.1: Spine of a striatal neuron showing input from both cortical and dopamine neurons.\r
Axons of cortical neurons influence striatal neurons via corticostriatal synapses releasing the\r
neurotransmitter glutamate at the tips of spines covering the dendrites of striatal neurons.\r
An axon of a VTA or SNpc dopamine neuron is shown passing by the spine (from the lower\r
right). “Dopamine varicosities” on this axon release dopamine at or near the spine stem, in an\r
arrangement that brings together presynaptic input from cortex, postsynaptic activity of the\r
striatal neuron, and dopamine, making it possible that several types of learning rules govern the\r
plasticity of corticostriatal synapses. Each axon of a dopamine neuron makes synaptic contact\r
with the stems of roughly 500,000 spines. Some of the complexity omitted from our discussion\r
is shown here by other neurotransmitter pathways and multiple receptor types, such as D1 an\r
D2 dopamine receptors by which dopamine can produce di↵erent e↵ects at spines and other\r
postsynaptic sites. From Journal of Neurophysiology, W. Schultz, vol. 80, 1998, page 10."""

[[sections]]
number = "15.5"
title = "Experimental Support for the Reward Prediction Error Hypothesis 387"
text = """
postsynaptic activity of medium spiny neurons, and input from dopamine neurons. What\r
actually occurs at these spines is complex and not completely understood. Figure 15.1\r
hints at the complexity by showing two types of receptors for dopamine, receptors for\r
glutamate—the neurotransmitter of the cortical inputs—and multiple ways that the\r
various signals can interact. But evidence is mounting that changes in the ecacies of\r
the synapses on the pathway from the cortex to the striatum, which neuroscientists call\r
corticostriatal synapses, depend critically on appropriately-timed dopamine signals."""

[[sections]]
number = "15.5"
title = "Experimental Support"
text = """
for the Reward Prediction Error Hypothesis\r
Dopamine neurons respond with bursts of activity to intense, novel, or unexpected visual\r
and auditory stimuli that trigger eye and body movements, but very little of their activity is\r
related to the movements themselves. This is surprising because degeneration of dopamine\r
neurons is a cause of Parkinson’s disease, whose symptoms include motor disorders,\r
particularly deficits in self-initiated movement. Motivated by the weak relationship\r
between dopamine neuron activity and stimulus-triggered eye and body movements,\r
Romo and Schultz (1990) and Schultz and Romo (1990) took the first steps toward the\r
reward prediction error hypothesis by recording the activity of dopamine neurons and\r
muscle activity while monkeys moved their arms.\r
They trained two monkeys to reach from a resting hand position into a bin containing\r
a bit of apple, a piece of cookie, or a raisin, when the monkey saw and heard the bin’s\r
door open. The monkey could then grab and bring the food to its mouth. After a monkey\r
became good at this, it was trained on two additional tasks. The purpose of the first\r
task was to see what dopamine neurons do when movements are self-initiated. The bin\r
was left open but covered from above so that the monkey could not see inside but could\r
reach in from below. No triggering stimuli were presented, and after the monkey reached\r
for and ate the food morsel, the experimenter usually (though not always), silently and\r
unseen by the monkey, replaced food in the bin by sticking it onto a rigid wire. Here too,\r
the activity of the dopamine neurons Romo and Schultz monitored was not related to the\r
monkey’s movements, but a large percentage of these neurons produced phasic responses\r
whenever the monkey first touched a food morsel. These neurons did not respond when\r
the monkey touched just the wire or explored the bin when no food was there. This was\r
good evidence that the neurons were responding to the food and not to other aspects of\r
the task.\r
The purpose of Romo and Schultz’s second task was to see what happens when\r
movements are triggered by stimuli. This task used a di↵erent bin with a movable cover.\r
The sight and sound of the bin opening triggered reaching movements to the bin. In this\r
case, Romo and Schultz found that after some period of training, the dopamine neurons\r
no longer responded to the touch of the food but instead responded to the sight and sound\r
of the opening cover of the food bin. The phasic responses of these neurons had shifted\r
from the reward itself to stimuli predicting the availability of the reward. In a followup\r
study, Romo and Schultz found that most of the dopamine neurons whose activity they"""

[[sections]]
number = "388"
title = "Chapter 15: Neuroscience"
text = """
monitored did not respond to the sight and sound of the bin opening outside the context\r
of the behavioral task. These observations suggested that the dopamine neurons were\r
responding neither to the initiation of a movement nor to the sensory properties of the\r
stimuli, but were rather signaling an expectation of reward.\r
Schultz’s group conducted many additional studies involving both SNpc and VTA\r
dopamine neurons. A particular series of experiments was influential in suggesting that\r
the phasic responses of dopamine neurons correspond to TD errors and not to simpler\r
errors like those in the Rescorla–Wagner model (14.3). In the first of these experiments\r
(Ljungberg, Apicella, and Schultz, 1992), monkeys were trained to depress a lever after\r
a light was illuminated as a ‘trigger cue’ to obtain a drop of apple juice. As Romo\r
and Schultz had observed earlier, many dopamine neurons initially responded to the\r
reward—the drop of juice (Figure 15.2, top panel). But many of these neurons lost that\r
reward response as training continued and developed responses instead to the illumination\r
of the light that predicted the reward (Figure 15.2, middle panel). With continued\r
training, lever pressing became faster while the number of dopamine neurons responding\r
to the trigger cue decreased.\r
Following this study, the same monkeys were trained on a new task (Schultz, Apicella,\r
and Ljungberg, 1993). Here the monkeys faced two levers, each with a light above it.\r
Illuminating one of these lights was an ‘instruction cue’ indicating which of the two levers\r
Figure 15.2: The response of dopamine neurons shifts from initial responses to primary reward\r
to earlier predictive stimuli. These are plots of the number of action potentials produced\r
by monitored dopamine neurons within small time intervals, averaged over all the monitored\r
dopamine neurons (ranging from 23 to 44 neurons for these data). Top: dopamine neurons are\r
activated by the unpredicted delivery of drop of apple juice. Middle: with learning, dopamine\r
neurons developed responses to the reward-predicting trigger cue and lost responsiveness to the\r
delivery of reward. Bottom: with the addition of an instruction cue preceding the trigger cue by\r
1 second, dopamine neurons shifted their responses from the trigger cue to the earlier instruction\r
cue. From Schultz et al. (1995), MIT Press."""

[[sections]]
number = "15.5"
title = "Experimental Support for the Reward Prediction Error Hypothesis 389"
text = """
Figure 15.3: The response of dopamine neurons drops\r
below baseline shortly after the time when an expected\r
reward fails to occur. Top: dopamine neurons are\r
activated by the unpredicted delivery of a drop of\r
apple juice. Middle: dopamine neurons respond to a\r
conditioned stimulus (CS) that predicts reward and\r
do not respond to the reward itself. Bottom: when\r
the reward predicted by the CS fails to occur, the\r
activity of dopamine neurons drops below baseline\r
shortly after the time the reward is expected to occur.\r
At the top of each of these panels is shown the average\r
number of action potentials produced by monitored\r
dopamine neurons within small time intervals around\r
the indicated times. The raster plots below show the\r
activity patterns of the individual dopamine neurons\r
that were monitored; each dot represents an action\r
potential. From Schultz, Dayan, and Montague, A\r
Neural Substrate of Prediction and Reward, Science,\r
vol. 275, issue 5306, pages 1593-1598, March 14, 1997.\r
Reprinted with permission from AAAS.\r
would produce a drop of apple juice.\r
In this task, the instruction cue pre\u0002ceded the trigger cue of the previ\u0002ous task by a fixed interval of 1 sec\u0002ond. The monkeys learned to with\u0002hold reaching until seeing the trig\u0002ger cue, and dopamine neuron activ\u0002ity increased, but now the responses\r
of the monitored dopamine neurons\r
occurred almost exclusively to the\r
earlier instruction cue and not to\r
the trigger cue (Figure 15.2, bot\u0002tom panel). Here again the num\u0002ber of dopamine neurons responding\r
to the instruction cue was much re\u0002duced when the task was well learned.\r
During learning across these tasks,\r
dopamine neuron activity shifted\r
from initially responding to the re\u0002ward to responding to the earlier\r
predictive stimuli, first progressing\r
to the trigger stimulus then to the\r
still earlier instruction cue. As re\u0002sponding moved earlier in time it\r
disappeared from the later stimuli.\r
This shifting of responses to earlier\r
reward predictors, while losing re\u0002sponses to later predictors is a hall\u0002mark of TD learning (see, for exam\u0002ple, Figure 14.2).\r
The task just described revealed\r
another property of dopamine neu\u0002ron activity shared with TD learn\u0002ing. The monkeys sometimes pressed\r
the wrong key, that is, the key other\r
than the instructed one, and conse\u0002quently received no reward. In these\r
trials, many of the dopamine neu\u0002rons showed a sharp decrease in their\r
firing rates below baseline shortly af\u0002ter the reward’s usual time of deliv\u0002ery, and this happened without the\r
availability of any external cue to\r
mark the usual time of reward de\u0002livery (Figure 15.3). Somehow the"""

[[sections]]
number = "390"
title = "Chapter 15: Neuroscience"
text = """
monkeys were internally keeping track of the timing of the reward. (Response timing is\r
one area where the simplest version of TD learning needs to be modified to account for\r
some of the details of the timing of dopamine neuron responses. We consider this issue in\r
the following section.)\r
The observations from the studies described above led Schultz and his group to conclude\r
that dopamine neurons respond to unpredicted rewards, to the earliest predictors of\r
reward, and that dopamine neuron activity decreases below baseline if a reward, or a\r
predictor of reward, does not occur at its expected time. Researchers familiar with\r
reinforcement learning were quick to recognize that these results are strikingly similar\r
to how the TD error behaves as the reinforcement signal in a TD algorithm. The next\r
section explores this similarity by working through a specific example in detail."""

[[sections]]
number = "15.6"
title = "TD Error/Dopamine Correspondence"
text = """
This section explains the correspondence between the TD error  and the phasic responses\r
of dopamine neurons observed in the experiments just described. We examine how \r
changes over the course of learning in a task something like the one described above\r
where a monkey first sees an instruction cue and then a fixed time later has to respond\r
correctly to a trigger cue in order to obtain reward. We use a simple idealized version of\r
this task, but we go into a lot more detail than is usual because we want to emphasize\r
the theoretical basis of the parallel between TD errors and dopamine neuron activity.\r
The first simplifying assumption is that the agent has already learned the actions\r
required to obtain reward. Then its task is just to learn accurate predictions of future\r
reward for the sequence of states it experiences. This is then a prediction task, or\r
more technically, a policy-evaluation task: learning the value function for a fixed policy\r
(Sections 4.1 and 6.1). The value function to be learned assigns to each state a value that\r
predicts the return that will follow that state if the agent selects actions according to the\r
given policy, where the return is the (possibly discounted) sum of all the future rewards.\r
This is unrealistic as a model of the monkey’s situation because the monkey would likely\r
learn these predictions at the same time that it is learning to act correctly (as would a\r
reinforcement learning algorithm that learns policies as well as value functions, such as\r
an actor–critic algorithm), but this scenario is simpler to describe than one in which a\r
policy and a value function are learned simultaneously.\r
Now imagine that the agent’s experience divides into multiple trials, in each of which\r
the same sequence of states repeats, with a distinct state occurring on each time step\r
during the trial. Further imagine that the return being predicted is limited to the return\r
over a trial, which makes a trial analogous to a reinforcement learning episode as we have\r
defined it. In reality, of course, the returns being predicted are not confined to single\r
trials, and the time interval between trials is an important factor in determining what an\r
animal learns. This is true for TD learning as well, but here we assume that returns do\r
not accumulate over multiple trials. Given this, then, a trial in experiments like those\r
conducted by Schultz and colleagues is equivalent to an episode of reinforcement learning.\r
(Though in this discussion, we will use the term trial instead of episode to relate better\r
to the experiments.)"""

[[sections]]
number = "15.6"
title = "TD Error/Dopamine Correspondence 391"
text = """
As usual, we also need to make an assumption about how states are represented as\r
inputs to the learning algorithm, an assumption that influences how closely the TD error\r
corresponds to dopamine neuron activity. We discuss this issue later, but for now we\r
assume the same CSC representation used by Montague et al. (1996) in which there is a\r
separate internal stimulus for each state visited at each time step in a trial. This reduces\r
the process to the tabular case covered in the first part of this book. Finally, we assume\r
that the agent uses TD(0) to learn a value function, V , stored in a lookup table initialized\r
to be zero for all the states. We also assume that this is a deterministic task and that\r
the discount factor, , is very nearly one so that we can ignore it.\r
Figure 15.4 shows the time courses of R, V , and  at several stages of learning in this\r
policy-evaluation task. The time axes represent the time interval over which a sequence\r
of states is visited in a trial (where for clarity we omit showing individual states). The\r
reward signal is zero throughout each trial except when the agent reaches the rewarding\r
state, shown near the right end of the time line, when the reward signal becomes some\r
positive number, say R?. The goal of TD learning is to predict the return for each\r
state visited in a trial, which in this undiscounted case and given our assumption that\r
predictions are confined to individual trials, is simply R? for each state.\r
R\r
RR\r
R\r
Rt\r
R\r
t1 = Rt + Vt  Vt1\r
R\r
V\r
V\r
\r
\r
\r
early in\r
learning\r
learning\r
complete\r
R omitted\r
regular predictors of over this interval R\r
R?\r
Figure 15.4: The behavior of the TD error  during TD learning is consistent with features of\r
the phasic activation of dopamine neurons. (Here  is the TD error available at time t, i.e., t1).\r
Top: a sequence of states, shown as an interval of regular predictors, is followed by a non-zero\r
reward R?. Early in learning: the initial value function, V , and initial , which at first is equal\r
to R?. Learning complete: the value function accurately predicts future reward,  is positive at\r
the earliest predictive state, and  = 0 at the time of the non-zero reward. R? omitted: at the\r
time the predicted reward is omitted,  becomes negative. See text for a complete explanation\r
of why this happens."""

[[sections]]
number = "392"
title = "Chapter 15: Neuroscience"
text = """
Preceding the rewarding state is a sequence of reward-predicting states, with the\r
earliest reward-predicting state shown near the left end of the time line. This is like\r
the state near the start of a trial, for example like the state marked by the instruction\r
cue in a trial of the monkey experiment of Schultz et al. (1993) described above. It is\r
the first state in a trial that reliably predicts that trial’s reward. (Of course, in reality\r
states visited on preceding trials are even earlier reward-predicting states, but because\r
we are confining predictions to individual trials, these do not qualify as predictors of this\r
trial’s reward. Below we give a more satisfactory, though more abstract, description of an\r
earliest reward-predicting state.) The latest reward-predicting state in a trial is the state\r
immediately preceding the trial’s rewarding state. This is the state near the far right end\r
of the time line in Figure 15.4. Note that the rewarding state of a trial does not predict\r
the return for that trial: the value of this state would come to predict the return over all\r
the following trials, which here we are assuming to be zero in this episodic formulation.\r
Figure 15.4 shows the first-trial time courses of V and  as the graphs labeled ‘early in\r
learning.’ Because the reward signal is zero throughout the trial except when the rewarding\r
state is reached, and all the V -values are zero, the TD error is also zero until it becomes\r
R? at the rewarding state. This follows because t1 = Rt + Vt  Vt1 = Rt + 0  0 = Rt,\r
which is zero until it equals R? when the reward occurs. Here Vt and Vt1 are respectively\r
the estimated values of the states visited at times t and t  1 in a trial. The TD error at\r
this stage of learning is analogous to a dopamine neuron responding to an unpredicted\r
reward (e.g., a drop of apple juice) at the start of training.\r
Throughout this first trial and all successive trials, TD(0) updates occur at each\r
state transition as described in Chapter 6. This successively increases the values of the\r
reward-predicting states, with the increases spreading backwards from the rewarding\r
state, until the values converge to the correct return predictions. In this case (because\r
we are assuming no discounting) the correct predictions are equal to R? for all the\r
reward-predicting states. This can be seen in Figure 15.4 as the graph of V labeled\r
‘learning complete’ where the values of all the states from the earliest to the latest\r
reward-predicting states all equal R?. The values of the states preceding the earliest\r
reward-predicting state remain low (which Figure 15.4 shows as zero) because they are\r
not reliable predictors of reward.\r
When learning is complete, that is, when V attains its correct values, the TD errors\r
associated with transitions from any reward-predicting state are zero because the predic\u0002tions are now accurate. This is because for a transition from a reward-predicting state to\r
another reward-predicting state, we have t1 = Rt + Vt  Vt1 =0+ R?  R? = 0, and\r
for the transition from the latest reward-predicting state to the rewarding state, we have\r
t1 = Rt +Vt Vt1 = R? + 0R? = 0. On the other hand, the TD error on a transition\r
from any state to the earliest reward-predicting state is positive because of the mismatch\r
between this state’s low value and the larger value of the following reward-predicting\r
state. Indeed, if the value of a state preceding the earliest reward-predicting state were\r
zero, then after the transition to the earliest reward-predicting state, we would have\r
that t1 = Rt + Vt  Vt1 =0+ R?  0 = R?. The ‘learning complete’ graph of  in\r
Figure 15.4 shows this positive value at the earliest reward-predicting state, and zeros\r
everywhere else."""

[[sections]]
number = "15.6"
title = "TD Error/Dopamine Correspondence 393"
text = """
The positive TD error upon transitioning to the earliest reward-predicting state is\r
analogous to the persistence of dopamine responses to the earliest stimuli predicting\r
reward. By the same token, when learning is complete, a transition from the latest\r
reward-predicting state to the rewarding state produces a zero TD error because the\r
latest reward-predicting state’s value, being correct, cancels the reward. This parallels the\r
observation that fewer dopamine neurons generate a phasic response to a fully predicted\r
reward than to an unpredicted reward.\r
After learning, if the reward is suddenly omitted, the TD error goes negative at the\r
usual time of reward because the value of the latest reward-predicting state is then too\r
high: t1 = Rt + Vt  Vt1 =0+0  R? = R?, as shown at the right end of the ‘R\r
omitted’ graph of  in Figure 15.4. This is like dopamine neuron activity decreasing\r
below baseline at the time an expected reward is omitted as seen in the experiment of\r
Schultz et al. (1993) described above and shown in Figure 15.3.\r
The idea of an earliest reward-predicting state deserves more attention. In the scenario\r
described above, because experience is divided into trials, and we assumed that predictions\r
are confined to individual trials, the earliest reward-predicting state is always the first\r
state of a trial. Clearly this is artificial. A more general way to think of an earliest\r
reward-predicting state is that it is an unpredicted predictor of reward, and there can\r
be many such states. In an animal’s life, many di↵erent states may precede an earliest\r
reward-predicting state. However, because these states are more often followed by other\r
states that do not predict reward, their reward-predicting powers, that is, their values,\r
remain low. A TD algorithm, if operating throughout the animal’s life, would update the\r
values of these states too, but the updates would not consistently accumulate because, by\r
assumption, none of these states reliably precedes an earliest reward-predicting state. If\r
any of them did, they would be reward-predicting states as well. This might explain why\r
with overtraining, dopamine responses decrease to even the earliest reward-predicting\r
stimulus in a trial. With overtraining one would expect that even a formerly-unpredicted\r
predictor state would become predicted by stimuli associated with earlier states: the\r
animal’s interaction with its environment both inside and outside of an experimental\r
task would become commonplace. Upon breaking this routine with the introduction of a\r
new task, however, one would see TD errors reappear, as indeed is observed in dopamine\r
neuron activity.\r
The example described above explains why the TD error shares key features with\r
the phasic activity of dopamine neurons when the animal is learning in a task similar\r
to the idealized task of our example. But not every property of the phasic activity of\r
dopamine neurons coincides so neatly with properties of . One of the most troubling\r
discrepancies involves what happens when a reward occurs earlier than expected. We\r
have seen that the omission of an expected reward produces a negative prediction error\r
at the reward’s expected time, which corresponds to the activity of dopamine neurons\r
decreasing below baseline when this happens. If the reward arrives later than expected,\r
it is then an unexpected reward and generates a positive prediction error. This happens\r
with both TD errors and dopamine neuron responses. But when reward arrives earlier\r
than expected, dopamine neurons do not do what the TD error does—at least with the\r
CSC representation used by Montague et al. (1996) and by us in our example. Dopamine"""

[[sections]]
number = "394"
title = "Chapter 15: Neuroscience"
text = """
neurons do respond to the early reward, which is consistent with a positive TD error\r
because the reward is not predicted to occur then. However, at the later time when the\r
reward is expected but omitted, the TD error is negative whereas, in contrast to this\r
prediction, dopamine neuron activity does not drop below baseline in the way the TD\r
model predicts (Hollerman and Schultz, 1998). Something more complicated is going on\r
in the animal’s brain than simply TD learning with a CSC representation.\r
Some of the mismatches between the TD error and dopamine neuron activity can\r
be addressed by selecting suitable parameter values for the TD algorithm and by using\r
stimulus representations other than the CSC representation. For instance, to address\r
the early-reward mismatch just described, Suri and Schultz (1999) proposed a CSC\r
representation in which the sequences of internal signals initiated by earlier stimuli\r
are cancelled by the occurrence of a reward. Another proposal by Daw, Courville,\r
and Touretzky (2006) is that the brain’s TD system uses representations produced by\r
statistical modeling carried out in sensory cortex rather than simpler representations\r
based on raw sensory input. Ludvig, Sutton, and Kehoe (2008) found that TD learning\r
with a microstimulus (MS) representation (Figure 14.1) fits the activity of dopamine\r
neurons in the early-reward and other situations better than when a CSC representation\r
is used. Pan, Schmidt, Wickens, and Hyland (2005) found that even with the CSC\r
representation, prolonged eligibility traces improve the fit of the TD error to some aspects\r
of dopamine neuron activity. In general, many fine details of TD-error behavior depend\r
on subtle interactions between eligibility traces, discounting, and stimulus representations.\r
Findings like these elaborate and refine the reward prediction error hypothesis without\r
refuting its core claim that the phasic activity of dopamine neurons is well characterized\r
as signaling TD errors.\r
On the other hand, there are other discrepancies between the TD theory and exper\u0002imental data that are not so easily accommodated by selecting parameter values and\r
stimulus representations (we mention some of these discrepancies in the Bibliographical\r
and Historical Remarks section at the end of this chapter), and more mismatches are\r
likely to be discovered as neuroscientists conduct ever more refined experiments. But the\r
reward prediction error hypothesis has been functioning very e↵ectively as a catalyst for\r
improving our understanding of how the brain’s reward system works. Intricate experi\u0002ments have been designed to validate or refute predictions derived from the hypothesis,\r
and experimental results have, in turn, led to refinement and elaboration of the TD\r
error/dopamine hypothesis.\r
A remarkable aspect of these developments is that the reinforcement learning algo\u0002rithms and theory that connect so well with properties of the dopamine system were\r
developed from a computational perspective in total absence of any knowledge about the\r
relevant properties of dopamine neurons—remember, TD learning and its connections\r
to optimal control and dynamic programming were developed many years before any of\r
the experiments were conducted that revealed the TD-like nature of dopamine neuron\r
activity. This unplanned correspondence, despite not being perfect, suggests that the TD\r
error/dopamine parallel captures something significant about brain reward processes.\r
In addition to accounting for many features of the phasic activity of dopamine neurons,\r
the reward prediction error hypothesis links neuroscience to other aspects of reinforcement"""

[[sections]]
number = "15.7"
title = "Neural Actor–Critic 395"
text = """
learning, in particular, to learning algorithms that use TD errors as reinforcement signals.\r
Neuroscience is still far from reaching complete understanding of the circuits, molecular\r
mechanisms, and functions of the phasic activity of dopamine neurons, but evidence\r
supporting the reward prediction error hypothesis, along with evidence that phasic\r
dopamine responses are reinforcement signals for learning, suggest that the brain might\r
implement something like an actor–critic algorithm in which TD errors play critical roles.\r
Other reinforcement learning algorithms are plausible candidates too, but actor–critic\r
algorithms fit the anatomy and physiology of the mammalian brain particularly well, as\r
we describe in the following two sections."""

[[sections]]
number = "15.7"
title = "Neural Actor–Critic"
text = """
Actor–critic algorithms learn both policies and value functions. The ‘actor’ is the\r
component that learns policies, and the ‘critic’ is the component that learns about\r
whatever policy is currently being followed by the actor in order to ‘criticize’ the actor’s\r
action choices. The critic uses a TD algorithm to learn the state-value function for the\r
actor’s current policy. The value function allows the critic to critique the actor’s action\r
choices by sending TD errors, , to the actor. A positive  means that the action was\r
‘good’ because it led to a state with a better-than-expected value; a negative  means\r
that the action was ‘bad’ because it led to a state with a worse-than-expected value.\r
Based on these critiques, the actor continually updates its policy.\r
Two distinctive features of actor–critic algorithms are responsible for thinking that the\r
brain might implement an algorithm like this. First, the two components of an actor–critic\r
algorithm—the actor and the critic—suggest that two parts of the striatum—the dorsal\r
and ventral subdivisions (Section 15.4), both critical for reward-based learning—may\r
function respectively something like an actor and a critic. A second property of actor–\r
critic algorithms that suggests a brain implementation is that the TD error has the dual\r
role of being the reinforcement signal for both the actor and the critic, though it has a\r
di↵erent influence on learning in each of these components. This fits well with several\r
properties of the neural circuitry: axons of dopamine neurons target both the dorsal\r
and ventral subdivisions of the striatum; dopamine appears to be critical for modulating\r
synaptic plasticity in both structures; and how a neuromodulator such as dopamine\r
acts on a target structure depends on properties of the target structure and not just on\r
properties of the neuromodulator.\r
Section 13.5 presents actor–critic algorithms as policy gradient methods, but the actor–\r
critic algorithm of Barto, Sutton, and Anderson (1983) was simpler and was presented as\r
an artificial neural network (ANN). Here we describe an ANN implementation something\r
like that of Barto et al., and we follow Takahashi, Schoenbaum, and Niv (2008) in giving\r
a schematic proposal for how this ANN might be implemented by real neural networks in\r
the brain. We postpone discussion of the actor and critic learning rules until Section 15.8,\r
where we present them as special cases of the policy-gradient formulation and discuss\r
what they suggest about how dopamine might modulate synaptic plasticity."""

[[sections]]
number = "396"
title = "Chapter 15: Neuroscience"
text = """
Figure 15.5a shows an implementation of an actor–critic algorithm as an ANN with\r
component networks implementing the actor and the critic. The critic consists of a single\r
neuron-like unit, V , whose output activity represents state values, and a component\r
shown as the diamond labeled TD that computes TD errors by combining V ’s output\r
with reward signals and with previous state values (as suggested by the loop from the\r
TD diamond to itself). The actor network has a single layer of k actor units labeled Ai,\r
i = 1,...,k. The output of each actor unit is a component of a k-dimensional action\r
vector. An alternative is that there are k separate actions, one commanded by each actor\r
unit, that compete with one another to be executed, but here we will think of the entire\r
A-vector as an action.\r
Reward\r
Actor\r
. TD\r
. \r
.\r
. \r
. \r
.\r
Critic\r
. \r
. \r
.\r
δ\r
Actions\r
States/Stimuli\r
TD error\r
Environment\r
Actor\r
VTA\r
SNc\r
S2\r
S1\r
Sn\r
. \r
. \r
.\r
. \r
. \r
.\r
Critic\r
. \r
. \r
.\r
S1\r
Sn\r
S2\r
States/\r
Actions\r
Stimuli\r
Dopamine\r
Ventral\r
striatum Dorsal striatum\r
Cortex (multiple areas)\r
Environment\r
Reward\r
1\r
2\r
n\r
1\r
2\r
n\r
A1\r
A2\r
A3\r
Ak\r
V\r
\r
(a) (b)\r
x1\r
x2\r
xn\r
x1\r
x2\r
xn\r
Figure 15.5: Actor–critic ANN and a hypothetical neural implementation. a) Actor–critic\r
algorithm as an ANN. The actor adjusts a policy based on the TD error  it receives from the\r
critic; the critic adjusts state-value parameters using the same . The critic produces a TD error\r
from the reward signal, R, and the current change in its estimate of state values. The actor does\r
not have direct access to the reward signal, and the critic does not have direct access to the\r
action. b) Hypothetical neural implementation of an actor–critic algorithm. The actor and the\r
value-learning part of the critic are respectively placed in the dorsal and ventral subdivisions\r
of the striatum. The TD error is transmitted by dopamine neurons located in the VTA and\r
SNpc to modulate changes in synaptic ecacies of input from cortical areas to the ventral and\r
dorsal striatum. Adapted from Frontiers in Neuroscience, vol. 2(1), 2008, Y. Takahashi, G.\r
Schoenbaum, and Y. Niv, Silencing the critics: Understanding the e↵ects of cocaine sensitization\r
on dorsolateral and ventral striatum in the context of an Actor/Critic model."""

[[sections]]
number = "15.7"
title = "Neural Actor–Critic 397"
text = """
Both the critic and actor networks receive input consisting of multiple features repre\u0002senting the state of the agent’s environment. (Recall from Chapter 1 that the environment\r
of a reinforcement learning agent includes components both inside and outside of the\r
‘organism’ containing the agent.) The figure shows these features as the circles labeled\r
x1, x2,...,xn, shown twice just to keep the figure simple. A weight representing the\r
ecacy of a synapse is associated with each connection from each feature xi to the\r
critic unit, V , and to each of the action units, Ai. The weights in the critic network\r
parameterize the value function, and the weights in the actor network parameterize the\r
policy. The networks learn as these weights change according to the critic and actor\r
learning rules that we describe in the following section.\r
The TD error produced by circuitry in the critic is the reinforcement signal for changing\r
the weights in both the critic and the actor networks. This is shown in Figure 15.5a by\r
the line labeled ‘TD error ’ extending across all of the connections in the critic and\r
actor networks. This aspect of the network implementation, together with the reward\r
prediction error hypothesis and the fact that the activity of dopamine neurons is so\r
widely distributed by the extensive axonal arbors of these neurons, suggests that an\r
actor–critic network something like this may not be too farfetched as a hypothesis about\r
how reward-related learning might happen in the brain.\r
Figure 15.5b suggests—very schematically—how the ANN on the figure’s left might\r
map onto structures in the brain according to the hypothesis of Takahashi et al. (2008).\r
The hypothesis puts the actor and the value-learning part of the critic respectively in the\r
dorsal and ventral subdivisions of the striatum, the input structure of the basal ganglia.\r
Recall from Section 15.4 that the dorsal striatum is primarily implicated in influencing\r
action selection, and the ventral striatum is thought to be critical for di↵erent aspects of\r
reward processing, including the assignment of a↵ective value to sensations. The cerebral\r
cortex, along with other structures, sends input to the striatum conveying information\r
about stimuli, internal states, and motor activity.\r
In this hypothetical actor–critic brain implementation, the ventral striatum sends value\r
information to the VTA and SNpc, where dopamine neurons in these nuclei combine it\r
with information about reward to generate activity corresponding to TD errors (though\r
exactly how dopaminergic neurons calculate these errors is not yet understood). The ‘TD\r
error ’ line in Figure 15.5a becomes the line labeled ‘Dopamine’ in Figure 15.5b, which\r
represents the widely branching axons of dopamine neurons whose cell bodies are in the\r
VTA and SNpc. Referring back to Figure 15.1, these axons make synaptic contact with\r
the spines on the dendrites of medium spiny neurons, the main input/output neurons of\r
both the dorsal and ventral divisions of the striatum. Axons of the cortical neurons that\r
send input to the striatum make synaptic contact on the tips of these spines. According\r
to the hypothesis, it is at these spines where changes in the ecacies of the synapses\r
from cortical regions to the striatum are governed by learning rules that critically depend\r
on a reinforcement signal supplied by dopamine.\r
An important implication of the hypothesis illustrated in Figure 15.5b is that the\r
dopamine signal is not the ‘master’ reward signal like the scalar Rt of reinforcement\r
learning. In fact, the hypothesis implies that one should not necessarily be able to\r
probe the brain and record any signal like Rt in the activity of any single neuron."""

[[sections]]
number = "398"
title = "Chapter 15: Neuroscience"
text = """
Many interconnected neural systems generate reward-related information, with di↵erent\r
structures being recruited depending on di↵erent types of rewards. Dopamine neurons\r
receive information from many di↵erent brain areas, so the input to the SNpc and\r
VTA labeled ‘Reward’ in Figure 15.5b should be thought of as vector of reward-related\r
information arriving to neurons in these nuclei along multiple input channels. What the\r
theoretical scalar reward signal Rt might correspond to, then, is the net contribution of\r
all reward-related information to dopamine neuron activity. It is the result of a pattern\r
of activity across many neurons in di↵erent areas of the brain.\r
Although the actor–critic neural implementation illustrated in Figure 15.5b may be\r
correct on some counts, it clearly needs to be refined, extended, and modified to qualify\r
as a full-fledged model of the function of the phasic activity of dopamine neurons. The\r
Historical and Bibliographic Remarks section at the end of this chapter cites publications\r
that discuss in more detail both empirical support for this hypothesis and places where it\r
falls short. We now look in detail at what the actor and critic learning algorithms suggest\r
about the rules governing changes in synaptic ecacies of corticostriatal synapses."""

[[sections]]
number = "15.8"
title = "Actor and Critic Learning Rules"
text = """
If the brain does implement something like the actor–critic algorithm—and assuming\r
populations of dopamine neurons broadcast a common reinforcement signal to the corti\u0002costriatal synapses of both the dorsal and ventral striatum as illustrated in Figure 15.5b\r
(which is likely an oversimplification as we mentioned above)—then this reinforcement\r
signal a↵ects the synapses of these two structures in di↵erent ways. The learning rules for\r
the critic and the actor use the same reinforcement signal, the TD error , but its e↵ect\r
on learning is di↵erent for these two components. The TD error (combined with eligibility\r
traces) tells the actor how to update action probabilities in order to reach higher-valued\r
states. Learning by the actor is like instrumental conditioning using a Law-of-E↵ect-type\r
learning rule (Section 1.7): the actor works to keep  as positive as possible. On the\r
other hand, the TD error (when combined with eligibility traces) tells the critic the\r
direction and magnitude in which to change the parameters of the value function in order\r
to improve its predictive accuracy. The critic works to reduce ’s magnitude to be as\r
close to zero as possible using a learning rule like the TD model of classical conditioning\r
(Section 14.2). The di↵erence between the critic and actor learning rules is relatively\r
simple, but this di↵erence has a profound e↵ect on learning and is essential to how the\r
actor–critic algorithm works. The di↵erence lies solely in the eligibility traces each type\r
of learning rule uses.\r
More than one set of learning rules can be used in actor–critic neural networks like\r
those in Figure 15.5b but, to be specific, here we focus on the actor–critic algorithm for\r
continuing problems with eligibility traces presented in Section 13.6. On each transition\r
from state St to state St+1, taking action At and receiving reward Rt+1, that algorithm\r
computes the TD error () and then updates the eligibility trace vectors (zw\r
t and z✓t ) and"""

[[sections]]
number = "15.8"
title = "Actor and Critic Learning Rules 399"
text = """
the parameters for the critic and actor (w and ✓), according to\r
t = Rt+1 +  vˆ(St+1,w)  vˆ(St,w),\r
zw\r
t = wzwt1 + rvˆ(St,w),\r
z✓\r
t = ✓z✓t1 + r ln ⇡(At|St, ✓),\r
w w + ↵w t zw\r
t ,\r
✓ ✓ + ↵✓ z✓\r
t ,\r
where  2 [0, 1) is a discount-rate parameter, w 2 [0, 1] and ✓ 2 [0, 1] are bootstrapping\r
parameters for the critic and the actor respectively, and ↵w > 0 and ↵✓ > 0 are analogous\r
step-size parameters.\r
Think of the approximate value function vˆ as the output of a single linear neuron-like\r
unit, called the critic unit and labeled V in Figure 15.5a. Then the value function is a\r
linear function of the feature-vector representation of state s, x(s)=(x1(s),...,xn(s))>,\r
parameterized by a weight vector w = (w1,...,wn)>:\r
vˆ(s,w) = w>x(s). (15.1)\r
Each xi(s) is like the presynaptic signal to a neuron’s synapse whose ecacy is wi. The\r
weights of the critic are incremented according to the rule above by ↵wtzw\r
t , where the\r
reinforcement signal, t, corresponds to a dopamine signal being broadcast to all of the\r
critic unit’s synapses. The eligibility trace vector, zw\r
t , for the critic unit is a trace (average\r
of recent values) of rvˆ(St,w). Because vˆ(s,w) is linear in the weights, rvˆ(St,w) = x(St).\r
In neural terms, this means that each synapse has its own eligibility trace, which is\r
one component of the vector zw\r
t . A synapse’s eligibility trace accumulates according to\r
the level of activity arriving at that synapse, that is, the level of presynaptic activity,\r
represented here by the component of the feature vector x(St) arriving at that synapse.\r
The trace otherwise decays toward zero at a rate governed by the fraction w. A synapse\r
is eligible for modification as long as its eligibility trace is non-zero. How the synapse’s\r
ecacy is actually modified depends on the reinforcement signals that arrive while the\r
synapse is eligible. We call eligibility traces like these of the critic unit’s synapses non\u0002contingent eligibility traces because they only depend on presynaptic activity and are not\r
contingent in any way on postsynaptic activity.\r
The non-contingent eligibility traces of the critic unit’s synapses mean that the critic\r
unit’s learning rule is essentially the TD model of classical conditioning described in\r
Section 14.2. With the definition we have given above of the critic unit and its learning\r
rule, the critic in Figure 15.5a is the same as the critic in the ANN actor–critic of Barto\r
et al. (1983). Clearly, a critic like this consisting of just one linear neuron-like unit is the\r
simplest starting point; this critic unit is a proxy for a more complicated neural network\r
able to learn value functions of greater complexity.\r
The actor in Figure 15.5a is a one-layer network of k neuron-like actor units, each\r
receiving at time t the same feature vector, x(St), that the critic unit receives. Each\r
actor unit j, j = 1,...,k, has its own weight vector, ✓j , but because the actor units are\r
all identical, we describe just one of the units and omit the subscript. One way for these"""

[[sections]]
number = "400"
title = "Chapter 15: Neuroscience"
text = """
units to follow the actor–critic algorithm given in the equations above is for each to be a\r
Bernoulli-logistic unit. This means that the output of each actor unit at each time is\r
a random variable, At, taking value 0 or 1. Think of value 1 as the neuron firing, that\r
is, emitting an action potential. The weighted sum, ✓>x(St), of a unit’s input vector\r
determines the unit’s action probabilities via the exponential soft-max distribution (13.2),\r
which for two actions is the logistic function:\r
⇡(1|s, ✓)=1  ⇡(0|s, ✓) = 1\r
1 + exp(✓>x(s)). (15.2)\r
The weights of each actor unit are incremented, as above, by: ✓ ✓ + ↵✓ t z✓\r
t , where\r
 again corresponds to the dopamine signal: the same reinforcement signal that is sent to\r
all the critic unit’s synapses. Figure 15.5a shows t being broadcast to all the synapses\r
of all the actor units (which makes this actor network a team of reinforcement learning\r
agents, something we discuss in Section 15.10 below). The actor eligibility trace vector\r
z✓\r
t is a trace (average of recent values) of r ln ⇡(At|St, ✓). To understand this eligibility\r
trace refer to Exercise 13.5, which defines this kind of unit and asks you to give a learning\r
rule for it. That exercise asked you to express r ln ⇡(a|s, ✓) in terms of a, x(s), and\r
⇡(a|s, ✓) (for arbitrary state s and action a) by calculating the gradient. For the action\r
and state actually occurring at time t, the answer is\r
r ln ⇡(At|St, ✓) = At  ⇡(1|St, ✓)\r
\r
x(St). (15.3)\r
Unlike the non-contingent eligibility trace of a critic synapse that only accumulates\r
the presynaptic activity x(St), the eligibility trace of an actor unit’s synapse in addition\r
depends on the activity of the actor unit itself. We call this a contingent eligibility\r
trace because it is contingent on this postsynaptic activity. The eligibility trace at each\r
synapse continually decays, but increments or decrements depending on the activity of\r
the presynaptic neuron and whether or not the postsynaptic neuron fires. The factor\r
At ⇡(1|St, ✓) in (15.3) is positive when At = 1 and negative otherwise. The postsynaptic\r
contingency in the eligibility traces of actor units is the only di↵erence between the critic\r
and actor learning rules. By keeping information about what actions were taken in\r
what states, contingent eligibility traces allow credit for reward (positive ), or blame for\r
punishment (negative ), to be apportioned among the policy parameters (the ecacies\r
of the actor units’ synapses) according to the contributions these parameters made to the\r
units’ outputs that could have influenced later values of . Contingent eligibility traces\r
mark the synapses as to how they should be modified to alter the units’ future responses\r
to favor positive values of .\r
What do the critic and actor learning rules suggest about how ecacies of corticostriatal\r
synapses change? Both learning rules are related to Donald Hebb’s classic proposal that\r
whenever a presynaptic signal participates in activating the postsynaptic neuron, the\r
synapse’s ecacy increases (Hebb, 1949). The critic and actor learning rules share with\r
Hebb’s proposal the idea that changes in a synapse’s ecacy depend on the interaction\r
of several factors. In the critic learning rule the interaction is between the reinforcement\r
signal  and eligibility traces that depend only on presynaptic signals. Neuroscientists\r
call this a two-factor learning rule because the interaction is between two signals or"""

[[sections]]
number = "15.8"
title = "Actor and Critic Learning Rules 401"
text = """
quantities. The actor learning rule, on the other hand, is a three-factor learning rule\r
because, in addition to depending on , its eligibility traces depend on both presynaptic\r
and postsynaptic activity. Unlike Hebb’s proposal, however, the relative timing of the\r
factors is critical to how synaptic ecacies change, with eligibility traces intervening to\r
allow the reinforcement signal to a↵ect synapses that were active in the recent past.\r
Some subtleties about signal timing for the actor and critic learning rules deserve closer\r
attention. In defining the neuron-like actor and critic units, we ignored the small amount\r
of time it takes synaptic input to e↵ect the firing of a real neuron. When an action\r
potential from the presynaptic neuron arrives at a synapse, neurotransmitter molecules\r
are released that di↵use across the synaptic cleft to the postsynaptic neuron, where\r
they bind to receptors on the postsynaptic neuron’s surface; this activates molecular\r
machinery that causes the postsynaptic neuron to fire (or to inhibit its firing in the case of\r
inhibitory synaptic input). This process can take several tens of milliseconds. According\r
to (15.1) and (15.2), though, the input to a critic and actor unit instantaneously produces\r
the unit’s output. Ignoring activation time like this is common in abstract models of\r
Hebbian-style plasticity in which synaptic ecacies change according to a simple product\r
of simultaneous pre- and postsynaptic activity. More realistic models must take activation\r
time into account.\r
Activation time is especially important for a more realistic actor unit because it\r
influences how contingent eligibility traces have to work in order to properly apportion\r
credit for reinforcement to the appropriate synapses. The expression At⇡(1|St, ✓)\r
\r
x(St)\r
defining contingent eligibility traces for the actor unit’s learning rule given above includes\r
the postsynaptic factor At  ⇡(1|St, ✓)\r
\r
and the presynaptic factor x(St). This works\r
because by ignoring activation time, the presynaptic activity x(St) participates in causing\r
the postsynaptic activity appearing in At⇡(1|St, ✓)\r
\r
. To assign credit for reinforcement\r
correctly, the presynaptic factor defining the eligibility trace must be a cause of the\r
postsynaptic factor that also defines the trace. Contingent eligibility traces for a more\r
realistic actor unit would have to take activation time into account. (Activation time\r
should not be confused with the time required for a neuron to receive a reinforcement\r
signal influenced by that neuron’s activity. The function of eligibility traces is to span\r
this time interval which is generally much longer than the activation time. We discuss\r
this further in the following section.)\r
There are hints from neuroscience for how this process might work in the brain.\r
Neuroscientists have discovered a form of Hebbian plasticity called spike-timing-dependent\r
plasticity (STDP) that lends plausibility to the existence of actor-like synaptic plasticity\r
in the brain. STDP is a Hebbian-style plasticity, but changes in a synapse’s ecacy\r
depend on the relative timing of presynaptic and postsynaptic action potentials. The\r
dependence can take di↵erent forms, but in the one most studied, a synapse increases in\r
strength if spikes incoming via that synapse arrive shortly before the postsynaptic neuron\r
fires. If the timing relation is reversed, with a presynaptic spike arriving shortly after the\r
postsynaptic neuron fires, then the strength of the synapse decreases. STDP is a type of\r
Hebbian plasticity that takes the activation time of a neuron into account, which is one\r
of the ingredients needed for actor-like learning."""

[[sections]]
number = "402"
title = "Chapter 15: Neuroscience"
text = """
The discovery of STDP has led neuroscientists to investigate the possibility of a three\u0002factor form of STDP in which neuromodulatory input must follow appropriately-timed\r
pre- and postsynaptic spikes. This form of synaptic plasticity, called reward-modulated\r
STDP, is much like the actor learning rule discussed here. Synaptic changes that would\r
be produced by regular STDP only occur if there is neuromodulatory input within a time\r
window after a presynaptic spike is closely followed by a postsynaptic spike. Evidence\r
is accumulating that reward-modulated STDP occurs at the spines of medium spiny\r
neurons of the dorsal striatum, with dopamine providing the neuromodulatory factor—\r
the sites where actor learning takes place in the hypothetical neural implementation of\r
an actor–critic algorithm illustrated in Figure 15.5b. Experiments have demonstrated\r
reward-modulated STDP in which lasting changes in the ecacies of corticostriatal\r
synapses occur only if a neuromodulatory pulse arrives within a time window that can\r
last up to 10 seconds after a presynaptic spike is closely followed by a postsynaptic\r
spike (Yagishita et al. 2014). Although the evidence is indirect, these experiments point\r
to the existence of contingent eligibility traces having prolonged time courses. The\r
molecular mechanisms producing these traces, as well as the much shorter traces that\r
likely underly STDP, are not yet understood, but research focusing on time-dependent\r
and neuromodulator-dependent synaptic plasticity is continuing.\r
The neuron-like actor unit that we have described here, with its Law-of-E↵ect-style\r
learning rule, appeared in somewhat simpler form in the actor–critic network of Barto et\r
al. (1983). That network was inspired by the “hedonistic neuron” hypothesis proposed\r
by physiologist A. H. Klopf (1972, 1982). Not all the details of Klopf’s hypothesis are\r
consistent with what has been learned about synaptic plasticity, but the discovery of\r
STDP and the growing evidence for a reward-modulated form of STDP suggest that\r
Klopf’s ideas may not have been far o↵ the mark. We discuss Klopf’s hedonistic neuron\r
hypothesis next."""

[[sections]]
number = "15.9"
title = "Hedonistic Neurons"
text = """
In his hedonistic neuron hypothesis, Klopf (1972, 1982) conjectured that individual\r
neurons seek to maximize the di↵erence between synaptic input treated as rewarding\r
and synaptic input treated as punishing by adjusting the ecacies of their synapses\r
on the basis of rewarding or punishing consequences of their own action potentials. In\r
other words, individual neurons can be trained with response-contingent reinforcement\r
like an animal can be trained in an instrumental conditioning task. His hypothesis\r
included the idea that rewards and punishments are conveyed to a neuron via the same\r
synaptic input that excites or inhibits the neuron’s spike-generating activity. (Had Klopf\r
known what we know today about neuromodulatory systems, he might have assigned the\r
reinforcing role to neuromodulatory input, but he wanted to avoid any centralized source\r
of training information.) Synaptically-local traces of past pre- and postsynaptic activity\r
had the key function in Klopf’s hypothesis of making synapses eligible—the term he\r
introduced—for modification by later reward or punishment. He conjectured that these\r
traces are implemented by molecular mechanisms local to each synapse and therefore\r
di↵erent from the electrical activity of both the pre- and the postsynaptic neurons. In"""

[[sections]]
number = "15.9"
title = "Hedonistic Neurons 403"
text = """
the Bibliographical and Historical Remarks section of this chapter we bring attention to\r
some similar proposals made by others.\r
Klopf specifically conjectured that synaptic ecacies change in the following way. When\r
a neuron fires an action potential, all of its synapses that were active in contributing to\r
that action potential become eligible to undergo changes in their ecacies. If the action\r
potential is followed within an appropriate time period by an increase of reward, the\r
ecacies of all the eligible synapses increase. Symmetrically, if the action potential is\r
followed within an appropriate time period by an increase of punishment, the ecacies\r
of eligible synapses decrease. This is implemented by triggering an eligibility trace at a\r
synapse upon a coincidence of presynaptic and postsynaptic activity (or more exactly,\r
upon pairing of presynaptic activity with the postsynaptic activity that that presynaptic\r
activity participates in causing)—what we call a contingent eligibility trace. This is\r
essentially the three-factor learning rule of an actor unit described in the previous section.\r
The shape and time course of an eligibility trace in Klopf’s theory reflects the durations\r
of the many feedback loops in which the neuron is embedded, some of which lie entirely\r
within the brain and body of the organism, while others extend out through the organism’s\r
external environment as mediated by its motor and sensory systems. His idea was that\r
the shape of a synaptic eligibility trace is like a histogram of the durations of the feedback\r
loops in which the neuron is embedded. The peak of an eligibility trace would then occur\r
at the duration of the most prevalent feedback loops in which that neuron participates.\r
The eligibility traces used by algorithms described in this book are simplified versions\r
of Klopf’s original idea, being exponentially (or geometrically) decreasing functions\r
controlled by the parameters  and . This simplifies simulations as well as theory, but\r
we regard these simple eligibility traces as a placeholders for traces closer to Klopf’s\r
original conception, which would have computational advantages in complex reinforcement\r
learning systems by refining the credit-assignment process.\r
Klopf’s hedonistic neuron hypothesis is not as implausible as it may at first appear.\r
A well-studied example of a single cell that seeks some stimuli and avoids others is the\r
bacterium Escherichia coli. The movement of this single-cell organism is influenced by\r
chemical stimuli in its environment, behavior known as chemotaxis. It swims in its liquid\r
environment by rotating hairlike structures called flagella attached to its surface. (Yes,\r
it rotates them!) Molecules in the bacterium’s environment bind to receptors on its\r
surface. Binding events modulate the frequency with which the bacterium reverses flagellar\r
rotation. Each reversal causes the bacterium to tumble in place and then head o↵ in a\r
random new direction. A little chemical memory and computation causes the frequency\r
of flagellar reversal to decrease when the bacterium swims toward higher concentrations\r
of molecules it needs to survive (attractants) and increase when the bacterium swims\r
toward higher concentrations of molecules that are harmful (repellants). The result is\r
that the bacterium tends to persist in swimming up attractant gradients and tends to\r
avoid swimming up repellant gradients.\r
The chemotactic behavior just described is called klinokinesis. It is a kind of trial\u0002and-error behavior, although it is unlikely that learning is involved: the bacterium needs\r
a modicum of short-term memory to detect molecular concentration gradients, but it\r
probably does not maintain long-term memories. Artificial intelligence pioneer Oliver"""

[[sections]]
number = "404"
title = "Chapter 15: Neuroscience"
text = """
Selfridge called this strategy “run and twiddle,” pointing out its utility as a basic adaptive\r
strategy: “keep going in the same way if things are getting better, and otherwise move\r
around” (Selfridge, 1978, 1984). Similarly, one might think of a neuron “swimming” (not\r
literally of course) in a medium composed of the complex collection of feedback loops\r
in which it is embedded, acting to obtain one type of input signal and to avoid others.\r
Unlike the bacterium, however, the neuron’s synaptic strengths retain information about\r
its past trial-and-error behavior. If this view of the behavior of a neuron (or just one type\r
of neuron) is plausible, then the closed-loop nature of how the neuron interacts with its\r
environment is important for understanding its behavior, where the neuron’s environment\r
consists of the rest of the animal together with the environment with which the animal\r
as a whole interacts.\r
Klopf’s hedonistic neuron hypothesis extended beyond the idea that individual neurons\r
are reinforcement learning agents. He argued that many aspects of intelligent behavior\r
can be understood as the result of the collective behavior of a population of self-interested\r
hedonistic neurons interacting with one another in an immense society or economic system\r
making up an animal’s nervous system. Whether or not this view of nervous systems\r
is useful, the collective behavior of reinforcement learning agents has implications for\r
neuroscience. We take up this subject next."""

[[sections]]
number = "15.10"
title = "Collective Reinforcement Learning"
text = """
The behavior of populations of reinforcement learning agents is deeply relevant to the\r
study of social and economic systems, and if anything like Klopf’s hedonistic neuron\r
hypothesis is correct, to neuroscience as well. The hypothesis described above about how\r
an actor–critic algorithm might be implemented in the brain only narrowly addresses\r
the implications of the fact that the dorsal and ventral subdivisions of the striatum, the\r
respective locations of the actor and the critic according to the hypothesis, each contain\r
millions of medium spiny neurons whose synapses undergo change modulated by phasic\r
bursts of dopamine neuron activity.\r
The actor in Figure 15.5a is a single-layer network of k actor units. The actions\r
produced by this network are vectors (A1, A2, ··· , Ak)> presumed to drive the animal’s\r
behavior. Changes in the ecacies of the synapses of all of these units depend on the\r
reinforcement signal . Because actor units attempt to make  as large as possible, \r
e↵ectively acts as a reward signal for them (so in this case reinforcement is the same\r
as reward). Thus, each actor unit is itself a reinforcement learning agent—a hedonistic\r
neuron if you will. Now, to make the situation as simple as possible, assume that each\r
of these units receives the same reward signal at the same time (although, as indicated\r
above, the assumption that dopamine is released at all the corticostriatal synapses under\r
the same conditions and at the same times is likely an oversimplification).\r
What can reinforcement learning theory tell us about what happens when all members\r
of a population of reinforcement learning agents learn according to a common reward\r
signal? The field of multi-agent reinforcement learning considers many aspects of learning\r
by populations of reinforcement learning agents. Although this field is beyond the scope\r
of this book, we believe that some of its basic concepts and results are relevant to thinking"""

[[sections]]
number = "15.10"
title = "Collective Reinforcement Learning 405"
text = """
about the brain’s di↵use neuromodulatory systems. In multi-agent reinforcement learning\r
(and in game theory), the scenario in which all the agents try to maximize a common\r
reward signal that they simultaneously receive is known as a cooperative game or a team\r
problem.\r
What makes a team problem interesting and challenging is that the common reward\r
signal sent to each agent evaluates the pattern of activity produced by the entire population,\r
that is, it evaluates the collective action of the team members. This means that any\r
individual agent has only limited ability to a↵ect the reward signal because any single\r
agent contributes just one component of the collective action evaluated by the common\r
reward signal. E↵ective learning in this scenario requires addressing a structural credit\r
assignment problem: which team members, or groups of team members, deserve credit for\r
a favorable reward signal, or blame for an unfavorable reward signal? It is a cooperative\r
game, or a team problem, because the agents are united in seeking to increase the same\r
reward signal: there are no conflicts of interest among the agents. The scenario would be\r
a competitive game if di↵erent agents receive di↵erent reward signals, where each reward\r
signal again evaluates the collective action of the population, and the objective of each\r
agent is to increase its own reward signal. In this case there might be conflicts of interest\r
among the agents, meaning that actions that are good for some agents are bad for others.\r
Even deciding what the best collective action should be is a non-trivial aspect of game\r
theory. This competitive setting might be relevant to neuroscience too (for example, to\r
account for heterogeneity of dopamine neuron activity), but here we focus only on the\r
cooperative, or team, case.\r
How can each reinforcement learning agent in a team learn to “do the right thing” so\r
that the collective action of the team is highly rewarded? An interesting result is that\r
if each agent can learn e↵ectively despite its reward signal being corrupted by a large\r
amount of noise, and despite its lack of access to complete state information, then the\r
population as a whole will learn to produce collective actions that improve as evaluated by\r
the common reward signal, even when the agents cannot communicate with one another.\r
Each agent faces its own reinforcement learning task in which its influence on the reward\r
signal is deeply buried in the noise created by the influences of other agents. In fact, for\r
any agent, all the other agents are part of its environment because its input, both the part\r
conveying state information and the reward part, depends on how all the other agents\r
are behaving. Furthermore, lacking access to the actions of the other agents, indeed\r
lacking access to the parameters determining their policies, each agent can only partially\r
observe the state of its environment. This makes each team member’s learning task very\r
dicult, but if each uses a reinforcement learning algorithm able to increase a reward\r
signal even under these dicult conditions, teams of reinforcement learning agents can\r
learn to produce collective actions that improve over time as evaluated by the team’s\r
common reward signal.\r
If the team members are neuron-like units, then each unit has to have the goal of\r
increasing the amount of reward it receives over time, as the actor unit does that we\r
described in Section 15.8. Each unit’s learning algorithm has to have two essential features.\r
First, it has to use contingent eligibility traces. Recall that a contingent eligibility trace,\r
in neural terms, is initiated (or increased) at a synapse when its presynaptic input"""

[[sections]]
number = "406"
title = "Chapter 15: Neuroscience"
text = """
participates in causing the postsynaptic neuron to fire. A non-contingent eligibility trace,\r
in contrast, is initiated or increased by presynaptic input independently of what the\r
postsynaptic neuron does. As explained in Section 15.8, by keeping information about\r
what actions were taken in what states, contingent eligibility traces allow credit for\r
reward, or blame for punishment, to be apportioned to an agent’s policy parameters\r
according to the contribution the values of these parameters made in determining the\r
agent’s action. By similar reasoning, a team member must remember its recent action so\r
that it can either increase or decrease the likelihood of producing that action according\r
to the reward signal that is subsequently received. The action component of a contingent\r
eligibility trace implements this action memory. Because of the complexity of the learning\r
task, however, contingent eligibility is merely a preliminary step in the credit assignment\r
process: the relationship between a single team member’s action and changes in the\r
team’s reward signal is a statistical correlation that has to be estimated over many trials.\r
Contingent eligibility is an essential but preliminary step in this process.\r
Learning with non-contingent eligibility traces does not work at all in the team setting\r
because it does not provide a way to correlate actions with consequent changes in the\r
reward signal. Non-contingent eligibility traces are adequate for learning to predict, as\r
the critic component of the actor–critic algorithm does, but they do not support learning\r
to control, as the actor component must do. The members of a population of critic-like\r
agents may still receive a common reinforcement signal, but they would all learn to\r
predict the same quantity (which in the case of an actor–critic method, would be the\r
expected return for the current policy). How successful each member of the population\r
would be in learning to predict the expected return would depend on the information it\r
receives, which could be very di↵erent for di↵erent members of the population. There\r
would be no need for the population to produce di↵erentiated patterns of activity. This\r
is not a team problem as defined here.\r
A second requirement for collective learning in a team problem is that there has to be\r
variability in the actions of the team members in order for the team to explore the space\r
of collective actions. The simplest way for a team of reinforcement learning agents to do\r
this is for each member to independently explore its own action space through persistent\r
variability in its output. This will cause the team as a whole to vary its collective actions.\r
For example, a team of the actor units described in Section 15.8 explores the space\r
of collective actions because the output of each unit, being a Bernoulli-logistic unit,\r
probabilistically depends on the weighted sum of its input vector’s components. The\r
weighted sum biases firing probability up or down, but there is always variability. Because\r
each unit uses a REINFORCE policy gradient algorithm (Chapter 13), each unit adjusts\r
its weights with the goal of maximizing the average reward rate it experiences while\r
stochastically exploring its own action space. One can show, as Williams (1992) did, that\r
a team of Bernoulli-logistic REINFORCE units implements a policy gradient algorithm\r
as a whole with respect to average rate of the team’s common reward signal, where the\r
actions are the collective actions of the team.\r
Further, Williams (1992) showed that a team of Bernoulli-logistic units using REIN\u0002FORCE ascends the average reward gradient when the units in the team are interconnected\r
to form a multilayer ANN. In this case, the reward signal is broadcast to all the units in"""

[[sections]]
number = "15.11"
title = "Model-based Methods in the Brain 407"
text = """
the network, though reward may depend only on the collective actions of the network’s\r
output units. This means that a multilayer team of Bernoulli-logistic REINFORCE\r
units learns like a multilayer network trained by the widely-used error backpropagation\r
method, but in this case the backpropagation process is replaced by the broadcasted\r
reward signal. In practice, the error backpropagation method is considerably faster,\r
but the reinforcement learning team method is more plausible as a neural mechanism,\r
especially in light of what is being learned about reward-modulated STDP as discussed\r
in Section 15.8.\r
Exploration through independent exploration by team members is only the simplest\r
way for a team to explore; more sophisticated methods are possible if the team members\r
coordinate their actions to focus on particular parts of the collective action space, either\r
by communicating with one another or by responding to common inputs. There are also\r
mechanisms more sophisticated than contingent eligibility traces for addressing structural\r
credit assignment, which is easier in a team problem when the set of possible collective\r
actions is restricted in some way. An extreme case is a winner-take-all arrangement (for\r
example, the result of lateral inhibition in the brain) that restricts collective actions to\r
those to which only one, or a few, team members contribute. In this case the winners get\r
the credit or blame for resulting reward or punishment.\r
Details of learning in cooperative games (or team problems) and non-cooperative\r
game problems are beyond the scope of this book. The Bibliographical and Historical\r
Remarks section at the end of this chapter cites a selection of the relevant publications,\r
including extensive references to research on implications for neuroscience of collective\r
reinforcement learning."""

[[sections]]
number = "15.11"
title = "Model-based Methods in the Brain"
text = """
Reinforcement learning’s distinction between model-free and model-based algorithms is\r
proving to be useful for thinking about animal learning and decision processes. Section 14.6\r
discusses how this distinction aligns with that between habitual and goal-directed animal\r
behavior. The hypothesis discussed above about how the brain might implement an\r
actor–critic algorithm is relevant only to an animal’s habitual mode of behavior because\r
the basic actor–critic method is model-free. What neural mechanisms are responsible\r
for producing goal-directed behavior, and how do they interact with those underlying\r
habitual behavior?\r
One way to investigate questions about the brain structures involved in these modes\r
of behavior is to inactivate an area of a rat’s brain and then observe what the rat does in\r
an outcome-devaluation experiment (Section 14.6). Results from experiments like these\r
indicate that the actor–critic hypothesis described above is too simple in placing the\r
actor in the dorsal striatum. Inactivating one part of the dorsal striatum, the dorsolateral\r
striatum (DLS), impairs habit learning, causing the animal to rely more on goal-directed\r
processes. On the other hand, inactivating the dorsomedial striatum (DMS) impairs\r
goal-directed processes, requiring the animal to rely more on habit learning. Results\r
like these support the view that the DLS in rodents is more involved in model-free\r
processes, whereas their DMS is more involved in model-based processes. Results of"""

[[sections]]
number = "408"
title = "Chapter 15: Neuroscience"
text = """
studies with human subjects in similar experiments using functional neuroimaging, and\r
with non-human primates, support the view that the analogous structures in the primate\r
brain are di↵erentially involved in habitual and goal-directed modes of behavior.\r
Other studies identify activity associated with model-based processes in the prefrontal\r
cortex of the human brain, the front-most part of the frontal cortex implicated in\r
executive function, including planning and decision making. Specifically implicated is the\r
orbitofrontal cortex (OFC), the part of the prefrontal cortex immediately above the eyes.\r
Functional neuroimaging in humans, and also recordings of the activities of single neurons\r
in monkeys, reveals strong activity in the OFC related to the subjective reward value\r
of biologically significant stimuli, as well as activity related to the reward expected as a\r
consequence of actions. Although not free of controversy, these results suggest significant\r
involvement of the OFC in goal-directed choice. It may be critical for the reward part of\r
an animal’s environment model.\r
Another structure involved in model-based behavior is the hippocampus, a structure\r
critical for memory and spatial navigation. A rat’s hippocampus plays a critical role\r
in the rat’s ability to navigate a maze in the goal-directed manner that led Tolman to\r
the idea that animals use models, or cognitive maps, in selecting actions (Section 14.5).\r
The hippocampus may also be a critical component of our human ability to imagine\r
new experiences (Hassabis and Maguire, 2007; Olafsd´ottir, Barry, Saleem, Hassabis, and ´\r
Spiers, 2015).\r
The findings that most directly implicate the hippocampus in planning—the process\r
needed to enlist an environment model in making decisions—come from experiments\r
that decode the activity of neurons in the hippocampus to determine what part of space\r
hippocampal activity is representing on a moment-to-moment basis. When a rat pauses\r
at a choice point in a maze, the representation of space in the hippocampus sweeps\r
forward (and not backwards) along the possible paths the animal can take from that\r
point (Johnson and Redish, 2007). Furthermore, the spatial trajectories represented by\r
these sweeps closely correspond to the rat’s subsequent navigational behavior (Pfei↵er\r
and Foster, 2013). These results suggest that the hippocampus is critical for the state\u0002transition part of an animal’s environment model, and that it is part of a system that\r
uses the model to simulate possible future state sequences to assess the consequences of\r
possible courses of action: a form of planning.\r
The results described above add to a voluminous literature on neural mechanisms\r
underlying goal-directed, or model-based, learning and decision making, but many\r
questions remain unanswered. For example, how can areas as structurally similar as the\r
DLS and DMS be essential components of modes of learning and behavior that are as\r
di↵erent as model-free and model-based algorithms? Are separate structures responsible\r
for (what we call) the transition and reward components of an environment model? Is all\r
planning conducted at decision time via simulations of possible future courses of action\r
as the forward sweeping activity in the hippocampus suggests? In other words, is all\r
planning something like a rollout algorithm (Section 8.10)? Or are models sometimes\r
engaged in the background to refine or recompute value information as illustrated by the\r
Dyna architecture (Section 8.2)? How does the brain arbitrate between the use of the\r
habit and goal-directed systems? Is there, in fact, a clear separation between the neural\r
substrates of these systems?"""

[[sections]]
number = "15.12"
title = "Addiction 409"
text = """
The evidence is not pointing to a positive answer to this last question. Summarizing\r
the situation, Doll, Simon, and Daw (2012) wrote that “model-based influences appear\r
ubiquitous more or less wherever the brain processes reward information,” and this is\r
true even in the regions thought to be critical for model-free learning. This includes the\r
dopamine signals themselves, which can exhibit the influence of model-based information\r
in addition to the reward prediction errors thought to be the basis of model-free processes.\r
Continuing neuroscience research informed by reinforcement learning’s model-free and\r
model-based distinction has the potential to sharpen our understanding of habitual and\r
goal-directed processes in the brain. A better grasp of these neural mechanisms may lead\r
to algorithms combining model-free and model-based methods in ways that have not yet\r
been explored in computational reinforcement learning."""

[[sections]]
number = "15.12"
title = "Addiction"
text = """
Understanding the neural basis of drug abuse is a high-priority goal of neuroscience with\r
the potential to produce new treatments for this serious public health problem. One\r
view is that drug craving is the result of the same motivation and learning processes that\r
lead us to seek natural rewarding experiences that serve our biological needs. Addictive\r
substances, by being intensely reinforcing, e↵ectively co-opt our natural mechanisms\r
of learning and decision making. This is plausible given that many—though not all—\r
drugs of abuse increase levels of dopamine either directly or indirectly in regions around\r
terminals of dopamine neuron axons in the striatum, a brain structure firmly implicated in\r
normal reward-based learning (Section 15.7). But the self-destructive behavior associated\r
with drug addiction is not characteristic of normal learning. What is di↵erent about\r
dopamine-mediated learning when the reward is the result of an addictive drug? Is\r
addiction the result of normal learning in response to substances that were largely\r
unavailable throughout our evolutionary history, so that evolution could not select against\r
their damaging e↵ects? Or do addictive substances somehow interfere with normal\r
dopamine-mediated learning?\r
The reward prediction error hypothesis of dopamine neuron activity and its connection\r
to TD learning are the basis of a model due to Redish (2004) of some—but certainly not\r
all—features of addiction. The model is based on the observation that administration of\r
cocaine and some other addictive drugs produces a transient increase in dopamine. In the\r
model, this dopamine surge is assumed to increase the TD error, , in a way that cannot\r
be cancelled out by changes in the value function. In other words, whereas  is reduced\r
to the degree that a normal reward is predicted by antecedent events (Section 15.6), the\r
contribution to  due to an addictive stimulus does not decrease as the reward signal\r
becomes predicted: drug rewards cannot be “predicted away.” The model does this by\r
preventing  from ever becoming negative when the reward signal is due to an addictive\r
drug, thus eliminating the error-correcting feature of TD learning for states associated\r
with administration of the drug. The result is that the values of these states increase\r
without bound, making actions leading to these states preferred above all others."""

[[sections]]
number = "410"
title = "Chapter 15: Neuroscience"
text = """
Addictive behavior is much more complicated than this result from Redish’s model, but\r
the model’s main idea may be a piece of the puzzle. Or the model might be misleading.\r
Dopamine appears not to play a critical role in all forms of addiction, and not everyone\r
is equally susceptible to developing addictive behavior. Moreover, the model does not\r
include the changes in many circuits and brain regions that accompany chronic drug\r
taking, for example, changes that lead to a drug’s diminishing e↵ect with repeated use.\r
It is also likely that addiction involves model-based processes. Still, Redish’s model\r
illustrates how reinforcement learning theory can be enlisted in the e↵ort to understand\r
a major health problem. In a similar manner, reinforcement learning theory has been\r
influential in the development of the new field of computational psychiatry, which aims\r
to improve understanding of mental disorders through mathematical and computational\r
methods."""

[[sections]]
number = "15.13"
title = "Summary"
text = """
The neural pathways involved in the brain’s reward system are complex and incompletely\r
understood, but neuroscience research directed toward understanding these pathways\r
and their roles in behavior is progressing rapidly. This research is revealing striking\r
correspondences between the brain’s reward system and the theory of reinforcement\r
learning as presented in this book.\r
The reward prediction error hypothesis of dopamine neuron activity was proposed by\r
scientists who recognized striking parallels between the behavior of TD errors and the\r
activity of neurons that produce dopamine, a neurotransmitter essential in mammals\r
for reward-related learning and behavior. Experiments conducted in the late 1980s and\r
1990s in the laboratory of neuroscientist Wolfram Schultz showed that dopamine neurons\r
respond to rewarding events with substantial bursts of activity, called phasic responses,\r
only if the animal does not expect those events, suggesting that dopamine neurons are\r
signaling reward prediction errors instead of reward itself. Further, these experiments\r
showed that as an animal learns to predict a rewarding event on the basis of preceding\r
sensory cues, the phasic activity of dopamine neurons shifts to earlier predictive cues\r
while decreasing to later predictive cues. This parallels the backing-up e↵ect of the TD\r
error as a reinforcement learning agent learns to predict reward.\r
Other experimental results firmly establish that the phasic activity of dopamine neurons\r
is a reinforcement signal for learning that reaches multiple areas of the brain by means of\r
profusely branching axons of dopamine producing neurons. These results are consistent\r
with the distinction we make between a reward signal, Rt, and a reinforcement signal,\r
which is the TD error t in most of the algorithms we present. Phasic responses of\r
dopamine neurons are reinforcement signals, not reward signals.\r
A prominent hypothesis is that the brain implements something like an actor–critic\r
algorithm. Two structures in the brain (the dorsal and ventral subdivisions of the\r
striatum), both of which play critical roles in reward-based learning, may function\r
respectively like an actor and a critic. That the TD error is the reinforcement signal for\r
both the actor and the critic fits well with the facts that dopamine neuron axons target\r
both the dorsal and ventral subdivisions of the striatum; that dopamine appears to be"""

[[sections]]
number = "15.13"
title = "Summary 411"
text = """
critical for modulating synaptic plasticity in both structures; and that the e↵ect on a\r
target structure of a neuromodulator such as dopamine depends on properties of the\r
target structure and not just on properties of the neuromodulator.\r
The actor and the critic can be implemented by ANNs consisting of neuron-like units\r
having learning rules based on the policy-gradient actor–critic method described in\r
Section 13.5. Each connection in these networks is like a synapse between neurons in\r
the brain, and the learning rules correspond to rules governing how synaptic ecacies\r
change as functions of the activities of the presynaptic and the postsynaptic neurons,\r
together with neuromodulatory input corresponding to input from dopamine neurons. In\r
this setting, each synapse has its own eligibility trace that records past activity involving\r
that synapse. The only di↵erence between the actor and critic learning rules is that they\r
use di↵erent kinds of eligibility traces: the critic unit’s traces are non-contingent because\r
they do not involve the critic unit’s output, whereas the actor unit’s traces are contingent\r
because in addition to the actor unit’s input, they depend on the actor unit’s output. In\r
the hypothetical implementation of an actor–critic system in the brain, these learning\r
rules respectively correspond to rules governing plasticity of corticostriatal synapses that\r
convey signals from the cortex to the principal neurons in the dorsal and ventral striatal\r
subdivisions, synapses that also receive inputs from dopamine neurons.\r
The learning rule of an actor unit in the actor–critic network closely corresponds to\r
reward-modulated spike-timing-dependent plasticity. In spike-timing-dependent plasticity\r
(STDP), the relative timing of pre- and postsynaptic activity determines the direction of\r
synaptic change. In reward-modulated STDP, changes in synapses in addition depend\r
on a neuromodulator, such as dopamine, arriving within a time window that can last\r
up to 10 seconds after the conditions for STDP are met. Evidence is accumulating that\r
reward-modulated STDP occurs at corticostriatal synapses, where the actor’s learning\r
takes place in the hypothetical neural implementation of an actor–critic system, adds to\r
the plausibility of the hypothesis that something like an actor–critic system exists in the\r
brains of some animals.\r
The idea of synaptic eligibility and basic features of the actor learning rule derive\r
from Klopf’s hypothesis of the “hedonistic neuron” (Klopf, 1972, 1981). He conjectured\r
that individual neurons seek to obtain reward and to avoid punishment by adjusting the\r
ecacies of their synapses on the basis of rewarding or punishing consequences of their\r
action potentials. A neuron’s activity can a↵ect its later input because the neuron is\r
embedded in many feedback loops, some within the animal’s nervous system and body\r
and others passing through the animal’s external environment. Klopf’s idea of eligibility\r
is that synapses are temporarily marked as eligible for modification if they participated\r
in the neuron’s firing (making this the contingent form of eligibility trace). A synapse’s\r
ecacy is modified if a reinforcing signal arrives while the synapse is eligible. We alluded\r
to the chemotactic behavior of a bacterium as an example of a single cell that directs its\r
movements in order to seek some molecules and to avoid others.\r
A conspicuous feature of the dopamine system is that fibers releasing dopamine project\r
widely to multiple parts of the brain. Although it is likely that only some populations\r
of dopamine neurons broadcast the same reinforcement signal, if this signal reaches\r
the synapses of many neurons involved in actor-type learning, then the situation can"""

[[sections]]
number = "412"
title = "Chapter 15: Neuroscience"
text = """
be modeled as a team problem. In this type of problem, each agent in a collection of\r
reinforcement learning agents receives the same reinforcement signal, where that signal\r
depends on the activities of all members of the collection, or team. If each team member\r
uses a suciently capable learning algorithm, the team can learn collectively to improve\r
performance of the entire team as evaluated by the globally-broadcast reinforcement\r
signal, even if the team members do not directly communicate with one another. This\r
is consistent with the wide dispersion of dopamine signals in the brain and provides\r
a neurally plausible alternative to the widely-used error-backpropagation method for\r
training multilayer networks.\r
The distinction between model-free and model-based reinforcement learning is helping\r
neuroscientists investigate the neural bases of habitual and goal-directed learning and\r
decision making. Research so far points to their being some brain regions more involved\r
in one type of process than the other, but the picture remains unclear because model-free\r
and model-based processes do not appear to be neatly separated in the brain. Many\r
questions remain unanswered. Perhaps most intriguing is evidence that the hippocampus,\r
a structure traditionally associated with spatial navigation and memory, appears to be\r
involved in simulating possible future courses of action as part of an animal’s decision\u0002making process. This suggests that it is part of a system that uses an environment model\r
for planning.\r
Reinforcement learning theory is also influencing thinking about neural processes\r
underlying drug abuse. A model of some features of drug addiction is based on the reward\r
prediction error hypothesis. It proposes that an addicting stimulant, such as cocaine,\r
destabilizes TD learning to produce unbounded growth in the values of actions associated\r
with drug intake. This is far from a complete model of addiction, but it illustrates how\r
a computational perspective suggests theories that can be tested with further research.\r
The new field of computational psychiatry similarly focuses on the use of computational\r
models, some derived from reinforcement learning, to better understand mental disorders.\r
This chapter only touched the surface of how the neuroscience of reinforcement learning\r
and the development of reinforcement learning in computer science and engineering\r
have influenced one another. Most features of reinforcement learning algorithms owe\r
their design to purely computational considerations, but some have been influenced by\r
hypotheses about neural learning mechanisms. Remarkably, as experimental data has\r
accumulated about the brain’s reward processes, many of the purely computationally\u0002motivated features of reinforcement learning algorithms are turning out to be consistent\r
with neuroscience data. Other features of computational reinforcement learning, such as\r
eligibility traces and the ability of teams of reinforcement learning agents to learn to act\r
collectively under the influence of a globally-broadcast reinforcement signal, may also\r
turn out to parallel experimental data as neuroscientists continue to unravel the neural\r
basis of reward-based animal learning and behavior."""

[[sections]]
number = "15.13"
title = "Summary 413"
text = """
Bibliographical and Historical Remarks\r
The number of publications treating parallels between the neuroscience of learning and\r
decision making and the approach to reinforcement learning presented in this book is\r
enormous. We can cite only a small selection. Niv (2009), Dayan and Niv (2008), Gimcher\r
(2011), Ludvig, Bellemare, and Pearson (2011), and Shah (2012) are good places to start.\r
Together with economics, evolutionary biology, and mathematical psychology, reinforce\u0002ment learning theory is helping to formulate quantitative models of the neural mechanisms\r
of choice in humans and non-human primates. With its focus on learning, this chapter\r
only lightly touches upon the neuroscience of decision making. Glimcher (2003) introduced\r
the field of “neuroeconomics,” in which reinforcement learning contributes to the study\r
of the neural basis of decision making from an economics perspective. See also Glimcher\r
and Fehr (2013). The text on computational and mathematical modeling in neuroscience\r
by Dayan and Abbott (2001) includes reinforcement learning’s role in these approaches.\r
Sterling and Laughlin (2015) examined the neural basis of learning in terms of general\r
design principles that enable ecient adaptive behavior.\r
15.1 There are many good expositions of basic neuroscience. Kandel, Schwartz, Jessell,\r
Siegelbaum, and Hudspeth (2013) is an authoritative and very comprehensive\r
source.\r
15.2 Berridge and Kringelbach (2008) reviewed the neural basis of reward and pleasure,\r
pointing out that reward processing has many dimensions and involves many\r
neural systems. Space prevents discussion of the influential research of Berridge\r
and Robinson (1998), who distinguish between the hedonic impact of a stimulus,\r
which they call “liking,” and the motivational e↵ect, which they call “wanting.”\r
Hare, O’Doherty, Camerer, Schultz, and Rangel (2008) examined the neural basis\r
of value-related signals from an economic perspective, distinguishing between\r
goal values, decision values, and prediction errors. Decision value is goal value\r
minus action cost. See also Rangel, Camerer, and Montague (2008), Rangel and\r
Hare (2010), and Peters and B¨uchel (2010)."""

[[sections]]
number = "15.3"
title = "The reward prediction error hypothesis of dopamine neuron activity is most"
text = """
prominently discussed by Schultz, Dayan, and Montague (1997). The hypothesis\r
was first explicitly put forward by Montague, Dayan, and Sejnowski (1996). As\r
they stated the hypothesis, it referred to reward prediction errors (RPEs) but\r
not specifically to TD errors; however, their development of the hypothesis made\r
it clear that they were referring to TD errors. The earliest recognition of the\r
TD-error/dopamine connection of which we are aware is that of Montague, Dayan,\r
Nowlan, Pouget, and Sejnowski (1993), who proposed a TD-error-modulated\r
Hebbian learning rule motivated by results on dopamine signaling from Schultz’s\r
group. The connection was also pointed out in an abstract by Quartz, Dayan,\r
Montague, and Sejnowski (1992). Montague and Sejnowski (1994) emphasized\r
the importance of prediction in the brain and outlined how predictive Hebbian\r
learning modulated by TD errors could be implemented via a di↵use neuromod\u0002ulatory system, such as the dopamine system. Friston, Tononi, Reeke, Sporns,"""

[[sections]]
number = "414"
title = "Chapter 15: Neuroscience"
text = """
and Edelman (1994) presented a model of value-dependent learning in the brain\r
in which synaptic changes are mediated by a TD-like error provided by a global\r
neuromodulatory signal (although they did not single out dopamine). Mon\u0002tague, Dayan, Person, and Sejnowski (1995) presented a model of honeybee\r
foraging using the TD error. The model is based on research by Hammer, Men\u0002zel, and colleagues (Hammer and Menzel, 1995; Hammer, 1997) showing that\r
the neuromodulator octopamine acts as a reinforcement signal in the honeybee.\r
Montague et al. (1995) pointed out that dopamine likely plays a similar role\r
in the vertebrate brain. Barto (1995a) related the actor–critic architecture to\r
basal-ganglionic circuits and discussed the relationship between TD learning\r
and the main results from Schultz’s group. Houk, Adams, and Barto (1995)\r
suggested how TD learning and the actor–critic architecture might map onto the\r
anatomy, physiology, and molecular mechanism of the basal ganglia. Doya and\r
Sejnowski (1998) extended their earlier paper on a model of birdsong learning\r
(Doya and Sejnowski, 1995) by including a TD-like error identified with dopamine\r
to reinforce the selection of auditory input to be memorized. O’Reilly and Frank\r
(2006) and O’Reilly, Frank, Hazy, and Watz (2007) argued that phasic dopamine\r
signals are RPEs but not TD errors. In support of their theory they cited results\r
with variable interstimulus intervals that do not match predictions of a simple\r
TD model, as well as the observation that higher-order conditioning beyond\r
second-order conditioning is rarely observed, while TD learning is not so limited.\r
Dayan and Niv (2008) discussed “the good, the bad, and the ugly” of how\r
reinforcement learning theory and the reward prediction error hypothesis align\r
with experimental data. Glimcher (2011) reviewed the empirical findings that\r
support the reward prediction error hypothesis and emphasized the significance\r
of the hypothesis for contemporary neuroscience.\r
15.4 Graybiel (2000) is a brief primer on the basal ganglia. The experiments mentioned\r
that involve optogenetic activation of dopamine neurons were conducted by Tsai,\r
Zhang, Adamantidis, Stuber, Bonci, de Lecea, and Deisseroth (2009), Steinberg,\r
Keiflin, Boivin, Witten, Deisseroth, and Janak (2013), and Claridge-Chang,\r
Roorda, Vrontou, Sjulson, Li, Hirsh, and Miesenb¨ock (2009). Fiorillo, Yun, and\r
Song (2013), Lammel, Lim, and Malenka (2014), and Saddoris, Cacciapaglia,\r
Wightmman, and Carelli (2015) are among studies showing that the signaling\r
properties of dopamine neurons are specialized for di↵erent target regions. RPE\u0002signaling neurons may belong to one among multiple populations of dopamine\r
neurons having di↵erent targets and subserving di↵erent functions. Eshel, Tian,\r
Bukwich, and Uchida (2016) found homogeneity of reward prediction error\r
responses of dopamine neurons in the lateral VTA during classical conditioning\r
in mice, though their results do not rule out response diversity across wider areas.\r
Gershman, Pesaran, and Daw (2009) studied reinforcement learning tasks that\r
can be decomposed into independent components with separate reward signals,\r
finding evidence in human neuroimaging data suggesting that the brain exploits\r
this kind of structure."""

[[sections]]
number = "15.13"
title = "Summary 415"
text = """
15.5 Schultz’s 1998 survey article is a good entr´ee into the very extensive literature\r
on reward predicting signaling of dopamine neurons. Berns, McClure, Pagnoni,\r
and Montague (2001), Breiter, Aharon, Kahneman, Dale, and Shizgal (2001),\r
Pagnoni, Zink, Montague, and Berns (2002), and O’Doherty, Dayan, Friston,\r
Critchley, and Dolan (2003) described functional brain imaging studies supporting\r
the existence of signals like TD errors in the human brain."""

[[sections]]
number = "15.6"
title = "This section roughly follows Barto (1995a) in explaining how TD errors mimic the"
text = "main results from Schultz’s group on the phasic responses of dopamine neurons."

[[sections]]
number = "15.7"
title = "This section is largely based on Takahashi, Schoenbaum, and Niv (2008) and Niv"
text = """
(2009). To the best of our knowledge, Barto (1995a) and Houk, Adams, and Barto\r
(1995) first speculated about possible implementations of actor–critic algorithms\r
in the basal ganglia. On the basis of functional magnetic resonance imaging of\r
human subjects while engaged in instrumental conditioning, O’Doherty, Dayan,\r
Schultz, Deichmann, Friston, and Dolan (2004) suggested that the actor and\r
the critic are most likely located respectively in the dorsal and ventral striatum.\r
Gershman, Moustafa, and Ludvig (2014) focused on how time is represented in\r
reinforcement learning models of the basal ganglia, discussing evidence for, and\r
implications of, various computational approaches to time representation.\r
The hypothetical neural implementation of the actor–critic architecture described\r
in this section includes very little detail about known basal ganglia anatomy and\r
physiology. In addition to the more detailed hypothesis of Houk, Adams, and\r
Barto (1995), a number of other hypotheses include more specific connections to\r
anatomy and physiology and are claimed to explain additional data. These include\r
hypotheses proposed by Suri and Schultz (1998, 1999), Brown, Bullock, and\r
Grossberg (1999), Contreras-Vidal and Schultz (1999), Suri, Bargas, and Arbib\r
(2001), O’Reilly and Frank (2006), and O’Reilly, Frank, Hazy, and Watz (2007).\r
Joel, Niv, and Ruppin (2002) critically evaluated the anatomical plausibility of\r
several of these models and present an alternative intended to accommodate\r
some neglected features of basal ganglionic circuitry."""

[[sections]]
number = "15.8"
title = "The actor learning rule discussed here is more complicated than the one in the"
text = """
early actor–critic network of Barto et al. (1983). Actor-unit eligibility traces in\r
that network were traces of just At⇥x(St) instead of the full (At⇡(1|St, ✓))x(St).\r
That work did not benefit from the policy-gradient theory presented in Chapter 13\r
or the contributions of Williams (1986, 1992), who showed how an ANN of\r
Bernoulli-logistic units could implement a policy-gradient method.\r
Reynolds and Wickens (2002) proposed a three-factor rule for synaptic plasticity\r
in the corticostriatal pathway in which dopamine modulates changes in corti\u0002costriatal synaptic ecacy. They discussed the experimental support for this\r
kind of learning rule and its possible molecular basis. The definitive demon\u0002stration of spike-timing-dependent plasticity (STDP) is attributed to Markram,\r
L¨ubke, Frotscher, and Sakmann (1997), with evidence from earlier experiments"""

[[sections]]
number = "416"
title = "Chapter 15: Neuroscience"
text = """
by Levy and Steward (1983) and others that the relative timing of pre- and\r
postsynaptic spikes is critical for inducing changes in synaptic ecacy. Rao and\r
Sejnowski (2001) suggested how STDP could be the result of a TD-like mechanism\r
at synapses with non-contingent eligibility traces lasting about 10 milliseconds.\r
Dayan (2002) commented that this would require an error as in Sutton and\r
Barto’s (1981a) early model of classical conditioning and not a true TD error.\r
Representative publications from the extensive literature on reward-modulated\r
STDP are Wickens (1990), Reynolds and Wickens (2002), and Calabresi, Picconi,\r
Tozzi and Di Filippo (2007). Pawlak and Kerr (2008) showed that dopamine is\r
necessary to induce STDP at the corticostriatal synapses of medium spiny neurons.\r
See also Pawlak, Wickens, Kirkwood, and Kerr (2010). Yagishita, Hayashi-Takagi,\r
Ellis-Davies, Urakubo, Ishii, and Kasai (2014) found that dopamine promotes\r
spine enlargement of the medium spiny neurons of mice only during a time window\r
of from 0.3 to 2 seconds after STDP stimulation. Izhikevich (2007) proposed\r
and explored the idea of using STDP timing conditions to trigger contingent\r
eligibility traces. Fr´emaux, Sprekeler, and Gerstner (2010) proposed theoretical\r
conditions for successful learning by rules based on reward-modulated STDP.\r
15.9 Klopf’s hedonistic neuron hypothesis (Klopf 1972, 1982) inspired our actor–critic\r
algorithm implemented as an ANN with a single neuron-like unit, called the\r
actor unit, implementing a Law-of-E↵ect-like learning rule (Barto, Sutton, and\r
Anderson, 1983). Ideas related to Klopf’s synaptically-local eligibility have been\r
proposed by others. Crow (1968) proposed that changes in the synapses of\r
cortical neurons are sensitive to the consequences of neural activity. Emphasizing\r
the need to address the time delay between neural activity and its consequences\r
in a reward-modulated form of synaptic plasticity, he proposed a contingent form\r
of eligibility, but associated with entire neurons instead of individual synapses.\r
According to his hypothesis, a wave of neuronal activity\r
leads to a short-term change in the cells involved in the wave such that they\r
are picked out from a background of cells not so activated. ... such cells are\r
rendered sensitive by the short-term change to a reward signal ... in such\r
a way that if such a signal occurs before the end of the decay time of the\r
change the synaptic connexions between the cells are made more e↵ective.\r
(Crow, 1968)\r
Crow argued against previous proposals that reverberating neural circuits play\r
this role by pointing out that the e↵ect of a reward signal on such a circuit would\r
“...establish the synaptic connexions leading to the reverberation (that is to say,\r
those involved in activity at the time of the reward signal) and not those on the\r
path which led to the adaptive motor output.” Crow further postulated that\r
reward signals are delivered via a “distinct neural fiber system,” presumably the\r
one into which Olds and Milner (1954) tapped, that would transform synaptic\r
connections “from a short into a long-term form.”"""

[[sections]]
number = "15.13"
title = "Summary 417"
text = """
In another farsighted hypothesis, Miller proposed a Law-of-E↵ect-like learning\r
rule that includes synaptically-local contingent eligibility traces:\r
... it is envisaged that in a particular sensory situation neurone B, by chance,\r
fires a ‘meaningful burst’ of activity, which is then translated into motor acts,\r
which then change the situation. It must be supposed that the meaningful\r
burst has an influence, at the neuronal level, on all of its own synapses which\r
are active at the time ... thereby making a preliminary selection of the\r
synapses to be strengthened, though not yet actually strengthening them.\r
...The strengthening signal ... makes the final selection ... and accomplishes\r
the definitive change in the appropriate synapses. (Miller, 1981, p. 81)\r
Miller’s hypothesis also included a critic-like mechanism, which he called a “sen\u0002sory analyzer unit,” that worked according to classical conditioning principles to\r
provide reinforcement signals to neurons so that they would learn to move from\r
lower- to higher-valued states, thus anticipating the use of the TD error as a rein\u0002forcement signal in the actor–critic architecture. Miller’s idea not only parallels\r
Klopf’s (with the exception of its explicit invocation of a distinct “strengthening\r
signal”), it also anticipated the general features of reward-modulated STDP.\r
A related though di↵erent idea, which Seung (2003) called the “hedonistic\r
synapse,” is that synapses individually adjust the probability that they release\r
neurotransmitter in the manner of the Law of E↵ect: if reward follows release,\r
the release probability increases, and decreases if reward follows failure to release.\r
This is essentially the same as the learning scheme Minsky used in his 1954\r
Princeton PhD dissertation, where he called the synapse-like learning element\r
a SNARC (Stochastic Neural-Analog Reinforcement Calculator). Contingent\r
eligibility is involved in these ideas too, although it is contingent on the activity\r
of an individual synapse instead of the postsynaptic neuron. Also related is the\r
proposal of Unnikrishnan and Venugopal (1994) that uses the correlation-based\r
method of Harth and Tzanakou (1974) to adjust ANN weights.\r
Frey and Morris (1997) proposed the idea of a “synaptic tag” for the induction\r
of long-lasting strengthening of synaptic ecacy. Though not unlike Klopf’s\r
eligibility, their tag was hypothesized to consist of a temporary strengthening of a\r
synapse that could be transformed into a long-lasting strengthening by subsequent\r
neuron activation. The model of O’Reilly and Frank (2006) and O’Reilly, Frank,\r
Hazy, and Watz (2007) uses working memory to bridge temporal intervals instead\r
of eligibility traces. Wickens and Kotter (1995) discuss possible mechanisms\r
for synaptic eligibility. He, Huertas, Hong, Tie, Hell, Shouval, Kirkwood (2015)\r
provide evidence supporting the existence of contingent eligibility traces in\r
synapses of cortical neurons with time courses like those of the eligibility traces\r
Klopf postulated.\r
The metaphor of a neuron using a learning rule related to bacterial chemotaxis was\r
discussed by Barto (1989). Koshland’s extensive study of bacterial chemotaxis\r
was in part motivated by similarities between features of bacteria and features of\r
neurons (Koshland, 1980). See also Berg (1975). Shimansky (2009) proposed a"""

[[sections]]
number = "418"
title = "Chapter 15: Neuroscience"
text = """
synaptic learning rule somewhat similar to Seung’s mentioned above in which\r
each synapse individually acts like a chemotactic bacterium. In this case a\r
collection of synapses “swims” toward attractants in the high-dimensional space\r
of synaptic weight values. Montague, Dayan, Person, and Sejnowski (1995)\r
proposed a chemotactic-like model of the bee’s foraging behavior involving the\r
neuromodulator octopamine."""

[[sections]]
number = "15.10"
title = "Research on the behavior of reinforcement learning agents in team and game"
text = """
problems has a long history roughly occurring in three phases. To the best\r
of our knowledge, the first phase began with investigations by the Russian\r
mathematician and physicist M. L. Tsetlin. A collection of his work was published\r
as Tsetlin (1973) after his death in 1966. Our Sections 1.7 and 4.8 refer to his\r
study of learning automata in connection to bandit problems. The Tsetlin\r
collection also includes studies of learning automata in team and game problems,\r
which led to later work in this area using stochastic learning automata as\r
described by Narendra and Thathachar (1974, 1989), Viswanathan and Narendra\r
(1974), Lakshmivarahan and Narendra (1982), Narendra and Wheeler (1983), and\r
Thathachar and Sastry (2002). Thathachar and Sastry (2011) is a more recent\r
comprehensive account. These studies were mostly restricted to non-associative\r
learning automata, meaning that they did not address associative, or contextual,\r
bandit problems (Section 2.9).\r
The second phase began with the extension of learning automata to the associative,\r
or contextual, case. Barto, Sutton, and Brouwer (1981) and Barto and Sutton\r
(1981b) experimented with associative stochastic learning automata in single\u0002layer ANNs to which a global reinforcement signal was broadcast. The learning\r
algorithm was an associative extension of the Alopex algorithm of Harth and\r
Tzanakou (1974). Barto et al. called neuron-like elements implementing this\r
kind of learning associative search elements (ASEs). Barto and Anandan (1985)\r
introduced an associative reinforcement learning algorithm called the associative\r
reward-penalty (ARP) algorithm. They proved a convergence result by combining\r
theory of stochastic learning automata with theory of pattern classification. Barto\r
(1985, 1986) and Barto and Jordan (1987) described results with teams of ARP\r
units connected into multi-layer ANNs, showing that they could learn nonlinear\r
functions, such as XOR and others, with a globally-broadcast reinforcement signal.\r
Barto (1985) extensively discussed this approach to ANNs and how this type of\r
learning rule is related to others in the literature at that time. Williams (1992)\r
mathematically analyzed and broadened this class of learning rules and related\r
their use to the error backpropagation method for training multilayer ANNs.\r
Williams (1988) described several ways that backpropagation and reinforcement\r
learning can be combined for training ANNs. Williams (1992) showed that a\r
special case of the ARP algorithm is a REINFORCE algorithm, although better\r
results were obtained with the general ARP algorithm (Barto, 1985)."""

[[sections]]
number = "15.13"
title = "Summary 419"
text = """
The third phase of interest in teams of reinforcement learning agents was influ\u0002enced by increased understanding of the role of dopamine as a widely broadcast\r
neuromodulator and speculation about the existence of reward-modulated STDP.\r
Much more so than earlier research, this research considers details of synaptic\r
plasticity and other constraints from neuroscience. Publications include the\r
following (chronologically and alphabetically): Bartlett and Baxter (1999, 2000),\r
Xie and Seung (2004), Baras and Meir (2007), Farries and Fairhall (2007), Florian\r
(2007), Izhikevich (2007), Pecevski, Maass, and Legenstein (2008), Legenstein,\r
Pecevski, and Maass (2008), Kolodziejski, Porr, and W¨org¨otter (2009), Urbanczik\r
and Senn (2009), and Vasilaki, Fr´emaux, Urbanczik, Senn, and Gerstner (2009).\r
Now´e, Vrancx, and De Hauwere (2012) reviewed more recent developments in\r
the wider field of multi-agent reinforcement learning\r
15.11 Yin and Knowlton (2006) reviewed findings from outcome-devaluation experi\u0002ments with rodents supporting the view that habitual and goal-directed behavior\r
(as psychologists use the phrase) are respectively most associated with process\u0002ing in the dorsolateral striatum (DLS) and the dorsomedial striatum (DMS).\r
Results of functional imaging experiments with human subjects in the outcome\u0002devaluation setting by Valentin, Dickinson, and O’Doherty (2007) suggest that\r
the orbitofrontal cortex (OFC) is an important component of goal-directed choice.\r
Single unit recordings in monkeys by Padoa-Schioppa and Assad (2006) support\r
the role of the OFC in encoding values guiding choice behavior. Rangel, Camerer,\r
and Montague (2008) and Rangel and Hare (2010) reviewed findings from the\r
perspective of neuroeconomics about how the brain makes goal-directed decisions.\r
Pezzulo, van der Meer, Lansink, and Pennartz (2014) reviewed the neuroscience\r
of internally generated sequences and presented a model of how these mecha\u0002nisms might be components of model-based planning. Daw and Shohamy (2008)\r
proposed that while dopamine signaling connects well to habitual, or model-free,\r
behavior, other processes are involved in goal-directed, or model-based, behavior.\r
Data from experiments by Bromberg-Martin, Matsumoto, Hong, and Hikosaka\r
(2010) indicate that dopamine signals contain information pertinent to both\r
habitual and goal-directed behavior. Doll, Simon, and Daw (2012) argued that\r
there may not a clear separation in the brain between mechanisms that subserve\r
habitual and goal-directed learning and choice."""

[[sections]]
number = "15.12"
title = "Keiflin and Janak (2015) reviewed connections between TD errors and addiction."
text = """
Nutt, Lingford-Hughes, Erritzoe, and Stokes (2015) critically evaluated the\r
hypothesis that addiction is due to a disorder of the dopamine system. Montague,\r
Dolan, Friston, and Dayan (2012) outlined the goals and early e↵orts in the field\r
of computational psychiatry, and Adams, Huys, and Roiser (2015) reviewed more\r
recent progress.

Chapter 16\r
Applications and Case Studies\r
In this chapter we present a few case studies of reinforcement learning. Several of these\r
are substantial applications of potential economic significance. One, Samuel’s checkers\r
player, is primarily of historical interest. Our presentations are intended to illustrate some\r
of the trade-o↵s and issues that arise in real applications. For example, we emphasize how\r
domain knowledge is incorporated into the formulation and solution of the problem. We\r
also highlight the representation issues that are so often critical to successful applications.\r
The algorithms used in some of these case studies are substantially more complex than\r
those we have presented in the rest of the book. Applications of reinforcement learning\r
are still far from routine and typically require as much art as science. Making applications\r
easier and more straightforward is one of the goals of current research in reinforcement\r
learning."""

[[sections]]
number = "16.1"
title = "TD-Gammon"
text = """
One of the most impressive applications of reinforcement learning to date is that by\r
Gerald Tesauro to the game of backgammon (Tesauro, 1992, 1994, 1995, 2002). Tesauro’s\r
program, TD-Gammon, required little backgammon knowledge, yet learned to play\r
extremely well, near the level of the world’s strongest grandmasters. The learning\r
algorithm in TD-Gammon was a straightforward combination of the TD() algorithm\r
and nonlinear function approximation using a multilayer artificial neural network (ANN)\r
trained by backpropagating TD errors.\r
Backgammon is a major game in the sense that it is played throughout the world,\r
with numerous tournaments and regular world championship matches. It is in part a\r
game of chance, and it is a popular vehicle for waging significant sums of money. There\r
are probably more professional backgammon players than there are professional chess\r
players. The game is played with 15 white and 15 black pieces on a board of 24 locations,\r
called points. To the right on the next page is shown a typical position early in the game,\r
seen from the perspective of the white player. White here has just rolled the dice and\r
obtained a 5 and a 2. This means that he can move one of his pieces 5 steps and one"""

[[sections]]
number = "422"
title = "Chapter 16: Applications and Case Studies"
text = """
white pieces move \r
 counterclockwise\r
12 34 5 6 7 8 9 10 11 12\r
192021222324 18 17 16 15 14 13\r
 black pieces \r
move clockwise\r
A backgammon position\r
(possibly the same piece) 2 steps. For\r
example, he could move two pieces from\r
the 12 point, one to the 17 point, and\r
one to the 14 point. White’s objective is\r
to advance all of his pieces into the last\r
quadrant (points 19–24) and then o↵\r
the board. The first player to remove\r
all his pieces wins. One complication\r
is that the pieces interact as they pass\r
each other going in di↵erent directions.\r
For example, if it were black’s move, he\r
could use the dice roll of 2 to move a\r
piece from the 24 point to the 22 point,\r
“hitting” the white piece there. Pieces that have been hit are placed on the “bar” in the\r
middle of the board (where we already see one previously hit black piece), from whence\r
they reenter the race from the start. However, if there are two pieces on a point, then\r
the opponent cannot move to that point; the pieces are protected from being hit. Thus,\r
white cannot use his 5–2 dice roll to move either of his pieces on the 1 point, because\r
their possible resulting points are occupied by groups of black pieces. Forming contiguous\r
blocks of occupied points to block the opponent is one of the elementary strategies of the\r
game.\r
Backgammon involves several further complications, but the above description gives\r
the basic idea. With 30 pieces and 24 possible locations (26, counting the bar and\r
o↵-the-board) it should be clear that the number of possible backgammon positions is\r
enormous, far more than the number of memory elements one could have in any physically\r
realizable computer. The number of moves possible from each position is also large. For a\r
typical dice roll there might be 20 di↵erent ways of playing. In considering future moves,\r
such as the response of the opponent, one must consider the possible dice rolls as well.\r
The result is that the game tree has an e↵ective branching factor of about 400. This is\r
far too large to permit e↵ective use of the conventional heuristic search methods that\r
have proved so e↵ective in games like chess and checkers.\r
On the other hand, the game is a good match to the capabilities of TD learning\r
methods. Although the game is highly stochastic, a complete description of the game’s\r
state is available at all times. The game evolves over a sequence of moves and positions\r
until finally ending in a win for one player or the other, ending the game. The outcome\r
can be interpreted as a final reward to be predicted. On the other hand, the theoretical\r
results we have described so far cannot be usefully applied to this task. The number of\r
states is so large that a lookup table cannot be used, and the opponent is a source of\r
uncertainty and time variation.\r
TD-Gammon used a nonlinear form of TD(). The estimated value, vˆ(s,w), of any\r
state (board position) s was meant to estimate the probability of winning starting from\r
state s. To achieve this, rewards were defined as zero for all time steps except those\r
on which the game is won. To implement the value function, TD-Gammon used a\r
standard multilayer ANN, much like that shown to the right on the next page. (The real"""

[[sections]]
number = "16.1"
title = "TD-Gammon 423"
text = """
Vt+1! Vt\r
hidden units (40-8\r
backgammon position (198 input units)\r
predicted probability\r
of winning, Vt\r
TD error,\r
. . . . . .\r
. . . . . .\r
. . . . . .\r
()\r
general update rule for this case is\r
wt+1 = wt + \r
h\r
Rt+1 + vˆ(St+1,wt)  vˆ(St,wt)\r
i\r
et, (15.1)\r
where wt is the vector of all modifiable parameters (in this case, the weights\r
of the network) and et is a vector of eligibility traces, one for each component\r
of wt, updated by\r
et = et1 + rwt vˆ(St,wt),\r
with e0 = 0. The gradient in this equation can be computed eciently by the\r
backpropagation procedure. For the backgammon application, in which  = 1\r
and the reward is always zero except upon winning, the TD error portion of the\r
learning rule is usually just ˆv(St+1,w)  vˆ(St,w), as suggested in Figure 15.2.\r
To apply the learning rule we need a source of backgammon games. Tesauro\r
obtained an unending sequence of games by playing his learning backgammon\r
player against itself. To choose its moves, TD-Gammon considered each of the\r
20 or so ways it could play its dice roll and the corresponding positions that\r
would result. The resulting positions are afterstates as discussed in Section 6.8.\r
The network was consulted to estimate each of their values. The move was\r
then selected that would lead to the position with the highest estimated value.\r
Continuing in this way, with TD-Gammon making the moves for both sides,\r
it was possible to easily generate large numbers of backgammon games. Each\r
game was treated as an episode, with the sequence of positions acting as\r
the states, S0, S1, S2,.... Tesauro applied the nonlinear TD rule (15.1) fully\r
incrementally, that is, after each individual move.\r
The weights of the network were set initially to small random values. The\r
initial evaluations were thus entirely arbitrary. Since the moves were selected\r
on the basis of these evaluations, the initial moves were inevitably poor, and\r
the initial games often lasted hundreds or thousands of moves before one side\r
or the other won, almost by accident. After a few dozen games however,\r
performance improved rapidly.\r
After playing about 300,000 games against itself, TD-Gammon 0.0 as de\u0002scribed above learned to play approximately as well as the best previous\r
backgammon computer programs. This was a striking result because all the\r
previous high-performance computer programs had used extensive backgam\u0002mon knowledge. For example, the reigning champion program at the time\r
was, arguably, Neurogammon, another program written by Tesauro that used\r
a neural network but not TD learning. Neurogammon’s network was trained\r
on a large training corpus of exemplary moves provided by backgammon ex\u0002perts, and, in addition, started with a set of features specially crafted for\r
TD error\r
+\r
h\r
+\r
(\r
+\r
) ()\r
i\r
(\r
where wt is the vector of all modifiable parameters (in this case, the weigh\r
of the network) and et is a vector of eligibility traces, one for each componen\r
of wt, updated by\r
et = et1 + rwt vˆ(St,wt),\r
with e0 = 0. The gradient in this equation can be computed eciently by th\r
backpropagation procedure. For the backgammon application, in which  = \r
and the reward is always zero except upon winning, the TD error portion of th\r
learning rule is usually just ˆv(St+1,w)  vˆ(St,w), as suggested in Figure 15.\r
To apply the learning rule we need a source of backgammon games. Tesaur\r
obtained an unending sequence of games by playing his learning backgammo\r
player against itself. To choose its moves, TD-Gammon considered each of th\r
20 or so ways it could play its dice roll and the corresponding positions tha\r
would result. The resulting positions are afterstates as discussed in Section 6.\r
The network was consulted to estimate each of their values. The move wa\r
then selected that would lead to the position with the highest estimated valu\r
Continuing in this way, with TD-Gammon making the moves for both side\r
it was possible to easily generate large numbers of backgammon games. Eac\r
game was treated as an episode, with the sequence of positions acting a\r
the states, S0, S1, S2,.... Tesauro applied the nonlinear TD rule (15.1) ful\r
incrementally, that is, after each individual move.\r
The weights of the network were set initially to small random values. Th\r
initial evaluations were thus entirely arbitrary. Since the moves were selecte\r
on the basis of these evaluations, the initial moves were inevitably poor, an\r
the initial games often lasted hundreds or thousands of moves before one sid\r
or the other won, almost by accident. After a few dozen games howeve\r
performance improved rapidly.\r
After playing about 300,000 games against itself, TD-Gammon 0.0 as d\r
scribed above learned to play approximately as well as the best previou\r
backgammon computer programs. This was a striking result because all th\r
previous high-performance computer programs had used extensive backgam\r
mon knowledge. For example, the reigning champion program at the tim\r
was, arguably, Neurogammon, another program written by Tesauro that use\r
a neural network but not TD learning. Neurogammon’s network was traine\r
on a large training corpus of exemplary moves provided by backgammon ex\r
perts, and, in addition, started with a set of features specially crafted fo\r
hidden units\r
(40-80)\r
Figure 16.1: The TD-Gammon ANN\r
network had two additional units in its\r
final layer to estimate the probability of\r
each player’s winning in a special way\r
called a “gammon” or “backgammon.”)\r
The network consisted of a layer of input\r
units, a layer of hidden units, and a final\r
output unit. The input to the network\r
was a representation of a backgammon\r
position, and the output was an estimate\r
of the value of that position.\r
In the first version of TD-Gammon,\r
TD-Gammon 0.0, backgammon posi\u0002tions were represented to the network\r
in a relatively direct way that involved\r
little backgammon knowledge. It did,\r
however, involve substantial knowledge of how ANNs work and how information is best\r
presented to them. It is instructive to note the exact representation Tesauro chose. There\r
were a total of 198 input units to the network. For each point on the backgammon board,\r
four units indicated the number of white pieces on the point. If there were no white\r
pieces, then all four units took on the value zero. If there was one piece, then the first\r
unit took on the value 1. This encoded the elementary concept of a “blot,” i.e., a piece\r
that can be hit by the opponent. If there were two or more pieces, then the second unit\r
was set to 1. This encoded the basic concept of a “made point” on which the opponent\r
cannot land. If there were exactly three pieces on the point, then the third unit was set\r
to 1. This encoded the basic concept of a “single spare,” i.e., an extra piece in addition\r
to the two pieces that made the point. Finally, if there were more than three pieces, the\r
fourth unit was set to a value proportionate to the number of additional pieces beyond\r
three. Letting n denote the total number of pieces on the point, if n > 3, then the fourth\r
unit took on the value (n3)/2. This encoded a linear representation of “multiple spares”\r
at the given point.\r
With four units for white and four for black at each of the 24 points, that made a\r
total of 192 units. Two additional units encoded the number of white and black pieces on\r
the bar (each took the value n/2, where n is the number of pieces on the bar), and two\r
more encoded the number of black and white pieces already successfully removed from\r
the board (these took the value n/15, where n is the number of pieces already borne\r
o↵). Finally, two units indicated in a binary fashion whether it was white’s or black’s\r
turn to move. The general logic behind these choices should be clear. Basically, Tesauro\r
tried to represent the position in a straightforward way, while keeping the number of\r
units relatively small. He provided one unit for each conceptually distinct possibility that\r
seemed likely to be relevant, and he scaled them to roughly the same range, in this case\r
between 0 and 1.\r
Given a representation of a backgammon position, the network computed its estimated\r
value in the standard way. Corresponding to each connection from an input unit to a\r
hidden unit was a real-valued weight. Signals from each input unit were multiplied by"""

[[sections]]
number = "424"
title = "Chapter 16: Applications and Case Studies"
text = """
their corresponding weights and summed at the hidden unit. The output, h(j), of hidden\r
unit j was a nonlinear sigmoid function of the weighted sum:\r
h(j) = \r
 X\r
i\r
wijxi\r
!\r
= 1\r
1 + eP\r
i wijxi\r
,\r
where xi is the value of the ith input unit and wij is the weight of its connection to the\r
jth hidden unit (all the weights in the network together make up the parameter vector w).\r
The output of the sigmoid is always between 0 and 1, and has a natural interpretation as\r
a probability based on a summation of evidence. The computation from hidden units\r
to the output unit was entirely analogous. Each connection from a hidden unit to the\r
output unit had a separate weight. The output unit formed the weighted sum and then\r
passed it through the same sigmoid nonlinearity.\r
TD-Gammon used the semi-gradient form of the TD() algorithm described in Sec\u0002tion 12.2, with the gradients computed by the error backpropagation algorithm (Rumel\u0002hart, Hinton, and Williams, 1986). Recall that the general update rule for this case is\r
wt+1\r
.\r
= wt + ↵\r
h\r
Rt+1 + vˆ(St+1,wt)  vˆ(St,wt)\r
i\r
zt, (16.1)\r
where wt is the vector of all modifiable parameters (in this case, the weights of the\r
network) and zt is a vector of eligibility traces, one for each component of wt, updated by\r
zt\r
.\r
= zt1 + rvˆ(St,wt),\r
with z0\r
.\r
= 0. The gradient in this equation can be computed eciently by the backpropa\u0002gation procedure. For the backgammon application, in which  = 1 and the reward is\r
always zero except upon winning, the TD error portion of the learning rule is usually\r
just ˆv(St+1,w)  vˆ(St,w), as suggested in Figure 16.1.\r
To apply the learning rule we need a source of backgammon games. Tesauro obtained\r
an unending sequence of games by playing his learning backgammon player against itself.\r
To choose its moves, TD-Gammon considered each of the 20 or so ways it could play\r
its dice roll and the corresponding positions that would result. The resulting positions\r
are afterstates as discussed in Section 6.8. The network was consulted to estimate each\r
of their values. The move was then selected that would lead to the position with the\r
highest estimated value. Continuing in this way, with TD-Gammon making the moves\r
for both sides, it was possible to easily generate large numbers of backgammon games.\r
Each game was treated as an episode, with the sequence of positions acting as the states,\r
S0, S1, S2,.... Tesauro applied the nonlinear TD rule (16.1) fully incrementally, that is,\r
after each individual move.\r
The weights of the network were set initially to small random values. The initial\r
evaluations were thus entirely arbitrary. Because the moves were selected on the basis\r
of these evaluations, the initial moves were inevitably poor, and the initial games often\r
lasted hundreds or thousands of moves before one side or the other won, almost by\r
accident. After a few dozen games however, performance improved rapidly.\r
After playing about 300,000 games against itself, TD-Gammon 0.0 as described above\r
learned to play approximately as well as the best previous backgammon computer"""

[[sections]]
number = "16.1"
title = "TD-Gammon 425"
text = """
programs. This was a striking result because all the previous high-performance computer\r
programs had used extensive backgammon knowledge. For example, the reigning champion\r
program at the time was, arguably, Neurogammon, another program written by Tesauro\r
that used an ANN but not TD learning. Neurogammon’s network was trained on a\r
large training corpus of exemplary moves provided by backgammon experts, and, in\r
addition, started with a set of features specially crafted for backgammon. Neurogammon\r
was a highly tuned, highly e↵ective backgammon program that decisively won the\r
World Backgammon Olympiad in 1989. TD-Gammon 0.0, on the other hand, was\r
constructed with essentially zero backgammon knowledge. That it was able to do as\r
well as Neurogammon and all other approaches is striking testimony to the potential of\r
self-play learning methods.\r
The tournament success of TD-Gammon 0.0 with zero expert backgammon knowledge\r
suggested an obvious modification: add the specialized backgammon features but keep\r
the self-play TD learning method. This produced TD-Gammon 1.0. TD-Gammon 1.0 was\r
clearly substantially better than all previous backgammon programs and found serious\r
competition only among human experts. Later versions of the program, TD-Gammon 2.0\r
(40 hidden units) and TD-Gammon 2.1 (80 hidden units), were augmented with a selective\r
two-ply search procedure. To select moves, these programs looked ahead not just to the\r
positions that would immediately result, but also to the opponent’s possible dice rolls\r
and moves. Assuming the opponent always took the move that appeared immediately\r
best for him, the expected value of each candidate move was computed and the best\r
was selected. To save computer time, the second ply of search was conducted only for\r
candidate moves that were ranked highly after the first ply, about four or five moves on\r
average. Two-ply search a↵ected only the moves selected; the learning process proceeded\r
exactly as before. The final versions of the program, TD-Gammon 3.0 and 3.1, used 160\r
hidden units and a selective three-ply search. TD-Gammon illustrates the combination\r
of learned value functions and decision-time search as in heuristic search and MCTS\r
methods. In follow-on work, Tesauro and Galperin (1997) explored trajectory sampling\r
methods as an alternative to full-width search, which reduced the error rate of live play\r
by large numerical factors (4x–6x) while keeping the think time reasonable at ⇠5–10\r
seconds per move.\r
During the 1990s, Tesauro was able to play his programs in a significant number of\r
games against world-class human players. A summary of the results is given in Table 16.1.\r
Program Hidden Training Opponents Results\r
Units Games\r
TD-Gammon 0.0 40 300,000 other programs tied for best\r
TD-Gammon 1.0 80 300,000 Robertie, Magriel, ... 13 pts / 51 games\r
TD-Gammon 2.0 40 800,000 various Grandmasters 7 pts / 38 games\r
TD-Gammon 2.1 80 1,500,000 Robertie 1 pt / 40 games\r
TD-Gammon 3.0 80 1,500,000 Kazaros +6 pts / 20 games\r
Table 16.1: Summary of TD-Gammon Results"""

[[sections]]
number = "426"
title = "Chapter 16: Applications and Case Studies"
text = """
Based on these results and analyses by backgammon grandmasters (Robertie, 1992; see\r
Tesauro, 1995), TD-Gammon 3.0 appeared to play at close to, or possibly better than,\r
the playing strength of the best human players in the world. Tesauro reported in a\r
subsequent article (Tesauro, 2002) the results of an extensive rollout analysis of the move\r
decisions and doubling decisions of TD-Gammon relative to top human players. The\r
conclusion was that TD-Gammon 3.1 had a “lopsided advantage” in piece-movement\r
decisions, and a “slight edge” in doubling decisions, over top humans.\r
TD-Gammon had a significant impact on the way the best human players play the\r
game. For example, it learned to play certain opening positions di↵erently than was\r
the convention among the best human players. Based on TD-Gammon’s success and\r
further analysis, the best human players now play these positions as TD-Gammon does\r
(Tesauro, 1995). The impact on human play was greatly accelerated when several other\r
self-teaching ANN backgammon programs inspired by TD-Gammon, such as Jellyfish,\r
Snowie, and GNUBackgammon, became widely available. These programs enabled wide\r
dissemination of new knowledge generated by the ANNs, resulting in great improvements\r
in the overall caliber of human tournament play (Tesauro, 2002)."""

[[sections]]
number = "16.2"
title = "Samuel’s Checkers Player"
text = """
An important precursor to Tesauro’s TD-Gammon was the seminal work of Arthur Samuel\r
(1959, 1967) in constructing programs for learning to play checkers. Samuel was one of\r
the first to make e↵ective use of heuristic search methods and of what we would now call\r
temporal-di↵erence learning. His checkers players are instructive case studies in addition\r
to being of historical interest. We emphasize the relationship of Samuel’s methods to\r
modern reinforcement learning methods and try to convey some of Samuel’s motivation\r
for using them.\r
Samuel first wrote a checkers-playing program for the IBM 701 in 1952. His first\r
learning program was completed in 1955 and was demonstrated on television in 1956.\r
Later versions of the program achieved good, though not expert, playing skill. Samuel\r
was attracted to game-playing as a domain for studying machine learning because games\r
are less complicated than problems “taken from life” while still allowing fruitful study of\r
how heuristic procedures and learning can be used together. He chose to study checkers\r
instead of chess because its relative simplicity made it possible to focus more strongly on\r
learning.\r
Samuel’s programs played by performing a lookahead search from each current position.\r
They used what we now call heuristic search methods to determine how to expand the\r
search tree and when to stop searching. The terminal board positions of each search were\r
evaluated, or “scored,” by a value function, or “scoring polynomial,” using linear function\r
approximation. In this and other respects Samuel’s work seems to have been inspired\r
by the suggestions of Shannon (1950). In particular, Samuel’s program was based on\r
Shannon’s minimax procedure to find the best move from the current position. Working\r
backward through the search tree from the scored terminal positions, each position was\r
given the score of the position that would result from the best move, assuming that the\r
machine would always try to maximize the score, while the opponent would always try to"""

[[sections]]
number = "16.2"
title = "Samuel’s Checkers Player 427"
text = """
minimize it. Samuel called this the “backed-up score” of the position. When the minimax\r
procedure reached the search tree’s root—the current position—it yielded the best move\r
under the assumption that the opponent would be using the same evaluation criterion,\r
shifted to its point of view. Some versions of Samuel’s programs used sophisticated search\r
control methods analogous to what are known as “alpha-beta” cuto↵s (e.g., see Pearl,\r
1984).\r
Samuel used two main learning methods, the simplest of which he called rote learning.\r
It consisted simply of saving a description of each board position encountered during play\r
together with its backed-up value determined by the minimax procedure. The result was\r
that if a position that had already been encountered were to occur again as a terminal\r
position of a search tree, the depth of the search was e↵ectively amplified because this\r
position’s stored value cached the results of one or more searches conducted earlier. One\r
initial problem was that the program was not encouraged to move along the most direct\r
path to a win. Samuel gave it a “a sense of direction” by decreasing a position’s value\r
a small amount each time it was backed up a level (called a ply) during the minimax\r
analysis. “If the program is now faced with a choice of board positions whose scores\r
di↵er only by the ply number, it will automatically make the most advantageous choice,\r
choosing a low-ply alternative if winning and a high-ply alternative if losing” (Samuel,\r
1959, p. 80). Samuel found this discounting-like technique essential to successful learning.\r
Rote learning produced slow but continual improvement that was most e↵ective for\r
opening and endgame play. His program became a “better-than-average novice” after\r
learning from many games against itself, a variety of human opponents, and from book\r
games in a supervised learning mode.\r
Rote learning and other aspects of Samuel’s work strongly suggest the essential idea\r
of temporal-di↵erence learning—that the value of a state should equal the value of\r
likely following states. Samuel came closest to this idea in his second learning method,\r
his “learning by generalization” procedure for modifying the parameters of the value\r
function. Samuel’s method was the same in concept as that used much later by Tesauro\r
in TD-Gammon. He played his program many games against another version of itself and\r
performed an update after each move. The idea of Samuel’s update is suggested by the\r
backup diagram in Figure 16.2. Each open circle represents a position where the program\r
moves next, an on-move position, and each solid circle represents a position where the\r
opponent moves next. An update was made to the value of each on-move position after a\r
move by each side, resulting in a second on-move position. The update was toward the\r
minimax value of a search launched from the second on-move position. Thus, the overall\r
e↵ect was that of a backing-up over one full move of real events and then a search over\r
possible events, as suggested by Figure 16.2. Samuel’s actual algorithm was significantly\r
more complex than this for computational reasons, but this was the basic idea.\r
Samuel did not include explicit rewards. Instead, he fixed the weight of the most\r
important feature, the piece advantage feature, which measured the number of pieces\r
the program had relative to how many its opponent had, giving higher weight to kings,\r
and including refinements so that it was better to trade pieces when winning than when\r
losing. Thus, the goal of Samuel’s program was to improve its piece advantage, which in\r
checkers is highly correlated with winning."""

[[sections]]
number = "428"
title = "Chapter 16: Applications and Case Studies"
text = """
hypothetical events\r
actual events\r
backup\r
Figure 16.2: The backup diagram for Samuel’s checkers player.\r
However, Samuel’s learning method may have been missing an essential part of a sound\r
temporal-di↵erence algorithm. Temporal-di↵erence learning can be viewed as a way of\r
making a value function consistent with itself, and this we can clearly see in Samuel’s\r
method. But also needed is a way of tying the value function to the true value of the\r
states. We have enforced this via rewards and by discounting or giving a fixed value to\r
the terminal state. But Samuel’s method included no rewards and no special treatment of\r
the terminal positions of games. As Samuel himself pointed out, his value function could\r
have become consistent merely by giving a constant value to all positions. He hoped to\r
discourage such solutions by giving his piece-advantage term a large, nonmodifiable weight.\r
But although this may decrease the likelihood of finding useless evaluation functions,\r
it does not prohibit them. For example, a constant function could still be attained by\r
setting the modifiable weights so as to cancel the e↵ect of the nonmodifiable one.\r
Because Samuel’s learning procedure was not constrained to find useful evaluation\r
functions, it should have been possible for it to become worse with experience. In fact,\r
Samuel reported observing this during extensive self-play training sessions. To get the\r
program improving again, Samuel had to intervene and set the weight with the largest\r
absolute value back to zero. His interpretation was that this drastic intervention jarred\r
the program out of local optima, but another possibility is that it jarred the program out\r
of evaluation functions that were consistent but had little to do with winning or losing\r
the game.\r
Despite these potential problems, Samuel’s checkers player using the generalization\r
learning method approached “better-than-average” play. Fairly good amateur opponents\r
characterized it as “tricky but beatable” (Samuel, 1959). In contrast to the rote-learning\r
version, this version was able to develop a good middle game but remained weak in\r
opening and endgame play. This program also included an ability to search through sets of\r
features to find those that were most useful in forming the value function. A later version\r
(Samuel, 1967) included refinements in its search procedure, such as alpha-beta pruning,"""

[[sections]]
number = "16.3"
title = "Watson’s Daily-Double Wagering 429"
text = """
extensive use of a supervised learning mode called “book learning,” and hierarchical\r
lookup tables called signature tables (Grith, 1966) to represent the value function\r
instead of linear function approximation. This version learned to play much better than\r
the 1959 program, though still not at a master level. Samuel’s checkers-playing program\r
was widely recognized as a significant achievement in artificial intelligence and machine\r
learning."""

[[sections]]
number = "16.3"
title = "Watson’s Daily-Double Wagering"
text = """
IBM Watson1 is the system developed by a team of IBM researchers to play the popular\r
TV quiz show Jeopardy!."""

[[sections]]
number = "2"
title = "It gained fame in 2011 by winning first prize in an exhibition"
text = """
match against human champions. Although the main technical achievement demonstrated\r
by Watson was its ability to quickly and accurately answer natural language questions\r
over broad areas of general knowledge, its winning Jeopardy! performance also relied on\r
sophisticated decision-making strategies for critical parts of the game. Tesauro, Gondek,\r
Lechner, Fan, and Prager (2012, 2013) adapted Tesauro’s TD-Gammon system described\r
above to create the strategy used by Watson in “Daily-Double” (DD) wagering in its\r
celebrated winning performance against human champions. These authors report that the\r
e↵ectiveness of this wagering strategy went well beyond what human players are able to\r
do in live game play, and that it, along with other advanced strategies, was an important\r
contributor to Watson’s impressive winning performance. Here we focus only on DD\r
wagering because it is the component of Watson that owes the most to reinforcement\r
learning.\r
Jeopardy! is played by three contestants who face a board showing 30 squares, each of\r
which hides a clue and has a dollar value. The squares are arranged in six columns, each\r
corresponding to a di↵erent category. A contestant selects a square, the host reads the\r
square’s clue, and each contestant may choose to respond to the clue by sounding a buzzer\r
(“buzzing in”). The first contestant to buzz in gets to try responding to the clue. If this\r
contestant’s response is correct, their score increases by the dollar value of the square; if\r
their response is not correct, or if they do not respond within five seconds, their score\r
decreases by that amount, and the other contestants get a chance to buzz in to respond\r
to the same clue. One or two squares (depending on the game’s current round) are\r
special DD squares. A contestant who selects one of these gets an exclusive opportunity\r
to respond to the square’s clue and has to decide—before the clue is revealed—on how\r
much to wager, or bet. The bet has to be greater than five dollars but not greater than\r
the contestant’s current score. If the contestant responds correctly to the DD clue, their\r
score increases by the bet amount; otherwise it decreases by the bet amount. At the end\r
of each game is a “Final Jeopardy” (FJ) round in which each contestant writes down\r
a sealed bet and then writes an answer after the clue is read. The contestant with the\r
highest score after three rounds of play (where a round consists of revealing all 30 clues)\r
is the winner. The game has many other details, but these are enough to appreciate\r
1Registered trademark of IBM Corp.\r
2Registered trademark of Jeopardy Productions Inc."""

[[sections]]
number = "430"
title = "Chapter 16: Applications and Case Studies"
text = """
the importance of DD wagering. Winning or losing often depends on a contestant’s DD\r
wagering strategy.\r
Whenever Watson selected a DD square, it chose its bet by comparing action values,\r
qˆ(s, bet), that estimated the probability of a win from the current game state, s, for\r
each round-dollar legal bet. Except for some risk-abatement measures described below,\r
Watson selected the bet with the maximum action value. Action values were computed\r
whenever a betting decision was needed by using two types of estimates that were learned\r
before any live game play took place. The first were estimated values of the afterstates\r
(Section 6.8) that would result from selecting each legal bet. These estimates were obtained\r
from a state-value function, vˆ(·,w), defined by parameters w, that gave estimates of the\r
probability of a win for Watson from any game state. The second estimates used to\r
compute action values gave the “in-category DD confidence,” pDD, which estimated the\r
likelihood that Watson would respond correctly to the as-yet unrevealed DD clue.\r
Tesauro et al. used the reinforcement learning approach of TD-Gammon described above\r
to learn vˆ(·,w): a straightforward combination of nonlinear TD() using a multilayer\r
ANN with weights w trained by backpropagating TD errors during many simulated\r
games. States were represented to the network by feature vectors specifically designed\r
for Jeopardy!. Features included the current scores of the three players, how many DDs\r
remained, the total dollar value of the remaining clues, and other information related to\r
the amount of play left in the game. Unlike TD-Gammon, which learned by self-play,\r
Watson’s vˆ was learned over millions of simulated games against carefully-crafted models\r
of human players. In-category confidence estimates were conditioned on the number of\r
right responses r and wrong responses w that Watson gave in previously-played clues in\r
the current category. The dependencies on (r, w) were estimated from Watson’s actual\r
accuracies over many thousands of historical categories.\r
With the previously learned value function vˆ and in-category DD confidence pDD,\r
Watson computed ˆq(s, bet) for each legal round-dollar bet as follows:\r
qˆ(s, bet) = pDD ⇥ vˆ(SW + bet,. . .) + (1  pDD) ⇥ vˆ(SW  bet,. . .), (16.2)\r
where SW is Watson’s current score, and vˆ gives the estimated value for the game state\r
after Watson’s response to the DD clue, which is either correct or incorrect. Computing\r
an action value this way corresponds to the insight from Exercise 3.19 that an action\r
value is the expected next state value given the action (except that here it is the expected\r
next afterstate value because the full next state of the entire game depends on the next\r
square selection).\r
Tesauro et al. found that selecting bets by maximizing action values incurred “a\r
frightening amount of risk,” meaning that if Watson’s response to the clue happened to\r
be wrong, the loss could be disastrous for its chances of winning. To decrease the downside\r
risk of a wrong answer, Tesauro et al. adjusted (16.2) by subtracting a small fraction of\r
the standard deviation over Watson’s correct/incorrect afterstate evaluations. They\r
further reduced risk by prohibiting bets that would cause the wrong-answer afterstate\r
value to decrease below a certain limit. These measures slightly reduced Watson’s\r
expectation of winning, but they significantly reduced downside risk, not only in terms of\r
average risk per DD bet, but even more so in extreme-risk scenarios where a risk-neutral\r
Watson would bet most or all of its bankroll."""

[[sections]]
number = "16.3"
title = "Watson’s Daily-Double Wagering 431"
text = """
Why was the TD-Gammon method of self-play not used to learn the critical value\r
function vˆ? Learning from self-play in Jeopardy! would not have worked very well\r
because Watson was so di↵erent from any human contestant. Self-play would have led to\r
exploration of state space regions that are not typical for play against human opponents,\r
particularly human champions. In addition, unlike backgammon, Jeopardy! is a game\r
of imperfect information because contestants do not have access to all the information\r
influencing their opponents’ play. In particular, Jeopardy! contestants do not know how\r
much confidence their opponents have for responding to clues in the various categories.\r
Self-play would have been something like playing poker with someone who is holding the\r
same cards that you hold.\r
As a result of these complications, much of the e↵ort in developing Watson’s DD\u0002wagering strategy was devoted to creating good models of human opponents. The models\r
did not address the natural language aspect of the game, but were instead stochastic\r
process models of events that can occur during play. Statistics were extracted from an\r
extensive fan-created archive of game information from the beginning of the show to the\r
present day. The archive includes information such as the ordering of the clues, right and\r
wrong contestant answers, DD locations, and DD and FJ bets for nearly 300,000 clues.\r
Three models were constructed: an Average Contestant model (based on all the data), a\r
Champion model (based on statistics from games with the 100 best players), and a Grand\r
Champion model (based on statistics from games with the 10 best players). In addition\r
to serving as opponents during learning, the models were used to assess the benefits\r
produced by the learned DD-wagering strategy. Watson’s win rate in simulation when it\r
used a baseline heuristic DD-wagering strategy was 61%; when it used the learned values\r
and a default confidence value, its win rate increased to 64%; and with live in-category\r
confidence, it was 67%. Tesauro et al. regarded this as a significant improvement, given\r
that the DD wagering was needed only about 1.5 to 2 times in each game.\r
Because Watson had only a few seconds to bet, as well as to select squares and decide\r
whether or not to buzz in, the computation time needed to make these decisions was\r
a critical factor. The ANN implementation of vˆ allowed DD bets to be made quickly\r
enough to meet the time constraints of live play. However, once games could be simulated\r
fast enough through improvements in the simulation software, near the end of a game\r
it was feasible to estimate the value of bets by averaging over many Monte-Carlo trials\r
in which the consequence of each bet was determined by simulating play to the game’s\r
end. Selecting endgame DD bets in live play based on Monte-Carlo trials instead of the\r
ANN significantly improved Watson’s performance because errors in value estimates\r
in endgames could seriously a↵ect its chances of winning. Making all the decisions via\r
Monte-Carlo trials might have led to better wagering decisions, but this was simply\r
impossible given the complexity of the game and the time constraints of live play.\r
Although its ability to quickly and accurately answer natural language questions\r
stands out as Watson’s major achievement, all of its sophisticated decision strategies\r
contributed to its impressive defeat of human champions. According to Tesauro et al.\r
(2012):\r
... it is plainly evident that our strategy algorithms achieve a level of quanti\u0002tative precision and real-time performance that exceeds human capabilities."""

[[sections]]
number = "432"
title = "Chapter 16: Applications and Case Studies"
text = """
This is particularly true in the cases of DD wagering and endgame buzzing,\r
where humans simply cannot come close to matching the precise equity and\r
confidence estimates and complex decision calculations performed by Watson."""

[[sections]]
number = "16.4"
title = "Optimizing Memory Control"
text = """
Most computers use dynamic random access memory (DRAM) as their main memory\r
because of its low cost and high capacity. The job of a DRAM memory controller is to\r
eciently use the interface between the processor chip and an o↵-chip DRAM system\r
to provide the high-bandwidth and low-latency data transfer necessary for high-speed\r
program execution. A memory controller needs to deal with dynamically changing\r
patterns of read/write requests while adhering to a large number of timing and resource\r
constraints required by the hardware. This is a formidable scheduling problem, especially\r
with modern processors with multiple cores sharing the same DRAM.\r
˙\r
Ipek, Mutlu, Mart´ınez, and Caruana (2008) (also Mart´ınez and ˙Ipek, 2009) designed\r
a reinforcement learning memory controller and demonstrated that it can significantly\r
improve the speed of program execution over what was possible with conventional\r
controllers at the time of their research. They were motivated by limitations of existing\r
state-of-the-art controllers that used policies that did not take advantage of past scheduling\r
experience and did not account for long-term consequences of scheduling decisions. ˙Ipek\r
et al.’s project was carried out by means of simulation, but they designed the controller\r
at the detailed level of the hardware needed to implement it—including the learning\r
algorithm—directly on a processor chip.\r
Accessing DRAM involves a number of steps that have to be done according to strict\r
time constraints. DRAM systems consist of multiple DRAM chips, each containing\r
multiple rectangular arrays of storage cells arranged in rows and columns. Each cell\r
stores a bit as the charge on a capacitor. Because the charge decreases over time, each\r
DRAM cell needs to be recharged—refreshed—every few milliseconds to prevent memory\r
content from being lost. This need to refresh the cells is why DRAM is called “dynamic.”\r
Each cell array has a row bu↵er that holds a row of bits that can be transferred into\r
or out of one of the array’s rows. An activate command “opens a row,” which means\r
moving the contents of the row whose address is indicated by the command into the\r
row bu↵er. With a row open, the controller can issue read and write commands to the\r
cell array. Each read command transfers a word (a short sequence of consecutive bits)\r
in the row bu↵er to the external data bus, and each write command transfers a word\r
in the external data bus to the row bu↵er. Before a di↵erent row can be opened, a\r
precharge command must be issued which transfers the (possibly updated) data in the\r
row bu↵er back into the addressed row of the cell array. After this, another activate\r
command can open a new row to be accessed. Read and write commands are column\r
commands because they sequentially transfer bits into or out of columns of the row bu↵er;\r
multiple bits can be transferred without re-opening the row. Read and write commands\r
to the currently-open row can be carried out more quickly than accessing a di↵erent row,\r
which would involve additional row commands: precharge and activate; this is sometimes"""

[[sections]]
number = "16.4"
title = "Optimizing Memory Control 433"
text = """
referred to as “row locality.” A memory controller maintains a memory transaction queue\r
that stores memory-access requests from the processors sharing the memory system. The\r
controller has to process requests by issuing commands to the memory system while\r
adhering to a large number of timing constraints.\r
A controller’s policy for scheduling access requests can have a large e↵ect on the\r
performance of the memory system, such as the average latency with which requests\r
can be satisfied and the throughput the system is capable of achieving. The simplest\r
scheduling strategy handles access requests in the order in which they arrive by issuing\r
all the commands required by the request before beginning to service the next one. But if\r
the system is not ready for one of these commands, or executing a command would result\r
in resources being underutilized (e.g., due to timing constraints arising from servicing\r
that one command), it makes sense to begin servicing a newer request before finishing\r
the older one. Policies can gain eciency by reordering requests, for example, by giving\r
priority to read requests over write requests, or by giving priority to read/write commands\r
to already open rows. The policy called First-Ready, First-Come-First-Serve (FR-FCFS),\r
gives priority to column commands (read and write) over row commands (activate and\r
precharge), and in case of a tie gives priority to the oldest command. FR-FCFS was\r
shown to outperform other scheduling policies in terms of average memory-access latency\r
under conditions commonly encountered (Rixner, 2004).\r
Figure 16.3 is a high-level view of ˙Ipek et al.’s reinforcement learning memory controller.\r
They modeled the DRAM access process as an MDP whose states are the contents of the\r
transaction queue and whose actions are commands to the DRAM system: precharge,\r
activate, read, write, and NoOp. The reward signal is 1 whenever the action is read or\r
write, and otherwise it is 0. State transitions were considered to be stochastic because\r
the next state of the system not only depends on the scheduler’s command, but also on\r
aspects of the system’s behavior that the scheduler cannot control, such as the workloads\r
of the processor cores accessing the DRAM system.\r
Figure 16.3: High-level view of the reinforcement learning DRAM controller. The scheduler is\r
the reinforcement learning agent. Its environment is represented by features of the transaction\r
queue, and its actions are commands to the DRAM system. ©2009 IEEE. Reprinted, with\r
permission, from J. F. Mart´ınez and E. ˙Ipek, Dynamic multicore resource management: A\r
machine learning approach, Micro, IEEE, 29(5), p. 12."""

[[sections]]
number = "434"
title = "Chapter 16: Applications and Case Studies"
text = """
Critical to this MDP are constraints on the actions available in each state. Recall from\r
Chapter 3 that the set of available actions can depend on the state: At 2 A(St), where\r
At is the action at time step t and A(St) is the set of actions available in state St. In\r
this application, the integrity of the DRAM system was assured by not allowing actions\r
that would violate timing or resource constraints. Although ˙Ipek et al. did not make it\r
explicit, they e↵ectively accomplished this by pre-defining the sets A(St) for all possible\r
states St.\r
These constraints explain why the MDP has a NoOp action and why the reward signal\r
is 0 except when a read or write command is issued. NoOp is issued when it is the sole\r
legal action in a state. To maximize utilization of the memory system, the controller’s\r
task is to drive the system to states in which either a read or a write action can be\r
selected: only these actions result in sending data over the external data bus, so it is only\r
these that contribute to the throughput of the system. Although precharge and activate\r
produce no immediate reward, the agent needs to select these actions to make it possible\r
to later select the rewarded read and write actions.\r
The scheduling agent used Sarsa (Section 6.4) to learn an action-value function. States\r
were represented by six integer-valued features. To approximate the action-value function,\r
the algorithm used linear function approximation implemented by tile coding with hashing\r
(Section 9.5.4). The tile coding had 32 tilings, each storing 256 action values as 16-bit\r
fixed point numbers. Exploration was "-greedy with " = 0.05.\r
State features included the number of read requests in the transaction queue, the\r
number of write requests in the transaction queue, the number of write requests in the\r
transaction queue waiting for their row to be opened, and the number of read requests in\r
the transaction queue waiting for their row to be opened that are the oldest issued by\r
their requesting processors. (The other features depended on how the DRAM interacts\r
with cache memory, details we omit here.) The selection of the state features was based\r
on ˙Ipek et al.’s understanding of factors that impact DRAM performance. For example,\r
balancing the rate of servicing reads and writes based on how many of each are in the\r
transaction queue can help avoid stalling the DRAM system’s interaction with cache\r
memory. The authors in fact generated a relatively long list of potential features, and\r
then pared them down to a handful using simulations guided by stepwise feature selection.\r
An interesting aspect of this formulation of the scheduling problem as an MDP is\r
that the features input to the tile coding for defining the action-value function were\r
di↵erent from the features used to specify the action-constraint sets A(St). Whereas the\r
tile coding input was derived from the contents of the transaction queue, the constraint\r
sets depended on a host of other features related to timing and resource constraints that\r
had to be satisfied by the hardware implementation of the entire system. In this way, the\r
action constraints ensured that the learning algorithm’s exploration could not endanger\r
the integrity of the physical system, while learning was e↵ectively limited to a “safe”\r
region of the much larger state space of the hardware implementation.\r
Because an objective of this work was that the learning controller could be implemented\r
on a chip so that learning could occur online while a computer is running, hardware\r
implementation details were important considerations. The design included two five-stage\r
pipelines to calculate and compare two action values at every processor clock cycle, and"""

[[sections]]
number = "16.4"
title = "Optimizing Memory Control 435"
text = """
to update the appropriate action value. This included accessing the tile coding which\r
was stored on-chip in static RAM. For the configuration ˙Ipek et al. simulated, which was\r
a 4GHz 4-core chip typical of high-end workstations at the time of their research, there\r
were 10 processor cycles for every DRAM cycle. Considering the cycles needed to fill\r
the pipes, up to 12 actions could be evaluated in each DRAM cycle. ˙Ipek et al. found\r
that the number of legal commands for any state was rarely greater than this, and that\r
performance loss was negligible if enough time was not always available to consider all\r
legal commands. These and other clever design details made it feasible to implement the\r
complete controller and learning algorithm on a multi-processor chip.\r
˙\r
Ipek et al. evaluated their learning controller in simulation by comparing it with three\r
other controllers: (1) the FR-FCFS controller mentioned above that produces the best\r
on-average performance, (2) a conventional controller that processes each request in\r
order, and (3) an unrealizable ideal controller, called the Optimistic controller, able\r
to sustain 100% DRAM throughput if given enough demand by ignoring all timing\r
and resource constraints, but otherwise modeling DRAM latency (as row bu↵er hits)\r
and bandwidth. They simulated nine memory-intensive parallel workloads consisting of\r
scientific and data-mining applications. Figure 16.4 shows the performance (the inverse\r
of execution time normalized to the performance of FR-FCFS) of each controller for\r
the nine applications, together with the geometric mean of their performances over the\r
applications. The learning controller, labeled RL in the figure, improved over that of\r
FR-FCFS by from 7% to 33% over the nine applications, with an average improvement of\r
19%. Of course, no realizable controller can match the performance of Optimistic, which\r
ignores all timing and resource constraints, but the learning controller’s performance\r
closed the gap with Optimistic’s upper bound by an impressive 27%.\r
Because the rationale for on-chip implementation of the learning algorithm was to\r
allow the scheduling policy to adapt online to changing workloads, ˙Ipek et al. analyzed\r
the impact of online learning compared to a previously-learned fixed policy. They trained\r
Figure 16.4: Performances of four controllers over a suite of 9 simulated benchmark applications.\r
The controllers are: the simplest ‘in-order’ controller, FR-FCFS, the learning controller RL,\r
and the unrealizable Optimistic controller which ignores all timing and resource constraints to\r
provide a performance upper bound. Performance, normalized to that of FR-FCFS, is the inverse\r
of execution time. At far right is the geometric mean of performances over the 9 benchmark\r
applications for each controller. Controller RL comes closest to the ideal performance. ©2009\r
IEEE. Reprinted, with permission, from J. F. Mart´ınez and E. ˙Ipek, Dynamic multicore resource\r
management: A machine learning approach, Micro, IEEE, 29(5), p. 13."""

[[sections]]
number = "436"
title = "Chapter 16: Applications and Case Studies"
text = """
their controller with data from all nine benchmark applications and then held the resulting\r
action values fixed throughout the simulated execution of the applications. They found\r
that the average performance of the controller that learned online was 8% better than\r
that of the controller using the fixed policy, leading them to conclude that online learning\r
is an important feature of their approach.\r
This learning memory controller was never committed to physical hardware because of\r
the large cost of fabrication. Nevertheless, ˙Ipek et al. could convincingly argue on the basis\r
of their simulation results that a memory controller that learns online via reinforcement\r
learning has the potential to improve performance to levels that would otherwise require\r
more complex and more expensive memory systems, while removing from human designers\r
some of the burden required to manually design ecient scheduling policies. Mukundan\r
and Mart´ınez (2012) took this project forward by investigating learning controllers with\r
additional actions, other performance criteria, and more complex reward functions derived\r
using genetic algorithms. They considered additional performance criteria related to\r
energy eciency. The results of these studies surpassed the earlier results described\r
above and significantly surpassed the 2012 state-of-the-art for all of the performance\r
criteria they considered. The approach is especially promising for developing sophisticated\r
power-aware DRAM interfaces."""

[[sections]]
number = "16.5"
title = "Human-level Video Game Play"
text = """
One of the greatest challenges in applying reinforcement learning to real-world problems\r
is deciding how to represent and store value functions and/or policies. Unless the state\r
set is finite and small enough to allow exhaustive representation by a lookup table—as in\r
many of our illustrative examples—one must use a parameterized function approximation\r
scheme. Whether linear or nonlinear, function approximation relies on features that\r
have to be readily accessible to the learning system and able to convey the information\r
necessary for skilled performance. Most successful applications of reinforcement learning\r
owe much to sets of features carefully handcrafted based on human knowledge and\r
intuition about the specific problem to be tackled.\r
A team of researchers at Google DeepMind developed an impressive demonstration that\r
a deep multi-layer ANN can automate the feature design process (Mnih et al., 2013, 2015).\r
Multi-layer ANNs have been used for function approximation in reinforcement learning\r
ever since the 1986 popularization of the backpropagation algorithm as a method for\r
learning internal representations (Rumelhart, Hinton, and Williams, 1986; see Section 9.7).\r
Striking results have been obtained by coupling reinforcement learning with backpropa\u0002gation. The results obtained by Tesauro and colleagues with TD-Gammon and Watson\r
discussed above are notable examples. These and other applications benefited from the\r
ability of multi-layer ANNs to learn task-relevant features. However, in all the examples\r
of which we are aware, the most impressive demonstrations required the network’s input\r
to be represented in terms of specialized features handcrafted for the given problem.\r
This is vividly apparent in the TD-Gammon results. TD-Gammon 0.0, whose network\r
input was essentially a “raw” representation of the backgammon board, meaning that it\r
involved very little knowledge of backgammon, learned to play approximately as well as"""

[[sections]]
number = "16.5"
title = "Human-level Video Game Play 437"
text = """
the best previous backgammon computer programs. Adding specialized backgammon\r
features produced TD-Gammon 1.0 which was substantially better than all previous\r
backgammon programs and competed well against human experts.\r
Mnih et al. developed a reinforcement learning agent called deep Q-network (DQN)\r
that combined Q-learning with a deep convolutional ANN, a many-layered, or deep,\r
ANN specialized for processing spatial arrays of data such as images. We describe deep\r
convolutional ANNs in Section 9.7. By the time of Mnih et al.’s work with DQN, deep\r
ANNs, including deep convolutional ANNs, had produced impressive results in many\r
applications, but they had not been widely used in reinforcement learning.\r
Mnih et al. used DQN to show how a reinforcement learning agent can achieve a high\r
level of performance on any of a collection of di↵erent problems without having to use\r
di↵erent problem-specific feature sets. To demonstrate this, they let DQN learn to play\r
49 di↵erent Atari 2600 video games by interacting with a game emulator. DQN learned a\r
di↵erent policy for each of the 49 games (because the weights of its ANN were reset to\r
random values before learning on each game), but it used the same raw input, network\r
architecture, and parameter values (e.g., step size, discount rate, exploration parameters,\r
and many more specific to the implementation) for all the games. DQN achieved levels\r
of play at or beyond human level on a large fraction of these games. Although the games\r
were alike in being played by watching streams of video images, they varied widely in other\r
respects. Their actions had di↵erent e↵ects, they had di↵erent state-transition dynamics,\r
and they needed di↵erent policies for learning high scores. The deep convolutional ANN\r
learned to transform the raw input common to all the games into features specialized for\r
representing the action values required for playing at the high level DQN achieved for\r
most of the games.\r
The Atari 2600 is a home video game console that was sold in various versions by Atari\r
Inc. from 1977 to 1992. It introduced or popularized many arcade video games that are\r
now considered classics, such as Pong, Breakout, Space Invaders, and Asteroids. Although\r
much simpler than modern video games, Atari 2600 games are still entertaining and\r
challenging for human players, and they have been attractive as testbeds for developing\r
and evaluating reinforcement learning methods (Diuk, Cohen, Littman, 2008; Naddaf,\r
2010; Cobo, Zang, Isbell, and Thomaz, 2011; Bellemare, Veness, and Bowling, 2013).\r
Bellemare, Naddaf, Veness, and Bowling (2012) developed the publicly available Arcade\r
Learning Environment (ALE) to encourage and simplify using Atari 2600 games to study\r
learning and planning algorithms.\r
These previous studies and the availability of ALE made the Atari 2600 game collection\r
a good choice for Mnih et al.’s demonstration, which was also influenced by the impressive\r
human-level performance that TD-Gammon was able to achieve in backgammon. DQN\r
is similar to TD-Gammon in using a multi-layer ANN as the function approximation\r
method for a semi-gradient form of a TD algorithm, with the gradients computed by\r
the backpropagation algorithm. However, instead of using TD() as TD-Gammon did,\r
DQN used the semi-gradient form of Q-learning. TD-Gammon estimated the values of\r
afterstates, which were easily obtained from the rules for making backgammon moves.\r
To use the same algorithm for the Atari games would have required generating the next\r
states for each possible action (which would not have been afterstates in that case).\r
This could have been done by using the game emulator to run single-step simulations"""

[[sections]]
number = "438"
title = "Chapter 16: Applications and Case Studies"
text = """
for all the possible actions (which ALE makes possible). Or a model of each game’s\r
state-transition function could have been learned and used to predict next states (Oh,\r
Guo, Lee, Lewis, and Singh, 2015). While these methods might have produced results\r
comparable to DQN’s, they would have been more complicated to implement and would\r
have significantly increased the time needed for learning. Another motivation for using\r
Q-learning was that DQN used the experience replay method, described below, which\r
requires an o↵-policy algorithm. Being model-free and o↵-policy made Q-learning a\r
natural choice.\r
Before describing the details of DQN and how the experiments were conducted, we look\r
at the skill levels DQN was able to achieve. Mnih et al. compared the scores of DQN with\r
the scores of the best performing learning system in the literature at the time, the scores\r
of a professional human games tester, and the scores of an agent that selected actions at\r
random. The best system from the literature used linear function approximation with\r
features designed using some knowledge about Atari 2600 games (Bellemare, Naddaf,\r
Veness, and Bowling, 2013). DQN learned on each game by interacting with the game\r
emulator for 50 million frames, which corresponds to about 38 days of experience with\r
the game. At the start of learning on each game, the weights of DQN’s network were reset\r
to random values. To evaluate DQN’s skill level after learning, its score was averaged\r
over 30 sessions on each game, each lasting up to 5 minutes and beginning with a random\r
initial game state. The professional human tester played using the same emulator (with\r
the sound turned o↵ to remove any possible advantage over DQN which did not process\r
audio). After 2 hours of practice, the human played about 20 episodes of each game for up\r
to 5 minutes each and was not allowed to take any break during this time. DQN learned\r
to play better than the best previous reinforcement learning systems on all but 6 of the\r
games, and played better than the human player on 22 of the games. By considering any\r
performance that scored at or above 75% of the human score to be comparable to, or\r
better than, human-level play, Mnih et al. concluded that the levels of play DQN learned\r
reached or exceeded human level on 29 of the 46 games. See Mnih et al. (2015) for a\r
more detailed account of these results.\r
For an artificial learning system to achieve these levels of play would be impressive\r
enough, but what makes these results remarkable—and what many at the time considered\r
to be breakthrough results for artificial intelligence—is that the very same learning system\r
achieved these levels of play on widely varying games without relying on any game-specific\r
modifications.\r
A human playing any of these 49 Atari games sees 210⇥160 pixel image frames with\r
128 colors at 60Hz. In principle, exactly these images could have formed the raw input to\r
DQN, but to reduce memory and processing requirements, Mnih et al. preprocessed each\r
frame to produce an 84⇥84 array of luminance values. Because the full states of many\r
of the Atari games are not completely observable from the image frames, Mnih et al.\r
“stacked” the four most recent frames so that the inputs to the network had dimension\r
84⇥84⇥4. This did not eliminate partial observability for all of the games, but it was\r
helpful in making many of them more Markovian.\r
An essential point here is that these preprocessing steps were exactly the same for all 46\r
games. No game-specific prior knowledge was involved beyond the general understanding\r
that it should still be possible to learn good policies with this reduced dimension and"""

[[sections]]
number = "16.5"
title = "Human-level Video Game Play 439"
text = """
that stacking adjacent frames should help with the partial observability of some of the\r
games. Because no game-specific prior knowledge beyond this minimal amount was used\r
in preprocessing the image frames, we can think of the 84⇥84⇥4 input vectors as being\r
“raw” input to DQN.\r
The basic architecture of DQN is similar to the deep convolutional ANN illustrated in\r
Figure 9.15 (though unlike that network, subsampling in DQN is treated as part of each\r
convolutional layer, with feature maps consisting of units having only a selection of the\r
possible receptive fields). DQN has three hidden convolutional layers, followed by one\r
fully connected hidden layer, followed by the output layer. The three successive hidden\r
convolutional layers of DQN produce 32 20⇥20 feature maps, 64 9⇥9 feature maps,\r
and 64 7⇥7 feature maps. The activation function of the units of each feature map is a\r
rectifier nonlinearity (max(0, x)). The 3,136 (64⇥7⇥7) units in this third convolutional\r
layer all connect to each of 512 units in the fully connected hidden layer, which then each\r
connect to all 18 units in the output layer, one for each possible action in an Atari game.\r
The activation levels of DQN’s output units were the estimated optimal action values\r
of the corresponding state–action pairs, for the state represented by the network’s input.\r
The assignment of output units to a game’s actions varied from game to game, and\r
because the number of valid actions varied between 4 and 18 for the games, not all output\r
units had functional roles in all of the games. It helps to think of the network as if it\r
were 18 separate networks, one for estimating the optimal action value of each possible\r
action. In reality, these networks shared their initial layers, but the output units learned\r
to use the features extracted by these layers in di↵erent ways.\r
DQN’s reward signal indicated how a games’s score changed from one time step to\r
the next: +1 whenever it increased, 1 whenever it decreased, and 0 otherwise. This\r
standardized the reward signal across the games and made a single step-size parameter\r
work well for all the games despite their varying ranges of scores. DQN used an "-greedy\r
policy, with " decreasing linearly over the first million frames and remaining at a low\r
value for the rest of the learning session. The values of the various other parameters,\r
such as the learning step size, discount rate, and others specific to the implementation,\r
were selected by performing informal searches to see which values worked best for a small\r
selection of the games. These values were then held fixed for all of the games.\r
After DQN selected an action, the action was executed by the game emulator, which\r
returned a reward and the next video frame. The frame was preprocessed and added\r
to the four-frame stack that became the next input to the network. Skipping for the\r
moment the changes to the basic Q-learning procedure made by Mnih et al., DQN used\r
the following semi-gradient form of Q-learning to update the network’s weights:\r
wt+1 = wt + ↵\r
h\r
Rt+1 +  maxa qˆ(St+1, a, wt)  qˆ(St, At, wt)\r
i\r
rqˆ(St, At, wt), (16.3)\r
where wt is the vector of the network’s weights, At is the action selected at time step t,\r
and St and St+1 are respectively the preprocessed image stacks input to the network at\r
time steps t and t + 1.\r
The gradient in (16.3) was computed by backpropagation. Imagining again that there\r
was a separate network for each action, for the update at time step t, backpropagation\r
was applied only to the network corresponding to At. Mnih et al. took advantage of\r
techniques shown to improve the basic backpropagation algorithm when applied to large"""

[[sections]]
number = "440"
title = "Chapter 16: Applications and Case Studies"
text = """
networks. They used a mini-batch method that updated weights only after accumulating\r
gradient information over a small batch of images (here after 32 images). This yielded\r
smoother sample gradients compared to the usual procedure that updates weights after\r
each action. They also used a gradient-ascent algorithm called RMSProp (Tieleman and\r
Hinton, 2012) that accelerates learning by adjusting the step-size parameter for each\r
weight based on a running average of the magnitudes of recent gradients for that weight.\r
Mnih et al. modified the basic Q-learning procedure in three ways. First, they used\r
a method called experience replay first studied by Lin (1992). This method stores the\r
agent’s experience at each time step in a replay memory that is accessed to perform the\r
weight updates. It worked like this in DQN. After the game emulator executed action\r
At in a state represented by the image stack St, and returned reward Rt+1 and image\r
stack St+1, it added the tuple (St, At, Rt+1, St+1) to the replay memory. This memory\r
accumulated experiences over many plays of the same game. At each time step multiple Q\u0002learning updates—a mini-batch—were performed based on experiences sampled uniformly\r
at random from the replay memory. Instead of St+1 becoming the new St for the next\r
update as it would in the usual form of Q-learning, a new unconnected experience was\r
drawn from the replay memory to supply data for the next update. Because Q-learning\r
is an o↵-policy algorithm, it does not need to be applied along connected trajectories.\r
Q-learning with experience replay provided several advantages over the usual form of\r
Q-learning. The ability to use each stored experience for many updates allowed DQN to\r
learn more eciently from its experiences. Experience replay reduced the variance of the\r
updates because successive updates were not correlated with one another as they would\r
be with standard Q-learning. And by removing the dependence of successive experiences\r
on the current weights, experience replay eliminated one source of instability.\r
Mnih et al. modified standard Q-learning in a second way to improve its stability. As\r
in other methods that bootstrap, the target for a Q-learning update depends on the\r
current action-value function estimate. When a parameterized function approximation\r
method is used to represent action values, the target is a function of the same parameters\r
that are being updated. For example, the target in the update given by (16.3) is\r
 maxa qˆ(St+1, a, wt). Its dependence on wt complicates the process compared to the\r
simpler supervised-learning situation in which the targets do not depend on the parameters\r
being updated. As discussed in Chapter 11 this can lead to oscillations and/or divergence.\r
To address this problem Mnih et al. used a technique that brought Q-learning closer\r
to the simpler supervised-learning case while still allowing it to bootstrap. Whenever\r
a certain number, C, of updates had been done to the weights w of the action-value\r
network, they inserted the network’s current weights into another network and held\r
these duplicate weights fixed for the next C updates of w. The outputs of this duplicate\r
network over the next C updates of w were used as the Q-learning targets. Letting q˜\r
denote the output of this duplicate network, then instead of (16.3) the update rule was:\r
wt+1 = wt + ↵\r
h\r
Rt+1 +  maxa q˜(St+1, a, wt)  qˆ(St, At, wt)\r
i\r
rqˆ(St, At, wt).\r
A final modification of standard Q-learning was also found to improve stability. They\r
clipped the error term Rt+1 +  maxa q˜(St+1, a, wt)  qˆ(St, At, wt) so that it remained in\r
the interval [1, 1]."""

[[sections]]
number = "16.6"
title = "Mastering the Game of Go 441"
text = """
Mnih et al. conducted a large number of learning runs on 5 of the games to gain\r
insight into the e↵ect that various of DQN’s design features had on its performance.\r
They ran DQN with the four combinations of experience replay and the duplicate\r
target network being included or not included. Although the results varied from game\r
to game, each of these features alone significantly improved performance, and very\r
dramatically improved performance when used together. Mnih et al. also studied the role\r
played by the deep convolutional ANN in DQN’s learning ability by comparing the deep\r
convolutional version of DQN with a version having a network of just one linear layer, both\r
receiving the same stacked preprocessed video frames. Here, the improvement of the deep\r
convolutional version over the linear version was particularly striking across all 5 of the test\r
games.\r
Creating artificial agents that excel over a diverse collection of challenging tasks has\r
been an enduring goal of artificial intelligence. The promise of machine learning as\r
a means for achieving this has been frustrated by the need to craft problem-specific\r
representations. DeepMind’s DQN stands as a major step forward by demonstrating\r
that a single agent can learn problem-specific features enabling it to acquire human\u0002competitive skills over a range of tasks. This demonstration did not produce one agent\r
that simultaneously excelled at all the tasks (because learning occurred separately for\r
each task), but it showed that deep learning can reduce, and possibly eliminate, the need\r
for problem-specific design and tuning. As Mnih et al. point out, however, DQN is not\r
a complete solution to the problem of task-independent learning. Although the skills\r
needed to excel on the Atari games were markedly diverse, all the games were played by\r
observing video images, which made a deep convolutional ANN a natural choice for this\r
collection of tasks. In addition, DQN’s performance on some of the Atari 2600 games\r
fell considerably short of human skill levels on these games. The games most dicult\r
for DQN—especially Montezuma’s Revenge on which DQN learned to perform about as\r
well as the random player—require deep planning beyond what DQN was designed to\r
do. Further, learning control skills through extensive practice, like DQN learned how to\r
play the Atari games, is just one of the types of learning humans routinely accomplish.\r
Despite these limitations, DQN advanced the state-of-the-art in machine learning by\r
impressively demonstrating the promise of combining reinforcement learning with modern\r
methods of deep learning."""

[[sections]]
number = "16.6"
title = "Mastering the Game of Go"
text = """
The ancient Chinese game of Go has challenged artificial intelligence researchers for many\r
decades. Methods that achieve human-level skill, or even superhuman-level skill, in other\r
games have not been successful in producing strong Go programs. Thanks to a very\r
active community of Go programmers and international competitions, the level of Go\r
program play has improved significantly over the years. Until recently, however, no Go\r
program had been able to play anywhere near the level of a human Go master.\r
A team at DeepMind (Silver et al., 2016) developed the program AlphaGo that broke\r
this barrier by combining deep ANNs (Section 9.7), supervised learning, Monte Carlo"""

[[sections]]
number = "442"
title = "Chapter 16: Applications and Case Studies"
text = """
tree search (MCTS, Section 8.11), and reinforcement learning. By the time of Silver et\r
al.’s 2016 publication, AlphaGo had been shown to be decisively stronger than other Go\r
programs, and it had defeated the European Go champion Fan Hui 5 games to 0. These\r
were the first victories of a Go program over a professional human Go player without\r
handicap in full Go games. Shortly thereafter, a similar version of AlphaGo won stunning\r
victories over the 18-time world champion Lee Sedol, winning 4 out of a 5 games in\r
a challenge match, making worldwide headline news. Artificial intelligence researchers\r
thought that it would be many more years, perhaps decades, before a program reached\r
this level of play.\r
Here we describe AlphaGo and a successor program called AlphaGo Zero (Silver et al.\r
2017a). Where in addition to reinforcement learning, AlphaGo relied on supervised learn\u0002ing from a large database of expert human moves, AlphaGo Zero used only reinforcement\r
learning and no human data or guidance beyond the basic rules of the game (hence the\r
Zero in its name). We first describe AlphaGo in some detail in order to highlight the\r
relative simplicity of AlphaGo Zero, which is both higher-performing and more of a pure\r
reinforcement learning program.\r
In many ways, both AlphaGo and AlphaGo Zero are descendants of Tesauro’s TD\u0002Gammon (Section 16.1), itself a descendant of Samuel’s checkers player (Section 16.2).\r
All these programs included reinforcement learning over simulated games of self-play.\r
AlphaGo and AlphaGo Zero also built upon the progress made by DeepMind on playing\r
Atari games with the program DQN (Section 16.5) that used deep convolutional ANNs\r
to approximate optimal value functions.\r
A Go board configuration\r
Go is a game between two players who alter\u0002nately place black and white ‘stones’ on unoccu\u0002pied intersections, or ‘points,’ on a board with\r
a grid of 19 horizontal and 19 vertical lines to\r
produce positions like that shown to the right.\r
The game’s goal is to capture an area of the\r
board larger than that captured by the oppo\u0002nent. Stones are captured according to simple\r
rules. A player’s stones are captured if they\r
are completely surrounded by the other player’s\r
stones, meaning that there is no horizontally\r
or vertically adjacent point that is unoccupied.\r
For example, the left panel of Figure 16.5 (on\r
the next page) shows three white stones with\r
an unoccupied adjacent point (labeled X). If\r
black were to place a stone on X, then the three\r
white stones would be captured and taken o↵\r
the board (middle panel). However, if white were to place a stone on point X first, then\r
the possibility of this capture would be blocked (right panel). Other rules are needed to\r
prevent infinite capturing/recapturing loops. The game ends when neither player wishes\r
to place another stone. These rules are simple, but they produce a very complex game\r
that has had wide appeal for thousands of years."""

[[sections]]
number = "16.6"
title = "Mastering the Game of Go 443"
text = """
X\r
Figure 16.5: Go capturing rule. Left: the three white stones are not surrounded because point\r
X is unoccupied. Middle: if black places a stone on X, the three white stones are captured and\r
removed from the board. Right: if white places a stone on point X first, the capture is blocked.\r
Methods that produce strong play for other games, such as chess, have not worked as\r
well for Go. The search space for Go is significantly larger than that of chess because\r
Go has a larger number of legal moves per position than chess (⇡ 250 versus ⇡ 35) and\r
Go games tend to involve more moves than chess games (⇡ 150 versus ⇡ 80). But the\r
size of the search space is not the major factor that makes Go so dicult. Exhaustive\r
search is infeasible for both chess and Go, and Go on smaller boards (e.g., 9 ⇥ 9) has\r
proven to be exceedingly dicult as well. Experts agree that the major stumbling block\r
to creating stronger-than-amateur Go programs is the diculty of defining an adequate\r
position evaluation function. A good evaluation function allows search to be truncated at\r
a feasible depth by providing relatively easy-to-compute predictions of what deeper search\r
would likely yield. According to M¨uller (2002): “No simple yet reasonable evaluation\r
function will ever be found for Go.” A major step forward was the introduction of MCTS\r
to Go programs. The strongest programs at the time of AlphaGo’s development all\r
included MCTS, but master-level skill remained elusive.\r
Recall from Section 8.11 that MCTS is a decision-time planning procedure that does\r
not attempt to learn and store a global evaluation function. Like a rollout algorithm\r
(Section 8.10), it runs many Monte Carlo simulations of entire episodes (here, entire\r
Go games) to select each action (here, each Go move: where to place a stone or to\r
resign). Unlike a simple rollout algorithm, however, MCTS is an iterative procedure that\r
incrementally extends a search tree whose root node represents the current environment\r
state. As illustrated in Figure 8.10, each iteration traverses the tree by simulating\r
actions guided by statistics associated with the tree’s edges. In its basic version, when\r
a simulation reaches a leaf node of the search tree, MCTS expands the tree by adding\r
some, or all, of the leaf node’s children to the tree. From the leaf node, or one of its\r
newly added child nodes, a rollout is executed: a simulation that typically proceeds all\r
the way to a terminal state, with actions selected by a rollout policy. When the rollout\r
completes, the statistics associated with the search tree’s edges that were traversed in\r
this iteration are updated by backing up the return produced by the rollout. MCTS\r
continues this process, starting each time at the search tree’s root at the current state, for\r
as many iterations as possible given the time constraints. Then, finally, an action from\r
the root node (which still represents the current environment state) is selected according\r
to statistics accumulated in the root node’s outgoing edges. This is the action the agent\r
takes. After the environment transitions to its next state, MCTS is executed again with\r
the root node set to represent the new current state. The search tree at the start of this\r
next execution might be just this new root node, or it might include descendants of this\r
node left over from MCTS’s previous execution. The remainder of the tree is discarded."""

[[sections]]
number = "444"
title = "Chapter 16: Applications and Case Studies"
text = ""

[[sections]]
number = "16.6.1"
title = "AlphaGo"
text = """
The main innovation that made AlphaGo such a strong player is that it selected moves by\r
a novel version of MCTS that was guided by both a policy and a value function learned\r
by reinforcement learning with function approximation provided by deep convolutional\r
ANNs. Another key feature is that instead of reinforcement learning starting from random\r
network weights, it started from weights that were the result of previous supervised\r
learning from a large collection of human expert moves.\r
The DeepMind team called AlphaGo’s modification of basic MCTS “asynchronous\r
policy and value MCTS,” or APV-MCTS. It selected actions via basic MCTS as described\r
above but with some twists in how it extended its search tree and how it evaluated action\r
edges. In contrast to basic MCTS, which expands its current search tree by using stored\r
action values to select an unexplored edge from a leaf node, APV-MCTS, as implemented\r
in AlphaGo, expanded its tree by choosing an edge according to probabilities supplied by\r
a 13-layer deep convolutional ANN, called the SL-policy network, trained previously by\r
supervised learning to predict moves contained in a database of nearly 30 million human\r
expert moves.\r
Then, also in contrast to basic MCTS, which evaluates the newly-added state node\r
solely by the return of a rollout initiated from it, APV-MCTS evaluated the node in two\r
ways: by this return of the rollout, but also by a value function, v✓, learned previously by\r
a reinforcement learning method. If s was the newly-added node, its value became\r
v(s) = (1  ⌘)v✓(s) + ⌘G, (16.4)\r
where G was the return of the rollout and ⌘ controlled the mixing of the values resulting\r
from these two evaluation methods. In AlphaGo, these values were supplied by the\r
value network, another 13-layer deep convolutional ANN that was trained as we describe\r
below to output estimated values of board positions. APV-MCTS’s rollouts in AlphaGo\r
were simulated games with both players using a fast rollout policy provided by a simple\r
linear network, also trained by supervised learning before play. Throughout its execution,\r
APV-MCTS kept track of how many simulations passed through each edge of the search\r
tree, and when its execution completed, the most-visited edge from the root node was\r
selected as the action to take, here the move AlphaGo actually made in a game.\r
The value network had the same structure as the deep convolutional SL policy network\r
except that it had a single output unit that gave estimated values of game positions\r
instead of the SL policy network’s probability distributions over legal actions. Ideally,\r
the value network would output optimal state values, and it might have been possible to\r
approximate the optimal value function along the lines of TD-Gammon described above:\r
self-play with nonlinear TD() coupled to a deep convolutional ANN. But the DeepMind\r
team took a di↵erent approach that held more promise for a game as complex as Go.\r
They divided the process of training the value network into two stages. In the first stage,\r
they created the best policy they could by using reinforcement learning to train an RL\r
policy network. This was a deep convolutional ANN with the same structure as the SL\r
policy network. It was initialized with the final weights of the SL policy network that\r
were learned via supervised learning, and then policy-gradient reinforcement learning was\r
used to improve upon the SL policy. In the second stage of training the value network,"""

[[sections]]
number = "16.6"
title = "Mastering the Game of Go 445"
text = """
the team used Monte Carlo policy evaluation on data obtained from a large number of\r
simulated self-play games with moves selected by the RL policy network.\r
Figure 16.6 illustrates the networks used by AlphaGo and the steps taken to train them\r
in what the DeepMind team called the “AlphaGo pipeline.” All these networks were\r
trained before any live game play took place, and their weights remained fixed throughout\r
live play.\r
sampled state-action pairs (s, a), using stochastic gradient ascent to \r
maximize the likelihood of the human move a selected in state s\r
∆σ\r
σ\r
∝\r
∂ ( | )\r
∂\r
σ log p a s\r
We trained a 13-layer policy network, which we call the SL policy \r
network, from 30 million positions from the KGS Go Server. The net\u0002work predicted expert moves on a held out test set with an accuracy of \r
57.0% using all input features, and 55.7% using only raw board posi\u0002tion and move history as inputs, compared to the state-of-the-art from \r
other research groups of 44.4% at date of submission24 (full results in \r
Extended Data Table 3). Small improvements in accuracy led to large \r
improvements in playing strength (Fig. 2a); larger networks achieve \r
bttbt ltltdihWl\r
and its weights ρ are in\r
games between the curre\r
previous iteration of the \r
of opponents in this way \r
to the current policy. We \r
non-terminal time steps \r
nal reward at the end of \r
player at time step t: +1 \r
then updated at each tim\r
direction that maximizes \r
∆\r
Figure 1 | Neural network training pipeline and architecture. a, A fast \r
rollout policy pπ and supervised learning (SL) policy network pσ are \r
trained to predict human expert moves in a data set of positions. \r
A reinforcement learning (RL) policy network pρ is initialized to the SL \r
policy network, and is then improved by policy gradient learning to \r
maximize the outcome (that is, winning more games) against previous \r
versions of the policy network. A new data set is generated by playing \r
games of self-play with the RL policy network. Finally, a value network vθ\r
is trained by regression to predict the expected outcome (that is, whether \r
the current player wins) in \r
b, Schematic representatio\r
AlphaGo. The policy netw\r
s as its input, passes it thro\r
σ (SL policy network) or ρ \r
distribution ( | ) σp a s or ( \r
ρ\r
p a \r
probability map over the b\r
convolutional layers with p\r
that predicts the expected \r
Regression\r
Classi!cation\r
Classi!cation\r
Self Play\r
Policy gradient\r
a b\r
Human expert positions Self-play positions\r
Neural network Data\r
Rollout policy\r
pS pV \r
p \r
U QT\r
SL policy network RL policy network Value network Poli\r
Rollout Policy SL policy network RL policy Network Value Network\r
Policy gradient\r
Supervised Learning\r
MC Policy Evaluation\r
Self Play\r
Supervised Learning\r
]\r
]\r
Networks Data\r
Figure 16.6: AlphaGo pipeline. Adapted with permission from Macmillan Publishers Ltd:\r
Nature, vol. 529(7587), p. 485, ©2016.\r
Here is some more detail about AlphaGo’s ANNs and their training. The identically\u0002structured SL and RL policy networks were similar to DQN’s deep convolutional network\r
described in Section 16.5 for playing Atari games, except that they had 13 convolutional\r
layers with the final layer consisting of a soft-max unit for each point on the 19 ⇥ 19\r
Go board. The networks’ input was a 19 ⇥ 19 ⇥ 48 image stack in which each point\r
on the Go board was represented by the values of 48 binary or integer-valued features.\r
For example, for each point, one feature indicated if the point was occupied by one of\r
AlphaGo’s stones, one of its opponent’s stones, or was unoccupied, thus providing the\r
“raw” representation of the board configuration. Other features were based on the rules\r
of Go, such as the number of adjacent points that were empty, the number of opponent\r
stones that would be captured by placing a stone there, the number of turns since a stone\r
was placed there, and other features that the design team considered to be important.\r
Training the SL policy network took approximately 3 weeks using a distributed\r
implementation of stochastic gradient ascent on 50 processors. The network achieved 57%\r
accuracy, where the best accuracy achieved by other groups at the time of publication\r
was 44.4%. Training the RL policy network was done by policy gradient reinforcement\r
learning over simulated games between the RL policy network’s current policy and\r
opponents using policies randomly selected from policies produced by earlier iterations\r
of the learning algorithm. Playing against a randomly selected collection of opponents"""

[[sections]]
number = "446"
title = "Chapter 16: Applications and Case Studies"
text = """
prevented overfitting to the current policy. The reward signal was +1 if the current\r
policy won, 1 if it lost, and zero otherwise. These games directly pitted the two policies\r
against one another without involving MCTS. By simulating many games in parallel on\r
50 processors, the DeepMind team trained the RL policy network on a million games\r
in a single day. In testing the final RL policy, they found that it won more than 80%\r
of games played against the SL policy, and it won 85% of games played against a Go\r
program using MCTS that simulated 100,000 games per move.\r
The value network, whose structure was similar to that of the SL and RL policy\r
networks except for its single output unit, received the same input as the SL and RL\r
policy networks with the exception that there was an additional binary feature giving\r
the current color to play. Monte Carlo policy evaluation was used to train the network\r
from data obtained from a large number of self-play games played using the RL policy.\r
To avoid overfitting and instability due to the strong correlations between positions\r
encountered in self-play, the DeepMind team constructed a data set of 30 million positions\r
each chosen randomly from a unique self-play game. Then training was done using 50\r
million mini-batches each of 32 positions drawn from this data set. Training took one\r
week on 50 GPUs.\r
The rollout policy was learned prior to play by a simple linear network trained by\r
supervised learning from a corpus of 8 million human moves. The rollout policy network\r
had to output actions quickly while still being reasonably accurate. In principle, the SL\r
or RL policy networks could have been used in the rollouts, but the forward propagation\r
through these deep networks took too much time for either of them to be used in rollout\r
simulations, a great many of which had to be carried out for each move decision during\r
live play. For this reason, the rollout policy network was less complex than the other\r
policy networks, and its input features could be computed more quickly than the features\r
used for the policy networks. The rollout policy network allowed approximately 1,000\r
complete game simulations per second to be run on each of the processing threads that\r
AlphaGo used.\r
One may wonder why the SL policy was used instead of the better RL policy to select\r
actions in the expansion phase of APV-MCTS. These policies took the same amount of\r
time to compute because they used the same network architecture. The team actually\r
found that AlphaGo played better against human opponents when APV-MCTS used as\r
the SL policy instead of the RL policy. They conjectured that the reason for this was\r
that the latter was tuned to respond to optimal moves rather than to the broader set\r
of moves characteristic of human play. Interestingly, the situation was reversed for the\r
value function used by APV-MCTS. They found that when APV-MCTS used the value\r
function derived from the RL policy, it performed better than if it used the value function\r
derived from the SL policy.\r
Several methods worked together to produce AlphaGo’s impressive playing skill. The\r
DeepMind team evaluated di↵erent versions of AlphaGo in order to assess the contributions\r
made by these various components. The parameter ⌘ in (16.4) controlled the mixing\r
of game state evaluations produced by the value network and by rollouts. With ⌘ = 0,\r
AlphaGo used just the value network without rollouts, and with ⌘ = 1, evaluation\r
relied just on rollouts. They found that AlphaGo using just the value network played"""

[[sections]]
number = "16.6"
title = "Mastering the Game of Go 447"
text = """
better than the rollout-only AlphaGo, and in fact played better than the strongest of all\r
other Go programs existing at the time. The best play resulted from setting ⌘ = 0.5,\r
indicating that combining the value network with rollouts was particularly important\r
to AlphaGo’s success. These evaluation methods complemented one another: the value\r
network evaluated the high-performance RL policy that was too slow to be used in live\r
play, while rollouts using the weaker but much faster rollout policy were able to add\r
precision to the value network’s evaluations for specific states that occurred during games.\r
Overall, AlphaGo’s remarkable success fueled a new round of enthusiasm for the promise\r
of artificial intelligence, specifically for systems combining reinforcement learning with\r
deep ANNs, to address problems in other challenging domains."""

[[sections]]
number = "16.6.2"
title = "AlphaGo Zero"
text = """
Building upon the experience with AlphaGo, a DeepMind team developed AlphaGo Zero\r
(Silver et al. 2017a). In contrast to AlphaGo, this program used no human data or\r
guidance beyond the basic rules of the game (hence the Zero in its name). It learned\r
exclusively from self-play reinforcement learning, with input giving just “raw” descriptions\r
of the placements of stones on the Go board. AlphaGo Zero implemented a form of\r
policy iteration (Section 4.3), interleaving policy evaluation with policy improvement.\r
Figure 16.7 is an overview of AlphaGo Zero’s algorithm. A significant di↵erence between\r
AlphaGo Zero and AlphaGo is that AlphaGo Zero used MCTS to select moves throughout\r
self-play reinforcement learning, whereas AlphaGo used MCTS for live play after—but not\r
during—learning. Other di↵erences besides not using any human data or human-crafted\r
features are that AlphaGo Zero used only one deep convolutional ANN and used a simpler\r
version of MCTS.\r
AlphaGo Zero’s MCTS was simpler than the version used by AlphaGo in that it did\r
not include rollouts of complete games, and therefore did not need a rollout policy. Each\r
iteration of AlphaGo Zero’s MCTS ran a simulation that ended at a leaf node of the\r
current search tree instead of at the terminal position of a complete game simulation.\r
But as in AlphaGo, each iteration of MCTS in AlphaGo Zero was guided by the output of\r
a deep convolutional network, labeled f✓ in Figure 16.7, where ✓ is the network’s weight\r
vector. The input to the network, whose architecture we describe below, consisted of raw\r
representations of board positions, and its output had two parts: a scalar value, v, an\r
estimate of the probability that the current player will win from from the current board\r
position, and a vector, p, of move probabilities, one for each possible stone placement on\r
the current board, plus the pass, or resign, move.\r
Instead of selecting self-play actions according to the probabilities p, however, AlphaGo\r
Zero used these probabilities, together with the network’s value output, to direct each\r
execution of MCTS, which returned new move probabilities, shown in Figure 16.7 as the\r
policies ⇡i. These policies benefitted from the many simulations that MCTS conducted\r
each time it executed. The result was that the policy actually followed by AlphaGo\r
Zero was an improvement over the policy given by the network’s outputs p. Silver et al.\r
(2017a) wrote that “MCTS may therefore be viewed as a powerful policy improvement\r
operator.”"""

[[sections]]
number = "448"
title = "Chapter 16: Applications and Case Studies"
text = """
Figure 1: Self-play reinforcement learning in AlphaGo Zero. a The program plays a game\r
s1, ..., sT against itself. In each position st, a Monte-Carlo tree search (MCTS) is executed (see\r
Figure 2) using the latest neural network f. Moves are selected according to the search probabil\u0002ities computed by the MCTS, at ⇠ t. The terminal position sT is scored to compute the game\r
winner z. b Neural network training in AlphaGo Zero. The neural network takes the raw board\r
position s as its input, passes it through many convolutional layers with parameters , and outputs\r
both a vector p, representing a probability distribution over moves, and a scalar value v, represent\u0002ing the probability of the current player winning in position s. The neural network is trained on\r
randomly sampled steps from recent games of self-play, (s,, z). The parameters  are updated so\r
as to maximise the similarity of the policy vector p to the search probabilities , and to minimise\r
the error between the predicted winner v and the game winner z (see Equation 1).\r
4\r
Figure 16.7: AlphaGo Zero self-play reinforcement learning. a) The program played many\r
games against itself, one shown here as a sequence of board positions si, i = 1, 2,...,T, with\r
moves ai, i = 1, 2,...,T, and winner z. Each move ai was determined by action probabilities ⇡i\r
returned by MCTS executed from root node si and guided by a deep convolutional network,\r
here labeled f✓, with latest weights ✓. Shown here for just one position s but repeated for all\r
si, the network’s inputs were raw representations of board positions si (together with several\r
past positions, though not shown here), and its outputs were vectors p of move probabilities\r
that guided MCTS’s forward searches, and scalar values v that estimated the probability of the\r
current player winning from each position si. b) Deep convolutional network training. Training\r
examples were randomly sampled steps from recent self-play games. Weights ✓ were updated\r
to move the policy vector p toward the probabilities ⇡ returned by MCTS, and to include the\r
winners z in the estimated win probability v. Reprinted from draft of Silver et al. (2017a) with\r
permission of the authors and DeepMind.\r
Here is more detail about AlphaGo Zero’s ANN and how it was trained. The network\r
took as input a 19 ⇥ 19 ⇥ 17 image stack consisting of 17 binary feature planes. The first\r
8 feature planes were raw representations of the positions of the current player’s stones in\r
the current and seven past board configurations: a feature value was 1 if a player’s stone\r
was on the corresponding point, and was 0 otherwise. The next 8 feature planes similarly\r
coded the positions of the opponent’s stones. A final input feature plane had a constant\r
value indicating the color of the current play: 1 for black; 0 for white. Because repetition\r
is not allowed in Go and one player is given some number of “compensation points” for\r
not getting the first move, the current board position is not a Markov state of Go. This\r
is why features describing past board positions and the color feature were needed.\r
The network was “two-headed,” meaning that after a number of initial layers, the\r
network split into two separate “heads” of additional layers that separately fed into two\r
sets of output units. In this case, one head fed 362 output units producing 192 + 1 move"""

[[sections]]
number = "16.6"
title = "Mastering the Game of Go 449"
text = """
probabilities p, one for each possible stone placement plus pass; the other head fed just\r
one output unit producing the scalar v, an estimate of the probability that the current\r
player will win from the current board position. The network before the split consisted of\r
41 convolutional layers, each followed by batch normalization, and with skip connections\r
added to implement residual learning by pairs of layers (see Section 9.7). Overall, move\r
probabilities and values were computed by 43 and 44 layers respectively.\r
Starting with random weights, the network was trained by stochastic gradient descent\r
(with momentum, regularization, and step-size parameter decreasing as training continues)\r
using batches of examples sampled uniformly at random from all the steps of the most\r
recent 500,000 games of self-play with the current best policy. Extra noise was added\r
to the network’s output p to encourage exploration of all possible moves. At periodic\r
checkpoints during training, which Silver et al. (2017a) chose to be at every 1,000 training\r
steps, the policy output by the ANN with the latest weights was evaluated by simulating\r
400 games (using MCTS with 1,600 iterations to select each move) against the current\r
best policy. If the new policy won (by a margin set to reduce noise in the outcome), then\r
it became the best policy to be used in subsequent self-play. The network’s weights were\r
updated to make the network’s policy output p more closely match the policy returned\r
by MCTS, and to make its value output, v, more closely match the probability that the\r
current best policy wins from the board position represented by the network’s input.\r
The DeepMind team trained AlphaGo Zero over 4.9 million games of self-play, which\r
took about 3 days. Each move of each game was selected by running MCTS for 1,600\r
iterations, taking approximately 0.4 second per move. Network weights were updated over\r
700,000 batches each consisting of 2,048 board configurations. They then ran tournaments\r
with the trained AlphaGo Zero playing against the version of AlphaGo that defeated Fan\r
Hui by 5 games to 0, and against the version that defeated Lee Sedol by 4 games to 1.\r
They used the Elo rating system to evaluate the relative performances of the programs.\r
The di↵erence between two Elo ratings is meant to predict the outcome of games between\r
the players. The Elo ratings of AlphaGo Zero, the version of AlphaGo that played against\r
Fan Hui, and the version that played against Lee Sedol were respectively 4,308, 3,144,\r
and 3,739. The gaps in these Elo ratings translate into predictions that AlphaGo Zero\r
would defeat these other programs with probabilities very close to one. In a match of 100\r
games between AlphaGo Zero, trained as described, and the exact version of AlphaGo\r
that defeated Lee Sedol held under the same conditions that were used in that match,\r
AlphaGo Zero defeated AlphaGo in all 100 games.\r
The DeepMind team also compared AlphaGo Zero with a program using an ANN with\r
the same architecture but trained by supervised learning to predict human moves in a\r
data set containing nearly 30 million positions from 160,000 games. They found that the\r
supervised-learning player initially played better than AlphaGo Zero, and was better at\r
predicting human expert moves, but played less well after AlphaGo Zero was trained for\r
a day. This suggested that AlphaGo Zero had discovered a strategy for playing that was\r
di↵erent from how humans play. In fact, AlphaGo Zero discovered, and came to prefer,\r
some novel variations of classical move sequences.\r
Final tests of AlphaGo Zero’s algorithm were conducted with a version having a larger\r
ANN and trained over 29 million self-play games, which took about 40 days, again starting"""

[[sections]]
number = "450"
title = "Chapter 16: Applications and Case Studies"
text = """
with random weights. This version achieved an Elo rating of 5,185. The team pitted\r
this version of AlphaGo Zero against a program called AlphaGo Master , the strongest\r
program at the time, that was identical to AlphaGo Zero but, like AlphaGo, used human\r
data and features. AlphaGo Master ’s Elo rating was 4,858, and it had defeated the\r
strongest human professional players 60 to 0 in online games. In a 100 game match,\r
AlphaGo Zero with the larger network and more extensive learning defeated AlphaGo\r
Master 89 games to 11, thus providing a convincing demonstration of the problem-solving\r
power of AlphaGo Zero’s algorithm.\r
AlphaGo Zero soundly demonstrated that superhuman performance can be achieved\r
by pure reinforcement learning, augmented by a simple version of MCTS, and deep ANNs\r
with very minimal knowledge of the domain and no reliance on human data or guidance.\r
We will surely see systems inspired by the DeepMind accomplishments of both AlphaGo\r
and AlphaGo Zero applied to challenging problems in other domains.\r
Recently, yet a better program, AlphaZero, was described by Silver et al. (2017b) that\r
does not even incorporate knowledge of Go. AlphaZero is a general reinforcement learning\r
algorithm that improves over the world’s hitherto best programs in the diverse games of\r
Go, chess, and shogi."""

[[sections]]
number = "16.7"
title = "Personalized Web Services"
text = """
Personalizing web services such as the delivery of news articles or advertisements is one\r
approach to increasing users’ satisfaction with a website or to increase the yield of a\r
marketing campaign. A policy can recommend content considered to be the best for each\r
particular user based on a profile of that user’s interests and preferences inferred from\r
their history of online activity. This is a natural domain for machine learning, and in\r
particular, for reinforcement learning. A reinforcement learning system can improve a\r
recommendation policy by making adjustments in response to user feedback. One way\r
to obtain user feedback is by means of website satisfaction surveys, but for acquiring\r
feedback in real time it is common to monitor user clicks as indicators of interest in a\r
link.\r
A method long used in marketing called A/B testing is a simple type of reinforcement\r
learning used to decide which of two versions, A or B, of a website users prefer. Because\r
it is non-associative, like a two-armed bandit problem, this approach does not personalize\r
content delivery. Adding context consisting of features describing individual users and\r
the content to be delivered allows personalizing service. This has been formalized as a\r
contextual bandit problem (or an associative reinforcement learning problem, Section 2.9)\r
with the objective of maximizing the total number of user clicks. Li, Chu, Langford, and\r
Schapire (2010) applied a contextual bandit algorithm to the problem of personalizing\r
the Yahoo! Front Page Today webpage (one of the most visited pages on the internet at\r
the time of their research) by selecting the news story to feature. Their objective was to\r
maximize the click-through rate (CTR), which is the ratio of the total number of clicks\r
all users make on a webpage to the total number of visits to the page. Their contextual\r
bandit algorithm improved over a standard non-associative bandit algorithm by 12.5%.\r
Theocharous, Thomas, and Ghavamzadeh (2015) argued that better results are possible"""

[[sections]]
number = "16.7"
title = "Personalized Web Services 451"
text = """
by formulating personalized recommendation as a Markov decision problem (MDP) with\r
the objective of maximizing the total number of clicks users make over repeated visits to\r
a website. Policies derived from the contextual bandit formulation are greedy in the sense\r
that they do not take long-term e↵ects of actions into account. These policies e↵ectively\r
treat each visit to a website as if it were made by a new visitor uniformly sampled from\r
the population of the website’s visitors. By not using the fact that many users repeatedly\r
visit the same websites, greedy policies do not take advantage of possibilities provided by\r
long-term interactions with individual users.\r
As an example of how a marketing strategy might take advantage of long-term user\r
interaction, Theocharous et al. contrasted a greedy policy with a longer-term policy for\r
displaying ads for buying a product, say a car. The ad displayed by the greedy policy\r
might o↵er a discount if the user buys the car immediately. A user either takes the o↵er\r
or leaves the website, and if they ever return to the site, they would likely see the same\r
o↵er. A longer-term policy, on the other hand, can transition the user “down a sales\r
funnel” before presenting the final deal. It might start by describing the availability of\r
favorable financing terms, then praise an excellent service department, and then, on the\r
next visit, o↵er the final discount. This type of policy can result in more clicks by a user\r
over repeated visits to the site, and if the policy is suitably designed, more eventual sales.\r
Working at Adobe Systems Incorporated, Theocharous et al. conducted experiments\r
to see if policies designed to maximize clicks over the long term could in fact improve\r
over short-term greedy policies. The Adobe Marketing Cloud, a set of tools that many\r
companies use to run digital marketing campaigns, provides infrastructure for automating\r
user-targeted advertising and fund-raising campaigns. Actually deploying novel policies\r
using these tools entails significant risk because a new policy may end up performing\r
poorly. For this reason, the research team needed to assess what a policy’s performance\r
would be if it were to be actually deployed, but to do so on the basis of data collected\r
under the execution of other policies. A critical aspect of this research, then, was o↵-policy\r
evaluation. Further, the team wanted to do this with high confidence to reduce the\r
risk of deploying a new policy. Although high confidence o↵-policy evaluation was a\r
central component of this research (see also Thomas, 2015; Thomas, Theocharous, and\r
Ghavamzadeh, 2015), here we focus only on the algorithms and their results.\r
Theocharous et al. compared the results of two algorithms for learning ad recommen\u0002dation policies. The first algorithm, which they called greedy optimization, had the goal\r
of maximizing only the probability of immediate clicks. As in the standard contextual\r
bandit formulation, this algorithm did not take the long-term e↵ects of recommendations\r
into account. The other algorithm, a reinforcement learning algorithm based on an MDP\r
formulation, aimed at improving the number of clicks users made over multiple visits to\r
a website. They called this latter algorithm life-time value (LTV) optimization. Both\r
algorithms faced challenging problems because the reward signal in this domain is very\r
sparse because users usually do not click on ads, and user clicking is very random so that\r
returns have high variance.\r
Data sets from the banking industry were used for training and testing these algorithms.\r
The data sets consisted of many complete trajectories of customer interaction with a\r
bank’s website that showed each customer one out of a collection of possible o↵ers. If"""

[[sections]]
number = "452"
title = "Chapter 16: Applications and Case Studies"
text = """
a customer clicked, the reward was 1, and otherwise it was 0. One data set contained\r
approximately 200,000 interactions from a month of a bank’s campaign that randomly\r
o↵ered one of 7 o↵ers. The other data set from another bank’s campaign contained\r
4,000,000 interactions involving 12 possible o↵ers. All interactions included customer\r
features such as the time since the customer’s last visit to the website, the number of their\r
visits so far, the last time the customer clicked, geographic location, one of a collection of\r
interests, and features giving demographic information.\r
Greedy optimization was based on a mapping estimating the probability of a click\r
as a function of user features. The mapping was learned via supervised learning from\r
one of the data sets by means of a random forest (RF) algorithm (Breiman, 2001). RF\r
algorithms have been widely used for large-scale applications in industry because they are\r
e↵ective predictive tools that tend not to overfit and are relatively insensitive to outliers\r
and noise. Theocharous et al. then used the mapping to define an "-greedy policy that\r
selected with probability 1-" the o↵er predicted by the RF algorithm to have the highest\r
probability of producing a click, and otherwise selected from the other o↵ers uniformly at\r
random.\r
LTV optimization used a batch-mode reinforcement learning algorithm called fitted\r
Q iteration (FQI). It is a variant of fitted value iteration (Gordon, 1999) adapted to\r
Q-learning. Batch mode means that the entire data set for learning is available from\r
the start, as opposed to the online mode of the algorithms we focus on in this book in\r
which data are acquired sequentially while the learning algorithm executes. Batch-mode\r
reinforcement learning algorithms are sometimes necessary when online learning is not\r
practical, and they can use any batch-mode supervised learning regression algorithm,\r
including algorithms known to scale well to high-dimensional spaces. The convergence of\r
FQI depends on properties of the function approximation algorithm (Gordon, 1999). For\r
their application to LTV optimization, Theocharous et al. used the same RF algorithm\r
they used for the greedy optimization approach. Because in this case FQI convergence\r
is not monotonic, Theocharous et al. kept track of the best FQI policy by o↵-policy\r
evaluation using a validation training set. The final policy for testing the LTV approach\r
was the "-greedy policy based on the best policy produced by FQI with the initial\r
action-value function set to the mapping produced by the RF for the greedy optimization\r
approach.\r
To measure the performance of the policies produced by the greedy and LTV approaches,\r
Theocharous et al. used the CTR metric and a metric they called the LTV metric. These\r
metrics are similar, except that the LTV metric critically distinguishes between individual\r
website visitors:\r
CTR = Total # of Clicks\r
Total # of Visits,\r
LTV = Total # of Clicks\r
Total # of Visitors.\r
Figure 16.8 illustrates how these metrics di↵er. Each circle represents a user visit to\r
the site; black circles are visits at which the user clicks. Each row represents visits by\r
a particular user. By not distinguishing between visitors, the CTR for these sequences\r
is 0.35, whereas the LTV is 1.5. Because LTV is larger than CTR to the extent that"""

[[sections]]
number = "16.8"
title = "Thermal Soaring 453"
text = """
individual users revisit the site, it is an indicator of how successful a policy is in encouraging\r
users to engage in extended interactions with the site.\r
Figure 16.8: Click through rate (CTR) versus life-time value (LTV). Each circle represents\r
a user visit; black circles are visits at which the user clicks. Adapted from Theocharous et\r
al. (2015).\r
Testing the policies produced by the greedy and LTV approaches was done using\r
a high confidence o↵-policy evaluation method on a test data set consisting of real\u0002world interactions with a bank website served by a random policy. As expected, results\r
showed that greedy optimization performed best as measured by the CTR metric, while\r
LTV optimization performed best as measured by the LTV metric. Furthermore—\r
although we have omitted its details—the high confidence o↵-policy evaluation method\r
provided probabilistic guarantees that the LTV optimization method would, with high\r
probability, produce policies that improve upon policies currently deployed. Assured by\r
these probabilistic guarantees, Adobe announced in 2016 that the new LTV algorithm\r
would be a standard component of the Adobe Marketing Cloud so that a retailer could\r
issue a sequence of o↵ers following a policy likely to yield higher return than a policy\r
that is insensitive to long-term results."""

[[sections]]
number = "16.8"
title = "Thermal Soaring"
text = """
Birds and gliders take advantage of upward air currents—thermals—to gain altitude in\r
order to maintain flight while expending little, or no, energy. Thermal soaring, as this\r
behavior is called, is a complex skill requiring responding to subtle environmental cues\r
to increase altitude by exploiting a rising column of air for as long as possible. Reddy,\r
Celani, Sejnowski, and Vergassola (2016) used reinforcement learning to investigate\r
thermal soaring policies that are e↵ective in the strong atmospheric turbulence usually\r
accompanying rising air currents. Their primary goal was to provide insight into the\r
cues birds sense and how they use them to achieve their impressive thermal soaring\r
performance, but the results also contribute to technology relevant to autonomous gliders.\r
Reinforcement learning had previously been applied to the problem of navigating eciently\r
to the vicinity of a thermal updraft (Woodbury, Dunn, and Valasek, 2014) but not to the\r
more challenging problem of soaring within the turbulence of the updraft itself.\r
Reddy et al. modeled the soaring problem as a continuing MDP with discounting.\r
The agent interacted with a detailed model of a glider flying in turbulent air. They"""

[[sections]]
number = "454"
title = "Chapter 16: Applications and Case Studies"
text = """
devoted significant e↵ort toward making the model generate realistic thermal soaring\r
conditions, including investigating several di↵erent approaches to atmospheric modeling.\r
For the learning experiments, air flow in a three-dimensional box with one kilometer\r
sides, one of which was at ground level, was modeled by a sophisticated physics-based\r
set of partial di↵erential equations involving air velocity, temperature, and pressure.\r
Introducing small random perturbations into the numerical simulation caused the model\r
to produce analogs of thermal updrafts and accompanying turbulence (Figure 16.9 Left)\r
Glider flight was modeled by aerodynamic equations involving velocity, lift, drag, and\r
other factors governing powerless flight of a fixed-wing aircraft. Maneuvering the glider\r
involved changing its angle of attack (the angle between the glider’s wing and the direction\r
of air flow) and its bank angle (Figure 16.9 Right).\r
A\r
C z\r
y\r
Lift L\r
z\r
x\r
Lift L\r
Drag D\r
velocity direction\r
wing direction\r
bank angle\r
glide angle\r
angle of attack\r
B\r
D\r
of the vertical velocity (A) and the temperature fields (B) in our numerical simulations of 3D Rayleigh–Bénard conv\r
ed and blue colors indicate regions of large upward and downward flow, respectively. For the temperature field, t\r
high and low temperature, respectively. Notice that the hot and cold regions drive the upward and downward bran\r
with the basic physics of convection. (C) The force-body diagram of flight with no thrust, that is, without any engine \r
ows the bank angle μ (blue), the angle of attack α (green), and the glide angle γ (red). (D) The range of horizontal s\r
olling the angle of attack. At small angles of attack, the glider moves fast but also sinks fast, whereas at larger angles\r
If the angle of attack is too highat about 16°the glider stallsleading to a sudden drop in liftThe vertical black \r
contribute significantly and more exploratory strategies are\r
preferred.\r
The SARSA algorithm finds the optimal policy by estimating\r
for every state–action pair its Q function defined as the expected\r
sum of future rewards given the current state s and the action a.\r
At each step, the Q function is updated as follows:\r
Qðs, aÞ → Qðs, aÞ+ηðr +βQðs′, a′Þ − Qðs, aÞÞ, [5]\r
where r is the received reward and η is the learning rate. The\r
update is made online and does not require any prior model of\r
the flow or the flight. This feature is particularly relevant in\r
modeling decision-making processes in animals. When the algo\u0002rithm is close to convergence, the Q function approaches the\r
solution to Bellman’s dynamic programming equations (12).\r
In the sequel, w\r
as optimal. It sho\r
algorithm (as othe\r
identifies an appr\r
is skipped only fo\r
Results\r
Sensorimotor Cues \r
aspects of the lea\r
motor cues that th\r
of the reward use\r
state and action sp\r
necessary to discre\r
lookup table rep\r
averaged over dif\r
performance crite\r
A\r
C z\r
y\r
Lift L\r
z\r
x\r
Lift L\r
Drag D\r
velocity direction\r
wing direction\r
bank angle\r
glide angle\r
angle of attack\r
B\r
D\r
Fig. 1. Snapshots of the vertical velocity (A) and the temperature fields (B) in our numerical simulati\r
velocity field, the red and blue colors indicate regions of large upward and downward flow, respectiv\r
indicate regions of high and low temperature, respectively. Notice that the hot and cold regions drive \r
cell, in agreement with the basic physics of convection. (C) The force-body diagram of flight with no t\r
The figure also shows the bank angle μ (blue), the angle of attack α (green), and the glide angle γ (re\r
accessible by controlling the angle of attack. At small angles of attack, the glider moves fast but also si\r
sinks more slowly. If the angle of attack is too high, at about 16°, the glider stalls, leading to a sudde\r
fixed angle of attack for most of the simulations (Results, Control over the Angle of Attack).\r
↵\r
Figure 16.9: Thermal soaring model: Left: snapshot of the vertical velocity field of the\r
simulated cube of air: in red (blue) is a region of large upward (downward) flow. Right: diagram\r
of powerless flight showing bank angle µ and angle of attack ↵. Adapted with permission From\r
PNAS vol. 113(22), p. E4879, 2016, Reddy, Celani, Sejnowski, and Vergassola, Learning to Soar\r
in Turbulent Environments.\r
The interface between the agent and the environment required defining the agent’s\r
actions, the state information the agent receives from the environment, and the reward\r
signal. By experimenting with various possibilities, Reddy et al. decided that three actions\r
each for the angle of attack and the bank angle were enough for their purposes: increment\r
or decrement the current bank angle and angle of attack by 5 and 2.5, respectively, or\r
leave them unchanged. This resulted in 32 possible actions. The bank angle was bounded\r
to remain between 15 and +15.\r
Because a goal of their study was to try to determine what minimal set of sensory\r
cues are necessary for e↵ective soaring, both to shed light on the cues birds might use for\r
soaring and to minimize the sensing complexity required for automated glider soaring,\r
the authors tried various sets of signals as input to the reinforcement learning agent.\r
They started by using state aggregation (Section 9.3) of a four-dimensional state space\r
with dimensions giving local vertical wind speed, local vertical wind acceleration, torque\r
depending on the di↵erence between the vertical wind velocities at the left and right wing\r
tips, and the local temperature. Each dimension was discretized into three bins: positive"""

[[sections]]
number = "16.8"
title = "Thermal Soaring 455"
text = """
high, negative high, and small. Results, described below, showed that only two of these\r
dimensions were critical for e↵ective soaring behavior.\r
The overall objective of thermal soaring is to gain as much altitude as possible from\r
each rising column of air. Reddy et al. tried a straightforward reward signal that rewarded\r
the agent at the end of each episode based on the altitude gained over the episode, a\r
large negative reward signal if the glider touched the ground, and zero otherwise. They\r
found that learning was not successful with this reward signal for episodes of realistic\r
duration and that eligibility traces did not help. By experimenting with various reward\r
signals, they found that learning was best with a reward signal that at each time step\r
linearly combined the vertical wind velocity and vertical wind acceleration observed on\r
the previous time step.\r
Learning was by one-step Sarsa, with actions selected according to a soft-max dis\u0002tribution based on normalized action values. Specifically, the action probabilities were\r
computed according to (13.2) with action preferences:\r
h(s, a, ✓) = qˆ(s, a, ✓)  minb qˆ(s, b, ✓)\r
⌧\r
\r
maxb qˆ(s, b, ✓)  minb qˆ(s, b, ✓)\r
 ,\r
where ✓ is a parameter vector with one component for each action and aggregated group\r
of states, and qˆ(s, a, ✓) merely returned the component corresponding to s, a in the usual\r
way for state aggregation methods. The above equation forms the action preferences\r
by normalizing the approximate action values to the interval [0, 1] then dividing by ⌧ , a\r
positive “temperature parameter.”3 As ⌧ increases, the probability of selecting an action\r
becomes less dependent on its preference; as ⌧ decreases toward zero, the probability of\r
selecting the most highly-preferred action approaches one, making the policy approach\r
the greedy policy. The temperature parameter ⌧ was initialized to 2.0 and incrementally\r
decreased to 0.2 during learning. Action preferences were computed from the current\r
estimates of the action values: the action with the maximum estimated action value was\r
given preference 1/⌧ , the action with the minimum estimated action value was given\r
preference 0, and the preferences of the other actions were scaled between these extremes.\r
The step-size and discount-rate parameters were fixed at 0.1 and 0.98 respectively.\r
Each learning episode took place with the agent controlling simulated flight in an\r
independently generated period of simulated turbulent air currents. Each episode lasted\r
2.5 minutes simulated with a 1 second time step. Learning e↵ectively converged after a\r
few hundred episodes. The left panel of Figure 16.10 shows a sample trajectory before\r
learning where the agent selects actions randomly. Starting at the top of the volume\r
shown, the glider’s trajectory is in the direction indicated by the arrow and quickly loses\r
altitude. Figure 16.10’s right panel is a trajectory after learning. The glider starts at the\r
same place (here appearing at the bottom of the volume) and gains altitude by spiraling\r
within the rising column of air. Although Reddy et al. found that performance varied\r
widely over di↵erent simulated periods of air flow, the number of times the glider touched\r
the ground consistently decreased to nearly zero as learning progressed.\r
After experimenting with di↵erent sets of features available to the learning agent, it\r
turned out that the combination of just vertical wind acceleration and torques worked\r
3Reddy et al. described this slightly di↵erently, but our version is equivalent to theirs."""

[[sections]]
number = "456"
title = "Chapter 16: Applications and Case Studies"
text = """
(a) (b) Figure 16.10: Sample thermal soaring trajectories, with arrows showing the direction of\r
flight from the same starting point (note that the altitude scales are shifted). Left: before\r
learning: the agent selects actions randomly and the glider descends. Right: after learning:\r
the glider gains altitude by following a spiral trajectory. Adapted with permission from PNAS\r
vol. 113(22), p. E4879, 2016, Reddy, Celani, Sejnowski, and Vergassola, Learning to Soar in\r
Turbulent Environments.\r
best. The authors conjectured that because these features give information about the\r
gradient of vertical wind velocity in two di↵erent directions, they allow the controller to\r
select between turning by changing the bank angle or continuing along the same course\r
by leaving the bank angle alone. This allows the glider to stay within a rising column of\r
air. Vertical wind velocity is indicative of the strength of the thermal but does not help\r
in staying within the flow. They found that sensitivity to temperature was of little help.\r
They also found that controlling the angle of attack is not helpful in staying within a\r
particular thermal, being useful instead for traveling between thermals when covering\r
large distances, as in cross-country gliding and bird migration.\r
Due to the fact that soaring in di↵erent levels of turbulence requires di↵erent policies,\r
training was done in conditions ranging from weak to strong turbulence. In strong\r
turbulence the rapidly changing wind and glider velocities allowed less time for the\r
controller to react. This reduced the amount of control possible compared to what\r
was possible for maneuvering when fluctuations were weak. Reddy et al. examined the\r
policies Sarsa learned under these di↵erent conditions. Common to policies learned in all\r
regimes were these features: when sensing negative wind acceleration, bank sharply in the\r
direction of the wing with the higher lift; when sensing large positive wind acceleration\r
and no torque, do nothing. However, di↵erent levels of turbulence led to policy di↵erences.\r
Policies learned in strong turbulence were more conservative in that they preferred small\r
bank angles, whereas in weak turbulence, the best action was to turn as much as possible\r
by banking sharply. Systematic study of the bank angles preferred by the policies learned\r
under the di↵erent conditions led the authors to suggest that by detecting when vertical"""

[[sections]]
number = "16.8"
title = "Thermal Soaring 457"
text = """
wind acceleration crosses a certain threshold the controller can adjust its policy to cope\r
with di↵erent turbulence regimes.\r
Reddy et al. also conducted experiments to investigate the e↵ect of the discount-rate\r
parameter  on the performance of the learned policies. They found that the altitude\r
gained in an episode increased as  increased, reaching a maximum for  = .99, suggesting\r
that e↵ective thermal soaring requires taking into account long-term e↵ects of control\r
decisions.\r
This computational study of thermal soaring illustrates how reinforcement learning\r
can further progress toward di↵erent kinds of objectives. Learning policies having\r
access to di↵erent sets of environmental cues and control actions contributes to both\r
the engineering objective of designing autonomous gliders and the scientific objective of\r
improving understanding of the soaring skills of birds. In both cases, hypotheses resulting\r
from the learning experiments can be tested in the field by instrumenting real gliders4\r
and by comparing predictions with observed bird soaring behavior.\r
4This work has recently been applied to real gliders. See Reddy, Wong-Ng, Celani, Sejnowski, and\r
Vergassola, “Glider soaring via reinforcement learning in the field.” Nature 562:236–239, 2018.

Chapter 17\r
Frontiers\r
In this final chapter we touch on some topics that are beyond the scope of this book but\r
that we see as particularly important for the future of reinforcement learning. Many of\r
these topics bring us beyond what is reliably known, and some bring us beyond the MDP\r
framework."""

[[sections]]
number = "17.1"
title = "General Value Functions and Auxiliary Tasks"
text = """
Over the course of this book, our notion of value function has become quite general.\r
With o↵-policy learning we allowed a value function to be conditional on an arbitrary\r
target policy. Then in Section 12.8 we generalized discounting to a termination function\r
 : S 7! [0, 1], so that a di↵erent discount rate could be applied at each time step in\r
determining the return (12.17). This allowed us to express predictions about how much\r
reward we will get over an arbitrary, state-dependent horizon. The next, and perhaps\r
final, step is to generalize beyond rewards to permit predictions about arbitrary signals.\r
Rather than predicting the sum of future rewards, we might predict the sum of the future\r
values of a sound or color sensation, or of an internal, highly processed signal such as\r
another prediction. Whatever signal is added up in this way in a value-function-like\r
prediction, we call it the cumulant of that prediction. We formalize it in a cumulant\r
signal Ct 2 R. Using this, a general value function, or GVF, is written\r
v⇡,,C (s) .= E\r
"\r
X1\r
k=t\r
 Y\r
k\r
i=t+1\r
(Si)\r
!\r
Ck+1\r
\r
\r
\r
\r
\r
St =s, At:1 ⇠⇡\r
#\r
. (17.1)\r
As with conventional value functions (such as v⇡ or q⇤) this is an ideal function that\r
we seek to approximate with a parameterized form, which we might continue to denote\r
vˆ(s,w), although of course there would have to be a di↵erent w for each prediction, that\r
is, for each choice of ⇡, , and C. Because a GVF has no necessary connection to reward,\r
it is perhaps a misnomer to call it a value function. You might simply call it a prediction\r
or, to make it more distinctive, a forecast (Ring, in preparation). Whatever it is called, it"""

[[sections]]
number = "460"
title = "Chapter 17: Frontiers"
text = """
is in the form of a value function and thus can be learned in the usual ways using the\r
methods developed in this book for learning approximate value functions. Along with\r
the learned predictions, we might also learn policies to maximize the predictions in the\r
usual ways by Generalized Policy Iteration (Section 4.6) or by actor–critic methods. In\r
this way an agent could learn to predict and control great numbers of signals, not just\r
long-term reward.\r
Why might it be useful to predict and control signals other than long-term reward?\r
These are auxiliary tasks in that they are extra (in addition to) the main task of\r
maximizing reward. One answer is that the ability to predict and control a diverse\r
multitude of signals can constitute a powerful kind of environmental model. As we saw\r
in Chapter 8, a good model can enable the agent to get reward more eciently. It takes\r
a couple of further concepts to develop this answer clearly, so we postpone it to the next\r
section. First let’s consider two simpler ways in which a multitude of diverse predictions\r
can be helpful to a reinforcement learning agent.\r
One simple way in which auxiliary tasks can help on the main task is that they may\r
require some of the same representations as are needed on the main task. Some of the\r
auxiliary tasks may be easier, with less delay and a clearer connection between actions\r
and outcomes. If good features can be found early on easy auxiliary tasks, then those\r
features may significantly speed learning on the main task. There is no necessary reason\r
why this has to be true, but in many cases it seems plausible. For example, if you learn\r
to predict and control your sensors over short time scales, say seconds, then you might\r
plausibly come up with part of the idea of physical objects, which would then greatly\r
help with the prediction and control of long-term reward.\r
We might imagine an artificial neural network (ANN) in which the last layer is split\r
into multiple parts, or heads, each working on a di↵erent task. One head might produce\r
the approximate value function for the main task (with reward as its cumulant) whereas\r
the others would produce solutions to various auxiliary tasks. All heads could propagate\r
errors by stochastic gradient descent into the same body—the shared preceding part\r
of the network—which would then try to form representations, in its next-to-last layer,\r
to support all the heads. Researchers have experimented with auxiliary tasks such as\r
predicting change in pixels, predicting the next time step’s reward, and predicting the\r
distribution of the return. In many cases this approach has been shown to greatly\r
accelerate learning on the main task (Jaderberg et al., 2017). Multiple predictions\r
have similarly been repeatedly proposed as a way of directing the construction of state\r
estimates (see Section 17.3).\r
Another simple way in which the learning of auxiliary tasks can improve performance\r
is best explained by analogy to the psychological phenomena of classical conditioning\r
(Section 14.2). One way of understanding classical conditioning is that evolution has\r
built in a reflexive (non-learned) association to a particular action from the prediction\r
of a particular signal. For example, humans and many other animals appear to have a\r
built-in reflex to blink whenever their prediction of being poked in the eye exceeds some\r
threshold. The prediction is learned, but the association from prediction to eye closure\r
is built in, and thus the animal is saved many unprotected pokes in its eye. Similarly,\r
the association from fear to increased heart rate, or to freezing, may be built in. Agent"""

[[sections]]
number = "17.2"
title = "Temporal Abstraction via Options 461"
text = """
designers can do something similar, connecting by design (without learning) predictions\r
of specific events to predetermined actions. For example, a self-driving car that learns to\r
predict whether going forward will produce a collision could be given a built-in reflex to\r
stop, or to turn away, whenever the prediction is above some threshold. Or consider a\r
vacuum-cleaning robot that learned to predict whether it might run out of battery power\r
before returning to the charger and that reflexively headed back to the charger whenever\r
the prediction became non-zero. The correct prediction would depend on the size of the\r
house, the room the robot was in, and the age of the battery, all of which would be hard\r
for the robot designer to know. It would be dicult for the designer to build in a reliable\r
algorithm for deciding whether to head back to the charger in sensory terms, but it might\r
be easy to do this in terms of the learned prediction. We foresee many possible ways\r
like this in which learned predictions might combine usefully with built-in algorithms for\r
controlling behavior.\r
Finally, perhaps the most important role for auxiliary tasks is in moving beyond the\r
assumption we have made throughout this book that the state representation is fixed\r
and given to the agent. To explain this role, we first have to take a few steps back to\r
appreciate the magnitude of this assumption and the implications of removing it. We do\r
that in Section 17.3."""

[[sections]]
number = "17.2"
title = "Temporal Abstraction via Options"
text = """
An appealing aspect of the MDP formalism is that it can be applied usefully to tasks\r
at many di↵erent time scales. It can be used to formalize the task of deciding which\r
muscles to twitch to grasp an object, which airplane flight to take to arrive conveniently\r
at a distant city, and which job to take to lead a satisfying life. These tasks di↵er greatly\r
in their time scales, yet each can be usefully formulated as an MDP that can be solved\r
by planning or learning processes as described in this book. All involve interaction with\r
the world, sequential decision making, and a goal usefully conceived of as accumulating\r
rewards over time, and so all can be formulated as MDPs.\r
Although all these tasks can be formulated as MDPs, you might think that they cannot\r
be formulated as a single MDP. They involve such di↵erent time scales, such di↵erent\r
notions of choice and action! It would be no good, for example, to plan a flight across a\r
continent at the level of muscle twitches. Yet for other tasks—such as grasping objects,\r
throwing darts, or hitting a baseball—low-level muscle twitches may be just the right\r
level. People do all these things seamlessly without appearing to switch between levels.\r
Can the MDP framework be stretched to cover all the levels simultaneously?\r
Perhaps it can. One popular idea is to formalize an MDP at a detailed level, with a\r
small time step, yet enable planning at higher levels using extended courses of action that\r
correspond to many base-level time steps. To do this we need a notion of course of action\r
that extends over many time steps and includes a notion of termination. A general way to\r
formulate these two ideas is as a policy, ⇡, and a state-dependent termination function, ,\r
as in GVFs. We define a pair of these as a generalized notion of action termed an option.\r
To execute an option ! = h⇡!, !i at time t is to obtain the action to take, At, from\r
⇡!(·|St), then terminate at time t + 1 with probability 1  !(St+1). If the option does"""

[[sections]]
number = "462"
title = "Chapter 17: Frontiers"
text = """
not terminate at t+ 1, then At+1 is selected from ⇡!(·|St+1), and the option terminates at\r
t + 2 with probability 1  !(St+2), and so on until eventual termination. It is convenient\r
to consider low-level actions to be special cases of options—each action a corresponds\r
to an option h⇡!, !i whose policy picks the action (⇡!(s)=a for all s 2 S) and whose\r
termination function is zero (!(s) = 0 for all s 2 S+). Options e↵ectively extend the\r
action space. The agent can either select a low-level action/option, terminating after one\r
time step, or select an extended option that might execute for many time steps before\r
terminating.\r
Options are designed so that they are interchangable with low-level actions. For\r
example, the notion of an action-value function q⇡ naturally generalizes to an option\u0002value function that takes a state and option as input and returns the expected return\r
starting from that state, executing that option to termination, and thereafter following\r
the policy, ⇡. We can also generalize the notion of policy to a hierarchical policy that\r
selects from options rather than actions, where options, when selected, execute until\r
termination. With these ideas, many of the algorithms in this book can be generalized to\r
learn approximate option-value functions and hierarchical policies. In the simplest case,\r
the learning process ‘jumps’ from option initiation to option termination, with an update\r
only occurring when an option terminates. More subtly, updates can be made on each\r
time step, using “intra-option” learning algorithms, which in general require o↵-policy\r
learning.\r
Perhaps the most important generalization made possible by option ideas is that of the\r
environmental model as developed in Chapters 3, 4, and 8. The conventional model of an\r
action is the state-transition probabilities and the expected immediate reward for taking\r
the action in each state. How do conventional action models generalize to option models?\r
For options, the appropriate model is again of two parts, one corresponding to the state\r
transition resulting from executing the option and one corresponding to the expected\r
cumulative reward along the way. The reward part of an option model, analogous to the\r
expected reward for state–action pairs (3.5), is\r
r(s, !) .= E\r
⇥\r
R1 + R2 + 2R3 + ··· + ⌧1R⌧\r
\r
 S0 =s, A0:⌧1 ⇠⇡!, ⌧ ⇠!\r
⇤\r
, (17.2)\r
for all options ! and all states s 2 S, where ⌧ is the random time step at which the option\r
terminates according to !. Note the role of the overall discounting parameter  in this\r
equation—discounting is according to , but termination of the option is according to\r
!. The state-transition part of an option model is a little more subtle. This part of\r
the model characterizes the probability of each possible resulting state (as in (3.4)), but\r
now this state may result after various numbers of time steps, each of which must be\r
discounted di↵erently. The model for option ! specifies, for each state s that ! might\r
start executing in, and for each state s0 that ! might terminate in,\r
p(s0|s, !) .= X1\r
k=1\r
kPr{Sk =s0, ⌧ =k | S0 =s, A0:k1 ⇠⇡!, ⌧ ⇠!}. (17.3)\r
Note that, because of the factor of k, this p(s0|s, !) is no longer a transition probability\r
and no longer sums to one over all values of s0. (Nevertheless, we continue to use the ‘|’\r
notation in p.)"""

[[sections]]
number = "17.2"
title = "Temporal Abstraction via Options 463"
text = """
The above definition of the transition part of an option model allows us to formulate\r
Bellman equations and dynamic programming algorithms that apply to all options,\r
including low-level actions as a special case. For example, the general Bellman equation\r
for the state values of a hierarchical policy ⇡ is\r
v⇡(s) = X\r
!2⌦(s)\r
⇡(!|s)\r
"\r
r(s, !) +X\r
s0\r
p(s0|s, !)v⇡(s0)\r
#\r
, (17.4)\r
where ⌦(s) denotes the set of options available in state s. If ⌦(s) includes only the\r
low-level actions, then this equation reduces to a version of the usual Bellman equation\r
(3.14), except of course  is included in the new p (17.3) and thus does not appear.\r
Similarly, the corresponding planning algorithms also have no . For example, the value\r
iteration algorithm with options, analogous to (4.10), is\r
vk+1(s) .= max !2⌦(s)\r
"\r
r(s, !) +X\r
s0\r
p(s0|s, !)vk(s0)\r
#\r
, for all s 2 S.\r
If ⌦(s) includes all the low-level actions available in each state s, then this algorithm\r
converges to the conventional v⇤, from which the optimal policy can be computed.\r
However, it is particularly useful to plan with options when only a subset of the possible\r
options are considered (in ⌦(s)) in each state. Value iteration will then converge to the\r
best hierarchical policy limited to the restricted set of options. Although this policy may\r
be sub-optimal, convergence can be much faster because fewer options are considered\r
and because each option can jump over many time steps.\r
To plan with options, the agent must either be given the option models, or learn them.\r
One natural way to learn an option model is to formulate it as a collection of GVFs (as\r
defined in the preceding section) and then learn the GVFs using the methods presented\r
in this book. It is not dicult to see how this could be done for the reward part of the\r
option model. You merely choose one GVF’s cumulant to be the reward (Ct = Rt), its\r
policy to be the option’s policy (⇡=⇡!), and its termination function to be the discount\r
rate times the option’s termination function ((s) =  · !(s)). The true GVF then\r
equals the reward part of the option model, v⇡,,C (s) = r(s, !), and the learning methods\r
described in this book can be used to approximate it. The state-transition part of the\r
option model is a little more complicated. You need to allocate one GVF for each state\r
that the option might terminate in. We don’t want these GVFs to accumulate anything\r
except when the option terminates, and then only when termination is in the appropriate\r
state. This can be achieved by choosing the cumulant of the GVF that predicts transition\r
to state s0 to be Ct = (1  !(St)) St=s0 . The GVF’s policy and termination functions\r
are chosen the same as for the reward part of the option model. The true GVF then\r
equals the s0 portion of the option’s state-transition model, v⇡,,C (s) = p(s0 |s, !), and\r
again this book’s methods could be employed to learn it. Although each of these steps\r
is seemingly natural, putting them all together (including function approximation and\r
other essential components) is quite challenging and beyond the current state of the art."""

[[sections]]
number = "464"
title = "Chapter 17: Frontiers"
text = """
Exercise 17.1 This section has presented options for the discounted case, but discounting\r
is arguably inappropriate for control when using function approximation (Section 10.4).\r
What is the natural Bellman equation for a hierarchical policy, analogous to (17.4), but\r
for the average reward setting (Section 10.3)? What are the two parts of the option\r
model, analogous to (17.2) and (17.3), for the average reward setting? ⇤"""

[[sections]]
number = "17.3"
title = "Observations and State"
text = """
Throughout this book we have written the learned approximate value functions (and\r
the policies in Chapter 13) as functions of the environment’s state. This is a significant\r
limitation of the methods presented in Part I, in which the learned value function was\r
implemented as a table such that any value function could be exactly approximated;\r
that case is tantamount to assuming that the state of the environment is completely\r
observed by the agent. But in many cases of interest, and certainly in the lives of all\r
natural intelligences, the sensory input gives only partial information about the state of\r
the world. Some objects may be occluded by others, or behind the agent, or miles away.\r
In these cases, potentially important aspects of the environment’s state are not directly\r
observable, and it is a strong, unrealistic, and limiting assumption to assume that the\r
learned value function is implemented as a table over the environment’s state space.\r
The framework of parametric function approximation that we developed in Part II is far\r
less restrictive and, arguably, no limitation at all. In Part II we retained the assumption\r
that the learned value functions (and policies) are functions of the environment’s state,\r
but allowed these functions to be arbitrarily restricted by the parameterization. It is\r
somewhat surprising and not widely recognized that function approximation includes\r
important aspects of partial observability. For example, if there is a state variable that is\r
not observable, then the parameterization can be chosen such that the approximate value\r
does not depend on that state variable. The e↵ect is just as if the state variable were not\r
observable. Because of this, all the results obtained for the parameterized case apply to\r
partial observability without change. In this sense, the case of parameterized function\r
approximation includes the case of partial observability.\r
Nevertheless, there are many issues that cannot be investigated without a more explicit\r
treatment of partial observability. Although we cannot give them a full treatment here,\r
we can outline the changes that would be needed to do so. There are four steps.\r
First, we would change the problem. The environment would emit not its states, but\r
only observations—signals that depend on its state but, like a robot’s sensors, provide\r
only partial information about it. For convenience, without loss of generality, we assume\r
that the reward is a direct, known function of the observation (perhaps the observation is\r
a vector, and the reward is one of its components). The environmental interaction would\r
then have no explicit states or rewards, but could simply be an alternating sequence of\r
actions At 2 A and observations Ot 2 O:\r
A0, O1, A1, O2, A2, O3, A3, O4,...,\r
going on forever (cf. Equation 3.1) or forming episodes each ending with a special terminal\r
observation."""

[[sections]]
number = "17.3"
title = "Observations and State 465"
text = """
Second, we can recover the idea of state as used in this book from the sequence of\r
observations and actions. Let us use the word history, and the notation Ht, for an initial\r
portion of the trajectory up to an observation: Ht\r
.\r
= A0, O1,...,At1, Ot. The history\r
represents the most that we can know about the past without looking outside of the\r
data stream (because the history is the whole past data stream). Of course, the history\r
grows with t and can become large and unwieldy. The idea of state is that of a compact\r
summary of the history that is useful for predicting future sequences. To be a summary\r
of the history, a state must be a function of history, St = f(Ht). The summary would be\r
informationally perfect if it retained all information about the history (and thus could\r
be used to predict futures as accurately as could be done from the full history). In this\r
case, the state St and the function f are said to have the Markov property, and St is a\r
state as we have used the term in this book. Let us henceforth call it a Markov state\r
to distinguish it from states that are summaries of the history but are not sucient to\r
predict all futures. In practice, the states of real agents will not be Markov but may\r
approach it as an ideal.\r
To be more explicit about the Markov property it is useful to formalize possible futures.\r
Let a test be any specific sequence of alternating actions and observations that might\r
occur in the future. For example, a three-step test might be denoted ⌧ = a1 o1 a2 o2 a3 o3.\r
The probability of this test given a specific history h is defined as\r
p(⌧ |h) .= Pr{Ot+1 =o1, Ot+2 =o2, Ot+3 =o3 | Ht =h, At =a1, At+1 =a2, At+2 =a3}.(17.5)\r
Formally, f is Markov if and only if, for any test ⌧ , and for any histories h and h0 that\r
map to the same state under f, the test’s probabilities given the two histories are equal:\r
f(h) = f(h0) ) p(⌧ |h) = p(⌧ |h0), for all h, h0, ⌧ 2 {A ⇥ O}⇤. (17.6)\r
A Markov state summarizes all the information in the history necessary for determining\r
any test’s probability. In fact, it summarizes all that is necessary for making any prediction,\r
including any GVF. It also summarizes all that is necessary for optimal behavior: if f is\r
Markov, then there is always a deterministic function ⇡ such that choosing At\r
.\r
= ⇡(f(Ht))\r
is an optimal policy.\r
The third step in extending reinforcement learning to partial observability is to deal\r
with certain computational considerations. As mentioned earlier, we want the state to be\r
compact—relatively small compared to the history. (The identity function, for example,\r
is not a good f even though it is Markov, because the corresponding state St =Ht would\r
grow unboundedly with time.) In addition, we don’t really want a function f that takes\r
whole histories. Instead, we want an f that can be compactly implemented with an\r
incremental, recursive update that computes St+1 from St, incorporating only the next\r
increment of data, At and Ot+1:\r
St+1\r
.\r
= u(St, At, Ot+1), for all t  0, (17.7)\r
with the first state S0 given. The function u is called the state-update function. For\r
example, if f were the identity (St =Ht), then u would merely extend St by appending At\r
and Ot+1 to it. Given f, it is always possible to construct a corresponding u, but it may"""

[[sections]]
number = "466"
title = "Chapter 17: Frontiers"
text = """
Policy, \r
Value fn.\r
World\r
Planner\r
Model\r
O A\r
R\r
A, R\r
S A u\r
Figure 17.1: A conceptual agent architecture including a model, a planner, and a state-update\r
function. The world in this case receives actions A and emits observations O. The observations\r
and a copy of the action are used by the state-update function u to produce the new state. The\r
new state is input to the policy and value function, producing the next action, and is also input\r
to the planner (and to u). The information flows most responsible for learning are shown by\r
dashed lines that pass diagonally across the boxes that they change. The reward R directly\r
changes the policy and value function. The action, reward, and state change the model, which\r
works closely with the planner to also change the policy and value function. Note that the\r
operation of the planner can be decoupled from the agent–environment interaction, whereas the\r
other processes should operate in lock step with this interaction to keep up with the arrival of\r
new data. Also note that the model and planner do not deal with observations directly, but only\r
with the states produced by u, which can act as targets for model learning.\r
not be computationally convenient and, as in the identity example, it may not produce\r
a compact state. The state-update function is a central part of any agent architecture\r
that handles partial observability. It must be eciently computable, as no actions or\r
predictions can be made until the state is available. An overall diagram of such an agent\r
architecture is given in Figure 17.1.\r
A common strategy for finding a Markov state is to look for something compact that\r
is recursively updatable and enables accurate short-term predictions. In fact, it is only\r
necessary to make accurate one-step predictions. An important fact is that, if an f\r
is incrementally updatable, then it is Markov if and only if all one-step tests can be\r
accurately predicted, that is, if and only if\r
f(h) = f(h0) ) Pr{Ot+1 =o|Ht =h, At =a} = Pr{Ot+1 =o|Ht =h0, At =a}, (17.8)\r
for all h, h0 2 {A ⇥ O}⇤, o 2 O and a 2 A. Accurate one-step predictions are informa\u0002tionally sucient, together with the state-update function, to accurately predict the\r
probability of any test of any length. This can be done by iteratively and alternately\r
making one-step predictions and applying the state-update function. From the whole\r
tree of possibilities the exact probability of any test or the expectation of any GVF\r
can be determined. These observations have led many researchers to focus on one-step\r
predictions rather than directly on multi-step predictions such as GVFs. However, note"""

[[sections]]
number = "17.3"
title = "Observations and State 467"
text = """
that determining long-term predictions from single-step predictions is exponentially com\u0002plex in the length of the predictions. Moreover, one-step predictions can be iterated\r
to give accurate long-term predictions only if they are exact. If there is any error or\r
approximation in the one-step predictions, then it can compound to make the long-term\r
predictions wildly inaccurate. In practice this is often what happens.\r
An example of obtaining Markov states through a state-update function is provided\r
by the popular Bayesian approach known as Partially Observable MDPs, or POMDPs.\r
In this approach the environment is assumed to have a well defined latent state Xt that\r
underlies and produces the environment’s observations, but is never available to the agent\r
(and is not to be confused with the state St used by the agent to make predictions and\r
decisions). The natural Markov state, St, for a POMDP is the distribution over the latent\r
states given the history, called the belief state. For concreteness, assume the usual case in\r
which there are a finite number of hidden states, Xt 2 {1, 2,...,d}. Then the belief state\r
is the vector St\r
.\r
= st 2 [0, 1]d with components\r
st[i] .= Pr{Xt =i | Ht}, for all possible latent states i 2 {1, 2,...,d}. (17.9)\r
The belief state remains the same size (same number of components) even as t grows. It\r
can also be incrementally updated by Bayes’ rule, assuming complete knowledge of the\r
internal workings of the environment. Specifically, the ith component of the belief-state\r
update function is\r
u(s, a, o)[i] .=\r
Pd\r
x=1 s[x]p(i, o|x, a)\r
Pd\r
x=1\r
Pd\r
x0=1 s[x]p(x0\r
, o|x, a)\r
, for all a 2 A, o 2 O, (17.10)\r
and for all belief states s with components s[x], where the four-argument p function here\r
is not the usual one for MDPs (as in Chapter 3), but the analogous one for POMDPs,\r
in terms of the latent state: p(x0, o|x, a) .= Pr{Xt =x0, Ot =o |Xt1 =x, At1 =a}. This\r
approach is popular in theoretical work and has many significant applications, but its\r
assumptions and computational complexity scale poorly, and we do not recommend it as\r
an approach to artificial intelligence.\r
Another example of Markov states is provided by Predictive State Representations,\r
or PSRs. PSRs address the weakness of the POMDP approach that the semantics of\r
its agent state St are grounded in the environment state, Xt, which is never observed\r
and thus is dicult to learn about. In PSRs and related approaches, the semantics of\r
the agent state is instead grounded in predictions about future observations and actions,\r
which are readily observable. In PSRs, a Markov state is defined as a d-vector of the\r
probabilities of d specially chosen “core” tests as defined above (17.5). The vector is then\r
updated by a state-update function u that is analogous to Bayes rule, but with a semantics\r
grounded in observable data, which arguably makes it easier to learn. This approach has\r
been extended in many ways, including end-tests, compositional tests, powerful “spectral”\r
methods, and closed-loop and temporally abstract tests learned by TD methods. Some of\r
the best theoretical developments are for systems known as Observable Operator Models\r
(OOMs) and Sequential Systems (Thon, 2017).\r
The fourth and final step in our brief outline of how to handle partial observability in\r
reinforcement learning is to re-introduce approximation. As discussed in the introduction"""

[[sections]]
number = "468"
title = "Chapter 17: Frontiers"
text = """
to Part II, to approach artificial intelligence ambitiously we must embrace approximation.\r
This is just as true for states as it is for value functions. We must accept and work with\r
an approximate notion of state. The approximate state will play the same role in our\r
algorithms as before, so we continue to use the notation St for the state used by the\r
agent, even though it may not be Markov.\r
Perhaps the simplest example of an approximate state is just the latest observation,\r
St\r
.\r
=Ot. Of course this approach cannot handle any hidden state information. It would\r
be better to use the last k observations and actions, St\r
.\r
= Ot, At1, Ot1,...,Atk, for\r
some k  1, which can be achieved by a state-update function that just shifts the new\r
data in and the oldest data out. This kth-order history approach is still very simple,\r
but can greatly increase the agent’s capabilities compared to trying to use the single\r
immediate observation directly as the state.\r
What happens when the Markov property (17.8) is only approximately satisfied?\r
Unfortunately, long-term prediction performance can degrade dramatically when one\u0002step predictions become even slightly inaccurate. Longer-term tests, GVFs, and state\u0002update functions may or may not approximate better. The short-term and long-term\r
approximation objectives are just di↵erent, and there are no useful theoretical guarantees\r
at present.\r
Nevertheless, there are still reasons to think that the general idea outlined in this\r
section applies to the approximate case. The general idea is that a state that is good for\r
some predictions is also good for others—in particular, that a Markov state, sucient for\r
one-step predictions, is also sucient for all others. If we step back from that specific\r
result for the Markov case, the general idea is similar to what we discussed in Section 17.1\r
with multi-headed learning and auxiliary tasks. We discussed how representations that\r
were good for the auxiliary tasks were often also good for the main task. Taken together,\r
these suggest an approach to both partial observability and representation learning in\r
which multiple predictions are pursued and used to direct the construction of state\r
features. The guarantee provided by the perfect-but-impractical Markov property is\r
replaced by the heuristic that what’s good for some predictions may be good for others.\r
This approach scales well with computational resources. With a powerful computer we\r
could experiment with large numbers of predictions, perhaps favoring those that are most\r
similar to the ones of ultimate interest, that are easiest to learn reliably, or that satisfy\r
other criteria. It is important here to move beyond selecting the predictions manually.\r
The agent should do it. This would require a general language for predictions, so that\r
the agent can systematically explore a large space of possible predictions, sifting through\r
them for the ones that are most useful.\r
In particular, both POMDP and PSR approaches can be applied with approximate\r
states. The semantics of the state is often useful in forming the state-update function, as\r
it is in these two approaches and in the kth-order history approach. However, there is not\r
a strong need for the state to be accurate with respect to its semantics in order to retain\r
useful information. Some approaches to state augmentation, such as Echo state networks\r
(Jaeger, 2002), keep almost arbitrary information about the history and can nevertheless\r
perform well. There are many possibilities, and we expect more work and ideas in this\r
area. Learning the state-update function for an approximate state is a major part of the\r
representation learning problem as it arises in reinforcement learning."""

[[sections]]
number = "17.4"
title = "Designing Reward Signals 469"
text = ""

[[sections]]
number = "17.4"
title = "Designing Reward Signals"
text = """
A major advantage of reinforcement learning over supervised learning is that reinforcement\r
learning does not rely on detailed instructional information: generating a reward signal\r
does not depend on knowledge of what the agent’s correct actions should be. But the\r
success of a reinforcement learning application strongly depends on how well the reward\r
signal frames the goal of the application’s designer and how well the signal assesses\r
progress in reaching that goal. For these reasons, designing a reward signal is a critical\r
part of any application of reinforcement learning.\r
By designing a reward signal we mean designing the part of an agent’s environment\r
that is responsible for computing each scalar reward Rt and sending it to the agent at\r
each time t. In our discussion of terminology at the end of Chapter 14, we said that Rt\r
is more like a signal generated inside an animal’s brain than it is like an object or event\r
in the animal’s external environment. The parts of our brains that generate these signals\r
for us evolved over millions of years to be well suited to the challenges our ancestors\r
had to face in their struggles to propagate their genes to future generations. We should\r
therefore not think that designing a good reward signal is always an easy thing to do!\r
One challenge is to design a reward signal so that as an agent learns, its behavior\r
approaches, and ideally eventually achieves, what the application’s designer actually\r
desires. This can be easy if the designer’s goal is simple and easy to identify, such as\r
finding the solution to a well-defined problem or earning a high score in a well-defined\r
game. In cases like these, it is usual to reward the agent according to its success in solving\r
the problem or its success in improving its score. But some problems involve goals that\r
are dicult to translate into reward signals. This is especially true when the problem\r
requires the agent to skillfully perform a complex task or set of tasks, such as would be\r
required of a useful household robotic assistant. Further, reinforcement learning agents\r
can discover unexpected ways to make their environments deliver reward, some of which\r
might be undesirable, or even dangerous. This is a longstanding and critical challenge for\r
any method, like reinforcement learning, that is based on optimization. We discuss this\r
issue more in Section 17.6, the final section of this book.\r
Even when there is a simple and easily identifiable goal, the problem of sparse reward\r
often arises. Delivering non-zero reward frequently enough to allow the agent to achieve\r
the goal once, let alone to learn to achieve it eciently from multiple initial conditions,\r
can be a daunting challenge. State–action pairs that clearly deserve to trigger reward may\r
be few and far between, and rewards that mark progress toward a goal can be infrequent\r
because progress is dicult or even impossible to detect. The agent may wander aimlessly\r
for long periods of time (what Minsky, 1961, called the “plateau problem”).\r
In practice, designing a reward signal is often left to an informal trial-and-error search\r
for a signal that produces acceptable results. If the agent fails to learn, learns too slowly,\r
or learns the wrong thing, then the designer tweaks the reward signal and tries again.\r
To do this, the designer judges the agent’s performance by criteria that he or she is\r
attempting to translate into a reward signal so that the agent’s goal matches his or her\r
own. And if learning is too slow, the designer may try to design a non-sparse reward signal\r
that e↵ectively guides learning throughout the agent’s interaction with its environment."""

[[sections]]
number = "470"
title = "Chapter 17: Frontiers"
text = """
It is tempting to address the sparse reward problem by rewarding the agent for achieving\r
subgoals that the designer thinks are important way stations to the overall goal. But\r
augmenting the reward signal with well-intentioned supplemental rewards may lead the\r
agent to behave di↵erently from what is intended; the agent may end up not achieving\r
the overall goal. A better way to provide such guidance is to leave the reward signal\r
alone and instead augment the value-function approximation with an initial guess of what\r
it should ultimately be, or augment it with initial guesses as to what certain parts of it\r
should be. For example, suppose we wants to o↵er v0 : S ! R as an initial guess at the\r
true optimal value function v⇤, and that we are using linear function approximation with\r
features x : S ! Rd. Then we would define the initial value function approximation as\r
vˆ(s,w) .= w>x(s) + v0(s), (17.11)\r
and update the weights w as usual. If the initial weight vector is 0, then the initial\r
value function will be v0, but the asymptotic solution quality will be determined by the\r
feature vectors as usual. This initialization can also be done for arbitrary nonlinear\r
approximators and arbitrary forms of v0, though it is not guaranteed to always accelerate\r
learning.\r
A particularly e↵ective approach to the sparse reward problem is the shaping tech\u0002nique introduced by the psychologist B. F. Skinner and described in Section 14.3. The\r
e↵ectiveness of this technique relies on the fact that sparse reward problems are not just\r
problems with the reward signal; they are also problems with an agent’s policy in that\r
it prevents the agent from frequently encountering rewarding states. Shaping involves\r
changing the reward signal as learning proceeds, starting from a reward signal that is\r
not sparse given the agent’s initial behavior, and gradually modifying it toward a reward\r
signal suited to the problem of original interest. Shaping might also involve modifying\r
the dynamics of the task as learning proceeds. Each modification is made so that the\r
agent is frequently rewarded given its current behavior. The agent faces a sequence of\r
increasingly-dicult reinforcement learning problems, where what is learned at each stage\r
makes the next-harder problem relatively easy because the agent now encounters reward\r
more frequently than it would if it did not have prior experience with easier problems.\r
This kind of shaping is an essential technique in training animals, and it is e↵ective in\r
computational reinforcement learning as well.\r
What if you have no idea what the rewards should be, but there is another agent,\r
perhaps a person, who is already expert at the task and whose behavior can be observed?\r
In this case you could use methods known variously as “imitation learning,” “learning\r
from demonstration,” and “apprenticeship learning.” The idea here is to benefit from\r
the expert agent but leave open the possibility of eventually performing better. Learning\r
from an expert’s behavior can be done either by learning directly by supervised learning\r
or by extracting a reward signal using what is known as “inverse reinforcement learning”\r
and then using a reinforcement learning algorithm with that reward signal to learn a\r
policy. The task of inverse reinforcement learning as explored by Ng and Russell (2000)\r
is to try to recover the expert’s reward signal from the expert’s behavior alone. This\r
cannot be done exactly because a policy can be optimal with respect to many di↵erent\r
reward signals (for example, all policies are optimal with respect to a constant reward\r
signal), but it is possible to find plausible reward signal candidates. Unfortunately, strong"""

[[sections]]
number = "17.4"
title = "Designing Reward Signals 471"
text = """
assumptions are required, including knowledge of the environment’s dynamics and of the\r
feature vectors in which the reward signal is linear. The method also requires completely\r
solving the problem (e.g., by dynamic programming methods) multiple times. These\r
diculties notwithstanding, Abbeel and Ng (2004) argue that the inverse reinforcement\r
learning approach can sometimes be more e↵ective than supervised learning for benefiting\r
from the behavior of an expert.\r
Another approach to finding a good reward signal is to automate the trial-and-error\r
search for a good signal that we mentioned above. From an application perspective, the\r
reward signal is a parameter of the learning algorithm. As is true for other algorithm\r
parameters, the search for a good reward signal can be automated by defining a space of\r
feasible candidates and applying an optimization algorithm. The optimization algorithm\r
evaluates each candidate reward signal by running the reinforcement learning system with\r
that signal for some number of steps, and then scoring the overall result by a “high-level”\r
objective function intended to encode the designer’s true goal, ignoring the limitations\r
of the agent. Reward signals can even be improved via online gradient ascent, where\r
the gradient is that of the high-level objective function (Sorg, Lewis, and Singh, 2010).\r
Relating this approach to the natural world, the algorithm for optimizing the high-level\r
objective function is analogous to evolution, where the high-level objective function is an\r
animal’s evolutionary fitness determined by the number of its o↵spring that survive to\r
reproductive age.\r
Computational experiments with this bilevel optimization approach—one level analo\u0002gous to evolution, and the other due to reinforcement learning by individual agents—have\r
confirmed that intuition alone is not always adequate to devise a good reward signal\r
(Singh, Lewis, and Barto, 2009). The performance of a reinforcement learning agent as\r
evaluated by the high-level objective function can be very sensitive to details of the agent’s\r
reward signal in subtle ways determined by the agent’s limitations and the environment\r
in which it acts and learns. These experiments have also demonstrated that an agent’s\r
goal should not always be the same as the goal of the agent’s designer.\r
At first this seems counterintuitive, but it may be impossible for the agent to achieve the\r
designer’s goal no matter what its reward signal is. The agent has to learn under various\r
kinds of constraints, such as limited computational power, limited access to information\r
about its environment, or limited time to learn. When there are constraints like these,\r
learning to achieve a goal that is di↵erent from the designer’s goal can sometimes end up\r
getting closer to the designer’s goal than if that goal were pursued directly (Sorg, Singh,\r
and Lewis, 2010; Sorg, 2011). Examples of this in the natural world are easy to find.\r
Because we cannot directly assess the nutritional value of most foods, evolution—the\r
designer of our reward signal—gave us a reward signal that makes us seek certain tastes.\r
Though certainly not infallible (indeed, possibly detrimental in environments that di↵er in\r
certain ways from ancestral environments), this compensates for many of our limitations:\r
our limited sensory abilities, the limited time over which we can learn, and the risks\r
involved in finding a healthy diet through personal experimentation. Similarly, because\r
an animal cannot always observe its own evolutionary fitness, that objective function\r
does not work as a reward signal for learning. Evolution instead provides reward signals\r
that are sensitive to observable predictors of evolutionary fitness.\r
Finally, remember that a reinforcement learning agent is not necessarily like a complete"""

[[sections]]
number = "472"
title = "Chapter 17: Frontiers"
text = """
organism or robot; it can be a component of a larger behaving system. This means\r
that reward signals may be influenced by things inside the larger behaving agent, such\r
as motivational states, memories, ideas, or even hallucinations. Reward signals may\r
also depend on properties of the learning process itself, such as measures of how much\r
progress learning is making. Making reward signals sensitive to information about internal\r
factors such as these makes it possible for an agent to learn how to control the “cognitive\r
architecture” of which it is a part, as well as to acquire knowledge and skills that would be\r
dicult to learn from a reward signal that depended only on external events. Possibilities\r
like these led to the idea of “intrinsically-motivated reinforcement learning” that we\r
briefly discuss further at the end of the following section."""

[[sections]]
number = "17.5"
title = "Remaining Issues"
text = """
In this book we have presented the foundations of a reinforcement learning approach to\r
artificial intelligence. Roughly speaking, that approach is based on model-free and model\u0002based methods working together, as in the Dyna architecture of Chapter 8, combined\r
with function approximation as developed in Part II. The focus has been on online and\r
incremental algorithms, which we see as fundamental even to model-based methods, and\r
on how these can be applied in o↵-policy training situations. The full rationale for the\r
latter has been presented only in this last chapter. That is, we have all along presented o↵-\r
policy learning as an appealing way to deal with the explore/exploit dilemma, but only in\r
this chapter have we discussed learning about many diverse auxiliary tasks simultaneously\r
with GVFs and learning about the world hierarchically in terms of temporally-abstract\r
option models, both of which involve o↵-policy learning. Much remains to be worked\r
out, as we have indicated throughout the book and as evidenced by the directions for\r
additional research discussed in this chapter. But suppose we are generous and grant the\r
broad outlines of everything that we have done in the book and everything that has been\r
outlined so far in this chapter. What would remain after that? Of course we can’t know\r
for sure what will be required, but we can make some guesses. In this section we highlight\r
six further issues which it seems to us will still need to be addressed by future research.\r
First, we still need powerful parametric function approximation methods that work well\r
in fully incremental and online settings. Methods based on deep learning and ANNs are\r
a major step in this direction but, still, only work well with batch training on large data\r
sets, with training from extensive o↵-line self play, or with learning from the interleaved\r
experience of multiple agents on the same task. These and other settings are ways of\r
working around a basic limitation of today’s deep learning methods, which struggle to\r
learn rapidly in the incremental, online settings that are most natural for the reinforcement\r
learning algorithms emphasized in this book. The problem is sometimes described as\r
one of “catastrophic interference” or “correlated data.” When something new is learned\r
it tends to replace what has previously been learned rather than adding to it, with the\r
result that the benefit of the older learning is lost. Techniques such as “replay bu↵ers”\r
are often used to retain and replay old data so that its benefits are not permanently lost.\r
An honest assessment has to be that current deep learning methods are not well suited to\r
online learning. We see no reason that this limitation is insurmountable, but algorithms"""

[[sections]]
number = "17.5"
title = "Remaining Issues 473"
text = """
that address it, while at the same time retaining the advantages of deep learning, have\r
not yet been devised. Most current deep learning research is directed toward working\r
around this limitation rather than removing it.\r
Second (and perhaps closely related), we still need methods for learning features such\r
that subsequent learning generalizes well. This issue is an instance of a general problem\r
variously called “representation learning,” “constructive induction,” and “meta-learning”—\r
how can we use experience not just to learn a given desired function, but to learn inductive\r
biases such that future learning generalizes better and is thus faster? This is an old\r
problem, dating back to the origins of artificial intelligence and pattern recognition in\r
the 1950s and 1960s.1 Such age should give one pause. Perhaps there is no solution. But\r
it is equally likely that the time for finding a solution and demonstrating its e↵ectiveness\r
has not yet arrived. Today machine learning is conducted at a far larger scale than it has\r
been in the past, and the potential benefits of a good representation learning method have\r
become much more apparent. We note that a new annual conference—the International\r
Conference on Learning Representations—has been exploring this and related topics\r
every year since 2013. It is also less common to explore representation learning within\r
a reinforcement learning context. Reinforcement learning brings some new possibilities\r
to this old issue, such as the auxiliary tasks discussed in Section 17.1. In reinforcement\r
learning, the problem of representation learning can be identified with the problem of\r
learning the state-update function discussed in Section 17.3.\r
Third, we still need scalable methods for planning with learned environment models.\r
Planning methods have proven extremely e↵ective in applications such as AlphaGo\r
Zero and computer chess in which the model of the environment is known from the\r
rules of the game or can otherwise be supplied by human designers. But cases of full\r
model-based reinforcement learning, in which the environment model is learned from\r
data and then used for planning, are rare. The Dyna system described in Chapter 8 is\r
one example, but as described there and in most subsequent work it uses a tabular model\r
without function approximation, which greatly limits its applicability. Only a few studies\r
have included learned linear models, and even fewer have also explored the inclusion of\r
temporally-abstract models using options as discussed in Section 17.2.\r
More work is needed before planning with learned models can be e↵ective. For example,\r
the learning of the model needs to be selective because the scope of a model strongly\r
a↵ects planning eciency. If a model focuses on the key consequences of the most\r
important options, then planning can be ecient and rapid, but if a model includes\r
details of unimportant consequences of options that are unlikely to be selected, then\r
planning may be almost useless. Environment models should be constructed judiciously\r
with regard to both their states and dynamics with the goal of optimizing the planning\r
process. The various parts of the model should be continually monitored as to the degree\r
to which they contribute to, or detract from, planning eciency. The field has not\r
yet addressed this complex of issues or designed model-learning methods that take into\r
account their implications.\r
1Some would claim that deep learning solves this problem, for example, that DQN as described in\r
Section 16.5 illustrates a solution, but we are unconvinced. There is as yet little evidence that deep\r
learning alone solves the representation learning problem in a general and ecient way."""

[[sections]]
number = "474"
title = "Chapter 17: Frontiers"
text = """
A fourth issue that needs to be addressed in future research is that of automating the\r
choice of tasks on which an agent works and uses to structure its developing competence.\r
It is usual in machine learning for human designers to set the tasks that the learning\r
agent is expected to master. Because these tasks are known in advance and remain fixed,\r
they can be built into the learning algorithm code. However, looking ahead, we will want\r
the agent to make its own choices about what tasks it should try to master. These might\r
be subtasks of a specific overall task that is already known, or they might be intended to\r
create building blocks that permit more ecient learning of many di↵erent tasks that the\r
agent is likely to face in the future but which are currently unknown.\r
These tasks may be like the auxiliary tasks or the GVFs discussed in Section 17.1, or\r
tasks solved by options as discussed in Section 17.2. In forming a GVF, for example, what\r
should the cumulant, the policy, and the termination function be? The current state of\r
the art is to select these manually, but far greater power and generality would come from\r
making these task choices automatically, particularly when they derive from what the\r
agent has previously constructed as a result of representation learning or experience with\r
previous subproblems. If GVF design is automated, then the design choices themselves\r
will have to be explicitly represented. Rather than the task choices being in the mind\r
of the designer and built into the code, they will have to be in the machine itself in\r
such a way that they can be set and changed, monitored, filtered, and searched among\r
automatically. Tasks could then be built hierarchically upon others much like features are\r
in an ANN. The tasks are the questions, and the contents of the ANN are the answers to\r
those questions. We expect there will need to be a full hierarchy of questions to match\r
the hierarchy of answers provided by modern deep learning methods.\r
The fifth issue that we would like to highlight for future research is that of the\r
interaction between behavior and learning via some computational analog of curiosity.\r
In this chapter we have been imagining a setting in which many tasks are being learned\r
simultaneously, using o↵-policy methods, from the same stream of experience. The actions\r
taken will of course influence this stream of experience, which in turn will determine how\r
much learning occurs and which tasks are learned. When reward is not available, or not\r
strongly influenced by behavior, the agent is free to choose actions that maximize in\r
some sense the learning on the tasks, that is, to use some measure of learning progress\r
as an internal or “intrinsic” reward, implementing a computational form of curiosity. In\r
addition to measuring learning progress, intrinsic reward can, among other possibilities,\r
signal the receipt of unexpected, novel, or otherwise interesting input, or can assess the\r
agent’s ability to cause changes in its environment. Intrinsic reward signals generated in\r
these ways can be used by an agent to pose tasks for itself by defining auxiliary tasks,\r
GVFs, or options, as discussed above, so that skills learned in this way can contribute\r
to the agent’s ability to master future tasks. The result is a computational analog of\r
something like play. Many preliminary studies of such uses of intrinsic reward signals\r
have been conducted, and exciting topics for future research remain in this general area.\r
A final issue that demands attention in future research is that of developing methods to\r
make it acceptably safe to embed reinforcement learning agents into physical environments.\r
This is one of the most pressing areas for future research, and we discuss it further in the\r
following section."""

[[sections]]
number = "17.6"
title = "Reinforcement Learning and the Future of Artificial Intelligence 475"
text = ""

[[sections]]
number = "17.6"
title = "Reinforcement Learning and the Future of"
text = """
Artificial Intelligence\r
When we were writing the first edition of this book in the mid-1990s, artificial intelligence\r
was making significant progress and was having an impact on society, though it was\r
mostly still the promise of artificial intelligence that was inspiring developments. Machine\r
learning was part of that outlook, but it had not yet become indispensable to artificial\r
intelligence. By today that promise has transitioned to applications that are changing the\r
lives of millions of people, and machine learning has come into its own as a key technology.\r
As we write this second edition, some of the most remarkable developments in artificial\r
intelligence have involved reinforcement learning, most notably “deep reinforcement\r
learning”—reinforcement learning with function approximation by deep artificial neural\r
networks. We are at the beginning of a wave of real-world applications of artificial\r
intelligence, many of which will include reinforcement learning, deep and otherwise, that\r
will impact our lives in ways that are hard to predict.\r
But an abundance of successful real-world applications does not mean that true\r
artificial intelligence has arrived. Despite great progress in many areas, the gulf between\r
artificial intelligence and the intelligence of humans, and other animals, remains great.\r
Superhuman performance can be achieved in some domains, even formidable domains\r
like Go, but it remains a significant challenge to develop systems that are like us in\r
being complete, interactive agents having general adaptability and problem-solving skills,\r
emotional sophistication, creativity, and the ability to learn quickly from experience.\r
With its focus on learning by interacting with dynamic environments, reinforcement\r
learning, as it develops over the future, will be a critical component of agents with these\r
abilities.\r
Reinforcement learning’s connections to psychology and neuroscience (Chapters 14\r
and 15) underscore its relevance to another longstanding goal of artificial intelligence:\r
shedding light on fundamental questions about the mind and how it emerges from the\r
brain. Reinforcement learning theory is already contributing to our understanding of\r
the brain’s reward, motivation, and decision-making processes, and there is good reason\r
to believe that through its links to computational psychiatry, reinforcement learning\r
theory will contribute to methods for treating mental disorders, including drug abuse\r
and addiction.\r
Another contribution that reinforcement learning can make over the future is as an\r
aid to human decision making. Policies derived by reinforcement learning in simulated\r
environments can advise human decision makers in such areas as education, healthcare,\r
transportation, energy, and public-sector resource allocation. Particularly relevant is the\r
key feature of reinforcement learning that it takes long-term consequences of decisions\r
into account. This is very clear in games like backgammon and Go, where some of\r
the most impressive results of reinforcement learning have been demonstrated, but it\r
is also a property of many high-stakes decisions that a↵ect our lives and our planet.\r
Reinforcement learning follows related methods for advising human decision making that\r
have been developed in the past by decision analysts in many disciplines. With advanced\r
function approximation methods and massive computational power, reinforcement learning"""

[[sections]]
number = "476"
title = "Chapter 17: Frontiers"
text = """
methods have the potential to overcome some of the diculties of scaling up traditional\r
decision-support methods to larger and more complex problems.\r
The rapid pace of advances in artificial intelligence has led to warnings that artificial\r
intelligence poses serious threats to our societies, even to humanity itself. The renowned\r
scientist and artificial intelligence pioneer Herbert Simon anticipated the warnings we are\r
hearing today in a presentation at the Earthware Symposium at CMU in 2000 (Simon,\r
2000). He spoke of the eternal conflict between the promise and perils of any new\r
knowledge, reminding us of the Greek myths of Prometheus, the idealized hero of modern\r
science, who stole fire from the gods for the benefit of mankind, and of Pandora, whose\r
mythical box could be opened by a small and innocent action to release untold perils on\r
the world. While accepting that this conflict is inevitable, Simon urged us to recognize\r
that as designers of our future and not mere spectators, the decisions we make can tilt\r
the scale in Prometheus’ favor. This is certainly true for reinforcement learning, which\r
can benefit society but can also produce undesirable outcomes if it is carelessly deployed.\r
Thus, the safety of artificial intelligence applications involving reinforcement learning is a\r
topic that deserves careful attention.\r
A reinforcement learning agent can learn by interacting with either the real world or\r
with a simulation of some piece of the real world, or by a mixture of these two sources of\r
experience. Simulators provide safe environments in which an agent can explore and learn\r
without risking real damage to itself or to its environment. In most current applications,\r
policies are learned from simulated experience instead of direct interaction with the\r
real world. In addition to avoiding undesirable real-world consequences, learning from\r
simulated experience can make virtually unlimited data available for learning, generally\r
at less cost than needed to obtain real experience, and because simulations typically run\r
much faster than real time, learning can often occur more quickly than if it relied on real\r
experience.\r
Nevertheless, the full potential of reinforcement learning requires reinforcement learning\r
agents to be embedded into the flow of real-world experience, where they act, explore,\r
and learn in our world, and not just in their worlds. After all, reinforcement learning\r
algorithms—at least those upon which we focus in this book—are designed to learn online,\r
and they emulate many aspects of how animals are able to survive in nonstationary and\r
hostile environments. Embedding reinforcement learning agents in the real world can be\r
transformative in realizing the promises of artificial intelligence to amplify and extend\r
human abilities.\r
A major reason for wanting a reinforcement learning agent to act and learn in the real\r
world is that it is often dicult, sometimes impossible, to simulate real-world experience\r
with enough fidelity to make the resulting policies, whether derived by reinforcement\r
learning or by other methods, work well—and safely—when directing real actions. This\r
is especially true for environments whose dynamics depend on the behavior of humans,\r
such as in education, healthcare, transportation, and public policy—domains that can\r
surely benefit from improved decision making. However, it is for real-world embedded\r
agents that warnings about potential dangers of artificial intelligence need to be heeded.\r
Some of these warnings are particularly relevant to reinforcement learning. Because\r
reinforcement learning is based on optimization, it inherits the plusses and minuses of all\r
optimization methods. On the minus side is the problem of devising objective functions,"""

[[sections]]
number = "17.6"
title = "Reinforcement Learning and the Future of Artificial Intelligence 477"
text = """
or reward signals in the case of reinforcement learning, so that optimization produces\r
the desired results while avoiding undesirable results. We said in Section 17.4 that\r
reinforcement learning agents can discover unexpected ways to make their environments\r
deliver reward, some of which might be undesirable, or even dangerous. When we specify\r
what we want a system to learn only indirectly, as we do in designing a reinforcement\r
learning system’s reward signal, we will not know how closely the agent will fulfill our desire\r
until its learning is complete. This is hardly a new problem with reinforcement learning;\r
recognition of it has a long history in both literature and engineering. For example, in\r
Goethe’s poem “The Sorcerer’s Apprentice” (Goethe, 1878), the apprentice uses magic to\r
enchant a broom to do his job of fetching water, but the result is an unintended flood due\r
to the apprentice’s inadequate knowledge of magic. In the engineering context, Norbert\r
Wiener, the founder of cybernetics, warned of this problem more than half a century ago\r
by relating the supernatural story of “The Monkey’s Paw” (Wiener, 1964): “... it grants\r
what you ask for, not what you should have asked for or what you intend” (p. 59). The\r
problem has also been discussed at length in a modern context by Nick Bostrom (2014).\r
Anyone having experience with reinforcement learning has likely seen their systems\r
discover unexpected ways to obtain a lot of reward. Sometimes the unexpected behavior\r
is good: it solves a problem in a nice new way. In other instances, what the agent\r
learns violates considerations that the system designer may never have thought about.\r
Careful design of reward signals is essential if an agent is to act in the real world with no\r
opportunity for human vetting of its actions or means to easily interrupt its behavior.\r
Despite the possibility of unintended negative consequences, optimization has been used\r
for hundreds of years by engineers, architects, and others whose designs have positively\r
impacted the world. We owe much that is good in our environment to the application\r
of optimization methods. Many approaches have been developed to mitigate the risk\r
of optimization, such as adding hard and soft constraints, restricting optimization to\r
robust and risk-sensitive policies, and optimizing with multiple objective functions. Some\r
of these approaches have been adapted to reinforcement learning, and more research is\r
needed to address these concerns. The problem of ensuring that a reinforcement learning\r
agent’s goal is attuned to our own remains a challenge.\r
Another challenge if reinforcement learning agents are to act and learn in the real world\r
is not just about what they might learn eventually, but about how they will behave while\r
they are learning. How do you make sure that an agent gets enough experience to learn a\r
high-performing policy, all the while not harming its environment, other agents, or itself\r
(or more realistically, while keeping the probability of harm acceptably low)? This problem\r
is also not novel or unique to reinforcement learning. Risk management and mitigation\r
for embedded reinforcement learning is similar to what control engineers have had to\r
confront from the beginning of using automatic control in situations where a controller’s\r
behavior can have unacceptable, possibly catastrophic, consequences, as in the control of\r
an aircraft or a delicate chemical process. Control applications rely on careful system\r
modeling, model validation, and extensive testing, and there is a highly-developed body\r
of theory aimed at ensuring convergence and stability of adaptive controllers designed for\r
use when the dynamics of the system to be controlled are not fully known. Theoretical\r
guarantees are never iron-clad because they depend on the validity of the assumptions\r
underlying the mathematics, but without this theory, combined with risk-management"""

[[sections]]
number = "478"
title = "Chapter 17: Frontiers"
text = """
and mitigation practices, automatic control—adaptive and otherwise—would not be as\r
beneficial as it is today in improving the quality, eciency, and cost-e↵ectiveness of\r
processes on which we have come to rely. One of the most pressing areas for future\r
reinforcement learning research is to adapt and extend methods developed in control\r
engineering with the goal of making it acceptably safe to fully embed reinforcement\r
learning agents into physical environments.\r
In closing, we return to Simon’s call for us to recognize that we are designers of our\r
future and not simply spectators. By decisions we make as individuals, and by the\r
influence we can exert on how our societies are governed, we can work toward ensuring\r
that the benefits made possible by a new technology outweigh the harm it can cause.\r
There is ample opportunity to do this in the case of reinforcement learning, which can\r
help improve the quality, fairness, and sustainability of life on our planet, but which\r
can also release new perils. A threat already here is the displacement of jobs caused\r
by applications of artificial intelligence. Still there are good reasons to believe that the\r
benefits of artificial intelligence can outweigh the disruption it causes. As to safety,\r
hazards possible with reinforcement learning are not completely di↵erent from those\r
that have been managed successfully for related applications of optimization and control\r
methods. As reinforcement learning moves out into the real world in future applications,\r
developers have an obligation to follow best practices that have evolved for similar\r
technologies, while at the same time extending them to make sure that Prometheus keeps\r
the upper hand.\r
Bibliographical and Historical Remarks\r
17.1 General value functions were first explicitly identified by Sutton and colleagues\r
(Sutton, 1995a; Sutton et al., 2011; Modayil, White, and Sutton, 2013). Ring (in\r
preparation) developed an extensive thought experiment with GVFs (“forecasts”)\r
that has been influential despite not yet having been published.\r
The first demonstrations of multi-headed learning in reinforcement learning\r
were by Jaderberg et al. (2017). Bellemare, Dabney, and Munos (2017) showed\r
that predicting more things about the distribution of reward could significantly\r
accelerate learning to optimize its expectation, an instance of auxiliary tasks.\r
Many others have since taken up this line of research.\r
The general theory of classical conditioning as learned predictions together with\r
built-in, reflexive reactions to the predictions has not to our knowledge been\r
clearly articulated in the psychological literature. Modayil and Sutton (2014)\r
describe it as an approach to the engineering of robots and other agents, calling\r
it “Pavlovian control” to allude to its roots in classical conditioning.\r
17.2 The formalization of temporally abstract courses of action as options was intro\u0002duced by Sutton, Precup, and Singh (1999), building on prior work by Parr (1998)\r
and Sutton (1995a), and on classical work on Semi-MDPs (e.g., see Puterman,\r
1994). Precup’s (2000) PhD thesis developed option ideas fully. An important\r
limitation of these early works is that they did not treat the o↵-policy case"""

[[sections]]
number = "17.6"
title = "Reinforcement Learning and the Future of Artificial Intelligence 479"
text = """
with function approximation. Intra-option learning in general requires o↵-policy\r
learning, which could not be done reliably with function approximation at that\r
time. Although now we have a variety of stable o↵-policy learning methods using\r
function approximation, their combination with option ideas had not been signif\u0002icantly explored at the time of publication of this book. Barto and Mahadevan\r
(2003) and Hengst (2012) review the options formalism and other approaches to\r
temporal abstraction.\r
Using GVFs to implement option models has not previously been described. Our\r
presentation uses the trick introduced by Modayil, White, and Sutton (2014) for\r
predicting signals at the termination of policies.\r
Among the few works that have learned option models with function approxi\u0002mation are those by Sorg and Singh (2010), and by Bacon, Harb, and Precup\r
(2017).\r
The extension of options and option models to the average-reward setting has\r
not yet been developed in the literature."""

[[sections]]
number = "17.3"
title = "A good presentation of the POMDP approach is given by Monahan (1982). PSRs"
text = """
and tests were introduced by Littman, Sutton, and Singh (2002). OOMs were\r
introduced by Jaeger (1997, 1998, 2000). Sequential Systems, which unify PSRs,\r
OOMs, and many other works, were introduced in the PhD thesis of Michael Thon\r
(2017; Thon and Jaeger, 2015). Extensions to networks of temporal relationships\r
were developed by Tanner (2006; Sutton and Tanner, 2005) and then extended\r
to options (Sutton, Rafols, and Koop, 2006).\r
The theory of reinforcement learning with a non-Markov state representation was\r
developed explicitly by Singh, Jaakkola, and Jordan (1994; Jaakkola, Singh, and\r
Jordan, 1995). Early reinforcement learning approaches to partial observability\r
were developed by Chrisman (1992), McCallum (1993, 1995), Parr and Russell\r
(1995), Littman, Cassandra, and Kaelbling (1995), and by Lin and Mitchell\r
(1992)."""

[[sections]]
number = "17.4"
title = "Early e↵orts to include advice and teaching in reinforcement learning include"
text = """
those by Lin (1992), Maclin and Shavlik (1994), Clouse (1996), and Clouse and\r
Utgo↵ (1992).\r
Skinner’s shaping should not be confused with the “potential-based shaping”\r
technique introduced by Ng, Harada, and Russell (1999). Their technique has\r
been shown by Wiewiora (2003) to be equivalent to the simpler idea of providing\r
an initial approximation to the value function, as in (17.11).\r
17.5 We recommend the book by Goodfellow, Bengio, and Courville (2016) for discus\u0002sion of today’s deep learning techniques. The problem of catastrophic interference\r
in ANNs was developed by McCloskey and Cohen (1989), Ratcli↵ (1990), and\r
French (1999). The idea of a replay bu↵er was introduced by Lin (1992) and used\r
prominently in deep learning in the Atari game playing system (Section 16.5,\r
Mnih et al., 2013, 2015)."""

[[sections]]
number = "480"
title = "Chapter 17: Frontiers"
text = """
Minsky (1961) was one of the first to identify the problem of representation\r
learning.\r
Among the few works to consider planning with learned, approximate models\r
are those by Kuvayev and Sutton (1996), Sutton, Szepesvari, Geramifard, and\r
Bowling (2008), Nouri and Littman (2009), and Hester and Stone (2012).\r
The need to be selective in model construction to avoid slowing planning is well\r
known in artificial intelligence. Some of the classic work is by Minton (1990) and\r
Tambe, Newell, and Rosenbloom (1990). Hauskrecht, Meuleau, Kaelbling, Dean,\r
and Boutilier (1998) showed this e↵ect in MDPs with deterministic options.\r
Schmidhuber (1991a, b) proposed how something like curiosity would result if\r
reward signals were a function of how quickly an agent’s environment model\r
is improving. The empowerment function proposed by Klyubin, Polani, and\r
Nehaniv (2005) is an information-theoretic measure of an agent’s ability to control\r
its environment that can function as an intrinsic reward signal. Baldassarre and\r
Mirolli (2013) is a collection of contributions by researchers studying intrinsic\r
reward and motivation from both biological and computational perspectives,\r
including a perspective on “intrinsically-motivated reinforcement learning,” to\r
use the term introduced by Singh, Barto, and Chentenez (2004). See also Oudeyer\r
and Kaplan (2007), Oudeyer, Kaplan, and Hafner (2007), and Barto (2013).

References\r
Abbeel, P., Ng, A. Y. (2004). Apprenticeship learning via inverse reinforcement learning. In\r
Proceedings of the 21st International Conference on Machine Learning. ACM, New York.\r
Abramson, B. (1990). Expected-outcome: A general model of static evaluation. IEEE Transac\u0002tions on Pattern Analysis and Machine Intelligence, 12 (2):182–193.\r
Adams, C. D. (1982). Variations in the sensitivity of instrumental responding to reinforcer\r
devaluation. The Quarterly Journal of Experimental Psychology, 34(2):77–98.\r
Adams, C. D., Dickinson, A. (1981). Instrumental responding following reinforcer devaluation.\r
The Quarterly Journal of Experimental Psychology, 33(2):109–121.\r
Adams, R. A., Huys, Q. J. M., Roiser, J. P. (2015). Computational Psychiatry: towards a\r
mathematically informed understanding of mental illness. Journal of Neurology, Neurosurgery\r
& Psychiatry. doi:10.1136/jnnp-2015-310737\r
Agrawal, R. (1995). Sample mean based index policies with O(logn) regret for the multi-armed\r
bandit problem. Advances in Applied Probability, 27 (4):1054–1078.\r
Agre, P. E. (1988). The Dynamic Structure of Everyday Life. PhD thesis, Massachusetts\r
Institute of Technology, Cambridge MA. AI-TR 1085, MIT Artificial Intelligence Laboratory.\r
Agre, P. E., Chapman, D. (1990). What are plans for? Robotics and Autonomous Systems,\r
6(1-2):17–34.\r
Aizerman, M. A., Braverman, E. ´I., Rozonoer, L. I. (1964). Probability problem of pattern\r
recognition learning and potential functions method. Avtomat. i Telemekh, 25 (9):1307–1323.\r
Albus, J. S. (1971). A theory of cerebellar function. Mathematical Biosciences, 10(1-2):25–61.\r
Albus, J. S. (1981). Brain, Behavior, and Robotics. Byte Books, Peterborough, NH.\r
Aleksandrov, V. M., Sysoev, V. I., Shemeneva, V. V. (1968). Stochastic optimization of systems.\r
Izv. Akad. Nauk SSSR, Tekh. Kibernetika:14–19.\r
Amari, S. I. (1998). Natural gradient works eciently in learning. Neural Computation,\r
10 (2):251–276.\r
An, P. C. E. (1991). An Improved Multi-dimensional CMAC Neural network: Receptive Field\r
Function and Placement. PhD thesis, University of New Hampshire, Durham.\r
An, P. C. E., Miller, W. T., Parks, P. C. (1991). Design improvements in associative memories for\r
cerebellar model articulation controllers (CMAC). Artificial Neural Networks, pp. 1207–1210,\r
Elsevier North-Holland. http://www.incompleteideas.net/papers/AnMillerParks1991.pdf\r
Anderson, C. W. (1986). Learning and Problem Solving with Multilayer Connectionist Systems.\r
PhD thesis, University of Massachusetts, Amherst.\r
Anderson, C. W. (1987). Strategy learning with multilayer connectionist representations. In\r
Proceedings of the 4th International Workshop on Machine Learning, pp. 103–114. Morgan\r
Kaufmann."""

[[sections]]
number = "482"
title = "References"
text = """
Anderson, C. W. (1989). Learning to control an inverted pendulum using neural networks. IEEE\r
Control Systems Magazine, 9 (3):31–37.\r
Anderson, J. A., Silverstein, J. W., Ritz, S. A., Jones, R. S. (1977). Distinctive features,\r
categorical perception, and probability learning: Some applications of a neural model.\r
Psychological Review, 84(5):413–451.\r
Andreae, J. H. (1963). STELLA, A scheme for a learning machine. In Proceedings of the 2nd\r
IFAC Congress, Basle, pp. 497–502. Butterworths, London.\r
Andreae, J. H. (1969). Learning machines—a unified view. In A. R. Meetham and R. A. Hudson\r
(Eds.), Encyclopedia of Information, Linguistics, and Control, pp. 261–270. Pergamon,\r
Oxford.\r
Andreae, J. H. (1977). Thinking with the Teachable Machine. Academic Press, London.\r
Andreae, J. H. (2017a). A model of how the brain learns: A short introduction to multiple\r
context associative learning (MCAL) and the PP system. Unpublished report.\r
Andreae, J. H. (2017b). Working memory for the associative learning of language. Unpublished\r
report.\r
Andreae, J. H., Cashin, P. M. (1969). A learning machine with monologue. International\r
Journal of Man–Machine Studies, 1(1):1–20.\r
Arthur, W. B. (1991). Designing economic agents that act like human agents: A behavioral\r
approach to bounded rationality. The American Economic Review, 81 (2):353–359.\r
Asadi, K., Allen, C., Roderick, M., Mohamed, A. R., Konidaris, G., Littman, M. (2017). Mean\r
actor critic. ArXiv:1709.00503.\r
Atkeson, C. G. (1992). Memory-based approaches to approximating continuous functions. In\r
Sante Fe Institute Studies in the Sciences of Complexity, Proceedings Vol. 12, pp. 521–521.\r
Addison-Wesley.\r
Atkeson, C. G., Moore, A. W., Schaal, S. (1997). Locally weighted learning. Artificial Intelligence\r
Review, 11 :11–73.\r
Auer, P., Cesa-Bianchi, N., Fischer, P. (2002). Finite-time analysis of the multiarmed bandit\r
problem. Machine learning, 47(2-3):235–256.\r
Bacon, P. L., Harb, J., Precup, D. (2017). The option-critic architecture. In Proceedings of the\r
Association for the Advancement of Artificial Intelligence, pp. 1726–1734.\r
Baird, L. C. (1995). Residual algorithms: Reinforcement learning with function approximation.\r
In Proceedings of the 12th International Conference on Machine Learning, pp. 30–37. Morgan\r
Kaufmann.\r
Baird, L. C. (1999). Reinforcement Learning through Gradient Descent. PhD thesis, Carnegie\r
Mellon University, Pittsburgh PA.\r
Baird, L. C., Klopf, A. H. (1993). Reinforcement learning with high-dimensional, continuous\r
actions. Wright Laboratory, Wright-Patterson Air Force Base, Tech. Rep. WL-TR-93-1147.\r
Baird, L., Moore, A. W. (1999). Gradient descent for general reinforcement learning. In Advances\r
in Neural Information Processing Systems 11, pp. 968–974. MIT Press, Cambridge MA.\r
Baldassarre, G., Mirolli, M. (Eds.) (2013). Intrinsically Motivated Learning in Natural and\r
Artificial Systems. Springer-Verlag, Berlin Heidelberg.\r
Balke, A., Pearl, J. (1994). Counterfactual probabilities: Computational methods, bounds\r
and applications. In Proceedings of the Tenth International Conference on Uncertainty in\r
Artificial Intelligence, pp. 46–54. Morgan Kaufmann.\r
Baras, D., Meir, R. (2007). Reinforcement learning, spike-time-dependent plasticity, and the\r
BCM rule. Neural Computation, 19(8):2245–2279.

References 483\r
Barnard, E. (1993). Temporal-di↵erence methods and Markov models. IEEE Transactions on\r
Systems, Man, and Cybernetics, 23(2):357–365.\r
Barreto, A. S., Precup, D., Pineau, J. (2011). Reinforcement learning using kernel-based\r
stochastic factorization. In Advances in Neural Information Processing Systems 24, pp. 720–"""

[[sections]]
number = "728"
title = "Curran Associates, Inc."
text = """
Bartlett, P. L., Baxter, J. (1999). Hebbian synaptic modifications in spiking neurons that\r
learn. Technical report, Research School of Information Sciences and Engineering, Australian\r
National University.\r
Bartlett, P. L., Baxter, J. (2000). A biologically plausible and locally optimal learning algorithm\r
for spiking neurons. Rapport technique, Australian National University.\r
Barto, A. G. (1985). Learning by statistical cooperation of self-interested neuron-like computing\r
elements. Human Neurobiology, 4(4):229–256.\r
Barto, A. G. (1986). Game-theoretic cooperativity in networks of self-interested units. In\r
J. S. Denker (Ed.), Neural Networks for Computing, pp. 41–46. American Institute of Physics,\r
New York.\r
Barto, A. G. (1989). From chemotaxis to cooperativity: Abstract exercises in neuronal learning\r
strategies. In R. Durbin, R. Maill and G. Mitchison (Eds.), The Computing Neuron, pp. 73–98.\r
Addison-Wesley, Reading, MA.\r
Barto, A. G. (1990). Connectionist learning for control: An overview. In T. Miller, R. S. Sutton,\r
and P. J. Werbos (Eds.), Neural Networks for Control, pp. 5–58. MIT Press, Cambridge,\r
MA.\r
Barto, A. G. (1991). Some learning tasks from a control perspective. In L. Nadel and D. L. Stein\r
(Eds.), 1990 Lectures in Complex Systems, pp. 195–223. Addison-Wesley, Redwood City, CA.\r
Barto, A. G. (1992). Reinforcement learning and adaptive critic methods. In D. A. White and\r
D. A. Sofge (Eds.), Handbook of Intelligent Control: Neural, Fuzzy, and Adaptive Approaches,\r
pp. 469–491. Van Nostrand Reinhold, New York.\r
Barto, A. G. (1995a). Adaptive critics and the basal ganglia. In J. C. Houk, J. L. Davis, and\r
D. G. Beiser (Eds.), Models of Information Processing in the Basal Ganglia, pp. 215–232.\r
MIT Press, Cambridge, MA.\r
Barto, A. G. (1995b). Reinforcement learning. In M. A. Arbib (Ed.), Handbook of Brain Theory\r
and Neural Networks, pp. 804–809. MIT Press, Cambridge, MA.\r
Barto, A. G. (2011). Adaptive real-time dynamic programming. In C. Sammut and G. I Webb\r
(Eds.), Encyclopedia of Machine Learning, pp. 19–22. Springer Science and Business Media.\r
Barto, A. G. (2013). Intrinsic motivation and reinforcement learning. In G. Baldassarre and M.\r
Mirolli (Eds.), Intrinsically Motivated Learning in Natural and Artificial Systems, pp. 17–47.\r
Springer-Verlag, Berlin Heidelberg.\r
Barto, A. G., Anandan, P. (1985). Pattern recognizing stochastic learning automata. IEEE\r
Transactions on Systems, Man, and Cybernetics, 15(3):360–375.\r
Barto, A. G., Anderson, C. W. (1985). Structural learning in connectionist systems. In\r
Proceedings of the Seventh Annual Conference of the Cognitive Science Society, pp. 43–54.\r
Barto, A. G., Anderson, C. W., Sutton, R. S. (1982). Synthesis of nonlinear control surfaces by\r
a layered associative search network. Biological Cybernetics, 43(3):175–185.\r
Barto, A. G., Bradtke, S. J., Singh, S. P. (1991). Real-time learning and control using\r
asynchronous dynamic programming. Technical Report 91-57. Department of Computer\r
and Information Science, University of Massachusetts, Amherst.\r
Barto, A. G., Bradtke, S. J., Singh, S. P. (1995). Learning to act using real-time dynamic\r
programming. Artificial Intelligence, 72(1-2):81–138."""

[[sections]]
number = "484"
title = "References"
text = """
Barto, A. G., Du↵, M. (1994). Monte Carlo matrix inversion and reinforcement learning. In\r
Advances in Neural Information Processing Systems 6, pp. 687–694. Morgan Kaufmann, San\r
Francisco.\r
Barto, A. G., Jordan, M. I. (1987). Gradient following without back-propagation in layered\r
networks. In M. Caudill and C. Butler (Eds.), Proceedings of the IEEE First Annual\r
Conference on Neural Networks, pp. II629–II636. SOS Printing, San Diego.\r
Barto, A. G., Mahadevan, S. (2003). Recent advances in hierarchical reinforcement learning.\r
Discrete Event Dynamic Systems, 13 (4):341–379.\r
Barto, A. G., Singh, S. P. (1990). On the computational economics of reinforcement learning. In\r
Connectionist Models: Proceedings of the 1990 Summer School. Morgan Kaufmann.\r
Barto, A. G., Sutton, R. S. (1981a). Goal seeking components for adaptive intelligence: An\r
initial assessment. Technical Report AFWAL-TR-81-1070. Air Force Wright Aeronautical\r
Laboratories/Avionics Laboratory, Wright-Patterson AFB, OH.\r
Barto, A. G., Sutton, R. S. (1981b). Landmark learning: An illustration of associative search.\r
Biological Cybernetics, 42(1):1–8.\r
Barto, A. G., Sutton, R. S. (1982). Simulation of anticipatory responses in classical conditioning\r
by a neuron-like adaptive element. Behavioural Brain Research, 4(3):221–235.\r
Barto, A. G., Sutton, R. S., Anderson, C. W. (1983). Neuronlike elements that can solve\r
dicult learning control problems. IEEE Transactions on Systems, Man, and Cybernetics,\r
13(5):835–846. Reprinted in J. A. Anderson and E. Rosenfeld (Eds.), Neurocomputing:\r
Foundations of Research, pp. 535–549. MIT Press, Cambridge, MA, 1988.\r
Barto, A. G., Sutton, R. S., Brouwer, P. S. (1981). Associative search network: A reinforcement\r
learning associative memory. Biological Cybernetics, 40(3):201–211.\r
Barto, A. G., Sutton, R. S., Watkins, C. J. C. H. (1990). Learning and sequential decision\r
making. In M. Gabriel and J. Moore (Eds.), Learning and Computational Neuroscience:\r
Foundations of Adaptive Networks, pp. 539–602. MIT Press, Cambridge, MA.\r
Baxter, J., Bartlett, P. L. (2001). Infinite-horizon policy-gradient estimation. Journal of Artificial\r
Intelligence Research, 15 :319–350.\r
Baxter, J., Bartlett, P. L., Weaver, L. (2001). Experiments with infinite-horizon, policy-gradient\r
estimation. Journal of Artificial Intelligence Research, 15 :351–381.\r
Bellemare, M. G., Dabney, W., Munos, R. (2017). A distributional perspective on reinforcement\r
learning. ArXiv:1707.06887.\r
Bellemare, M. G., Naddaf, Y., Veness, J., Bowling, M. (2013). The arcade learning environment:\r
An evaluation platform for general agents. Journal of Artificial Intelligence Research,\r
47:253–279.\r
Bellemare, M. G., Veness, J., Bowling, M. (2012). Investigating contingency awareness using\r
Atari 2600 games. In Proceedings of the Twenty-Sixth AAAI Conference on Artificial\r
Intelligence, pp. 864–871. AAAI Press, Menlo Park, CA.\r
Bellman, R. E. (1956). A problem in the sequential design of experiments. Sankhya, 16:221–229.\r
Bellman, R. E. (1957a). Dynamic Programming. Princeton University Press, Princeton.\r
Bellman, R. E. (1957b). A Markov decision process. Journal of Mathematics and Mechanics,\r
6(5):679–684.\r
Bellman, R. E., Dreyfus, S. E. (1959). Functional approximations and dynamic programming.\r
Mathematical Tables and Other Aids to Computation, 13:247–251.\r
Bellman, R. E., Kalaba, R., Kotkin, B. (1963). Polynomial approximation—A new computational\r
technique in dynamic programming: Allocation processes. Mathematical Computation,\r
17:155–161.

References 485\r
Bengio, Y. (2009). Learning deep architectures for AI. Foundations and Trends in Machine\r
Learning, 2(1):1–27.\r
Bengio, Y., Courville, A. C., Vincent, P. (2012). Unsupervised feature learning and deep learning:\r
A review and new perspectives. CoRR 1, ArXiv:1206.5538.\r
Bentley, J. L. (1975). Multidimensional binary search trees used for associative searching.\r
Communications of the ACM, 18 (9):509–517.\r
Berg, H. C. (1975). Chemotaxis in bacteria. Annual review of biophysics and bioengineering,\r
4(1):119–136.\r
Berns, G. S., McClure, S. M., Pagnoni, G., Montague, P. R. (2001). Predictability modulates\r
human brain response to reward. The journal of neuroscience, 21(8):2793–2798.\r
Berridge, K. C., Kringelbach, M. L. (2008). A↵ective neuroscience of pleasure: reward in humans\r
and animals. Psychopharmacology, 199(3):457–480.\r
Berridge, K. C., Robinson, T. E. (1998). What is the role of dopamine in reward: hedonic\r
impact, reward learning, or incentive salience? Brain Research Reviews, 28(3):309–369.\r
Berry, D. A., Fristedt, B. (1985). Bandit Problems. Chapman and Hall, London.\r
Bertsekas, D. P. (1982). Distributed dynamic programming. IEEE Transactions on Automatic\r
Control, 27(3):610–616.\r
Bertsekas, D. P. (1983). Distributed asynchronous computation of fixed points. Mathematical\r
Programming, 27(1):107–120.\r
Bertsekas, D. P. (1987). Dynamic Programming: Deterministic and Stochastic Models. Prentice\u0002Hall, Englewood Cli↵s, NJ.\r
Bertsekas, D. P. (2005). Dynamic Programming and Optimal Control, Volume 1, third edition.\r
Athena Scientific, Belmont, MA.\r
Bertsekas, D. P. (2012). Dynamic Programming and Optimal Control, Volume 2: Approximate\r
Dynamic Programming, fourth edition. Athena Scientific, Belmont, MA.\r
Bertsekas, D. P. (2013). Rollout algorithms for discrete optimization: A survey. In Handbook of\r
Combinatorial Optimization, pp. 2989–3013. Springer, New York.\r
Bertsekas, D. P., Tsitsiklis, J. N. (1989). Parallel and Distributed Computation: Numerical\r
Methods. Prentice-Hall, Englewood Cli↵s, NJ.\r
Bertsekas, D. P., Tsitsiklis, J. N. (1996). Neuro-Dynamic Programming. Athena Scientific,\r
Belmont, MA.\r
Bertsekas, D. P., Tsitsiklis, J. N., Wu, C. (1997). Rollout algorithms for combinatorial optimiza\u0002tion. Journal of Heuristics, 3 (3):245–262.\r
Bertsekas, D. P., Yu, H. (2009). Projected equation methods for approximate solution of large\r
linear systems. Journal of Computational and Applied Mathematics, 227 (1):27–50.\r
Bhat, N., Farias, V., Moallemi, C. C. (2012). Non-parametric approximate dynamic programming\r
via the kernel method. In Advances in Neural Information Processing Systems 25, pp. 386–394.\r
Curran Associates, Inc.\r
Bhatnagar, S., Sutton, R., Ghavamzadeh, M., Lee, M. (2009). Natural actor–critic algorithms.\r
Automatica, 45 (11).\r
Biermann, A. W., Fairfield, J. R. C., Beres, T. R. (1982). Signature table systems and learning.\r
IEEE Transactions on Systems, Man, and Cybernetics, 12(5):635–648.\r
Bishop, C. M. (1995). Neural Networks for Pattern Recognition. Clarendon, Oxford.\r
Bishop, C. M. (2006). Pattern Recognition and Machine Learning. Springer Science + Business\r
Media New York LLC.\r
Blodgett, H. C. (1929). The e↵ect of the introduction of reward upon the maze performance of\r
rats. University of California Publications in Psychology, 4:113–134."""

[[sections]]
number = "486"
title = "References"
text = """
Boakes, R. A., Costa, D. S. J. (2014). Temporal contiguity in associative learning: Iinterference\r
and decay from an historical perspective. Journal of Experimental Psychology: Animal\r
Learning and Cognition, 40(4):381–400.\r
Booker, L. B. (1982). Intelligent Behavior as an Adaptation to the Task Environment. PhD thesis,\r
University of Michigan, Ann Arbor.\r
Bostrom, N. (2014). Superintelligence: Paths, Dangers, Strategies. Oxford University Press.\r
Bottou, L., Vapnik, V. (1992). Local learning algorithms. Neural Computation, 4 (6):888–900.\r
Boyan, J. A. (1999). Least-squares temporal di↵erence learning. In Proceedings of the 16th\r
International Conference on Machine Learning, pp. 49–56.\r
Boyan, J. A. (2002). Technical update: Least-squares temporal di↵erence learning. Machine\r
Learning, 49(2):233–246.\r
Boyan, J. A., Moore, A. W. (1995). Generalization in reinforcement learning: Safely approximat\u0002ing the value function. In Advances in Neural Information Processing Systems 7, pp. 369–376.\r
MIT Press, Cambridge, MA.\r
Bradtke, S. J. (1993). Reinforcement learning applied to linear quadratic regulation. In Advances\r
in Neural Information Processing Systems 5, pp. 295–302. Morgan Kaufmann.\r
Bradtke, S. J. (1994). Incremental Dynamic Programming for On-Line Adaptive Optimal Control.\r
PhD thesis, University of Massachusetts, Amherst. Appeared as CMPSCI Technical Report\r
94-62.\r
Bradtke, S. J., Barto, A. G. (1996). Linear least–squares algorithms for temporal di↵erence\r
learning. Machine Learning, 22:33–57.\r
Bradtke, S. J., Ydstie, B. E., Barto, A. G. (1994). Adaptive linear quadratic control using policy\r
iteration. In Proceedings of the American Control Conference, pp. 3475–3479. American\r
Automatic Control Council, Evanston, IL.\r
Brafman, R. I., Tennenholtz, M. (2003). R-max – a general polynomial time algorithm for\r
near-optimal reinforcement learning. Journal of Machine Learning Research, 3 :213–231.\r
Breiman, L. (2001). Random forests. Machine Learning, 45 (1):5–32.\r
Breiter, H. C., Aharon, I., Kahneman, D., Dale, A., Shizgal, P. (2001). Functional imaging\r
of neural responses to expectancy and experience of monetary gains and losses. Neuron,\r
30(2):619–639.\r
Breland, K., Breland, M. (1961). The misbehavior of organisms. American Psychologist,\r
16(11):681–684.\r
Bridle, J. S. (1990). Training stochastic model recognition algorithms as networks can lead to\r
maximum mutual information estimates of parameters. In Advances in Neural Information\r
Processing Systems 2, pp. 211–217. Morgan Kaufmann, San Mateo, CA.\r
Broomhead, D. S., Lowe, D. (1988). Multivariable functional interpolation and adaptive networks.\r
Complex Systems, 2:321–355.\r
Bromberg-Martin, E. S., Matsumoto, M., Hong, S., Hikosaka, O. (2010). A pallidus-habenula\u0002dopamine pathway signals inferred stimulus values. Journal of Neurophysiology, 104(2):1068–\r
1076.\r
Browne, C.B., Powley, E., Whitehouse, D., Lucas, S.M., Cowling, P.I., Rohlfshagen, P., Tavener,\r
S., Perez, D., Samothrakis, S., Colton, S. (2012). A survey of monte carlo tree search methods.\r
IEEE Transactions on Computational Intelligence and AI in Games, 4(1):1–43.\r
Brown, J., Bullock, D., Grossberg, S. (1999). How the basal ganglia use parallel excitatory\r
and inhibitory learning pathways to selectively respond to unexpected rewarding cues. The\r
Journal of Neuroscience, 19(23):10502–10511.

References 487\r
Bryson, A. E., Jr. (1996). Optimal control—1950 to 1985. IEEE Control Systems, 13(3):26–33.\r
Buchanan, B. G., Mitchell, T., Smith, R. G., Johnson, C. R., Jr. (1978). Models of learning\r
systems. Encyclopedia of Computer Science and technology, 11.\r
Buhusi, C. V., Schmajuk, N. A. (1999). Timing in simple conditioning and occasion setting: A\r
neural network approach. Behavioural Processes, 45(1):33–57.\r
Bu¸soniu, L., Lazaric, A., Ghavamzadeh, M., Munos, R., Babu˘ska, R., De Schutter, B. (2012).\r
Least-squares methods for policy iteration. In M. Wiering and M. van Otterlo (Eds.),\r
Reinforcement Learning: State-of-the-Art, pp. 75–109. Springer-Verlag Berlin Heidelberg.\r
Bush, R. R., Mosteller, F. (1955). Stochastic Models for Learning. Wiley, New York.\r
Byrne, J. H., Gingrich, K. J., Baxter, D. A. (1990). Computational capabilities of single\r
neurons: Relationship to simple forms of associative and nonassociative learning in aplysia.\r
In R. D. Hawkins and G. H. Bower (Eds.), Computational Models of Learning, pp. 31–63.\r
Academic Press, New York.\r
Calabresi, P., Picconi, B., Tozzi, A., Filippo, M. D. (2007). Dopamine-mediated regulation of\r
corticostriatal synaptic plasticity. Trends in Neuroscience, 30(5):211–219.\r
Camerer, C. (2011). Behavioral Game Theory: Experiments in Strategic Interaction. Princeton\r
University Press.\r
Campbell, D. T. (1960). Blind variation and selective survival as a general strategy in knowledge\u0002processes. In M. C. Yovits and S. Cameron (Eds.), Self-Organizing Systems, pp. 205–231.\r
Pergamon, New York.\r
Cao, X. R. (2009). Stochastic learning and optimization—A sensitivity-based approach. Annual\r
Reviews in Control, 33 (1):11–24.\r
Cao, X. R., Chen, H. F. (1997). Perturbation realization, potentials, and sensitivity analysis of\r
Markov processes. IEEE Transactions on Automatic Control, 42 (10):1382–1393.\r
Carlstr¨om, J., Nordstr¨om, E. (1997). Control of self-similar ATM call trac by reinforcement\r
learning. In Proceedings of the International Workshop on Applications of Neural Networks\r
to Telecommunications 3, pp. 54–62. Erlbaum, Hillsdale, NJ.\r
Chapman, D., Kaelbling, L. P. (1991). Input generalization in delayed reinforcement learning:\r
An algorithm and performance comparisons. In Proceedings of the Twelfth International\r
Conference on Artificial Intelligence, pp. 726–731. Morgan Kaufmann, San Mateo, CA.\r
Chaslot, G., Bakkes, S., Szita, I., Spronck, P. (2008). Monte-Carlo tree search: A new framework\r
for game AI. In Proceedings of the Fourth AAAI Conference on Artificial Intelligence and\r
Interactive Digital Entertainment (AIDE-08), pp. 216–217. AAAI Press, Menlo Park, CA.\r
Chow, C.-S., Tsitsiklis, J. N. (1991). An optimal one-way multigrid algorithm for discrete-time\r
stochastic control. IEEE Transactions on Automatic Control, 36(8):898–914.\r
Chrisman, L. (1992). Reinforcement learning with perceptual aliasing: The perceptual distinc\u0002tions approach. In Proceedings of the Tenth National Conference on Artificial Intelligence,\r
pp. 183–188. AAAI/MIT Press, Menlo Park, CA.\r
Christensen, J., Korf, R. E. (1986). A unified theory of heuristic evaluation functions and\r
its application to learning. In Proceedings of the Fifth National Conference on Artificial\r
Intelligence, pp. 148–152. Morgan Kaufmann.\r
Cichosz, P. (1995). Truncating temporal di↵erences: On the ecient implementation of TD()\r
for reinforcement learning. Journal of Artificial Intelligence Research, 2:287–318.\r
Ciosek, K., Whiteson, S. (2017). Expected policy gradients. ArXiv:1706.05374v1. A revised ver\u0002sion appeared in Proceedings of the Annual Conference of the Association for the Advancement\r
of Artificial Intelligence, pp. 2868–2875.\r
Ciosek, K., Whiteson, S. (2018). Expected policy gradients for reinforcement learning. ArXiv:\r
1801.03326."""

[[sections]]
number = "488"
title = "References"
text = """
Claridge-Chang, A., Roorda, R. D., Vrontou, E., Sjulson, L., Li, H., Hirsh, J., Miesenb¨ock, G.\r
(2009). Writing memories with light-addressable reinforcement circuitry. Cell, 139(2):405–\r
415.\r
Clark, R. E., Squire, L. R. (1998). Classical conditioning and brain systems: the role of awareness.\r
Science, 280(5360):77–81.\r
Clark, W. A., Farley, B. G. (1955). Generalization of pattern recognition in a self-organizing\r
system. In Proceedings of the 1955 Western Joint Computer Conference, pp. 86–91.\r
Clouse, J. (1996). On Integrating Apprentice Learning and Reinforcement Learning TITLE2.\r
PhD thesis, University of Massachusetts, Amherst. Appeared as CMPSCI Technical Report\r
96-026.\r
Clouse, J., Utgo↵, P. (1992). A teaching method for reinforcement learning systems. In\r
Proceedings of the 9th International Workshop on Machine Learning, pp. 92–101. Morgan\r
Kaufmann.\r
Cobo, L. C., Zang, P., Isbell, C. L., Thomaz, A. L. (2011). Automatic state abstraction from\r
demonstration. In Proceedings of the Twenty-Second International Joint Conference on\r
Artificial Intelligence, pp. 1243-1248. AAAI Press.\r
Connell, J. (1989). A colony architecture for an artificial creature. Technical Report AI-TR-1151.\r
MIT Artificial Intelligence Laboratory, Cambridge, MA.\r
Connell, M. E., Utgo↵, P. E. (1987). Learning to control a dynamic physical system. Computa\u0002tional intelligence, 3 (1):330–337.\r
Contreras-Vidal, J. L., Schultz, W. (1999). A predictive reinforcement model of dopamine neurons\r
for learning approach behavior. Journal of Computational Neuroscience, 6(3):191–214.\r
Coulom, R. (2006). Ecient selectivity and backup operators in Monte-Carlo tree search. In\r
Proceedings of the 5th International Conference on Computers and Games (CG’06), pp. 72–83.\r
Springer-Verlag Berlin, Heidelberg.\r
Courville, A. C., Daw, N. D., Touretzky, D. S. (2006). Bayesian theories of conditioning in a\r
changing world. Trends in Cognitive Science, 10(7):294–300.\r
Craik, K. J. W. (1943). The Nature of Explanation. Cambridge University Press, Cambridge.\r
Cross, J. G. (1973). A stochastic learning model of economic behavior. The Quarterly Journal\r
of Economics, 87 (2):239–266.\r
Crow, T. J. (1968). Cortical synapses and reinforcement: a hypothesis. Nature, 219(5155):736–\r
737.\r
Curtiss, J. H. (1954). A theoretical comparison of the eciencies of two classical methods and a\r
Monte Carlo method for computing one component of the solution of a set of linear algebraic\r
equations. In H. A. Meyer (Ed.), Symposium on Monte Carlo Methods, pp. 191–233. Wiley,\r
New York.\r
Cybenko, G. (1989). Approximation by superpositions of a sigmoidal function. Mathematics of\r
control, signals and systems, 2(4):303–314.\r
Cziko, G. (1995). Without Miracles: Universal Selection Theory and the Second Darvinian\r
Revolution. MIT Press, Cambridge, MA.\r
Dabney, W. (2014). Adaptive step-sizes for reinforcement learning. PhD thesis, University of\r
Massachusetts, Amherst.\r
Dabney, W., Barto, A. G. (2012). Adaptive step-size for online temporal di↵erence learning. In\r
Proceedings of the Annual Conference of the Association for the Advancement of Artificial\r
Intelligence.\r
Daniel, J. W. (1976). Splines and eciency in dynamic programming. Journal of Mathematical\r
Analysis and Applications, 54:402–407.\r
Dann, C., Neumann, G., Peters, J. (2014). Policy evaluation with temporal di↵erences: A survey\r
and comparison. Journal of Machine Learning Research, 15 :809–883.

References 489\r
Daw, N. D., Courville, A. C., Touretzky, D. S. (2003). Timing and partial observability in the\r
dopamine system. In Advances in Neural Information Processing Systems 15, pp. 99–106.\r
MIT Press, Cambridge, MA.\r
Daw, N. D., Courville, A. C., Touretzky, D. S. (2006). Representation and timing in theories of\r
the dopamine system. Neural Computation, 18(7):1637–1677.\r
Daw, N. D., Niv, Y., Dayan, P. (2005). Uncertainty based competition between prefrontal and\r
dorsolateral striatal systems for behavioral control. Nature Neuroscience, 8(12):1704–1711.\r
Daw, N. D., Shohamy, D. (2008). The cognitive neuroscience of motivation and learning. Social\r
Cognition, 26(5):593–620.\r
Dayan, P. (1991). Reinforcement comparison. In D. S. Touretzky, J. L. Elman, T. J. Sejnowski,\r
and G. E. Hinton (Eds.), Connectionist Models: Proceedings of the 1990 Summer School,\r
pp. 45–51. Morgan Kaufmann.\r
Dayan, P. (1992). The convergence of TD() for general . Machine Learning, 8(3):341–362.\r
Dayan, P. (2002). Matters temporal. Trends in Cognitive Sciences, 6 (3):105–106.\r
Dayan, P., Abbott, L. F. (2001). Theoretical Neuroscience: Computational and Mathematical\r
Modeling of Neural Systems. MIT Press, Cambridge, MA.\r
Dayan, P., Berridge, K. C. (2014). Model-based and model-free Pavlovian reward learning:\r
Revaluation, revision, and revaluation. Cognitive, A↵ective, & Behavioral Neuroscience,\r
14(2):473–492.\r
Dayan, P., Niv, Y. (2008). Reinforcement learning: the good, the bad and the ugly. Current\r
Opinion in Neurobiology, 18(2):185–196.\r
Dayan, P., Niv, Y., Seymour, B., Daw, N. D. (2006). The misbehavior of value and the discipline\r
of the will. Neural Networks, 19 (8):1153–1160.\r
Dayan, P., Sejnowski, T. (1994). TD() converges with probability 1. Machine Learning,\r
14(3):295–301.\r
De Asis, K., Hernandez-Garcia, J. F., Holland, G. Z., Sutton, R. S. (2017). Multi-step Rein\u0002forcement Learning: A Unifying Algorithm. ArXiv:1703.01327.\r
de Farias, D. P. (2002). The Linear Programming Approach to Approximate Dynamic Program\u0002ming: Theory and Application. Stanford University PhD thesis.\r
de Farias, D. P., Van Roy, B. (2003). The linear programming approach to approximate dynamic\r
programming. Operations Research 51 (6):850–865.\r
Dean, T., Lin, S.-H. (1995). Decomposition techniques for planning in stochastic domains.\r
In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence,\r
pp. 1121–1127. Morgan Kaufmann. See also Technical Report CS-95-10, Brown University,\r
Department of Computer Science, 1995.\r
Degris, T., Pilarski, P. M., Sutton, R. S. (2012). Model-free reinforcement learning with\r
continuous action in practice. In 2012 American Control Conference, pp. 2177–2182. IEEE.\r
Degris, T., White, M., Sutton, R. S. (2012). O↵-policy actor–critic. In Proceedings of the 29th\r
International Conference on Machine Learning. ArXiv:1205.4839, 2012.\r
Denardo, E. V. (1967). Contraction mappings in the theory underlying dynamic programming.\r
SIAM Review, 9(2):165–177.\r
Dennett, D. C. (1978). Why the Law of E↵ect Will Not Go Away. Brainstorms, pp. 71–89.\r
Bradford/MIT Press, Cambridge, MA.\r
Derthick, M. (1984). Variations on the Boltzmann machine learning algorithm. Carnegie-Mellon\r
University Department of Computer Science Technical Report No. CMU-CS-84-120.\r
Deutsch, J. A. (1953). A new type of behaviour theory. British Journal of Psychology. General\r
Section, 44(4):304–317."""

[[sections]]
number = "490"
title = "References"
text = """
Deutsch, J. A. (1954). A machine with insight. Quarterly Journal of Experimental Psychology,\r
6(1):6–11.\r
Dick, T. (2015). Policy Gradient Reinforcement Learning Without Regret. M.Sc. thesis,\r
University of Alberta.\r
Dickinson, A. (1980). Contemporary Animal Learning Theory. Cambridge University Press.\r
Dickinson, A. (1985). Actions and habits: the development of behavioral autonomy. Phil. Trans.\r
R. Soc. Lond. B, 308(1135):67–78.\r
Dickinson, A., Balleine, B. W. (2002). The role of learning in motivation. In C. R. Gallistel\r
(Ed.), Stevens’ Handbook of Experimental Psychology, volume 3, pp. 497–533. Wiley, NY.\r
Dietterich, T. G., Buchanan, B. G. (1984). The role of the critic in learning systems. In O. G.\r
Selfridge, E. L. Rissland, and M. A. Arbib (Eds.), Adaptive Control of Ill-Defined Systems,\r
pp. 127–147. Plenum Press, NY.\r
Dietterich, T. G., Flann, N. S. (1995). Explanation-based learning and reinforcement learning:\r
A unified view. In A. Prieditis and S. Russell (Eds.), Proceedings of the 12th International\r
Conference on Machine Learning, pp. 176–184. Morgan Kaufmann.\r
Dietterich, T. G., Wang, X. (2002). Batch value function approximation via support vectors.\r
In Advances in Neural Information Processing Systems 14, pp. 1491–1498. MIT Press,\r
Cambridge, MA.\r
Diuk, C., Cohen, A., Littman, M. L. (2008). An object-oriented representation for ecient\r
reinforcement learning. In Proceedings of the 25th International Conference on Machine\r
Learning, pp. 240–247. ACM, New York.\r
Dolan, R. J., Dayan, P. (2013). Goals and habits in the brain. Neuron, 80(2):312–325.\r
Doll, B. B., Simon, D. A., Daw, N. D. (2012). The ubiquity of model-based reinforcement\r
learning. Current Opinion in Neurobiology, 22(6):1–7.\r
Donahoe, J. W., Burgos, J. E. (2000). Behavior analysis and revaluation. Journal of the\r
Experimental Analysis of Behavior, 74(3):331–346.\r
Dorigo, M., Colombetti, M. (1994). Robot shaping: Developing autonomous agents through\r
learning. Artificial Intelligence, 71(2):321–370.\r
Doya, K. (1996). Temporal di↵erence learning in continuous time and space. In Advances in\r
Neural Information Processing Systems 8, pp. 1073–1079. MIT Press, Cambridge, MA.\r
Doya, K., Sejnowski, T. J. (1995). A novel reinforcement model of birdsong vocalization\r
learning. In Advances in Neural Information Processing Systems 7, pp. 101–108. MIT Press,\r
Cambridge, MA.\r
Doya, K., Sejnowski, T. J. (1998). A computational model of birdsong learning by auditory\r
experience and auditory feedback. In P. W. F. Poon and J. F. Brugge (Eds.), Central\r
Auditory Processing and Neural Modeling, pp. 77–88. Springer, Boston, MA.\r
Doyle, P. G., Snell, J. L. (1984). Random Walks and Electric Networks. The Mathematical\r
Association of America. Carus Mathematical Monograph 22.\r
Dreyfus, S. E., Law, A. M. (1977). The Art and Theory of Dynamic Programming. Academic\r
Press, New York.\r
Du, S. S., Chen, J., Li, L., Xiao, L., Zhou, D. (2017). Stochastic variance reduction methods for\r
policy evaluation. Proceedings of the 34th International Conference on Machine Learning,\r
pp. 1049–1058. ArXiv:1702.07944.\r
Duda, R. O., Hart, P. E. (1973). Pattern Classification and Scene Analysis. Wiley, New York.

References 491\r
Du↵, M. O. (1995). Q-learning for bandit problems. In Proceedings of the 12th International\r
Conference on Machine Learning, pp. 209–217. Morgan Kaufmann.\r
Egger, D. M., Miller, N. E. (1962). Secondary reinforcement in rats as a function of information\r
value and reliability of the stimulus. Journal of Experimental Psychology, 64:97–104.\r
Eshel, N., Tian, J., Bukwich, M., Uchida, N. (2016). Dopamine neurons share common response\r
function for reward prediction error. Nature Neuroscience, 19 (3):479–486.\r
Estes, W. K. (1943). Discriminative conditioning. I. A discriminative property of conditioned\r
anticipation. Journal of Experimental Psychology, 32 (2):150–155.\r
Estes, W. K. (1948). Discriminative conditioning. II. E↵ects of a Pavlovian conditioned stimulus\r
upon a subsequently established operant response. Journal of Experimental Psychology,\r
38 (2):173–177.\r
Estes, W. K. (1950). Toward a statistical theory of learning. Psychololgical Review, 57(2):94–107.\r
Farley, B. G., Clark, W. A. (1954). Simulation of self-organizing systems by digital computer.\r
IRE Transactions on Information Theory, 4(4):76–84.\r
Farries, M. A., Fairhall, A. L. (2007). Reinforcement learning with modulated spike timing\u0002dependent synaptic plasticity. Journal of Neurophysiology, 98(6):3648–3665.\r
Feldbaum, A. A. (1965). Optimal Control Systems. Academic Press, New York.\r
Finch, G., Culler, E. (1934). Higher order conditioning with constant motivation. The American\r
Journal of Psychology:596–602.\r
Finnsson, H., Bj¨ornsson, Y. (2008). Simulation-based approach to general game playing. In\r
Proceedings of the Association for the Advancement of Artificial Intelligence, pp. 259–264.\r
Fiorillo, C. D., Yun, S. R., Song, M. R. (2013). Diversity and homogeneity in responses of\r
midbrain dopamine neurons. The Journal of Neuroscience, 33(11):4693–4709.\r
Florian, R. V. (2007). Reinforcement learning through modulation of spike-timing-dependent\r
synaptic plasticity. Neural Computation, 19(6):1468–1502.\r
Fogel, L. J., Owens, A. J., Walsh, M. J. (1966). Artificial Intelligence through Simulated Evolution.\r
John Wiley and Sons.\r
French, R. M. (1999). Catastrophic forgetting in connectionist networks. Trends in cognitive\r
sciences, 3 (4):128–135.\r
Frey, U., Morris, R. G. M. (1997). Synaptic tagging and long-term potentiation. Nature,\r
385(6616):533–536.\r
Fr´emaux, N., Sprekeler, H., Gerstner, W. (2010). Functional requirements for reward-modulated\r
spike-timing-dependent plasticity. The Journal of Neuroscience, 30 (40): 13326–13337\r
Friedman, J. H., Bentley, J. L., Finkel, R. A. (1977). An algorithm for finding best matches in\r
logarithmic expected time. ACM Transactions on Mathematical Software, 3 (3):209–226.\r
Friston, K. J., Tononi, G., Reeke, G. N., Sporns, O., Edelman, G. M. (1994). Value-dependent\r
selection in the brain: Simulation in a synthetic neural model. Neuroscience, 59(2):229–243.\r
Fu, K. S. (1970). Learning control systems—Review and outlook. IEEE Transactions on\r
Automatic Control, 15(2):210–221.\r
Galanter, E., Gerstenhaber, M. (1956). On thought: The extrinsic theory. Psychological Review,\r
63(4):218–227.\r
Gallistel, C. R. (2005). Deconstructing the law of e↵ect. Games and Economic Behavior,\r
52 (2):410–423.\r
Gardner, M. (1973). Mathematical games. Scientific American, 228(1):108–115.\r
Geist, M., Scherrer, B. (2014). O↵-policy learning with eligibility traces: A survey. Journal of\r
Machine Learning Research, 15 (1):289–333."""

[[sections]]
number = "492"
title = "References"
text = """
Gelly, S., Silver, D. (2007). Combining online and o✏ine knowledge in UCT. Proceedings of the\r
24th International Conference on Machine Learning, pp. 273–280.\r
Gelperin, A., Hopfield, J. J., Tank, D. W. (1985). The logic of limax learning. In A. Selverston\r
(Ed.), Model Neural Networks and Behavior, pp. 247–261. Plenum Press, New York.\r
Genesereth, M., Thielscher, M. (2014). General game playing. Synthesis Lectures on Artificial\r
Intelligence and Machine Learning, 8 (2):1–229.\r
Gershman, S. J., Moustafa, A. A., Ludvig, E. A. (2014). Time representation in reinforcement\r
learning models of the basal ganglia. Frontiers in Computational Neuroscience, 7:194.\r
Gershman, S. J., Pesaran, B., Daw, N. D. (2009). Human reinforcement learning subdivides\r
structured action spaces by learning e↵ector-specific values. The Journal of Neuroscience,\r
29 (43):13524–13531.\r
Ghiassian, S., Rafiee, B., Sutton, R. S. (2016). A first empirical study of emphatic temporal\r
di↵erence learning. Workshop on Continual Learning and Deep Learning at the Conference\r
on Neural Information Processing Systems. ArXiv:1705.04185.\r
Ghiassian, S., Patterson, A., White, M., Sutton, R. S., White, A. (2018). Online o↵-policy\r
prediction. ArXiv:1811.02597.\r
Gibbs, C. M., Cool, V., Land, T., Kehoe, E. J., Gormezano, I. (1991). Second-order conditioning\r
of the rabbit’s nictitating membrane response. Integrative Physiological and Behavioral\r
Science, 26 (4):282–295.\r
Gittins, J. C., Jones, D. M. (1974). A dynamic allocation index for the sequential design of\r
experiments. In J. Gani, K. Sarkadi, and I. Vincze (Eds.), Progress in Statistics, pp. 241–266.\r
North-Holland, Amsterdam–London.\r
Glimcher, P. W. (2011). Understanding dopamine and reinforcement learning: The dopamine\r
reward prediction error hypothesis. Proceedings of the National Academy of Sciences,\r
108(Supplement 3):15647–15654.\r
Glimcher, P. W. (2003). Decisions, Uncertainty, and the Brain: The science of Neuroeconomics.\r
MIT Press, Cambridge, MA.\r
Glimcher, P. W., Fehr, E. (Eds.) (2013). Neuroeconomics: Decision Making and the Brain,\r
Second Edition. Academic Press.\r
Goethe, J. W. V. (1878). The Sorcerer’s Apprentice. In The Permanent Goethe, p. 349. The\r
Dial Press, Inc., New York.\r
Goldstein, H. (1957). Classical Mechanics. Addison-Wesley, Reading, MA.\r
Goodfellow, I., Bengio, Y., Courville, A. (2016). Deep Learning. MIT Press, Cambridge, MA.\r
Goodwin, G. C., Sin, K. S. (1984). Adaptive Filtering Prediction and Control. Prentice-Hall,\r
Englewood Cli↵s, NJ.\r
Gopnik, A., Glymour, C., Sobel, D., Schulz, L. E., Kushnir, T., Danks, D. (2004). A theory of\r
causal learning in children: Causal maps and Bayes nets. Psychological Review, 111(1):3–32.\r
Gordon, G. J. (1995). Stable function approximation in dynamic programming. In A. Prieditis\r
and S. Russell (Eds.), Proceedings of the 12th International Conference on Machine Learning,\r
pp. 261–268. Morgan Kaufmann. An expanded version was published as Technical Report\r
CMU-CS-95-103. Carnegie Mellon University, Pittsburgh, PA, 1995.\r
Gordon, G. J. (1996a). Chattering in SARSA(). CMU learning lab internal report.\r
Gordon, G. J. (1996b). Stable fitted reinforcement learning. In Advances in Neural Information\r
Processing Systems 8, pp. 1052–1058. MIT Press, Cambridge, MA.\r
Gordon, G. J. (1999). Approximate Solutions to Markov Decision Processes. PhD thesis,\r
Carnegie Mellon University, Pittsburgh PA. Pittsburgh, PA.\r
Gordon, G. J. (2001). Reinforcement learning with function approximation converges to a

References 493\r
region. In Advances in Neural Information Processing Systems 13, pp. 1040–1046. MIT\r
Press, Cambridge, MA.\r
Graybiel, A. M. (2000). The basal ganglia. Current Biology, 10(14):R509–R511.\r
Greensmith, E., Bartlett, P. L., Baxter, J. (2002). Variance reduction techniques for gradient\r
estimates in reinforcement learning. In Advances in Neural Information Processing Systems\r
14, pp. 1507–1514. MIT Press, Cambridge, MA.\r
Greensmith, E., Bartlett, P. L., Baxter, J. (2004). Variance reduction techniques for gradient\r
estimates in reinforcement learning. Journal of Machine Learning Research, 5 (Nov):1471–\r
1530.\r
Grith, A. K. (1966). A new machine learning technique applied to the game of checkers.\r
Technical Report Project MAC, Artificial Intelligence Memo 94. Massachusetts Institute of\r
Technology, Cambridge, MA.\r
Grith, A. K. (1974). A comparison and evaluation of three machine learning procedures as\r
applied to the game of checkers. Artificial Intelligence, 5(2):137–148.\r
Grondman, I., Busoniu, L., Lopes, G. A., Babuska, R. (2012). A survey of actor–critic reinforce\u0002ment learning: Standard and natural policy gradients. IEEE Transactions on Systems, Man,\r
and Cybernetics, Part C (Applications and Reviews), 42 (6):1291–1307.\r
Grossberg, S. (1975). A neural model of attention, reinforcement, and discrimination learning.\r
International Review of Neurobiology, 18:263–327.\r
Grossberg, S., Schmajuk, N. A. (1989). Neural dynamics of adaptive timing and temporal\r
discrimination during associative learning. Neural Networks, 2(2):79–102.\r
Gullapalli, V. (1990). A stochastic reinforcement algorithm for learning real-valued functions.\r
Neural Networks, 3(6): 671–692.\r
Gullapalli, V., Barto, A. G. (1992). Shaping as a method for accelerating reinforcement learning.\r
In Proceedings of the 1992 IEEE International Symposium on Intelligent Control, pp. 554–559.\r
IEEE.\r
Gurvits, L., Lin, L.-J., Hanson, S. J. (1994). Incremental learning of evaluation functions\r
for absorbing Markov chains: New methods and theorems. Siemans Corporate Research,\r
Princeton, NJ.\r
Hackman, L. (2012). Faster Gradient-TD Algorithms. M.Sc. thesis, University of Alberta,\r
Edmonton.\r
Hallak, A., Tamar, A., Mannor, S. (2015). Emphatic TD Bellman operator is a contraction.\r
ArXiv:1508.03411.\r
Hallak, A., Tamar, A., Munos, R., Mannor, S. (2016). Generalized emphatic temporal di↵erence\r
learning: Bias-variance analysis. In Proceedings of the Thirtieth AAAI Conference on\r
Artificial Intelligence, pp. 1631–1637. AAAI Press, Menlo Park, CA.\r
Hammer, M. (1997). The neural basis of associative reward learning in honeybees. Trends in\r
Neuroscience, 20(6):245–252.\r
Hammer, M., Menzel, R. (1995). Learning and memory in the honeybee. The Journal of\r
Neuroscience, 15(3):1617–1630.\r
Hampson, S. E. (1983). A Neural Model of Adaptive Behavior. PhD thesis, University of\r
California, Irvine.\r
Hampson, S. E. (1989). Connectionist Problem Solving: Computational Aspects of Biological\r
Learning. Birkhauser, Boston.\r
Hare, T. A., O’Doherty, J., Camerer, C. F., Schultz, W., Rangel, A. (2008). Dissociating the role\r
of the orbitofrontal cortex and the striatum in the computation of goal values and prediction\r
errors. The Journal of Neuroscience, 28(22):5623–5630."""

[[sections]]
number = "494"
title = "References"
text = """
Harth, E., Tzanakou, E. (1974). Alopex: A stochastic method for determining visual receptive\r
fields. Vision Research, 14 (12):1475–1482.\r
Hassabis, D., Maguire, E. A. (2007). Deconstructing episodic memory with construction. Trends\r
in Cognitive Sciences, 11(7):299–306.\r
Hauskrecht, M., Meuleau, N., Kaelbling, L. P., Dean, T., Boutilier, C. (1998). Hierarchical\r
solution of Markov decision processes using macro-actions. In Proceedings of the Fourteenth\r
Conference on Uncertainty in Artificial Intelligence, pp. 220–229. Morgan Kaufmann.\r
Hawkins, R. D., Kandel, E. R. (1984). Is there a cell-biological alphabet for simple forms of\r
learning? Psychological Review, 91(3):375–391.\r
Haykin, S. (1994). Neural networks: A Comprehensive Foundation, Macmillan, New York.\r
He, K., Huertas, M., Hong, S. Z., Tie, X., Hell, J. W., Shouval, H., Kirkwood, A. (2015). Distinct\r
eligibility traces for LTP and LTD in cortical synapses. Neuron, 88(3):528–538.\r
He, K., Zhang, X., Ren, S., Sun, J. (2016). Deep residual learning for image recognition. In\r
Proceedings of the 1992 IEEE Conference on Computer Vision and Pattern Recognition,\r
pp. 770–778.\r
Hebb, D. O. (1949). The Organization of Behavior: A Neuropsychological Theory. John Wiley\r
and Sons Inc., New York. Reissued by Lawrence Erlbaum Associates Inc., Mahwah NJ, 2002.\r
Hengst, B. (2012). Hierarchical approaches. In M. Wiering and M. van Otterlo (Eds.), Rein\u0002forcement Learning: State-of-the-Art, pp. 293–323. Springer-Verlag Berlin Heidelberg.\r
Herrnstein, R. J. (1970). On the Law of E↵ect. Journal of the Experimental Analysis of Behavior,\r
13 (2):243–266.\r
Hersh, R., Griego, R. J. (1969). Brownian motion and potential theory. Scientific American,\r
220(3):66–74.\r
Hester, T., Stone, P. (2012). Learning and using models. In M. Wiering and M. van Otterlo (Eds.),\r
Reinforcement Learning: State-of-the-Art, pp. 111–141. Springer-Verlag Berlin Heidelberg.\r
Hesterberg, T. C. (1988), Advances in Importance Sampling, PhD thesis, Statistics Department,\r
Stanford University.\r
Hilgard, E. R. (1956). Theories of Learning, Second Edition. Appleton-Century-Cofts, Inc.,\r
New York.\r
Hilgard, E. R., Bower, G. H. (1975). Theories of Learning. Prentice-Hall, Englewood Cli↵s, NJ.\r
Hinton, G. E. (1984). Distributed representations. Technical Report CMU-CS-84-157. Depart\u0002ment of Computer Science, Carnegie-Mellon University, Pittsburgh, PA.\r
Hinton, G. E., Osindero, S., Teh, Y. (2006). A fast learning algorithm for deep belief nets.\r
Neural Computation, 18(7):1527–1554.\r
Hochreiter, S., Schmidhuber, J. (1997). LTSM can solve hard time lag problems. In Advances\r
in Neural Information Processing Systems 9, pp. 473–479. MIT Press, Cambridge, MA.\r
Holland, J. H. (1975). Adaptation in Natural and Artificial Systems. University of Michigan\r
Press, Ann Arbor.\r
Holland, J. H. (1976). Adaptation. In R. Rosen and F. M. Snell (Eds.), Progress in Theoretical\r
Biology, vol. 4, pp. 263–293. Academic Press, New York.\r
Holland, J. H. (1986). Escaping brittleness: The possibility of general-purpose learning algorithms\r
applied to rule-based systems. In R. S. Michalski, J. G. Carbonell, and T. M. Mitchell\r
(Eds.), Machine Learning: An Artificial Intelligence Approach, vol. 2, pp. 593–623. Morgan\r
Kaufmann.\r
Hollerman, J. R., Schultz, W. (1998). Dopmine neurons report an error in the temporal\r
prediction of reward during learning. Nature Neuroscience, 1(4):304–309.

References 495\r
Houk, J. C., Adams, J. L., Barto, A. G. (1995). A model of how the basal ganglia generates and\r
uses neural signals that predict reinforcement. In J. C. Houk, J. L. Davis, and D. G. Beiser\r
(Eds.), Models of Information Processing in the Basal Ganglia, pp. 249–270. MIT Press,\r
Cambridge, MA.\r
Howard, R. (1960). Dynamic Programming and Markov Processes. MIT Press, Cambridge, MA.\r
Hull, C. L. (1932). The goal-gradient hypothesis and maze learning. Psychological Review,\r
39(1):25–43.\r
Hull, C. L. (1943). Principles of Behavior. Appleton-Century, New York.\r
Hull, C. L. (1952). A Behavior System. Wiley, New York.\r
Io↵e, S., Szegedy, C. (2015). Batch normalization: Accelerating deep network training by\r
reducing internal covariate shift. ArXiv:1502.03167.\r
˙\r
Ipek, E., Mutlu, O., Mart´ınez, J. F., Caruana, R. (2008). Self-optimizing memory controllers: A\r
reinforcement learning approach. In ISCA’08:Proceedings of the 35th Annual International\r
Symposium on Computer Architecture, pp. 39–50. IEEE Computer Society Washington, DC.\r
Izhikevich, E. M. (2007). Solving the distal reward problem through linkage of STDP and\r
dopamine signaling. Cerebral Cortex, 17(10):2443–2452.\r
Jaakkola, T., Jordan, M. I., Singh, S. P. (1994). On the convergence of stochastic iterative\r
dynamic programming algorithms. Neural Computation, 6:1185–1201.\r
Jaakkola, T., Singh, S. P., Jordan, M. I. (1995). Reinforcement learning algorithm for partially\r
observable Markov decision problems. In Advances in Neural Information Processing Systems\r
7, pp. 345–352. MIT Press, Cambridge, MA.\r
Jacobs, R. A. (1988). Increased rates of convergence through learning rate adaptation. Neural\r
Networks, 1 (4):295–307.\r
Jaderberg, M., Mnih, V., Czarnecki, W. M., Schaul, T., Leibo, J. Z., Silver, D., Kavukcuoglu, K.\r
(2016). Reinforcement learning with unsupervised auxiliary tasks. ArXiv:1611.05397.\r
Jaeger, H. (1997). Observable operator models and conditioned continuation representations. Ar\u0002beitspapiere der GMD 1043, GMD Forschungszentrum Informationstechnik, Sankt Augustin,\r
Germany.\r
Jaeger, H. (1998). Discrete Time, Discrete Valued Observable Operator Models: A Tutorial.\r
GMD-Forschungszentrum Informationstechnik.\r
Jaeger, H. (2000). Observable operator models for discrete stochastic time series. Neural\r
Computation, 12 (6):1371–1398.\r
Jaeger, H. (2002). Tutorial on training recurrent neural networks, covering BPPT, RTRL, EKF\r
and the ‘echo state network’ approach. German National Research Center for Information\r
Technology, Technical Report GMD report 159, 2002.\r
Joel, D., Niv, Y., Ruppin, E. (2002). Actor–critic models of the basal ganglia: New anatomical\r
and computational perspectives. Neural Networks, 15(4):535–547.\r
Johnson, A., Redish, A. D. (2007). Neural ensembles in CA3 transiently encode paths forward\r
of the animal at a decision point. The Journal of Neuroscience, 27(45):12176–12189.\r
Kaelbling, L. P. (1993a). Hierarchical learning in stochastic domains: Preliminary results. In\r
Proceedings of the 10th International Conference on Machine Learning, pp. 167–173. Morgan\r
Kaufmann.\r
Kaelbling, L. P. (1993b). Learning in Embedded Systems. MIT Press, Cambridge, MA.\r
Kaelbling, L. P. (Ed.) (1996). Special triple issue on reinforcement learning, Machine Learning,\r
22(1/2/3).\r
Kaelbling, L. P., Littman, M. L., Moore, A. W. (1996). Reinforcement learning: A survey.\r
Journal of Artificial Intelligence Research, 4:237–285."""

[[sections]]
number = "496"
title = "References"
text = """
Kakade, S. M. (2002). A natural policy gradient. In Advances in Neural Information Processing\r
Systems 14, pp. 1531–1538. MIT Press, Cambridge, MA.\r
Kakade, S. M. (2003). On the Sample Complexity of Reinforcement Learning. PhD thesis,\r
University of London.\r
Kakutani, S. (1945). Markov processes and the Dirichlet problem. Proceedings of the Japan\r
Academy, 21(3-10):227–233.\r
Kalos, M. H., Whitlock, P. A. (1986). Monte Carlo Methods. Wiley, New York.\r
Kamin, L. J. (1968). “Attention-like” processes in classical conditioning. In M. R. Jones (Ed.),\r
Miami Symposium on the Prediction of Behavior, 1967: Aversive Stimulation, pp. 9–31.\r
University of Miami Press, Coral Gables, Florida.\r
Kamin, L. J. (1969). Predictability, surprise, attention, and conditioning. In B. A. Campbell and\r
R. M. Church (Eds.), Punishment and Aversive Behavior, pp. 279–296. Appleton-Century\u0002Crofts, New York.\r
Kandel, E. R., Schwartz, J. H., Jessell, T. M., Siegelbaum, S. A., Hudspeth, A. J. (Eds.) (2013).\r
Principles of Neural Science, Fifth Edition. McGraw-Hill Companies, Inc.\r
Karampatziakis, N., Langford, J. (2010). Online importance weight aware updates. ArXiv:1011.1576.\r
Kashyap, R. L., Blaydon, C. C., Fu, K. S. (1970). Stochastic approximation. In J. M. Mendel\r
and K. S. Fu (Eds.), Adaptive, Learning, and Pattern Recognition Systems: Theory and\r
Applications, pp. 329–355. Academic Press, New York.\r
Kearney, A., Veeriah, V, Travnik, J, Sutton, R. S., Pilarski, P. M. (in preparation). TIDBD:\r
Adapting Temporal-di↵erence Step-sizes Through Stochastic Meta-descent.\r
Kearns, M., Singh, S. (2002). Near-optimal reinforcement learning in polynomial time. Machine\r
Learning, 49 (2-3):209–232.\r
Keerthi, S. S., Ravindran, B. (1997). Reinforcement learning. In E. Fieslerm and R. Beale\r
(Eds.), Handbook of Neural Computation, C3. Oxford University Press, New York.\r
Kehoe, E. J. (1982). Conditioning with serial compound stimuli: Theoretical and empirical\r
issues. Experimental Animal Behavior, 1:30–65.\r
Kehoe, E. J., Schreurs, B. G., Graham, P. (1987). Temporal primacy overrides prior training\r
in serial compound conditioning of the rabbit’s nictitating membrane response. Animal\r
Learning & Behavior, 15(4):455–464.\r
Keiflin, R., Janak, P. H. (2015). Dopamine prediction errors in reward learning and addiction:\r
From theory to neural circuitry. Neuron, 88(2):247– 263.\r
Kimble, G. A. (1961). Hilgard and Marquis’ Conditioning and Learning. Appleton-Century\u0002Crofts, New York.\r
Kimble, G. A. (1967). Foundations of Conditioning and Learning. Appleton-Century-Crofts,\r
New York.\r
Kingma, D., Ba, J. (2014). Adam: A method for stochastic optimization. ArXiv:1412.6980.\r
Klopf, A. H. (1972). Brain function and adaptive systems—A heterostatic theory. Technical\r
Report AFCRL-72-0164, Air Force Cambridge Research Laboratories, Bedford, MA. A\r
summary appears in Proceedings of the International Conference on Systems, Man, and\r
Cybernetics (1974). IEEE Systems, Man, and Cybernetics Society, Dallas, TX.\r
Klopf, A. H. (1975). A comparison of natural and artificial intelligence. SIGART Newsletter,\r
53:11–13.\r
Klopf, A. H. (1982). The Hedonistic Neuron: A Theory of Memory, Learning, and Intelligence.\r
Hemisphere, Washington, DC.\r
Klopf, A. H. (1988). A neuronal model of classical conditioning. Psychobiology, 16(2):85–125.

References 497\r
Klyubin, A. S., Polani, D., Nehaniv, C. L. (2005). Empowerment: A universal agent-centric\r
measure of control. In Proceedings of the 2005 IEEE Congress on Evolutionary Computation\r
(Vol. 1, pp. 128–135). IEEE.\r
Kober, J., Peters, J. (2012). Reinforcement learning in robotics: A survey. In M. Wiering, M.\r
van Otterlo (Eds.), Reinforcement Learning: State-of-the-Art, pp. 579–610. Springer-Verlag.\r
Kocsis, L., Szepesv´ari, Cs. (2006). Bandit based Monte-Carlo planning. In Proceedings of the\r
European Conference on Machine Learning, pp. 282–293. Springer-Verlag Berlin Heidelberg.\r
Kohonen, T. (1977). Associative Memory: A System Theoretic Approach. Springer-Verlag,\r
Berlin.\r
Koller, D., Friedman, N. (2009). Probabilistic Graphical Models: Principles and Techniques.\r
MIT Press.\r
Kolodziejski, C., Porr, B., W¨org¨otter, F. (2009). On the asymptotic equivalence between\r
di↵erential Hebbian and temporal di↵erence learning. Neural Computation, 21(4):1173–1202.\r
Kolter, J. Z. (2011). The fixed points of o↵-policy TD. In Advances in Neural Information\r
Processing Systems 24, pp. 2169–2177. Curran Associates, Inc.\r
Konda, V. R., Tsitsiklis, J. N. (2000). Actor-critic algorithms. In Advances in Neural Information\r
Processing Systems 12, pp. 1008–1014. MIT Press, Cambridge, MA.\r
Konda, V. R., Tsitsiklis, J. N. (2003). On actor-critic algorithms. SIAM Journal on Control\r
and Optimization, 42 (4):1143–1166.\r
Konidaris, G. D., Osentoski, S., Thomas, P. S. (2011). Value function approximation in\r
reinforcement learning using the Fourier basis . In Proceedings of the Twenty-Fifth Conference\r
of the Association for the Advancement of Artificial Intelligence, pp. 380–385.\r
Korf, R. E. (1988). Optimal path finding algorithms. In L. N. Kanal and V. Kumar (Eds.),\r
Search in Artificial Intelligence, pp. 223–267. Springer-Verlag, Berlin.\r
Korf, R. E. (1990). Real-time heuristic search. Artificial Intelligence, 42 (2–3), 189–211.\r
Koshland, D. E. (1980). Bacterial Chemotaxis as a Model Behavioral System. Raven Press, New\r
York.\r
Koza, J. R. (1992). Genetic Programming: On the Programming of Computers by Means of\r
Natural Selection (Vol. 1). MIT Press., Cambridge, MA.\r
Kraft, L. G., Campagna, D. P. (1990). A summary comparison of CMAC neural network and\r
traditional adaptive control systems. In T. Miller, R. S. Sutton, and P. J. Werbos (Eds.),\r
Neural Networks for Control, pp. 143–169. MIT Press, Cambridge, MA.\r
Kraft, L. G., Miller, W. T., Dietz, D. (1992). Development and application of CMAC neural\r
network-based control. In D. A. White and D. A. Sofge (Eds.), Handbook of Intelligent\r
Control: Neural, Fuzzy, and Adaptive Approaches, pp. 215–232. Van Nostrand Reinhold,\r
New York.\r
Kumar, P. R., Varaiya, P. (1986). Stochastic Systems: Estimation, Identification, and Adaptive\r
Control. Prentice-Hall, Englewood Cli↵s, NJ.\r
Kumar, P. R. (1985). A survey of some results in stochastic adaptive control. SIAM Journal of\r
Control and Optimization, 23(3):329–380.\r
Kumar, V., Kanal, L. N. (1988). The CDP, A unifying formulation for heuristic search, dynamic\r
programming, and branch-and-bound. In L. N. Kanal and V. Kumar (Eds.), Search in\r
Artificial Intelligence, pp. 1–37. Springer-Verlag, Berlin.\r
Kushner, H. J., Dupuis, P. (1992). Numerical Methods for Stochastic Control Problems in\r
Continuous Time. Springer-Verlag, New York."""

[[sections]]
number = "498"
title = "References"
text = """
Kuvayev, L., Sutton, R.S. (1996). Model-based reinforcement learning with an approximate,\r
learned model. Proceedings of the Ninth Yale Workshop on Adaptive and Learning Systems,\r
pp. 101–105, Yale University, New Haven, CT.\r
Lagoudakis, M., Parr, R. (2003). Least squares policy iteration. Journal of Machine Learning\r
Research, 4 (Dec):1107–1149.\r
Lai, T. L., Robbins, H. (1985). Asymptotically ecient adaptive allocation rules. Advances in\r
Applied Mathematics, 6 (1):4–22.\r
Lakshmivarahan, S., Narendra, K. S. (1982). Learning algorithms for two-person zero-sum\r
stochastic games with incomplete information: A unified approach. SIAM Journal of Control\r
and Optimization, 20(4):541–552.\r
Lammel, S., Lim, B. K., Malenka, R. C. (2014). Reward and aversion in a heterogeneous\r
midbrain dopamine system. Neuropharmacology, 76:353–359.\r
Lane, S. H., Handelman, D. A., Gelfand, J. J. (1992). Theory and development of higher-order\r
CMAC neural networks. IEEE Control Systems, 12 (2):23–30.\r
LeCun, Y. (1985). Une proc´edure d’apprentissage pour r´eseau a seuil asymmetrique (a learning\r
scheme for asymmetric threshold networks). In Proceedings of Cognitiva 85, Paris, France.\r
LeCun, Y., Bottou, L., Bengio, Y., Ha↵ner, P. (1998). Gradient-based learning applied to\r
document recognition. Proceedings of the IEEE, 86(11):2278–2324.\r
Legenstein, R. W., Maass, D. P. (2008). A learning theory for reward-modulated spike-timing\u0002dependent plasticity with application to biofeedback. PLoS Computational Biology, 4(10).\r
Levy, W. B., Steward, D. (1983). Temporal contiguity requirements for long-term associative\r
potentiation/depression in the hippocampus. Neuroscience, 8(4):791–797.\r
Lewis, F. L., Liu, D. (Eds.) (2012). Reinforcement Learning and Approximate Dynamic\r
Programming for Feedback Control. John Wiley and Sons.\r
Lewis, R. L., Howes, A., Singh, S. (2014). Computational rationality: Linking mechanism and\r
behavior through utility maximization. Topics in Cognitive Science, 6(2):279–311.\r
Li, L. (2012). Sample complexity bounds of exploration. In M. Wiering and M. van Otterlo (Eds.),\r
Reinforcement Learning: State-of-the-Art, pp. 175–204. Springer-Verlag Berlin Heidelberg.\r
Li, L., Chu, W., Langford, J., Schapire, R. E. (2010). A contextual-bandit approach to personal\u0002ized news article recommendation. In Proceedings of the 19th International Conference on\r
World Wide Web, pp. 661–670. ACM, New York.\r
Lin, C.-S., Kim, H. (1991). CMAC-based adaptive critic self-learning control. IEEE Transactions\r
on Neural Networks, 2(5):530–533.\r
Lin, L.-J. (1992). Self-improving reactive agents based on reinforcement learning, planning and\r
teaching. Machine Learning, 8(3-4):293–321.\r
Lin, L.-J., Mitchell, T. (1992). Reinforcement learning with hidden states. In Proceedings of\r
the Second International Conference on Simulation of Adaptive Behavior: From Animals to\r
Animats, pp. 271–280. MIT Press, Cambridge, MA.\r
Littman, M. L., Cassandra, A. R., Kaelbling, L. P. (1995). Learning policies for partially\r
observable environments: Scaling up. In Proceedings of the 12th International Conference on\r
Machine Learning, pp. 362–370. Morgan Kaufmann.\r
Littman, M. L., Dean, T. L., Kaelbling, L. P. (1995). On the complexity of solving Markov\r
decision problems. In Proceedings of the Eleventh Annual Conference on Uncertainty in\r
Artificial Intelligence, pp. 394–402.\r
Littman, M. L., Sutton, R. S., Singh, S. (2002). Predictive representations of state. In Advances\r
in Neural Information Processing Systems 14, pp. 1555–1561. MIT Press, Cambridge, MA.\r
Liu, J. S. (2001). Monte Carlo Strategies in Scientific Computing. Springer-Verlag, Berlin.

References 499\r
Ljung, L. (1998). System identification. In A. Proch´azka, J. Uhl´ıˆr, P. W. J. Rayner, and N. G.\r
Kingsbury (Eds.), Signal Analysis and Prediction, pp. 163–173. Springer Science + Business\r
Media New York, LLC.\r
Ljung, L., S¨oderstrom, T. (1983). Theory and Practice of Recursive Identification. MIT Press,\r
Cambridge, MA.\r
Ljungberg, T., Apicella, P., Schultz, W. (1992). Responses of monkey dopamine neurons during\r
learning of behavioral reactions. Journal of Neurophysiology, 67(1):145–163.\r
Lovejoy, W. S. (1991). A survey of algorithmic methods for partially observed Markov decision\r
processes. Annals of Operations Research, 28(1):47–66.\r
Luce, D. (1959). Individual Choice Behavior. Wiley, New York.\r
Ludvig, E. A., Bellemare, M. G., Pearson, K. G. (2011). A primer on reinforcement learning\r
in the brain: Psychological, computational, and neural perspectives. In E. Alonso and E.\r
Mondrag´on (Eds.), Computational Neuroscience for Advancing Artificial Intelligence: Models,\r
Methods and Applications, pp. 111–44. Medical Information Science Reference, Hershey PA.\r
Ludvig, E. A., Sutton, R. S., Kehoe, E. J. (2008). Stimulus representation and the timing of\r
reward-prediction errors in models of the dopamine system. Neural Computation, 20(12):3034–\r
3054.\r
Ludvig, E. A., Sutton, R. S., Kehoe, E. J. (2012). Evaluating the TD model of classical\r
conditioning. Learning & behavior, 40(3):305–319.\r
Machado, A. (1997). Learning the temporal dynamics of behavior. Psychological Review,\r
104(2):241–265.\r
Mackintosh, N. J. (1975). A theory of attention: Variations in the associability of stimuli with\r
reinforcement. Psychological Review, 82(4):276–298.\r
Mackintosh, N. J. (1983). Conditioning and Associative Learning. Clarendon Press, Oxford.\r
Maclin, R., Shavlik, J. W. (1994). Incorporating advice into agents that learn from reinforcements.\r
In Proceedings of the Twelfth National Conference on Artificial Intelligence, pp. 694–699.\r
AAAI Press, Menlo Park, CA.\r
Maei, H. R. (2011). Gradient Temporal-Di↵erence Learning Algorithms. PhD thesis, University\r
of Alberta, Edmonton.\r
Maei, H. R. (2018). Convergent actor-critic algorithms under o↵-policy training and function\r
approximation. ArXiv:1802.07842.\r
Maei, H. R., Sutton, R. S. (2010). GQ(): A general gradient algorithm for temporal-di↵erence\r
prediction learning with eligibility traces. In Proceedings of the Third Conference on Artificial\r
General Intelligence, pp. 91–96.\r
Maei, H. R., Szepesv´ari, Cs., Bhatnagar, S., Precup, D., Silver, D., Sutton, R. S. (2009).\r
Convergent temporal-di↵erence learning with arbitrary smooth function approximation. In\r
Advances in Neural Information Processing Systems 22, pp. 1204–1212. Curran Associates,\r
Inc.\r
Maei, H. R., Szepesv´ari, Cs., Bhatnagar, S., Sutton, R. S. (2010). Toward o↵-policy learning\r
control with function approximation. In Proceedings of the 27th International Conference on\r
Machine Learning, pp. 719–726).\r
Mahadevan, S. (1996). Average reward reinforcement learning: Foundations, algorithms, and\r
empirical results. Machine Learning, 22(1):159–196.\r
Mahadevan, S., Liu, B., Thomas, P., Dabney, W., Giguere, S., Jacek, N., Gemp, I., Liu, J. (2014).\r
Proximal reinforcement learning: A new theory of sequential decision making in primal-dual\r
spaces. ArXiv:1405.6757.\r
Mahadevan, S., Connell, J. (1992). Automatic programming of behavior-based robots using"""

[[sections]]
number = "500"
title = "References"
text = """
reinforcement learning. Artificial Intelligence, 55(2-3):311–365.\r
Mahmood, A. R. (2017). Incremental O↵-Policy Reinforcement Learning Algorithms. PhD\r
thesis, University of Alberta, Edmonton.\r
Mahmood, A. R., Sutton, R. S. (2015). O↵-policy learning based on weighted importance sam\u0002pling with linear computational complexity. In Proceedings of the 31st Conference on Uncer\u0002tainty in Artificial Intelligence, pp. 552–561. AUAI Press Corvallis,\r
Oregon.\r
Mahmood, A. R., Sutton, R. S., Degris, T., Pilarski, P. M. (2012). Tuning-free step-size adapta\u0002tion. In 2012 IEEE International Conference on Acoustics, Speech and Signal Processing,\r
Proceedings, pp. 2121–2124. IEEE.\r
Mahmood, A. R., Yu, H, Sutton, R. S. (2017). Multi-step o↵-policy learning without importance\r
sampling ratios. ArXiv:1702.03006.\r
Mahmood, A. R., van Hasselt, H., Sutton, R. S. (2014). Weighted importance sampling for\r
o↵-policy learning with linear function approximation. Advances in Neural Information\r
Processing Systems 27, pp. 3014–3022. Curran Associates, Inc.\r
Marbach, P., Tsitsiklis, J. N. (1998). Simulation-based optimization of Markov reward processes.\r
MIT Technical Report LIDS-P-2411.\r
Marbach, P., Tsitsiklis, J. N. (2001). Simulation-based optimization of Markov reward processes.\r
IEEE Transactions on Automatic Control, 46 (2):191–209.\r
Markram, H., L¨ubke, J., Frotscher, M., Sakmann, B. (1997). Regulation of synaptic ecacy by\r
coincidence of postsynaptic APs and EPSPs. Science, 275(5297):213–215.\r
Mart´ınez, J. F., ˙Ipek, E. (2009). Dynamic multicore resource management: A machine learning\r
approach. Micro, IEEE, 29(5):8–17.\r
Mataric, M. J. (1994). Reward functions for accelerated learning. In Proceedings of the 11th\r
International Conference on Machine Learning, pp. 181–189. Morgan Kaufmann.\r
Matsuda, W., Furuta, T., Nakamura, K. C., Hioki, H., Fujiyama, F., Arai, R., Kaneko, T.\r
(2009). Single nigrostriatal dopaminergic neurons form widely spread and highly dense\r
axonal arborizations in the neostriatum. The Journal of Neuroscience, 29(2):444–453.\r
Mazur, J. E. (1994). Learning and Behavior, 3rd ed. Prentice-Hall, Englewood Cli↵s, NJ.\r
McCallum, A. K. (1993). Overcoming incomplete perception with utile distinction memory. In\r
Proceedings of the 10th International Conference on Machine Learning, pp. 190–196. Morgan\r
Kaufmann.\r
McCallum, A. K. (1995). Reinforcement Learning with Selective Perception and Hidden State.\r
PhD thesis, University of Rochester, Rochester NY.\r
McCloskey, M., Cohen, N. J. (1989). Catastrophic interference in connectionist networks: The\r
sequential learning problem. Psychology of Learning and Motivation, 24 :109–165.\r
McClure, S. M., Daw, N. D., Montague, P. R. (2003). A computational substrate for incentive\r
salience. Trends in Neurosciences, 26 (8):423–428.\r
McCulloch, W. S., Pitts, W. (1943). A logical calculus of the ideas immanent in nervous activity.\r
Bulletin of Mathematical Biophysics, 5 (4):115–133.\r
McMahan, H. B., Gordon, G. J. (2005). Fast Exact Planning in Markov Decision Processes.\r
In Proceedings of the International Conference on Automated Planning and Scheduling,\r
pp. 151-160.\r
Melo, F. S., Meyn, S. P., Ribeiro, M. I. (2008). An analysis of reinforcement learning with\r
function approximation. In Proceedings of the 25th International Conference on Machine\r
Learning, pp. 664–671.\r
Mendel, J. M. (1966). A survey of learning control systems. ISA Transactions, 5:297–303.

References 501\r
Mendel, J. M., McLaren, R. W. (1970). Reinforcement learning control and pattern recognition\r
systems. In J. M. Mendel and K. S. Fu (Eds.), Adaptive, Learning and Pattern Recognition\r
Systems: Theory and Applications, pp. 287–318. Academic Press, New York.\r
Michie, D. (1961). Trial and error. In S. A. Barnett and A. McLaren (Eds.), Science Survey,\r
Part 2, pp. 129–145. Penguin, Harmondsworth.\r
Michie, D. (1963). Experiments on the mechanisation of game learning. 1. characterization of\r
the model and its parameters. The Computer Journal, 6(3):232–263.\r
Michie, D. (1974). On Machine Intelligence. Edinburgh University Press, Edinburgh.\r
Michie, D., Chambers, R. A. (1968). BOXES, An experiment in adaptive control. In E. Dale\r
and D. Michie (Eds.), Machine Intelligence 2, pp. 137–152. Oliver and Boyd, Edinburgh.\r
Miller, R. (1981). Meaning and Purpose in the Intact Brain: A Philosophical, Psychological,\r
and Biological Account of Conscious Process. Clarendon Press, Oxford.\r
Miller, W. T., An, E., Glanz, F., Carter, M. (1990). The design of CMAC neural networks for\r
control. Adaptive and Learning Systems, 1 :140–145.\r
Miller, W. T., Glanz, F. H. (1996). UNH CMAC verison 2.1: The University of New Hampshire\r
Implementation of the Cerebellar Model Arithmetic Computer - CMAC. Robotics Laboratory\r
Technical Report, University of New Hampshire, Durham.\r
Miller, S., Williams, R. J. (1992). Learning to control a bioreactor using a neural net Dyna-Q\r
system. In Proceedings of the Seventh Yale Workshop on Adaptive and Learning Systems,\r
pp. 167–172. Center for Systems Science, Dunham Laboratory, Yale University, New Haven.\r
Miller, W. T., Scalera, S. M., Kim, A. (1994). Neural network control of dynamic balance for a\r
biped walking robot. In Proceedings of the Eighth Yale Workshop on Adaptive and Learning\r
Systems, pp. 156–161. Center for Systems Science, Dunham Laboratory, Yale University,\r
New Haven.\r
Minton, S. (1990). Quantitative results concerning the utility of explanation-based learning.\r
Artificial Intelligence, 42 (2-3):363–391.\r
Minsky, M. L. (1954). Theory of Neural-Analog Reinforcement Systems and Its Application to\r
the Brain-Model Problem. PhD thesis, Princeton University.\r
Minsky, M. L. (1961). Steps toward artificial intelligence. Proceedings of the Institute of Radio\r
Engineers, 49:8–30. Reprinted in E. A. Feigenbaum and J. Feldman (Eds.), Computers and\r
Thought, pp. 406–450. McGraw-Hill, New York, 1963.\r
Minsky, M. L. (1967). Computation: Finite and Infinite Machines. Prentice-Hall, Englewood\r
Cli↵s, NJ.\r
Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., Riedmiller, M.\r
(2013). Playing atari with deep reinforcement learning. ArXiv:1312.5602.\r
Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves,\r
A., Riedmiller, M., Fidjeland, A. K., Ostrovski, G., Petersen, S., Beattie, C., Sadik, A.,\r
Antonoglou, I., King, H., Kumaran, D., Wierstra, D., Legg, S., Hassabis, D. (2015). Human\u0002level control through deep reinforcement learning. Nature, 518(7540):529–533.\r
Modayil, J., Sutton, R. S. (2014). Prediction driven behavior: Learning predictions that drive\r
fixed responses. In AAAI-14 Workshop on Artificial Intelligence and Robotics, Quebec City,\r
Canada.\r
Modayil, J., White, A., Sutton, R. S. (2014). Multi-timescale nexting in a reinforcement learning\r
robot. Adaptive Behavior, 22(2):146–160.\r
Monahan, G. E. (1982). State of the art—a survey of partially observable Markov decision\r
processes: theory, models, and algorithms. Management Science, 28 (1):1–16."""

[[sections]]
number = "502"
title = "References"
text = """
Montague, P. R., Dayan, P., Nowlan, S. J., Pouget, A., Sejnowski, T. J. (1993). Using aperiodic\r
reinforcement for directed self-organization during development. In Advances in Neural\r
Information Processing Systems 5, pp. 969–976. Morgan Kaufmann.\r
Montague, P. R., Dayan, P., Person, C., Sejnowski, T. J. (1995). Bee foraging in uncertain\r
environments using predictive hebbian learning. Nature, 377(6551):725–728.\r
Montague, P. R., Dayan, P., Sejnowski, T. J. (1996). A framework for mesencephalic dopamine\r
systems based on predictive Hebbian learning. The Journal of Neuroscience, 16(5):1936–1947.\r
Montague, P. R., Dolan, R. J., Friston, K. J., Dayan, P. (2012). Computational psychiatry.\r
Trends in Cognitive Sciences, 16 (1):72–80.\r
Montague, P. R., Sejnowski, T. J. (1994). The predictive brain: Temporal coincidence and\r
temporal order in synaptic learningmechanisms. Learning & Memory, 1(1):1–33.\r
Moore, A. W. (1990). Ecient Memory-Based Learning for Robot Control. PhD thesis,\r
University of Cambridge.\r
Moore, A. W., Atkeson, C. G. (1993). Prioritized sweeping: Reinforcement learning with less\r
data and less real time. Machine Learning, 13(1):103–130.\r
Moore, A. W., Schneider, J., Deng, K. (1997). Ecient locally weighted polynomial regression\r
predictions. In Proceedings of the 14th International Conference on Machine Learning.\r
Morgan Kaufmann.\r
Moore, J. W., Blazis, D. E. J. (1989). Simulation of a classically conditioned response: A\r
cerebellar implementation of the sutton-barto-desmond model. In J. H. Byrne and W. O.\r
Berry (Eds.), Neural Models of Plasticity, pp. 187–207. Academic Press, San Diego, CA.\r
Moore, J. W., Choi, J.-S., Brunzell, D. H. (1998). Predictive timing under temporal uncertainty:\r
The time derivative model of the conditioned response. In D. A. Rosenbaum and C. E.\r
Collyer (Eds.), Timing of Behavior, pp. 3–34. MIT Press, Cambridge, MA.\r
Moore, J. W., Desmond, J. E., Berthier, N. E., Blazis, E. J., Sutton, R. S., Barto, A. G. (1986).\r
Simulation of the classically conditioned nictitating membrane response by a neuron-like\r
adaptive element: I. Response topography, neuronal firing, and interstimulus intervals.\r
Behavioural Brain Research, 21(2):143–154.\r
Moore, J. W., Marks, J. S., Castagna, V. E., Polewan, R. J. (2001). Parameter stability in the\r
TD model of complex CR topographies. In Society for Neuroscience Abstracts, 27:642.\r
Moore, J. W., Schmajuk, N. A. (2008). Kamin blocking. Scholarpedia, 3(5):3542.\r
Moore, J. W., Stickney, K. J. (1980). Formation of attentional-associative networks in real\r
time:Role of the hippocampus and implications for conditioning. Physiological Psychology,\r
8(2):207–217.\r
Mukundan, J., Mart´ınez, J. F. (2012). MORSE, Multi-objective reconfigurable self-optimizing\r
memory scheduler. In IEEE 18th International Symposium on High Performance Computer\r
Architecture, pp. 1–12.\r
M¨uller, M. (2002). Computer Go. Artificial Intelligence, 134(1):145–179.\r
Munos, R., Stepleton, T., Harutyunyan, A., Bellemare, M. (2016). Safe and ecient o↵-policy\r
reinforcement learning. In Advances in Neural Information Processing Systems 29, pp. 1046–"""

[[sections]]
number = "1054"
title = "Curran Associates, Inc."
text = """
Naddaf, Y. (2010). Game-Independent AI Agents for Playing Atari 2600 Console Games. PhD\r
thesis, University of Alberta, Edmonton.\r
Narendra, K. S., Thathachar, M. A. L. (1974). Learning automata—A survey. IEEE Transactions\r
on Systems, Man, and Cybernetics, 4:323–334.\r
Narendra, K. S., Thathachar, M. A. L. (1989). Learning Automata: An Introduction. Prentice\u0002Hall, Englewood Cli↵s, NJ.

References 503\r
Narendra, K. S., Wheeler, R. M. (1983). An N-player sequential stochastic game with identical\r
payo↵s. IEEE Transactions on Systems, Man, and Cybernetics, 6:1154–1158.\r
Narendra, K. S., Wheeler, R. M. (1986). Decentralized learning in finite Markov chains. IEEE\r
Transactions on Automatic Control, 31(6):519–526.\r
Nedi´c, A., Bertsekas, D. P. (2003). Least squares policy evaluation algorithms with linear\r
function approximation. Discrete Event Dynamic Systems, 13 (1-2):79–110.\r
Ng, A. Y. (2003). Shaping and Policy Search in Reinforcement Learning. PhD thesis, University\r
of California, Berkeley.\r
Ng, A. Y., Harada, D., Russell, S. (1999). Policy invariance under reward transformations:\r
Theory and application to reward shaping. In I. Bratko and S. Dzeroski (Eds.), Proceedings\r
of the 16th International Conference on Machine Learning, pp. 278–287.\r
Ng, A. Y., Russell, S. J. (2000). Algorithms for inverse reinforcement learning. In Proceedings of\r
the 17th International Conference on Machine Learning, pp. 663–670.\r
Niv, Y. (2009). Reinforcement learning in the brain. Journal of Mathematical Psychology,\r
53(3):139–154.\r
Niv, Y., Daw, N. D., Dayan, P. (2006). How fast to work: Response vigor, motivation and tonic\r
dopamine. In Advances in Neural Information Processing Systems 18, pp. 1019–1026. MIT\r
Press, Cambridge, MA.\r
Niv, Y., Daw, N. D., Joel, D., Dayan, P. (2007). Tonic dopamine: opportunity costs and the\r
control of response vigor. Psychopharmacology, 191(3):507–520.\r
Niv, Y., Joel, D., Dayan, P. (2006). A normative perspective on motivation. Trends in Cognitive\r
Sciences, 10(8):375–381.\r
Nouri, A., Littman, M. L. (2009). Multi-resolution exploration in continuous spaces. In Advances\r
in Neural Information Processing Systems 21, pp. 1209–1216. Curran Associates, Inc.\r
Now´e, A., Vrancx, P., Hauwere, Y.-M. D. (2012). Game theory and multi-agent reinforcement\r
learning. In M. Wiering and M. van Otterlo (Eds.), Reinforcement Learning: State-of-the-Art,\r
pp. 441–467. Springer-Verlag Berlin Heidelberg.\r
Nutt, D. J., Lingford-Hughes, A., Erritzoe, D., Stokes, P. R. A. (2015). The dopamine theory of\r
addiction: 40 years of highs and lows. Nature Reviews Neuroscience, 16(5):305–312.\r
O’Doherty, J. P., Dayan, P., Friston, K., Critchley, H., Dolan, R. J. (2003). Temporal di↵erence\r
models and reward-related learning in the human brain. Neuron, 38(2):329–337.\r
O’Doherty, J. P., Dayan, P., Schultz, J., Deichmann, R., Friston, K., Dolan, R. J. (2004).\r
Dissociable roles of ventral and dorsal striatum in instrumental conditioning. Science,\r
304(5669):452–454.\r
Olafsd´ottir, H. F., Barry, C., Saleem, A. B., Hassabis, D., Spiers, H. J. (2015). Hippocampal ´\r
place cells construct reward related sequences through unexplored space. Elife, 4:e06063.\r
Oh, J., Guo, X., Lee, H., Lewis, R. L., Singh, S. (2015). Action-conditional video prediction\r
using deep networks in Atari games. In Advances in Neural Information Processing Systems\r
28, pp. 2845–2853. Curran Associates, Inc.\r
Olds, J., Milner, P. (1954). Positive reinforcement produced by electrical stimulation of the septal\r
area and other regions of rat brain. Journal of Comparative and Physiological Psychology,\r
47(6):419–427.\r
O’Reilly, R. C., Frank, M. J. (2006). Making working memory work: A computational model of\r
learning in the prefrontal cortex and basal ganglia. Neural Computation, 18(2):283–328.\r
O’Reilly, R. C., Frank, M. J., Hazy, T. E., Watz, B. (2007). PVLV, the primary value and\r
learned value Pavlovian learning algorithm. Behavioral Neuroscience, 121(1):31–49."""

[[sections]]
number = "504"
title = "References"
text = """
Omohundro, S. M. (1987). Ecient algorithms with neural network behavior. Technical Report,\r
Department of Computer Science, University of Illinois at Urbana-Champaign.\r
Ormoneit, D., Sen, S. (2002). Kernel-based reinforcement learning. ´ Machine Learning, 49 (2-\r
3):161–178.\r
Oudeyer, P.-Y., Kaplan, F. (2007). What is intrinsic motivation? A typology of computational\r
approaches. Frontiers in Neurorobotics, 1:6.\r
Oudeyer, P.-Y., Kaplan, F., Hafner, V. V. (2007). Intrinsic motivation systems for autonomous\r
mental development. IEEE Transactions on Evolutionary Computation, 11(2):265–286.\r
Padoa-Schioppa, C., Assad, J. A. (2006). Neurons in the orbitofrontal cortex encode economic\r
value. Nature, 441 (7090):223–226.\r
Page, C. V. (1977). Heuristics for signature table analysis as a pattern recognition technique.\r
IEEE Transactions on Systems, Man, and Cybernetics, 7(2):77–86.\r
Pagnoni, G., Zink, C. F., Montague, P. R., Berns, G. S. (2002). Activity in human ventral\r
striatum locked to errors of reward prediction. Nature Neuroscience, 5(2):97–98.\r
Pan, W.-X., Schmidt, R., Wickens, J. R., Hyland, B. I. (2005). Dopamine cells respond to\r
predicted events during classical conditioning: Evidence for eligibility traces in the reward\u0002learning network. The Journal of Neuroscience, 25(26):6235–6242.\r
Park, J., Kim, J., Kang, D. (2005). An RLS-based natural actor–critic algorithm for locomotion\r
of a two-linked robot arm. Computational Intelligence and Security:65–72.\r
Parks, P. C., Militzer, J. (1991). Improved allocation of weights for associative memory storage\r
in learning control systems. In IFAC Design Methods of Control Systems, Zurich, Switzerland,\r
pp. 507–512.\r
Parr, R. (1988). Hierarchical Control and Learning for Markov Decision Processes. PhD thesis,\r
University of California, Berkeley.\r
Parr, R., Li, L., Taylor, G., Painter-Wakefield, C., Littman, M. L. (2008). An analysis of linear\r
models, linear value-function approximation, and feature selection for reinforcement learning.\r
In Proceedings of the 25th international conference on Machine learning, pp. 752–759).\r
Parr, R., Russell, S. (1995). Approximating optimal policies for partially observable stochastic\r
domains. In Proceedings of the Fourteenth International Joint Conference on Artificial\r
Intelligence, pp. 1088–1094. Morgan Kaufmann.\r
Pavlov, I. P. (1927). Conditioned Reflexes. Oxford University Press, London.\r
Pawlak, V., Kerr, J. N. D. (2008). Dopamine receptor activation is required for corticostriatal\r
spike-timing-dependent plasticity. The Journal of Neuroscience, 28(10):2435–2446.\r
Pawlak, V., Wickens, J. R., Kirkwood, A., Kerr, J. N. D. (2010). Timing is not every\u0002thing: neuromodulation opens the STDP gate. Frontiers in Synaptic Neuroscience, 2:146.\r
doi:10.3389/fnsyn.2010.00146.\r
Pearce, J. M., Hall, G. (1980). A model for Pavlovian learning: Variation in the e↵ectiveness of\r
conditioning but not unconditioned stimuli. Psychological Review, 87(6):532–552.\r
Pearl, J. (1984). Heuristics: Intelligent Search Strategies for Computer Problem Solving.\r
Addison-Wesley, Reading, MA.\r
Pearl, J. (1995). Causal diagrams for empirical research. Biometrika, 82(4):669-688.\r
Pecevski, D., Maass, W., Legenstein, R. A. (2008). Theoretical analysis of learning with\r
reward-modulated spike-timing-dependent plasticity. In Advances in Neural Information\r
Processing Systems 20, pp. 881–888. Curran Associates, Inc.\r
Peng, J. (1993). Ecient Dynamic Programming-Based Learning for Control. PhD thesis,\r
Northeastern University, Boston MA.

References 505\r
Peng, J. (1995). Ecient memory-based dynamic programming. In Proceedings of the 12th\r
International Conference on Machine Learning, pp. 438–446.\r
Peng, J., Williams, R. J. (1993). Ecient learning and planning within the Dyna framework.\r
Adaptive Behavior, 1(4):437–454.\r
Peng, J., Williams, R. J. (1994). Incremental multi-step Q-learning. In Proceedings of the\r
11th International Conference on Machine Learning, pp. 226–232. Morgan Kaufmann, San\r
Francisco.\r
Peng, J., Williams, R. J. (1996). Incremental multi-step Q-learning. Machine Learning,\r
22(1):283–290.\r
Perkins, T. J., Pendrith, M. D. (2002). On the existence of fixed points for Q-learning and\r
Sarsa in partially observable domains. In Proceedings of the 19th International Conference\r
on Machine Learning, pp. 490–497.\r
Perkins, T. J., Precup, D. (2003). A convergent form of approximate policy iteration. In Advances\r
in Neural Information Processing Systems 15, pp. 1627–1634. MIT Press, Cambridge, MA.\r
Peters, J., B¨uchel, C. (2010). Neural representations of subjective reward value. Behavioral\r
Brain Research, 213(2):135–141.\r
Peters, J., Schaal, S. (2008). Natural actor–critic. Neurocomputing, 71 (7):1180–1190.\r
Peters, J., Vijayakumar, S., Schaal, S. (2005). Natural actor–critic. In European Conference on\r
Machine Learning, pp. 280–291. Springer Berlin Heidelberg.\r
Pezzulo, G., van der Meer, M. A. A., Lansink, C. S., Pennartz, C. M. A. (2014). Internally\r
generated sequences in learning and executing goal-directed behavior. Trends in Cognitive\r
Science, 18(12):647–657.\r
Pfei↵er, B. E., Foster, D. J. (2013). Hippocampal place-cell sequences depict future paths to\r
remembered goals. Nature, 497(7447):74–79.\r
Phansalkar, V. V., Thathachar, M. A. L. (1995). Local and global optimization algorithms for\r
generalized learning automata. Neural Computation, 7(5):950–973.\r
Poggio, T., Girosi, F. (1989). A theory of networks for approximation and learning. A.I. Memo\r
1140. Artificial Intelligence Laboratory, Massachusetts Institute of Technology, Cambridge,\r
MA.\r
Poggio, T., Girosi, F. (1990). Regularization algorithms for learning that are equivalent to\r
multilayer networks. Science, 247(4945):978–982.\r
Polyak, B. T. (1990). New stochastic approximation type procedures. Automat. i Telemekh,\r
7 (98-107):2 (in Russian).\r
Polyak, B. T., Juditsky, A. B. (1992). Acceleration of stochastic approximation by averaging.\r
SIAM Journal on Control and Optimization, 30 (4):838–855.\r
Powell, M. J. D. (1987). Radial basis functions for multivariate interpolation: A review. In\r
J. C. Mason and M. G. Cox (Eds.), Algorithms for Approximation, pp. 143–167. Clarendon\r
Press, Oxford.\r
Powell, W. B. (2011). Approximate Dynamic Programming: Solving the Curses of Dimensionality,\r
Second edition. John Wiley and Sons.\r
Powers, W. T. (1973). Behavior: The Control of Perception. Aldine de Gruyter, Chicago. 2nd\r
expanded edition 2005.\r
Precup, D. (2000). Temporal Abstraction in Reinforcement Learning. PhD thesis, University of\r
Massachusetts, Amherst.\r
Precup, D., Sutton, R. S., Dasgupta, S. (2001). O↵-policy temporal-di↵erence learning with\r
function approximation. In Proceedings of the 18th International Conference on Machine\r
Learning, pp. 417–424."""

[[sections]]
number = "506"
title = "References"
text = """
Precup, D., Sutton, R. S., Paduraru, C., Koop, A., Singh, S. (2006). O↵-policy learning\r
with options and recognizers. In Advances in Neural Information Processing Systems 18,\r
pp. 1097–1104. MIT Press, Cambridge, MA.\r
Precup, D., Sutton, R. S., Singh, S. (2000). Eligibility traces for o↵-policy policy evaluation. In\r
Proceedings of the 17th International Conference on Machine Learning, pp. 759–766. Morgan\r
Kaufmann.\r
Puterman, M. L. (1994). Markov Decision Problems. Wiley, New York.\r
Puterman, M. L., Shin, M. C. (1978). Modified policy iteration algorithms for discounted\r
Markov decision problems. Management Science, 24(11):1127–1137.\r
Quartz, S., Dayan, P., Montague, P. R., Sejnowski, T. J. (1992). Expectation learning in the\r
brain using di↵use ascending connections. In Society for Neuroscience Abstracts, 18:1210.\r
Randløv, J., Alstrøm, P. (1998). Learning to drive a bicycle using reinforcement learning\r
and shaping. In Proceedings of the 15th International Conference on Machine Learning,\r
pp. 463–471.\r
Rangel, A., Camerer, C., Montague, P. R. (2008). A framework for studying the neurobiology of\r
value-based decision making. Nature Reviews Neuroscience, 9(7):545–556.\r
Rangel, A., Hare, T. (2010). Neural computations associated with goal-directed choice. Current\r
Opinion in Neurobiology, 20(2):262–270.\r
Rao, R. P., Sejnowski, T. J. (2001). Spike-timing-dependent Hebbian plasticity as temporal\r
di↵erence learning. Neural Computation, 13 (10):2221–2237.\r
Ratcli↵, R. (1990). Connectionist models of recognition memory: Constraints imposed by\r
learning and forgetting functions. Psychological Review, 97 (2):285–308.\r
Reddy, G., Celani, A., Sejnowski, T. J., Vergassola, M. (2016). Learning to soar in turbulent\r
environments. Proceedings of the National Academy of Sciences, 113(33):E4877–E4884.\r
Redish, D. A. (2004). Addiction as a computational process gone awry. Science, 306(5703):1944–\r
1947.\r
Reetz, D. (1977). Approximate solutions of a discounted Markovian decision process. Bonner\r
Mathematische Schriften, 98:77–92.\r
Rescorla, R. A., Wagner, A. R. (1972). A theory of Pavlovian conditioning: Variations in the\r
e↵ectiveness of reinforcement and nonreinforcement. In A. H. Black and W. F. Prokasy\r
(Eds.), Classical Conditioning II, pp. 64–99. Appleton-Century-Crofts, New York.\r
Revusky, S., Garcia, J. (1970). Learned associations over long delays. In G. Bower (Ed.), The\r
Psychology of Learning and Motivation, v. 4, pp. 1–84. Academic Press, Inc., New York.\r
Reynolds, J. N. J., Wickens, J. R. (2002). Dopamine-dependent plasticity of corticostriatal\r
synapses. Neural Networks, 15(4):507–521.\r
Ring, M. B. (in preparation). Representing knowledge as forecasts (and state as knowledge).\r
Ripley, B. D. (2007). Pattern Recognition and Neural Networks. Cambridge University Press.\r
Rixner, S. (2004). Memory controller optimizations for web servers. In Proceedings of the\r
37th annual IEEE/ACM International Symposium on Microarchitecture, p. 355–366. IEEE\r
Computer Society.\r
Robbins, H. (1952). Some aspects of the sequential design of experiments. Bulletin of the\r
American Mathematical Society, 58:527–535.\r
Robertie, B. (1992). Carbon versus silicon: Matching wits with TD-Gammon. Inside Backgam\u0002mon, 2(2):14–22.\r
Romo, R., Schultz, W. (1990). Dopamine neurons of the monkey midbrain: Contingencies of\r
responses to active touch during self-initiated arm movements. Journal of Neurophysiology,\r
63(3):592–624.

References 507\r
Rosenblatt, F. (1962). Principles of Neurodynamics: Perceptrons and the Theory of Brain\r
Mechanisms. Spartan Books, Washington, DC.\r
Ross, S. (1983). Introduction to Stochastic Dynamic Programming. Academic Press, New York.\r
Ross, T. (1933). Machines that think. Scientific American, 148(4):206–208.\r
Rubinstein, R. Y. (1981). Simulation and the Monte Carlo Method. Wiley, New York.\r
Rumelhart, D. E., Hinton, G. E., Williams, R. J. (1986). Learning internal representations by\r
error propagation. In D. E. Rumelhart and J. L. McClelland (Eds.), Parallel Distributed Pro\u0002cessing: Explorations in the Microstructure of Cognition, vol. I, Foundations. Bradford/MIT\r
Press, Cambridge, MA.\r
Rummery, G. A. (1995). Problem Solving with Reinforcement Learning. PhD thesis, University\r
of Cambridge.\r
Rummery, G. A., Niranjan, M. (1994). On-line Q-learning using connectionist systems. Technical\r
Report CUED/F-INFENG/TR 166. Engineering Department, Cambridge University.\r
Ruppert, D. (1988). Ecient estimations from a slowly convergent Robbins-Monro process.\r
Cornell University Operations Research and Industrial Engineering Technical Report No. 781.\r
Russell, S., Norvig, P. (2009). Artificial Intelligence: A Modern Approach, 3rd edition. Prentice\u0002Hall, Englewood Cli↵s, NJ.\r
Russo, D. J., Van Roy, B., Kazerouni, A., Osband, I., Wen, Z. (2018). A tutorial on Thompson\r
sampling, Foundations and Trends in Machine Learning. ArXiv:1707.02038.\r
Rust, J. (1996). Numerical dynamic programming in economics. In H. Amman, D. Kendrick, and\r
J. Rust (Eds.), Handbook of Computational Economics, pp. 614–722. Elsevier, Amsterdam.\r
Saddoris, M. P., Cacciapaglia, F., Wightmman, R. M., Carelli, R. M. (2015). Di↵erential\r
dopamine release dynamics in the nucleus accumbens core and shell reveal complemen\u0002tary signals for error prediction and incentive motivation. The Journal of Neuroscience,\r
35(33):11572–11582.\r
Saksida, L. M., Raymond, S. M., Touretzky, D. S. (1997). Shaping robot behavior using principles\r
from instrumental conditioning. Robotics and Autonomous Systems, 22(3):231–249.\r
Samuel, A. L. (1959). Some studies in machine learning using the game of checkers. IBM\r
Journal on Research and Development, 3(3), 210–229.\r
Samuel, A. L. (1967). Some studies in machine learning using the game of checkers. II—Recent\r
progress. IBM Journal on Research and Development, 11(6):601–617.\r
Schaal, S., Atkeson, C. G. (1994). Robot juggling: Implementation of memory-based learning.\r
IEEE Control Systems, 14 (1):57–71.\r
Schmajuk, N. A. (2008). Computational models of classical conditioning. Scholarpedia,\r
3(3):1664.\r
Schmidhuber, J. (1991a). Curious model-building control systems. In Proceedings of the IEEE\r
International Joint Conference on Neural Networks, pp. 1458–1463. IEEE.\r
Schmidhuber, J. (1991b). A possibility for implementing curiosity and boredom in model-building\r
neural controllers. In From Animals to Animats: Proceedings of the First International\r
Conference on Simulation of Adaptive Behavior, pp. 222–227. MIT Press, Cambridge, MA.\r
Schmidhuber, J. (2015). Deep learning in neural networks: An overview. Neural Networks,\r
6 :85–117.\r
Schmidhuber, J., Storck, J., Hochreiter, S. (1994). Reinforcement driven information acquisition\r
in nondeterministic environments. Technical report, Fakult¨at f¨ur Informatik, Technische\r
Universit¨at M¨unchen, M¨unchen, Germany.\r
Schraudolph, N. N. (1999). Local gain adaptation in stochastic gradient descent. In Proceedings\r
of the International Conference on Artificial Neural Networks, pp. 569–574. IEEE, London."""

[[sections]]
number = "508"
title = "References"
text = """
Schraudolph, N. N. (2002). Fast curvature matrix-vector products for second-order gradient\r
descent. Neural Computation, 14 (7):1723–1738.\r
Schraudolph, N. N., Yu, J., Aberdeen, D. (2006). Fast online policy gradient learning with SMD\r
gain vector adaptation. In Advances in Neural Information Processing Systems, pp. 1185–\r
1192.\r
Schulman, J., Chen, X., Abbeel, P. (2017). Equivalence between policy gradients and soft\r
Q-Learning. ArXiv:1704.06440.\r
Schultz, D. G., Melsa, J. L. (1967). State Functions and Linear Control Systems. McGraw-Hill,\r
New York.\r
Schultz, W. (1998). Predictive reward signal of dopamine neurons. Journal of Neurophysiology,\r
80(1):1–27.\r
Schultz, W., Apicella, P., Ljungberg, T. (1993). Responses of monkey dopamine neurons to\r
reward and conditioned stimuli during successive steps of learning a delayed response task.\r
The Journal of Neuroscience, 13 (3):900–913.\r
Schultz, W., Dayan, P., Montague, P. R. (1997). A neural substrate of prediction and reward.\r
Science, 275(5306):1593–1598.\r
Schultz, W., Romo, R. (1990). Dopamine neurons of the monkey midbrain: contingencies of\r
responses to stimuli eliciting immediate behavioral reactions. Journal of Neurophysiology,\r
63(3):607–624.\r
Schultz, W., Romo, R., Ljungberg, T., Mirenowicz, J., Hollerman, J. R., Dickinson, A. (1995).\r
Reward-related signals carried by dopamine neurons. In J. C. Houk, J. L. Davis, and D. G.\r
Beiser (Eds.), Models of Information Processing in the Basal Ganglia, pp. 233–248. MIT\r
Press, Cambridge, MA.\r
Schwartz, A. (1993). A reinforcement learning method for maximizing undiscounted rewards. In\r
Proceedings of the 10th International Conference on Machine Learning, pp. 298–305. Morgan\r
Kaufmann.\r
Schweitzer, P. J., Seidmann, A. (1985). Generalized polynomial approximations in Markovian\r
decision processes. Journal of Mathematical Analysis and Applications, 110(2):568–582.\r
Selfridge, O. G. (1978). Tracking and trailing: Adaptation in movement strategies. Technical\r
report, Bolt Beranek and Newman, Inc. Unpublished report.\r
Selfridge, O. G. (1984). Some themes and primitives in ill-defined systems. In O. G. Selfridge,\r
E. L. Rissland, and M. A. Arbib (Eds.), Adaptive Control of Ill-Defined Systems, pp. 21–26.\r
Plenum Press, NY. Proceedings of the NATO Advanced Research Institute on Adaptive\r
Control of Ill-defined Systems, NATO Conference Series II, Systems Science, Vol. 16.\r
Selfridge, O. J., Sutton, R. S., Barto, A. G. (1985). Training and tracking in robotics. In A. Joshi\r
(Ed.), Proceedings of the Ninth International Joint Conference on Artificial Intelligence,\r
pp. 670–672. Morgan Kaufmann.\r
Seo, H., Barraclough, D., Lee, D. (2007). Dynamic signals related to choices and outcomes in\r
the dorsolateral prefrontal cortex. Cerebral Cortex, 17(suppl 1):110–117.\r
Seung, H. S. (2003). Learning in spiking neural networks by reinforcement of stochastic synaptic\r
transmission. Neuron, 40(6):1063–1073.\r
Shah, A. (2012). Psychological and neuroscientific connections with reinforcement learning. In M.\r
Wiering and M. van Otterlo (Eds.), Reinforcement Learning: State-of-the-Art, pp. 507–537.\r
Springer-Verlag Berlin Heidelberg.\r
Shannon, C. E. (1950). Programming a computer for playing chess. Philosophical Magazine and\r
Journal of Science, 41(314):256–275.

References 509\r
Shannon, C. E. (1951). Presentation of a maze-solving machine. In H. V. Forester (Ed.), Cyber\u0002netics. Transactions of the Eighth Conference, pp. 173–180. Josiah Macy Jr. Foundation.\r
Shannon, C. E. (1952). “Theseus” maze-solving mouse. http://cyberneticzoo.com/mazesolvers/1952—\r
theseus-maze-solving-mouse—claude-shannon-american/.\r
Shelton, C. R. (2001). Importance Sampling for Reinforcement Learning with Multiple Objectives.\r
PhD thesis, Massachusetts Institute of Technology, Cambridge MA.\r
Shepard, D. (1968). A two-dimensional interpolation function for irregularly-spaced data. In\r
Proceedings of the 23rd ACM National Conference, pp. 517–524. ACM, New York.\r
Sherman, J., Morrison, W. J. (1949). Adjustment of an inverse matrix corresponding to changes\r
in the elements of a given column or a given row of the original matrix (abstract). Annals of\r
Mathematical Statistics, 20 (4):621.\r
Shewchuk, J., Dean, T. (1990). Towards learning time-varying functions with high input\r
dimensionality. In Proceedings of the Fifth IEEE International Symposium on Intelligent\r
Control, pp. 383–388. IEEE Computer Society Press, Los Alamitos, CA.\r
Shimansky, Y. P. (2009). Biologically plausible learning in neural networks: a lesson from\r
bacterial chemotaxis. Biological Cybernetics, 101(5-6):379–385.\r
Si, J., Barto, A., Powell, W., Wunsch, D. (Eds.) (2004). Handbook of Learning and Approximate\r
Dynamic Programming. John Wiley and Sons.\r
Silver, D. (2009). Reinforcement Learning and Simulation Based Search in the Game of Go.\r
PhD thesis, University of Alberta, Edmonton.\r
Silver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., van den Driessche, G., Schrittwieser,\r
J., Antonoglou, I., Panneershelvam, V., Lanctot, M., Dieleman, S., Grewe, D., Nham, J.,\r
Kalchbrenner, N., Sutskever, I., Lillicrap, T., Leach, M., Kavukcuoglu, K., Graepel, T.,\r
Hassabis, D. (2016). Mastering the game of Go with deep neural networks and tree search.\r
Nature, 529(7587):484–489.\r
Silver, D., Lever, G., Heess, N., Degris, T., Wierstra, D., Riedmiller, M. (2014). Deterministic\r
policy gradient algorithms. In Proceedings of the 31st International Conference on Machine\r
Learning, pp. 387–395.\r
Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez, A., Hubert, T.,\r
Baker, L., Lai, M., Bolton, A., Chen, Y., Lillicrap, L., Hui, F., Sifre, L., van den Driessche,\r
G., Graepel, T., Hassibis, D. (2017a). Mastering the game of Go without human knowledge.\r
Nature, 550 (7676):354–359.\r
Silver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai, M., Guez, A., Lanctot, M., Sifre, L.,\r
Kumaran, D., Graepel, T., Lillicrap, T., Simoyan, K., Hassibis, D. (2017b). Mastering chess\r
and shogi by self-play with a general reinforcement learning algorithm. ArXiv:1712.01815.\r
S¸im¸sek, O., Alg´orta, S., Kothiyal, A. (2016). Why most decisions are easy in tetris—And perhaps ¨\r
in other sequential decision problems, as well. In Proceedings of the 33rd International\r
Conference on Machine Learning, pp. 1757-1765.\r
Simon, H. (2000). Lecture at the Earthware Symposium, Carnegie Mellon University. https\r
://www.youtube.com/watch?v=EZhyi-8DBjc.\r
Singh, S. P. (1992a). Reinforcement learning with a hierarchy of abstract models. In Proceedings\r
of the Tenth National Conference on Artificial Intelligence, pp. 202–207. AAAI/MIT Press,\r
Menlo Park, CA.\r
Singh, S. P. (1992b). Scaling reinforcement learning algorithms by learning variable temporal\r
resolution models. In Proceedings of the 9th International Workshop on Machine Learning,\r
pp. 406–415. Morgan Kaufmann.\r
Singh, S. P. (1993). Learning to Solve Markovian Decision Processes. PhD thesis, University of\r
Massachusetts, Amherst."""

[[sections]]
number = "510"
title = "References"
text = """
Singh, S. P. (Ed.) (2002). Special double issue on reinforcement learning, Machine Learning,\r
49(2-3).\r
Singh, S., Barto, A. G., Chentanez, N. (2005). Intrinsically motivated reinforcement learning.\r
In Advances in Neural Information Processing Systems 17, pp. 1281–1288. MIT Press,\r
Cambridge, MA.\r
Singh, S. P., Bertsekas, D. (1997). Reinforcement learning for dynamic channel allocation\r
in cellular telephone systems. In Advances in Neural Information Processing Systems 9,\r
pp. 974–980. MIT Press, Cambridge, MA.\r
Singh, S. P., Jaakkola, T., Jordan, M. I. (1994). Learning without state-estimation in partially\r
observable Markovian decision problems. In Proceedings of the 11th International Conference\r
on Machine Learning, pp. 284–292. Morgan Kaufmann.\r
Singh, S., Jaakkola, T., Littman, M. L., Szepesv´ari, C. (2000). Convergence results for single-step\r
on-policy reinforcement-learning algorithms. Machine Learning, 38 (3):287–308.\r
Singh, S. P., Jaakkola, T., Jordan, M. I. (1995). Reinforcement learning with soft state\r
aggregation. In Advances in Neural Information Processing Systems 7, pp. 359–368. MIT\r
Press, Cambridge, MA.\r
Singh, S., Lewis, R. L., Barto, A. G. (2009). Where do rewards come from? In N. Taatgen\r
and H. van Rijn (Eds.), Proceedings of the 31st Annual Conference of the Cognitive Science\r
Society, pp. 2601–2606. Cognitive Science Society.\r
Singh, S., Lewis, R. L., Barto, A. G., Sorg, J. (2010). Intrinsically motivated reinforcement\r
learning: An evolutionary perspective. IEEE Transactions on Autonomous Mental Develop\u0002ment, 2(2):70–82. Special issue on Active Learning and Intrinsically Motivated Exploration\r
in Robots: Advances and Challenges.\r
Singh, S. P., Sutton, R. S. (1996). Reinforcement learning with replacing eligibility traces.\r
Machine Learning, 22(1-3):123–158.\r
Skinner, B. F. (1938). The Behavior of Organisms: An Experimental Analysis. Appleton-Century,\r
New York.\r
Skinner, B. F. (1958). Reinforcement today. American Psychologist, 13(3):94–99.\r
Skinner, B. F. (1963). Operant behavior. American Psychologist, 18 (8):503–515.\r
Sofge, D. A., White, D. A. (1992). Applied learning: Optimal control for manufacturing. In\r
D. A. White and D. A. Sofge (Eds.), Handbook of Intelligent Control: Neural, Fuzzy, and\r
Adaptive Approaches, pp. 259–281. Van Nostrand Reinhold, New York.\r
Sorg, J. D. (2011). The Optimal Reward Problem:Designing E↵ective Reward for Bounded\r
Agents. PhD thesis, University of Michigan, Ann Arbor.\r
Sorg, J., Lewis, R. L., Singh, S. P. (2010). Reward design via online gradient ascent. In Advances\r
in Neural Information Processing Systems 23, pp. 2190–2198. Curran Associates, Inc.\r
Sorg, J., Singh, S. (2010). Linear options. In Proceedings of the 9th International Conference on\r
Autonomous Agents and Multiagent Systems, pp. 31–38.\r
Sorg, J., Singh, S., Lewis, R. (2010). Internal rewards mitigate agent boundedness. In Proceedings\r
of the 27th International Conference on Machine Learning, pp. 1007–1014.\r
Spence, K. W. (1947). The role of secondary reinforcement in delayed reward learning. Psycho\u0002logical Review, 54(1):1–8.\r
Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., Salakhutdinov, R. (2014). Dropout:\r
A simple way to prevent neural networks from overfitting. Journal of Machine Learning\r
Research, 15(1):1929–1958.\r
Staddon, J. E. R. (1983). Adaptive Behavior and Learning. Cambridge University Press.

References 511\r
Stanfill, C., Waltz, D. (1986). Toward memory-based reasoning. Communications of the ACM,\r
29 (12):1213–1228.\r
Steinberg, E. E., Keiflin, R., Boivin, J. R., Witten, I. B., Deisseroth, K., Janak, P. H. (2013). A\r
causal link between prediction errors, dopamine neurons and learning. Nature Neuroscience,\r
16(7):966–973.\r
Sterling, P., Laughlin, S. (2015). Principles of Neural Design. MIT Press, Cambridge, MA.\r
Sternberg, S. (1963). Stochastic learning theory. In: Handbook of Mathematical Psychology,\r
Volume II, R. D. Luce, R. R. Bush, and E. Galanter (Eds.). John Wiley & Sons.\r
Sugiyama, M., Hachiya, H., Morimura, T. (2013). Statistical Reinforcement Learning: Modern\r
Machine Learning Approaches. Chapman & Hall/CRC.\r
Suri, R. E., Bargas, J., Arbib, M. A. (2001). Modeling functions of striatal dopamine modulation\r
in learning and planning. Neuroscience, 103(1):65–85.\r
Suri, R. E., Schultz, W. (1998). Learning of sequential movements by neural network model\r
with dopamine-like reinforcement signal. Experimental Brain Research, 121(3):350–354.\r
Suri, R. E., Schultz, W. (1999). A neural network model with dopamine-like reinforcement\r
signal that learns a spatial delayed response task. Neuroscience, 91(3):871–890.\r
Sutton, R. S. (1978a). Learning theory support for a single channel theory of the brain.\r
Unpublished report.\r
Sutton, R. S. (1978b). Single channel theory: A neuronal theory of learning. Brain Theory\r
Newsletter, 4:72–75. Center for Systems Neuroscience, University of Massachusetts, Amherst,\r
MA.\r
Sutton, R. S. (1978c). A unified theory of expectation in classical and instrumental conditioning.\r
Bachelors thesis, Stanford University.\r
Sutton, R. S. (1984). Temporal Credit Assignment in Reinforcement Learning. PhD thesis,\r
University of Massachusetts, Amherst.\r
Sutton, R. S. (1988). Learning to predict by the method of temporal di↵erences. Machine\r
Learning, 3(1):9–44 (important erratum p. 377).\r
Sutton, R. S. (1990). Integrated architectures for learning, planning, and reacting based on\r
approximating dynamic programming. In Proceedings of the 7th International Workshop on\r
Machine Learning, pp. 216–224. Morgan Kaufmann.\r
Sutton, R. S. (1991a). Dyna, an integrated architecture for learning, planning, and reacting.\r
SIGART Bulletin, 2(4):160–163. ACM, New York.\r
Sutton, R. S. (1991b). Planning by incremental dynamic programming. In Proceedings of the\r
8th International Workshop on Machine Learning, pp. 353–357. Morgan Kaufmann.\r
Sutton, R. S. (Ed.) (1992a). Reinforcement Learning. Kluwer Academic Press. Reprinting of a\r
special double issue on reinforcement learning, Machine Learning, 8(3-4).\r
Sutton, R. S. (1992b). Adapting bias by gradient descent: An incremental version of delta-bar\u0002delta. Proceedings of the Tenth National Conference on Artificial Intelligence, pp. 171–176,\r
MIT Press.\r
Sutton, R. S. (1992c). Gain adaptation beats least squares? Proceedings of the Seventh Yale\r
Workshop on Adaptive and Learning Systems, pp. 161–166, Yale University, New Haven, CT.\r
Sutton, R. S. (1995a). TD models: Modeling the world at a mixture of time scales. In\r
Proceedings of the 12th International Conference on Machine Learning, pp. 531–539. Morgan\r
Kaufmann.\r
Sutton, R. S. (1995b). On the virtues of linear learning and trajectory distributions. In\r
Proceedings of the Workshop on Value Function Approximation at The 12th International\r
Conference on Machine Learning."""

[[sections]]
number = "512"
title = "References"
text = """
Sutton, R. S. (1996). Generalization in reinforcement learning: Successful examples using sparse\r
coarse coding. In Advances in Neural Information Processing Systems 8, pp. 1038–1044.\r
MIT Press, Cambridge, MA.\r
Sutton, R. S. (2009). The grand challenge of predictive empirical abstract knowledge. Working\r
Notes of the IJCAI-09 Workshop on Grand Challenges for Reasoning from Experiences.\r
Sutton, R. S. (2015a) Introduction to reinforcement learning with function approximation.\r
Tutorial at the Conference on Neural Information Processing Systems, Montreal, December\r
7, 2015.\r
Sutton, R. S. (2015b) True online Emphatic TD(): Quick reference and implementation guide.\r
ArXiv:1507.07147. Code is available in Python and C++ by downloading the source files of\r
this arXiv paper as a zip archive.\r
Sutton, R. S., Barto, A. G. (1981a). Toward a modern theory of adaptive networks: Expectation\r
and prediction. Psychological Review, 88(2):135–170.\r
Sutton, R. S., Barto, A. G. (1981b). An adaptive network that constructs and uses an internal\r
model of its world. Cognition and Brain Theory, 3:217–246.\r
Sutton, R. S., Barto, A. G. (1987). A temporal-di↵erence model of classical conditioning. In\r
Proceedings of the Ninth Annual Conference of the Cognitive Science Society, pp. 355-378.\r
Erlbaum, Hillsdale, NJ.\r
Sutton, R. S., Barto, A. G. (1990). Time-derivative models of Pavlovian reinforcement. In\r
M. Gabriel and J. Moore (Eds.), Learning and Computational Neuroscience: Foundations of\r
Adaptive Networks, pp. 497–537. MIT Press, Cambridge, MA.\r
Sutton, R. S., Maei, H. R., Precup, D., Bhatnagar, S., Silver, D., Szepesv´ari, Cs., Wiewiora, E.\r
(2009a). Fast gradient-descent methods for temporal-di↵erence learning with linear function\r
approximation. In Proceedings of the 26th International Conference on Machine Learning,\r
pp. 993–1000. ACM, New York.\r
Sutton, R. S., Szepesv´ari, Cs., Maei, H. R. (2009b). A convergent O(d2) temporal-di↵erence\r
algorithm for o↵-policy learning with linear function approximation. In Advances in Neural\r
Information Processing Systems 21, pp. 1609–1616. Curran Associates, Inc.\r
Sutton, R. S., Mahmood, A. R., Precup, D., van Hasselt, H. (2014). A new Q() with interim\r
forward view and Monte Carlo equivalence. In Proceedings of the International Conference\r
on Machine Learning, 31. JMLR W&CP 32 (2).\r
Sutton, R. S., Mahmood, A. R., White, M. (2016). An emphatic approach to the problem of\r
o↵-policy temporal-di↵erence learning. Journal of Machine Learning Research, 17 (73):1–29.\r
Sutton, R. S., McAllester, D. A., Singh, S. P., Mansour, Y. (2000). Policy gradient methods\r
for reinforcement learning with function approximation. In Advances in Neural Information\r
Processing Systems 12, pp. 1057–1063. MIT Press, Cambridge, MA.\r
Sutton, R. S., Modayil, J., Delp, M., Degris, T., Pilarski, P. M., White, A., Precup, D. (2011).\r
Horde: A scalable real-time architecture for learning knowledge from unsupervised senso\u0002rimotor interaction. In Proceedings of the Tenth International Conference on Autonomous\r
Agents and Multiagent Systems, pp. 761–768, Taipei, Taiwan.\r
Sutton, R. S., Pinette, B. (1985). The learning of world models by connectionist networks. In\r
Proceedings of the Seventh Annual Conference of the Cognitive Science Society, pp. 54–64.\r
Sutton, R. S., Precup, D., Singh, S. (1999). Between MDPs and semi-MDPs: A framework for\r
temporal abstraction in reinforcement learning. Artificial Intelligence, 112 (1-2):181–211.\r
Sutton, R. S., Rafols, E., Koop, A. (2006). Temporal abstraction in temporal-di↵erence networks.\r
In Advances in neural information processing systems, pp. 1313–1320.\r
Sutton, R. S., Singh, S. P., McAllester, D. A. (2000). Comparing policy-gradient algorithms.\r
Unpublished manuscript.

References 513\r
Sutton, R. S., Szepesv´ari, Cs., Geramifard, A., Bowling, M., (2008). Dyna-style planning with\r
linear function approximation and prioritized sweeping. In Proceedings of the 24th Conference\r
on Uncertainty in Artificial Intelligence, pp. 528–536.\r
Sutton, R. S., Tanner, B. (2005). Temporal-di↵erence networks. In Advances in Neural\r
Information Processing Systems 17, p. 1377–1384.\r
Szepesv´ari, Cs. (2010). Algorithms for reinforcement learning. In Synthesis Lectures on Artificial\r
Intelligence and Machine Learning, 4(1):1–103. Morgan and Claypool.\r
Szita, I. (2012). Reinforcement learning in games. In M. Wiering and M. van Otterlo (Eds.),\r
Reinforcement Learning: State-of-the-Art, pp. 539–577. Springer-Verlag Berlin Heidelberg.\r
Tadepalli, P., Ok, D. (1994). H-learning: A reinforcement learning method to optimize\r
undiscounted average reward. Technical Report 94-30-01. Oregon State University, Computer\r
Science Department, Corvallis.\r
Tadepalli, P., Ok, D. (1996). Scaling up average reward reinforcement learning by approximating\r
the domain models and the value function. In Proceedings of the 13th International Conference\r
on Machine Learning, pp. 471–479.\r
Takahashi, Y., Schoenbaum, G., and Niv, Y. (2008). Silencing the critics: Understanding the\r
e↵ects of cocaine sensitization on dorsolateral and ventral striatum in the context of an\r
actor/critic model. Frontiers in Neuroscience, 2(1):86–99.\r
Tambe, M., Newell, A., Rosenbloom, P. S. (1990). The problem of expensive chunks and its\r
solution by restricting expressiveness. Machine Learning, 5 (3):299–348.\r
Tan, M. (1991). Learning a cost-sensitive internal representation for reinforcement learning. In\r
L. A. Birnbaum and G. C. Collins (Eds.), Proceedings of the 8th International Workshop on\r
Machine Learning, pp. 358–362. Morgan Kaufmann.\r
Tanner, B. (2006). Temporal-Di↵erence Networks. MSc thesis, University of Alberta.\r
Taylor, G., Parr, R. (2009). Kernelized value function approximation for reinforcement learning.\r
In Proceedings of the 26th International Conference on Machine Learning, pp. 1017–1024.\r
ACM, New York.\r
Taylor, M. E., Stone, P. (2009). Transfer learning for reinforcement learning domains: A survey.\r
Journal of Machine Learning Research, 10 :1633–1685.\r
Tesauro, G. (1986). Simple neural models of classical conditioning. Biological Cybernetics,\r
55(2-3):187–200.\r
Tesauro, G. (1992). Practical issues in temporal di↵erence learning. Machine Learning,\r
8(3-4):257–277.\r
Tesauro, G. (1994). TD-Gammon, a self-teaching backgammon program, achieves master-level\r
play. Neural Computation, 6(2):215–219.\r
Tesauro, G. (1995). Temporal di↵erence learning and TD-Gammon. Communications of the\r
ACM, 38(3):58–68.\r
Tesauro, G. (2002). Programming backgammon using self-teaching neural nets. Artificial\r
Intelligence, 134(1-2):181–199.\r
Tesauro, G., Galperin, G. R. (1997). On-line policy improvement using Monte-Carlo search. In\r
Advances in Neural Information Processing Systems 9, pp. 1068–1074. MIT Press, Cambridge,\r
MA.\r
Tesauro, G., Gondek, D. C., Lechner, J., Fan, J., Prager, J. M. (2012). Simulation, learning,\r
and optimization techniques in Watson’s game strategies. IBM Journal of Research and\r
Development, 56(3-4):16–1–16–11.\r
Tesauro, G., Gondek, D. C., Lenchner, J., Fan, J., Prager, J. M. (2013). Analysis of Watson’s\r
strategies for playing Jeopardy! Journal of Artificial Intelligence Research, 47:205–251.\r
Tham, C. K. (1994). Modular On-Line Function Approximation for Scaling up Reinforcement\r
Learning. PhD thesis, University of Cambridge."""

[[sections]]
number = "514"
title = "References"
text = """
Thathachar, M. A. L., Sastry, P. S. (1985). A new approach to the design of reinforcement\r
schemes for learning automata. IEEE Transactions on Systems, Man, and Cybernetics,\r
15(1):168–175.\r
Thathachar, M., Sastry, P. S. (2002). Varieties of learning automata: an overview. IEEE\r
Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics, 36(6):711–722.\r
Thathachar, M., Sastry, P. S. (2011). Networks of Learning Automata: Techniques for Online\r
Stochastic Optimization. Springer Science & Business Media.\r
Theocharous, G., Thomas, P. S., Ghavamzadeh, M. (2015). Personalized ad recommendation for\r
life-time value optimization guarantees. In Proceedings of the Twenty-Fourth International\r
Joint Conference on Artificial Intelligence. AAAI Press, Palo Alto, CA.\r
Thistlethwaite, D. (1951). A critical review of latent learning and related experiments. Psycho\u0002logical Bulletin, 48(2):97–129.\r
Thomas, P. S. (2014). Bias in natural actor–critic algorithms. In Proceedings of the 31st\r
International Conference on Machine Learning, JMLR W&CP 32 (1), pp. 441–448.\r
Thomas, P. S. (2015). Safe Reinforcement Learning. PhD thesis, University of Massachusetts,\r
Amherst.\r
Thomas, P. S., Brunskill, E. (2017). Policy gradient methods for reinforcement learning with\r
function approximation and action-dependent baselines. ArXiv:1706.06643.\r
Thomas, P. S., Theocharous, G., Ghavamzadeh, M. (2015). High-confidence o↵-policy evaluation.\r
In Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence, pp. 3000–3006.\r
AAAI Press, Menlo Park, CA.\r
Thompson, W. R. (1933). On the likelihood that one unknown probability exceeds another in\r
view of the evidence of two samples. Biometrika, 25(3-4):285–294.\r
Thompson, W. R. (1934). On the theory of apportionment. American Journal of Mathematics,\r
57: 450–457.\r
Thon, M. (2017). Spectral Learning of Sequential Systems. PhD thesis, Jacobs University\r
Bremen.\r
Thon, M., Jaeger, H. (2015). Links between multiplicity automata, observable operator models\r
and predictive state representations: a unified learning framework. The Journal of Machine\r
Learning Research, 16 (1):103–147.\r
Thorndike, E. L. (1898). Animal intelligence: An experimental study of the associative processes\r
in animals. The Psychological Review, Series of Monograph Supplements, II(4).\r
Thorndike, E. L. (1911). Animal Intelligence. Hafner, Darien, CT.\r
Thorp, E. O. (1966). Beat the Dealer: A Winning Strategy for the Game of Twenty-One.\r
Random House, New York.\r
Tian, T. (in preparation) An Empirical Study of Sliding-Step Methods in Temporal Di↵erence\r
Learning. M.Sc thesis, University of Alberta, Edmonton.\r
Tieleman, T., Hinton, G. (2012). Lecture 6.5–RMSProp. COURSERA: Neural networks for\r
machine learning 4.2:26–31.\r
Tolman, E. C. (1932). Purposive Behavior in Animals and Men. Century, New York.\r
Tolman, E. C. (1948). Cognitive maps in rats and men. Psychological Review, 55(4):189–208.\r
Tsai, H.-S., Zhang, F., Adamantidis, A., Stuber, G. D., Bonci, A., de Lecea, L., Deisseroth,\r
K. (2009). Phasic firing in dopaminergic neurons is sucient for behavioral conditioning.\r
Science, 324(5930):1080–1084.\r
Tsetlin, M. L. (1973). Automaton Theory and Modeling of Biological Systems. Academic Press,\r
New York.

References 515\r
Tsitsiklis, J. N. (1994). Asynchronous stochastic approximation and Q-learning. Machine\r
Learning, 16(3):185–202.\r
Tsitsiklis, J. N. (2002). On the convergence of optimistic policy iteration. Journal of Machine\r
Learning Research, 3:59–72.\r
Tsitsiklis, J. N., Van Roy, B. (1996). Feature-based methods for large scale dynamic programming.\r
Machine Learning, 22(1-3):59–94.\r
Tsitsiklis, J. N., Van Roy, B. (1997). An analysis of temporal-di↵erence learning with function\r
approximation. IEEE Transactions on Automatic Control, 42(5):674–690.\r
Tsitsiklis, J. N., Van Roy, B. (1999). Average cost temporal-di↵erence learning. Automatica,\r
35(11):1799–1808.\r
Turing, A. M. (1948). Intelligent machinery. In B. Jack Copeland (Ed.) (2004), The Essential\r
Turing, pp. 410–432. Oxford University Press, Oxford.\r
Ungar, L. H. (1990). A bioreactor benchmark for adaptive network-based process control. In\r
W. T. Miller, R. S. Sutton, and P. J. Werbos (Eds.), Neural Networks for Control, pp. 387–402.\r
MIT Press, Cambridge, MA.\r
Unnikrishnan, K. P., Venugopal, K. P. (1994). Alopex: A correlation-based learning algorithm\r
for feedforward and recurrent neural networks. N eural Computation, 6(3): 469–490.\r
Urbanczik, R., Senn, W. (2009). Reinforcement learning in populations of spiking neurons.\r
Nature neuroscience, 12(3):250–252.\r
Urbanowicz, R. J., Moore, J. H. (2009). Learning classifier systems: A complete introduction,\r
review, and roadmap. Journal of Artificial Evolution and Applications. 10.1155/2009/736398.\r
Valentin, V. V., Dickinson, A., O’Doherty, J. P. (2007). Determining the neural substrates of\r
goal-directed learning in the human brain. The Journal of Neuroscience, 27(15):4019–4026.\r
van Hasselt, H. (2010). Double Q-learning. In Advances in Neural Information Processing\r
Systems 23, pp. 2613–2621. Curran Associates, Inc.\r
van Hasselt, H. (2011). Insights in Reinforcement Learning: Formal Analysis and Empirical\r
Evaluation of Temporal-di↵erence Learning. SIKS dissertation series number 2011-04.\r
van Hasselt, H. (2012). Reinforcement learning in continuous state and action spaces. In M.\r
Wiering and M. van Otterlo (Eds.), Reinforcement Learning: State-of-the-Art, pp. 207–251.\r
Springer-Verlag Berlin Heidelberg.\r
van Hasselt, H., Sutton, R. S. (2015). Learning to predict independent of span. ArXiv:1508.04582.\r
Van Roy, B., Bertsekas, D. P., Lee, Y., Tsitsiklis, J. N. (1997). A neuro-dynamic programming\r
approach to retailer inventory management. In Proceedings of the 36th IEEE Conference on\r
Decision and Control, Vol. 4, pp. 4052–4057.\r
van Seijen, H. (2011). Reinforcement Learning under Space and Time Constraints. University of\r
Amsterdam PhD thesis. Hague: TNO.\r
van Seijen, H. (2016). E↵ective multi-step temporal-di↵erence learning for non-linear function\r
approximation. ArXiv:1608.05151.\r
van Seijen, H., Sutton, R. S. (2013). Ecient planning in MDPs by small backups. In: Proceedings\r
of the 30th International Conference on Machine Learning, pp. 361–369.\r
van Seijen, H., Sutton, R. S. (2014). True online TD(). In Proceedings of the 31st International\r
Conference on Machine Learning, pp. 692–700. JMLR W&CP 32(1),\r
van Seijen, H., Mahmood, A. R., Pilarski, P. M., Machado, M. C., Sutton, R. S. (2016). True\r
online temporal-di↵erence learning. Journal of Machine Learning Research, 17 (145):1–40.\r
van Seijen, H., Van Hasselt, H., Whiteson, S., Wiering, M. (2009). A theoretical and empirical\r
analysis of Expected Sarsa. In IEEE Symposium on Adaptive Dynamic Programming and\r
Reinforcement Learning, pp. 177–184."""

[[sections]]
number = "516"
title = "References"
text = """
van Seijen, H., Whiteson, S., van Hasselt, H., Wiering, M. (2011). Exploiting best-match\r
equations for ecient reinforcement learning. Journal of Machine Learning Research 12 :2045–\r
2094.\r
Varga, R. S. (1962). Matrix Iterative Analysis. Englewood Cli↵s, NJ: Prentice-Hall.\r
Vasilaki, E., Fr´emaux, N., Urbanczik, R., Senn, W., Gerstner, W. (2009). Spike-based rein\u0002forcement learning in continuous state and action space: when policy gradient methods fail.\r
PLoS Computational Biology, 5(12).\r
Viswanathan, R., Narendra, K. S. (1974). Games of stochastic automata. IEEE Transactions\r
on Systems, Man, and Cybernetics, 4(1):131–135.\r
Wagner, A. R. (2008). Evolution of an elemental theory of Pavlovian conditioning. Learning &\r
Behavior, 36 (3):253–265.\r
Walter, W. G. (1950). An imitation of life. Scientific American, 182(5):42–45.\r
Walter, W. G. (1951). A machine that learns. Scientific American, 185(2):60–63.\r
Waltz, M. D., Fu, K. S. (1965). A heuristic approach to reinforcement learning control systems.\r
IEEE Transactions on Automatic Control, 10(4):390–398.\r
Watkins, C. J. C. H. (1989). Learning from Delayed Rewards. PhD thesis, University of\r
Cambridge.\r
Watkins, C. J. C. H., Dayan, P. (1992). Q-learning. Machine Learning, 8(3-4):279–292.\r
Werbos, P. J. (1977). Advanced forecasting methods for global crisis warning and models of\r
intelligence. General Systems Yearbook, 22(12):25–38.\r
Werbos, P. J. (1982). Applications of advances in nonlinear sensitivity analysis. In R. F. Drenick\r
and F. Kozin (Eds.), System Modeling and Optimization, pp. 762–770. Springer-Verlag.\r
Werbos, P. J. (1987). Building and understanding adaptive systems: A statistical/numerical\r
approach to factory automation and brain research. IEEE Transactions on Systems, Man,\r
and Cybernetics, 17(1):7–20.\r
Werbos, P. J. (1988). Generalization of back propagation with applications to a recurrent gas\r
market model. Neural Networks, 1(4):339–356.\r
Werbos, P. J. (1989). Neural networks for control and system identification. In Proceedings of\r
the 28th Conference on Decision and Control, pp. 260–265. IEEE Control Systems Society.\r
Werbos, P. J. (1992). Approximate dynamic programming for real-time control and neural\r
modeling. In D. A. White and D. A. Sofge (Eds.), Handbook of Intelligent Control: Neural,\r
Fuzzy, and Adaptive Approaches, pp. 493–525. Van Nostrand Reinhold, New York.\r
Werbos, P. J. (1994). The Roots of Backpropagation: From Ordered Derivatives to Neural\r
Networks and Political Forecasting (Vol. 1). John Wiley and Sons.\r
Wiering, M., Van Otterlo, M. (2012). Reinforcement Learning: State-of-the-Art. Springer-Verlag\r
Berlin Heidelberg.\r
White, A. (2015). Developing a Predictive Approach to Knowledge. PhD thesis, University of\r
Alberta, Edmonton.\r
White, D. J. (1969). Dynamic Programming. Holden-Day, San Francisco.\r
White, D. J. (1985). Real applications of Markov decision processes. Interfaces, 15(6):73–83.\r
White, D. J. (1988). Further real applications of Markov decision processes. Interfaces,\r
18(5):55–61.\r
White, D. J. (1993). A survey of applications of Markov decision processes. Journal of the\r
Operational Research Society, 44(11):1073–1096.\r
White, A., White, M. (2016). Investigating practical linear temporal di↵erence learning. In\r
Proceedings of the 2016 International Conference on Autonomous Agents and Multiagent\r
Systems, pp. 494–502.

References 517\r
Whitehead, S. D., Ballard, D. H. (1991). Learning to perceive and act by trial and error.\r
Machine Learning, 7(1):45–83.\r
Whitt, W. (1978). Approximations of dynamic programs I. Mathematics of Operations Research,\r
3(3):231–243.\r
Whittle, P. (1982). Optimization over Time, vol. 1. Wiley, New York.\r
Whittle, P. (1983). Optimization over Time, vol. 2. Wiley, New York.\r
Wickens, J., K¨otter, R. (1995). Cellular models of reinforcement. In J. C. Houk, J. L. Davis and\r
D. G. Beiser (Eds.), Models of Information Processing in the Basal Ganglia, pp. 187–214.\r
MIT Press, Cambridge, MA.\r
Widrow, B., Gupta, N. K., Maitra, S. (1973). Punish/reward: Learning with a critic in adaptive\r
threshold systems. IEEE Transactions on Systems, Man, and Cybernetics, 3(5):455–465.\r
Widrow, B., Ho↵, M. E. (1960). Adaptive switching circuits. In 1960 WESCON Convention\r
Record Part IV, pp. 96–104. Institute of Radio Engineers, New York. Reprinted in J. A.\r
Anderson and E. Rosenfeld, Neurocomputing: Foundations of Research, pp. 126–134. MIT\r
Press, Cambridge, MA, 1988.\r
Widrow, B., Smith, F. W. (1964). Pattern-recognizing control systems. In J. T. Tou and\r
R. H. Wilcox (Eds.), Computer and Information Sciences, pp. 288–317. Spartan, Washington,\r
DC.\r
Widrow, B., Stearns, S. D. (1985). Adaptive Signal Processing. Prentice-Hall, Englewood Cli↵s,\r
NJ.\r
Wiener, N. (1964). God and Golem, Inc: A Comment on Certain Points where Cybernetics\r
Impinges on Religion. MIT Press, Cambridge, MA.\r
Wiewiora, E. (2003). Potential-based shaping and Q-value initialization are equivalent. Journal\r
of Artificial Intelligence Research, 19 :205–208.\r
Williams, R. J. (1986). Reinforcement learning in connectionist networks: A mathematical\r
analysis. Technical Report ICS 8605. Institute for Cognitive Science, University of California\r
at San Diego, La Jolla.\r
Williams, R. J. (1987). Reinforcement-learning connectionist systems. Technical Report\r
NU-CCS-87-3. College of Computer Science, Northeastern University, Boston.\r
Williams, R. J. (1988). On the use of backpropagation in associative reinforcement learning.\r
In Proceedings of the IEEE International Conference on Neural Networks, pp. I-263–I-270.\r
IEEE San Diego section and IEEE TAB Neural Network Committee.\r
Williams, R. J. (1992). Simple statistical gradient-following algorithms for connectionist\r
reinforcement learning. Machine Learning, 8(3-4):229–256.\r
Williams, R. J., Baird, L. C. (1990). A mathematical analysis of actor–critic architectures for\r
learning optimal controls through incremental dynamic programming. In Proceedings of the\r
Sixth Yale Workshop on Adaptive and Learning Systems, pp. 96–101. Center for Systems\r
Science, Dunham Laboratory, Yale University, New Haven.\r
Wilson, R. C., Takahashi, Y. K., Schoenbaum, G., Niv, Y. (2014). Orbitofrontal cortex as a\r
cognitive map of task space. Neuron, 81(2):267–279.\r
Wilson, S. W. (1994). ZCS, A zeroth order classifier system. Evolutionary Computation,\r
2(1):1–18.\r
Wise, R. A. (2004). Dopamine, learning, and motivation. Nature Reviews Neuroscience,\r
5(6):1–12.\r
Witten, I. H. (1976a). Learning to Control. University of Essex PhD thesis.\r
Witten, I. H. (1976b). The apparent conflict between estimation and control—A survey of the\r
two-armed problem. Journal of the Franklin Institute, 301(1-2):161–189."""

[[sections]]
number = "518"
title = "References"
text = """
Witten, I. H. (1977). An adaptive optimal controller for discrete-time Markov environments.\r
Information and Control, 34(4):286–295.\r
Witten, I. H., Corbin, M. J. (1973). Human operators and automatic adaptive controllers: A\r
comparative study on a particular control task. International Journal of Man–Machine\r
Studies, 5(1):75–104.\r
Woodbury, T., Dunn, C., and Valasek, J. (2014). Autonomous soaring using reinforcement\r
learning for trajectory generation. In 52nd Aerospace Sciences Meeting, p. 0990.\r
Woodworth, R. S. (1938). Experimental Psychology. New York: Henry Holt and Company.\r
Xie, X., Seung, H. S. (2004). Learning in neural networks by reinforcement of irregular spiking.\r
Physical Review E, 69(4):041909.\r
Xu, X., Xie, T., Hu, D., Lu, X. (2005). Kernel least-squares temporal di↵erence learning.\r
International Journal of Information Technology, 11 (9):54–63.\r
Yagishita, S., Hayashi-Takagi, A., Ellis-Davies, G. C. R., Urakubo, H., Ishii, S., Kasai, H. (2014).\r
A critical time window for dopamine actions on the structural plasticity of dendritic spines.\r
Science, 345(6204):1616–1619.\r
Yee, R. C., Saxena, S., Utgo↵, P. E., Barto, A. G. (1990). Explaining temporal di↵erences to\r
create useful concepts for evaluating states. In Proceedings of the Eighth National Conference\r
on Artificial Intelligence, pp. 882–888. AAAI Press, Menlo Park, CA.\r
Yin, H. H., Knowlton, B. J. (2006). The role of the basal ganglia in habit formation. Nature\r
Reviews Neuroscience, 7(6):464–476.\r
Young, P. (1984). Recursive Estimation and Time-Series Analysis. Springer-Verlag, Berlin.\r
Yu, H. (2010). Convergence of least squares temporal di↵erence methods under general conditions.\r
International Conference on Machine Learning 27, pp. 1207–1214.\r
Yu, H. (2012). Least squares temporal di↵erence methods: An analysis under general conditions.\r
SIAM Journal on Control and Optimization, 50 (6):3310–3343.\r
Yu, H. (2015). On convergence of emphatic temporal-di↵erence learning. In Proceedings of the\r
28th Annual Conference on Learning Theory, JMLR W&CP 40. Also ArXiv:1506.02582.\r
Yu, H. (2016). Weak convergence properties of constrained emphatic temporal-di↵erence learning\r
with constant and slowly diminishing stepsize. Journal of Machine Learning Research,\r
17 (220):1–58.\r
Yu, H. (2017). On convergence of some gradient-based temporal-di↵erences algorithms for\r
o↵-policy learning. ArXiv:1712.09652.\r
Yu, H., Mahmood, A. R., Sutton, R. S. (2017). On generalized bellman equations and temporal\u0002di↵erence learning. ArXiv:17041.04463. A summary appeared in Proceedings of the Canadian\r
Conference on Artificial Intelligence, pp. 3–14. Springer.

Index\r
Page numbers in italics are recommended to be consulted first. Page numbers in bold contain\r
boxed algorithms.\r
k-armed bandits, 25–45\r
absorbing state, 57\r
access-control queuing example, 256\r
action preferences, 322, 329, 336, 455\r
in bandit problems, 37, 42\r
action-value function, see value function, action\r
action-value methods, 321\r
for bandit problems, 27\r
actor–critic, 21, 239, 321, 331–332, 338, 406\r
advantage, A2C, 338\r
one-step (episodic), 332\r
with eligibility traces (episodic), 332\r
with eligibility traces (continuing), 333\r
neural, 395–415\r
addiction, 409–410\r
afterstates, 137, 140, 181, 182, 191, 424, 430\r
agent–environment interface, 47–58, 466\r
all-actions algorithm, 326\r
AlphaGo, AlphaGo Zero, AlphaZero, 441–450\r
Andreae, John, 17, 21, 69, 89\r
ANN, see artificial neural networks\r
applications and case studies, 421–457\r
approximate dynamic programming, 15\r
artificial intelligence, xvii, 1, 472, 478\r
artificial neural networks, 223–228, 238–240,\r
395–398, 423, 430, 436–450, 472\r
associative reinforcement learning, 45, 418\r
associative search, 41\r
asynchronous dynamic programming, 85, 88\r
Atari video game play, 436–441\r
auxiliary tasks, 460–461, 468, 474\r
average reward setting, 249–255, 258, 464\r
averagers, 264\r
backgammon, 11, 21, 182, 184, 421–426\r
backpropagation, 21, 225–227, 239, 407, 424,\r
436, 439\r
backup diagram, 60, 139\r
for dynamic programming, 59, 61, 64, 172\r
for Monte Carlo methods, 94\r
for Q-learning, 134\r
for TD(0), 121\r
for Sarsa, 129\r
for Expected Sarsa, 134\r
for Sarsa(), 304\r
for TD(), 289\r
for Q(), 313\r
for Tree Backup(), 314\r
for Truncated TD(), 296\r
for n-step Q(), 155\r
for n-step Expected Sarsa, 146\r
for n-step Sarsa, 146\r
for n-step TD, 142\r
for n-step Tree Backup, 152\r
for Samuel’s Checker Player, 428\r
compound, 288\r
half backups, 62\r
backward view of eligibility traces, 288, 293\r
Baird’s counterexample, 261–264, 280, 283, 285\r
bandit algorithm, simple, 32\r
bandit problems, 25–45\r
basal ganglia, 386\r
baseline, 37–40, 329, 330, 338\r
behavior policy, 103, 110, see o↵-policy learning\r
Bellman equation, 14\r
for v⇡, 59\r
for q⇡, 78\r
for optimal value functions: v⇤ and q⇤, 63\r
di↵erential, 250\r
for options, 463\r
Bellman error, 268, 270, 272, 273\r
learnability of, 274–278\r
vector, 267–269\r
Bellman operator, 267–269, 286\r
Bellman residual, 286, see Bellman error\r
Bellman, Richard, 14, 71, 89, 241\r
binary features, 215, 222, 245, 304, 305\r
bioreactor example, 51\r
blackjack example, 93–94, 99, 106\r
blocking maze example, 166\r
bootstrapping, 89, 189, 308\r
n-step, 141–158, 255\r
and dynamic programming, 89\r
and function approximation, 208, 264–274"""

[[sections]]
number = "520"
title = "Index"
text = """
and Monte Carlo methods, 95\r
and stability, 263–265\r
and TD learning, 120\r
assessment of, 124–128, 248, 264, 291, 318\r
in psychology, 345, 349, 354, 355\r
parameter ( or n), 291, 307, 399\r
BOXES, 18, 71, 237\r
branching factor, 173–177, 422\r
breakfast example, 5, 22\r
bucket-brigade algorithm, 19, 21, 139\r
catastrophic interference, 472\r
certainty-equivalence estimate, 128\r
chess, 4, 20, 54, 182, 450\r
classical conditioning, 20, 343–357\r
blocking, 371\r
and higher-order conditioning, 345–355\r
delay and trace conditioning, 344\r
Rescorla-Wagner model, 346–349\r
TD model, 349–357\r
classifier systems, 19, 21\r
cli↵ walking example, 132, 133\r
CMAC, see tile coding\r
coarse coding, 215–220, 238\r
cognitive maps, 363–364\r
collective reinforcement learning, 404–407\r
complex backups, see compound update\r
compound stimulus, 345, 346–356, 371, 382\r
compound update/backup, 288, 319\r
conditioned/unconditioned stimulus, conditioned\r
response (CS/US, CR), 344\r
constant-↵ MC, 120\r
contextual bandits, 41\r
continuing tasks, 54, 57, 70, 124, 249, 294\r
continuous action, 73, 244, 335–336\r
continuous state, 73, 223, 238\r
continuous time, 11, 71\r
control and prediction, 342\r
control theory, 4, 71\r
control variates, 150–152, 155, 281\r
and eligibility traces, 309–312\r
credit assignment, 11, 17, 19, 47, 294, 401\r
in psychology, 346, 361\r
structural, 385, 405, 407\r
critic, 18, 239, 346, 417, see actor–critic\r
cumulant, 459\r
curiosity, 474\r
curse of dimensionality, 4, 14, 221, 231\r
cybernetics, xvii, 477\r
deadly triad, 264\r
deep learning, 12, 223, 441, 472–474, 479\r
deep reinforcement learning, 236\r
deep residual learning, 227\r
delayed reinforcement, 361–363\r
delayed reward, 1, 47, 249\r
dimensions of reinforcement learning methods,\r
189–191\r
direct and indirect RL, 162, 164, 192\r
discounting, 55, 199, 243, 249, 282, 324, 328,\r
427, 459\r
in pole balancing, 56\r
state dependent, 307\r
deprecated, 253, 256\r
distribution models, 159, 185\r
dopamine, 377, 381–387, 413–419\r
and addiction, 409–410\r
double learning, 134–136, 140\r
DP, see dynamic programming\r
driving-home example, 122–123\r
Dyna architecture, 164, 161–170\r
dynamic programming, 13–15, 73–90, 174, 262\r
and artificial intelligence, 89\r
and function approximation, 241\r
and options, 463\r
and the deadly triad, 264\r
computational eciency of, 87\r
eligibility traces, 287–320, 350, 362, 398–403\r
accumulating, 300, 306, 310\r
replacing, 301, 306\r
dutch, 300–303\r
contingent/non-contingent, 399–403, 411\r
o↵-policy, 309–316\r
with state-dependent  and , 309–316\r
Emphatic-TD methods, 234–235, 315\r
o↵-policy, 281–282\r
environment, 47–58\r
episodes, episodic tasks, 11, 54–58, 91\r
error reduction property, 144, 288\r
evaluative feedback, 17, 25, 47\r
evolution, 7, 359, 374, 471\r
evolutionary methods, 7, 8, 9, 11, 19\r
expected approximate value, 148, 155\r
Expected Sarsa, 133, see also Sarsa, Expected\r
expected update, 75, 172–181, 189\r
experience replay, 440–441\r
explore/exploit dilemma, 3, 103, 472\r
exploring starts, 96, 98–100, 178

Index 521\r
feature construction, 210–223\r
final time step (T), 54\r
Fourier basis, 211–215\r
function approximation, 195–200\r
gambler’s example, 84\r
game theory, 19\r
gazelle calf example, 5\r
general value functions (GVFs), 459–463, 474\r
generalized policy iteration (GPI), 86–87, 92,\r
97, 138, 189\r
genetic algorithms, 19\r
Gittins index, 43\r
gliding/soaring case study, 453–457\r
goal, see reward signal\r
golf example, 61, 63, 66\r
gradient, 201\r
gradient descent, see stochastic gradient de\u0002scent\r
Gradient-TD methods, 278–281, 314–315\r
greedy or "-greedy\r
as exploiting, 26–28\r
as shortsighted, 64\r
"-greedy policies, 100\r
gridworld examples, 60, 65, 76, 147\r
cli↵ walking, 132\r
Dyna blocking maze, 166\r
Dyna maze, 164\r
Dyna shortcut maze, 167\r
windy, 130, 131\r
habitual and goal-directed control, 364–368\r
hedonistic neurons, 402–404\r
heuristic search, 181–183, 190\r
as sequences of backups, 183\r
in Samuel’s checkers player, 426\r
in TD-Gammon, 425\r
history of reinforcement learning, 13–22\r
Holland, John, 19, 21, 44, 139, 241\r
Hull, Clark, 16, 359, 360, 362–363\r
importance sampling, 103–117, 151, 257\r
ratio, 104, 148, 258\r
weighted and ordinary, 105, 106\r
and eligibility traces, 309–312\r
and infinite variance, 106\r
discounting aware, 112–113\r
incremental implementation, 109\r
per-decision, 114–115\r
n-step, 148–156\r
incremental implementation\r
of averages, 30–33\r
of weighted averages, 109\r
instrumental conditioning, 357–361, see also\r
Law of E↵ect\r
and motivation, 360–361\r
Thorndike’s puzzle boxes, 358\r
interest and emphasis, 234–235, 282, 316\r
inverse reinforcement learning, 470\r
Jack’s car rental example, 81–82, 137, 210\r
kernel-based function approximation, 232–233\r
Klopf, A. Harry, xv, xvii, 19–21, 402–404, 411\r
latent learning, 192, 363, 366\r
Law of E↵ect, 15–16, 45, 343, 358–361, 417\r
learning automata, 18\r
Least Mean Square (LMS) algorithm, 279, 301\r
Least-Squares TD (LSTD), 228–229\r
linear function approx., 204–209, 266–269\r
linear programming, 87, 90\r
local and global optima, 200\r
Markov decision process (MDP), 2, 14, 47–71\r
Markov property, 49, 115, 465–468\r
Markov reward process (MRP), 125\r
maximization bias, 134–136\r
maximum-likelihood estimate, 128\r
MC, see Monte Carlo methods\r
Mean Square\r
Bellman Error, BE, 268\r
Projected Bellman Error, PBE, 269\r
Return Error, RE, 275\r
TD Error, TDE, 270\r
Value Error, VE, 199–200\r
memory-based function approx., 230–232\r
Michie, Donald, 17, 71, 117\r
Minsky, Marvin, 16, 17, 20, 89\r
model of the environment, 7, 159\r
model-based and model-free methods, 7, 159\r
in animal learning, 363–368\r
model-based reinforcement learning, 159–193\r
in neuroscience, 407–409\r
Monte Carlo methods, 91–117\r
first- and every-visit MC, 92\r
first-visit MC control, 101\r
first-visit MC prediction, 92"""

[[sections]]
number = "522"
title = "Index"
text = """
gradient method for v⇡, 202\r
Monte Carlo ES (Exploring Starts), 99\r
o↵-policy control, 111, 110–112\r
o↵-policy prediction, 103–109, 110\r
Monte Carlo Tree Search (MCTS), 185–188\r
motivation, 360–361\r
mountain car example, 244–248, 305, 306\r
multi-armed bandits, 25–45\r
n-step methods, 141–158\r
Q(), 156\r
Sarsa, 147, 247\r
di↵erential, 255\r
o↵-policy, 149\r
TD, 144\r
Tree Backup, 154\r
truncated -return, 295\r
naughts and crosses, see tic-tac-toe\r
neural networks, see artificial neural networks\r
neurodynamic programming, 15\r
neuroeconomics, 413, 419\r
neuroscience, 4, 21, 377–419\r
nonstationarity, 30, 32–36, 44, 255\r
inherent, 91, 198\r
notation, xiii, xix\r
observations, 464\r
o↵-policy methods, 257–286\r
vs on-policy methods, 100, 103\r
Monte Carlo, 103–115\r
Q-learning, 131\r
Expected Sarsa, 133–134\r
n-step, 148–156\r
n-step Q(), 156\r
n-step Sarsa, 149\r
n-step Tree Backup, 154\r
and eligibility traces, 309–316\r
Emphatic-TD(), 315\r
GQ(), 315\r
GTD(), 314\r
HTD(), 315\r
Q(), 312–314\r
Tree Backup(), 312–314\r
reducing variance, 283–284\r
on-policy distribution, 175, 199, 208, 258, 262,\r
281, 282\r
vs uniform distribution, 176\r
on-policy methods, 100\r
actor–critic, 332, 333\r
approximate\r
control, 244, 247, 251, 255\r
prediction, 202, 203, 209\r
Monte Carlo, 101, 100–103, 328, 330\r
n-step, 144, 147\r
Sarsa, 130, 129–131\r
TD(0), 120, 119–128\r
with eligibility traces, 293, 300, 305, 307\r
operant conditioning, see instrumental learning\r
optimal control, 2, 14–15, 21\r
optimistic initial values, 34–35, 192\r
optimizing memory control, 432–436\r
options, 461–464\r
models of, 462\r
pain and pleasure, 6, 16, 413\r
Partially Observable MDPs (POMDPs), 467\r
Pavlov, Ivan, 16, 343–345, 362\r
Pavlovian\r
conditioning, see classical conditioning\r
control, 343, 371, 373, 478\r
personalizing web services, 450–453\r
planning, 3, 5, 7, 11, 138, 159–193\r
in psychology, 363, 364, 366\r
with learned models, 161–168, 473\r
with options, 461, 463\r
policy, 6, 41, 58\r
hierarchical, 462\r
soft and "-soft, 100–103, 110\r
policy approximation, 321–324\r
policy evaluation, 74–76, see also prediction\r
iterative, 75\r
policy gradient methods, 321–338\r
REINFORCE, 328, 330\r
actor–critic, 332, 333\r
policy gradient theorem, 324–326\r
proof, episodic case, 325\r
proof, continuing case, 334\r
policy improvement, 76–80\r
theorem, 78, 101\r
policy iteration, 14, 80, 80–82\r
polynomial basis, 210–211\r
prediction, 74–76, see also policy evaluation\r
and control, 342\r
Monte Carlo, 92–97\r
o↵-policy, 103–108\r
TD, 119–126\r
with approximation, 197–242\r
prior knowledge, 12, 34, 54, 137, 236, 324, 471

Index 523\r
prioritized sweeping, 170, 168–171\r
projected Bellman error, 285\r
vector, 267, 269\r
proximal TD methods, 286\r
pseudo termination, 282, 308\r
psychology, 4, 13, 19, 20, 341–376\r
Q(), Watkins’s, 312–314\r
Q-function, see action-value function\r
Q-learning, 21, 131, 131–135\r
double, 136\r
Q-planning, 161\r
Q(), 156, 154–156\r
queuing example, 252\r
R-learning, 256\r
racetrack exercise, 111\r
radial basis functions (RBFs), 221–222\r
random walk, 95\r
5-state, 125, 126, 127\r
19-state, 144, 291\r
TD() results on, 294, 295, 299\r
1000-state, 203–209, 217, 218\r
Fourier and polynomial bases, 214\r
real-time dynamic programming, 177–180\r
recycling robot example, 52\r
REINFORCE, 328, 326–331\r
with baseline, 330\r
reinforcement learning, 1–22\r
reinforcement signal, 380\r
representation learning, 473\r
residual-gradient algorithm, 272–274, 277\r
naive, 270, 271\r
return, 54–58\r
n-step, 143\r
for Q(), 155\r
for action values, 146\r
for Expected Sarsa, 148\r
for Tree Backup, 153\r
with control variates, 150, 151\r
with function approximation, 209\r
di↵erential, 250, 255, 334\r
flat partial, 113\r
with state-dependent termination, 308\r
-return, 288–291\r
truncated, 296\r
reward prediction error hypothesis, 381–383,\r
387–395\r
reward signal, 1, 6, 48, 53, 361, 380, 383, 397\r
and reinforcement, 373–375, 380–381\r
design of, 469–472, 477\r
intrinsic, 474\r
sparse, 469–470\r
rod maneuvering example, 171\r
rollout algorithms, 183–185\r
root mean square (RMS) error, 125\r
safety, 434, 478\r
sample and expected updates, 121, 170–174\r
sample or simulation model, 115\r
sample-average method, 27\r
Samuel’s checkers player, 20, 241, 426–429\r
Sarsa, 130, 129–131, 244\r
vs Q-learning, 132\r
di↵erential, one-step, 251\r
Expected, 133–134, 140\r
n-step, 148\r
n-step o↵-policy, 150\r
double, 136\r
n-step, 147, 145–148, 247\r
di↵erential, 255\r
o↵-policy, 149\r
Sarsa(), 305, 303–307\r
true online, 307\r
Schultz, Wolfram, 387–395, 410\r
search control, 163\r
secondary reinforcement, 20, 346, 354, 369\r
selective bootstrap adaptation, 239\r
semi-gradient methods, 202, 258–259\r
SGD, see stochastic gradient descent\r
Shannon, Claude, 16, 20, 71, 426\r
shaping, 360, 470\r
Skinner, B. F., 359–360, 375, 470, 479\r
soap bubble example, 95\r
soft and "-soft policies, 100–103, 110\r
soft-max, 322–323, 329, 336, 400, 445, 455\r
for bandits, 37, 45\r
spike-timing-dependent plasticity (STDP), 401\r
state, 7, 48, 49\r
kth-order history approach, 468\r
and observations, 464–468\r
Markov property, 465–468\r
belief, 467\r
latent, 467\r
observable operator models (OOMs), 467\r
partially observable MDPs, 14, 467\r
predictive state representations, 467\r
state-update function, 465"""

[[sections]]
number = "524"
title = "Index"
text = """
state aggregation, 203–204\r
state-update function, 465\r
step-size parameter, 10, 31–33, 120, 125, 126\r
automatic adaptation, 238\r
in DQN, 439, 440\r
in psychological models, 347, 348\r
selecting manually, 222–223\r
with coarse coding, 216\r
with Fourier features, 213\r
with tile coding, 217, 223\r
stochastic approx. convergence conditions, 33\r
stochastic gradient descent (SGD), 200–204\r
in the Bellman error, 269–278\r
strong and weak methods, 4\r
supervised learning, xvii, 2, 17–19, 198\r
sweeps, 75, 160, see also prioritized sweeping\r
synaptic plasticity, 379\r
Hebbian, 400\r
two-factor and three factor, 400\r
system identification, 364\r
tabular solution methods, 23\r
target\r
policy, 103, 110\r
of update, 31, 143, 198\r
TD, see temporal-di↵erence learning\r
TD error, 121\r
n-step, 255\r
di↵erential, 250\r
with function approximation, 270\r
TD(), 293, 292–295\r
truncated, 295–297\r
true online, 300, 299–301\r
TD-Gammon, 21, 421–426\r
temporal abstraction, 461–464\r
temporal-di↵erence learning, 10, 119–140\r
history of, 20–21\r
advantages of, 124–126\r
optimality of, 126–128\r
TD(0), 120, 203\r
TD(1), 294\r
TD(), 293, 292–295\r
true online, 300, 299–301\r
-return methods\r
o↵-line, 290\r
online, 297–299\r
n-step, 144, 141–158, 209\r
termination function, 307, 459\r
Thompson sampling, 43, 45\r
Thorndike, Edward, see Law of E↵ect\r
tic-tac-toe, 8–13, 17, 137\r
tile coding, 217–221, 223, 238, 246, 434, 435\r
Tolman, Edward, 364, 408\r
trace-decay parameter (), 287, 289, 290, 292\r
state dependent, 307\r
trajectory sampling, 174–177\r
transition probabilities, 49\r
Tree Backup\r
n-step, 152–153, 154\r
Tree-Backup(), 312–314\r
trial-and-error, 1, 7, 15–21, 403, 404, see also\r
instrumental conditioning\r
true online TD(), 300, 299–301\r
Tsitsiklis and Van Roy’s Counterexample, 263\r
undiscounted continuing tasks, see average re\u0002ward setting\r
unsupervised learning, 2, 226\r
value, 6, 26, 47\r
value function, 6, 58–67\r
for a given policy: v⇡ and q⇡, 58\r
for an optimal policy: v⇤ and q⇤, 62\r
action, 58, 63, 65, 71, 129, 131\r
approximate action values: qˆ(s, a, w), 243\r
approximate state values: ˆv(s,w), 197\r
di↵erential, 243\r
vs evolutionary methods, 11\r
value iteration, 83, 82–84\r
value-function approximation, 198\r
Watkins, Chris, 15, 21, 89, 320\r
Watson (Jeopardy! player), 429–432\r
Werbos, Paul, 14, 21, 70, 89, 139, 239\r
Witten, Ian, 21, 70"""

[[sections]]
number = "525"
title = "Adaptive Computation and Machine Learning"
text = """
Francis Bach, Editor\r
Bioinformatics: The Machine Learning Approach, Pierre Baldi and Søren Brunak\r
Reinforcement Learning: An Introduction, Richard S. Sutton and Andrew G. Barto\r
Graphical Models for Machine Learning and Digital Communication, Brendan J. Frey\r
Learning in Graphical Models, Michael I. Jordan\r
Causation, Prediction, and Search, second edition, Peter Spirtes, Clark Glymour, and\r
Richard Scheines\r
Principles of Data Mining, David Hand, Heikki Mannila, and Padhraic Smyth\r
Bioinformatics: The Machine Learning Approach, second edition, Pierre Baldi and Søren\r
Brunak\r
Learning Kernel Classifiers: Theory and Algorithms, Ralf Herbrich\r
Learning with Kernels: Support Vector Machines, Regularization, Optimization, and\r
Beyond, Bernhard Sch¨olkopf and Alexander J. Smola\r
Introduction to Machine Learning, Ethem Alpaydin\r
Gaussian Processes for Machine Learning, Carl Edward Rasmussen and Christopher K.I.\r
Williams\r
Semi-Supervised Learning, Olivier Chapelle, Bernhard Sch¨olkopf, and Alexander Zien,\r
Eds.\r
The Minimum Description Length Principle, Peter D. Gr¨unwald\r
Introduction to Statistical Relational Learning, Lise Getoor and Ben Taskar, Eds.

526\r
Probabilistic Graphical Models: Principles and Techniques, Daphne Koller and Nir Fried\u0002man\r
Introduction to Machine Learning, second edition, Ethem Alpaydin\r
Machine Learning in Non-Stationary Environments: Introduction to Covariate Shift\r
Adaptation, Masashi Sugiyama and Motoaki Kawanabe\r
Boosting: Foundations and Algorithms, Robert E. Schapire and Yoav Freund\r
Machine Learning: A Probabilistic Perspective, Kevin P. Murphy\r
Foundations of Machine Learning, Mehryar Mohri, Afshin Rostami, and Ameet Talwalker\r
Introduction to Machine Learning, third edition, Ethem Alpaydin\r
Deep Learning, Ian Goodfellow, Yoshua Bengio, and Aaron Courville\r
Elements of Causal Inference, Jonas Peters, Dominik Janzing, and Bernhard Sch¨olkopf\r
Machine Learning for Data Streams, with Practical Examples in MOA, Albert Bifet,\r
Ricard Gavald`a, Geo↵rey Holmes, Bernhard Pfahringer"""

[[equations]]
latex = """
10 by the national company.\r
If he is out of cars at that location, then the business is lost. Cars become available for\r
renting the day after they are returned. To help ensure that cars are available where\r
they are needed, Jack can move them between the two locations overnight, at a cost of"""
display = false

[[equations]]
latex = """
2, as do all cars\r
moved in the other direction. In addition, Jack has limited parking space at each location.\r
If more than 10 cars are kept overnight at a location (after any moving of cars), then an\r
additional cost of"""
display = false

[[tables]]
page_num = 4
headers = [
    "Description: Second edition.",
    "Cambridge, MA : The MIT Press, [2018]",
    "Series: Adaptive",
]
rows = [
    [
    "computation and machine learning series",
    "Includes bibliographical references and index.",
],
    [
    "Identifiers: LCCN 2018023826",
    "ISBN 9780262039246 (hardcover : alk. paper)",
],
]

[[tables]]
page_num = 54
headers = [
    "= ↵Rn + (1",
    "↵)Qn",
]
rows = [
    [
    "= ↵Rn + (1",
    "↵) [↵Rn1 + (1",
    "↵)Qn1]",
],
    [
    "= ↵Rn + (1",
    "↵)↵Rn1 + (1",
    "↵)",
],
]

[[tables]]
page_num = 54
headers = [
    "··· + (1",
    "↵)",
]
rows = [
    [
    "n1↵R1 + (1",
    "↵)nQ1",
],
    [
    "= (1",
    "↵)",
],
]

[[tables]]
page_num = 74
headers = [
    "it to low with probability 1",
    "↵. On the other hand, a period of searching undertaken",
]
rows = [
    [
    "when the energy level is low leaves it low with probability",
    "and depletes the battery with",
],
    [
    "probability 1",
    ". In the latter case, the robot must be rescued, and the battery is then",
],
]

[[tables]]
page_num = 74
headers = [
    "sa s0 p(s0",
    "s, a) r(s, a, s0)",
]
rows = [
    [
    "high search high ↵ rsearch high search low 1",
    "↵ rsearch low search high 1",
    "3",
],
    [
    "low search low",
    "rsearch high wait high 1 rwait high wait low 0 -",
],
]

[[tables]]
page_num = 75
headers = [
    "Exercise 3.4 Give a table analogous to that in Example 3.3, but for p(s0, r",
    "s, a). It",
]
rows = [
    [
    "should have columns for s, a, s0, r, and p(s0, r",
    "s, a), and a row for every 4-tuple for which",
],
    [
    "p(s0, r",
    "s, a) > 0. ⇤",
],
]

[[tables]]
page_num = 85
headers = [
    "= maxa E⇡⇤[Gt",
    "St =s, At =a]",
]
rows = [
    [
    "= maxa E⇡⇤[Rt+1 + Gt+1",
    "St =s, At =a] (by (3.9))",
],
    [
    "= maxa E[Rt+1 + v⇤(St+1)",
    "St =s, At =a] (3.18)",
],
]

[[tables]]
page_num = 87
headers = [
    "v⇤(h) = max ⇢ p(h",
    "h, s)[r(h, s, h) + v⇤(h)] + p(l",
    "h, s)[r(h, s, l) + v⇤(l)],",
]
rows = [
    [
    "p(h",
    "h, w)[r(h, w, h) + v⇤(h)] + p(l",
    "h, w)[r(h, w, l) + v⇤(l)]",
],
    [
    "= max ⇢ ↵[rs + v⇤(h)] + (1",
    "↵)[rs + v⇤(l)],",
],
]

[[tables]]
page_num = 96
headers = [
    "v⇡(s) .= E⇡[Gt",
    "St =s]",
]
rows = [
    [
    "= E⇡[Rt+1 + Gt+1",
    "St =s] (from (3.9))",
],
    [
    "= E⇡[Rt+1 + v⇡(St+1)",
    "St =s] (4.3)",
],
]

[[tables]]
page_num = 100
headers = [
    "= E[Rt+1 + v⇡(St+1)",
    "St =s, At =⇡0(s)] (by (4.6))",
]
rows = [
    [
    "= E⇡0[Rt+1 + v⇡(St+1)",
    "St =s]",
],
    [
    " E⇡0[Rt+1 + q⇡(St+1, ⇡0(St+1))",
    "St =s] (by (4.7))",
],
    [
    "= E⇡0[Rt+1 + E[Rt+2 + v⇡(St+2)",
    "St+1, At+1 =⇡0(St+1)]",
    "St =s]",
],
]

[[tables]]
page_num = 122
headers = [
    "A(s)",
    ", and the remaining bulk of",
]
rows = [
    [
    "the probability, 1",
    '" + "',
],
    [
    "A(s)",
    ', is given to the greedy action. The "-greedy policies are',
],
    [
    'examples of "-soft policies, defined as policies for which ⇡(a',
    's)  "',
],
    [
    "A(s)",
    "for all states and",
],
]

[[tables]]
page_num = 123
headers = [
    "⇡(a",
    "St)",
]
rows = [
    [
    '⇢ 1  " + "/',
    "A(St)",
    "if a = A⇤",
],
    [
    '"/',
    "A(St)",
    "if a 6= A⇤",
],
]

[[tables]]
page_num = 132
headers = [
    "C(St,At) [G",
    "Q(St, At)]",
]
rows = [
    [
    "W W ⇡(At",
    "St)",
],
    [
    "b(At",
    "St)",
],
]

[[tables]]
page_num = 134
headers = [
    "100 factors, ⇡(A0",
    "S0)",
]
rows = [
    [
    "b(A0",
    "S0)",
],
    [
    "⇡(A1",
    "S1)",
],
]

[[tables]]
page_num = 135
headers = [
    "degree (1",
    "), producing a return of R1 + R2, and so on. The latter degree corresponds",
]
rows = [
    [
    "to terminating on the second step, 1",
    ", and not having already terminated on the",
],
    [
    "first step, . The degree of termination on the third step is thus (1",
    ")2, with the 2",
],
]

[[tables]]
page_num = 135
headers = [
    "= (1",
    ")Rt+1",
]
rows = [
    [
    "+ (1",
    ") (Rt+1 + Rt+2)",
],
    [
    "+ (1",
    ")2 (Rt+1 + Rt+2 + Rt+3)",
],
]

[[tables]]
page_num = 136
headers = [
    "⇢t:T 1Rt+1 = ⇡(At",
    "St)",
]
rows = [
    [
    "b(At",
    "St)",
],
    [
    "⇡(At+1",
    "St+1)",
],
    [
    "b(At+1",
    "St+1)",
],
    [
    "⇡(At+2",
    "St+2)",
],
]

[[tables]]
page_num = 136
headers = [
    "b(a",
    "Sk)",
]
rows = [
    [
    "⇡(a",
    "Sk)",
],
    [
    "b(a",
    "Sk) = X",
],
]

[[tables]]
page_num = 142
headers = [
    "v⇡(s) .= E⇡[Gt",
    "St =s] (6.3)",
]
rows = [
    [
    "= E⇡[Rt+1 + Gt+1",
    "St =s] (from (3.9))",
],
    [
    "= E⇡[Rt+1 + v⇡(St+1)",
    "St =s] . (6.4)",
],
]

[[tables]]
page_num = 169
headers = [
    "Select and store an action At+1 ⇠ ⇡(·",
    "St+1)",
]
rows = [
    [
    "| ⌧ t",
    "n +1 (⌧ is the time whose estimate is being updated)",
],
    [
    "| If ⌧",
    "0:",
],
]

[[tables]]
page_num = 169
headers = [
    "| Q(S⌧ , A⌧ ) Q(S⌧ , A⌧ ) + ↵ [G",
    "Q(S⌧ , A⌧ )]",
]
rows = [
    [
    "If ⇡ is being learned, then ensure that ⇡(·",
    'S⌧ ) is "-greedy wrt Q',
],
    [
    "Until ⌧ = T",
    "1",
],
]

[[tables]]
page_num = 171
headers = [
    "Select and store an action At+1 ⇠ b(·",
    "St+1)",
]
rows = [
    [
    "| ⌧ t",
    "n +1 (⌧ is the time whose estimate is being updated)",
],
    [
    "| If ⌧",
    "0:",
],
]

[[tables]]
page_num = 171
headers = [
    "| Q(S⌧ , A⌧ ) Q(S⌧ , A⌧ ) + ↵⇢ [G",
    "Q(S⌧ , A⌧ )]",
]
rows = [
    [
    "If ⇡ is being learned, then ensure that ⇡(·",
    "S⌧ ) is greedy wrt Q",
],
    [
    "Until ⌧ = T",
    "1",
],
]

[[tables]]
page_num = 176
headers = [
    "| ⌧ t + 1",
    "n (⌧ is the time whose estimate is being updated)",
]
rows = [
    [
    "| If ⌧",
    "0:",
],
    [
    "| If t + 1",
    "T:",
],
]

[[tables]]
page_num = 176
headers = [
    "a6=Ak ⇡(a",
    "Sk)Q(Sk, a) + ⇡(Ak",
    "Sk)G",
]
rows = [
    [
    "| Q(S⌧ , A⌧ ) Q(S⌧ , A⌧ ) + ↵ [G",
    "Q(S⌧ , A⌧ )]",
],
    [
    "If ⇡ is being learned, then ensure that ⇡(·",
    "S⌧ ) is greedy wrt Q",
],
    [
    "Until ⌧ = T",
    "1",
],
]

[[tables]]
page_num = 177
headers = [
    "⇡(a",
    "St+1)Qh1(St+1, a) +  ⇡(At+1",
    "St+1)Gt+1:h",
]
rows = [
    [
    "= Rt+1 + V¯h1(St+1)  ⇡(At+1",
    "St+1)Qh1(St+1, At+1) + ⇡(At+1",
    "St+1)Gt+1:h",
],
    [
    "= Rt+1 + ⇡(At+1",
    "St+1)",
],
]

[[tables]]
page_num = 178
headers = [
    "Store ⇡(At+1",
    "St+1)",
]
rows = [
    [
    "b(At+1",
    "St+1) as ⇢t+1",
],
    [
    "| ⌧ t",
    "n +1 (⌧ is the time whose estimate is being updated)",
],
    [
    "| If ⌧",
    "0:",
],
]

[[tables]]
page_num = 178
headers = [
    "| Q(S⌧ , A⌧ ) Q(S⌧ , A⌧ ) + ↵ [G",
    "Q(S⌧ , A⌧ )]",
]
rows = [
    [
    "If ⇡ is being learned, then ensure that ⇡(·",
    "S⌧ ) is greedy wrt Q",
],
    [
    "Until ⌧ = T",
    "1",
],
]

[[tables]]
page_num = 229
headers = [
    "1>D(I",
    "P) = µ>(I",
    "P)",
]
rows = [
    [
    "= µ>",
    "µ>P",
],
    [
    "= µ>",
    "µ> (because µ is the stationary distribution)",
],
    [
    "= (1",
    ")µ>,",
],
]

[[tables]]
page_num = 251
headers = [
    "t1",
    "Ab 1",
]
rows = [
    [
    "t1xt1(xt1",
    "xt)>Ab 1t1",
],
    [
    "1+(xt1",
    "xt)>Ab 1",
],
]

[[tables]]
page_num = 269
headers = [
    "Select and store At+1 ⇠ ⇡(·",
    'St+1) or "-greedy wrt ˆq(St+1, ·, w)',
]
rows = [
    [
    "| ⌧ t",
    "n +1 (⌧ is the time whose estimate is being updated)",
],
    [
    "| If ⌧",
    "0:",
],
]

[[tables]]
page_num = 275
headers = [
    "The tth reward will appear undiscounted in the t",
    "1st return, discounted once in the",
]
rows = [
    [
    "t",
    "2nd return, and discounted 999 times in the t",
    "1000th return. The weight on the",
],
    [
    "tth reward is thus 1 +",
    "+ 2 + 3 + ··· = 1/(1",
    "). Because all states are the same,",
],
]

[[tables]]
page_num = 277
headers = [
    "Select and store an action At+1 ⇠ ⇡(·",
    'St+1), or "-greedy wrt ˆq(St+1, ·, w)',
]
rows = [
    [
    "⌧ t",
    "n +1 (⌧ is the time whose estimate is being updated)",
],
    [
    "If ⌧",
    "0:",
],
]

[[tables]]
page_num = 283
headers = [
    "b(dashed",
    "·)=6/7",
]
rows = [
    [
    "b(solid",
    "·)=1/7",
],
    [
    "⇡(solid",
    "·)=1",
],
]

[[tables]]
page_num = 312
headers = [
    "return. Thus, for",
    "= 1, updating according to the -return is a Monte Carlo algorithm.",
]
rows = [
    [
    "On the other hand, if",
    "= 0, then the -return reduces to Gt:t+1, the one-step return.",
],
    [
    "Thus, for",
    "= 0, updating according to the -return is a one-step TD method.",
],
]

[[tables]]
page_num = 349
headers = [
    "⇡(a",
    "St, ✓)q⇡(St, a)",
]
rows = [
    [
    "r⇡(a",
    "St, ✓)",
],
    [
    "⇡(a",
    "St, ✓)",
],
]

[[tables]]
page_num = 351
headers = [
    "Gt",
    "b(St)",
]
rows = [
    [
    "⌘r⇡(At",
    "St, ✓t)",
],
    [
    "⇡(At",
    "St, ✓t) . (13.11)",
],
]

[[tables]]
page_num = 353
headers = [
    "Gt:t+1",
    "vˆ(St,w)",
]
rows = [
    [
    "⌘r⇡(At",
    "St, ✓t)",
],
    [
    "⇡(At",
    "St, ✓t) (13.12)",
],
]

[[tables]]
page_num = 353
headers = [
    "Rt+1 + vˆ(St+1,w)",
    "vˆ(St,w)",
]
rows = [
    [
    "⌘r⇡(At",
    "St, ✓t)",
],
    [
    "⇡(At",
    "St, ✓t) (13.13)",
],
]

[[tables]]
page_num = 358
headers = [
    "(c) Express the eligibility r ln ⇡(a",
    "s, ✓) for a Bernoulli-logistic unit, in terms of a, x(s),",
]
rows = [
    [
    "and ⇡(a",
    "s, ✓) by calculating the gradient.",
],
    [
    "Hint for part (c): Define P = ⇡(1",
    "s, ✓) and compute the derivative of the logarithm, for",
],
]

[[tables]]
page_num = 406
headers = [
    "algorithms we discuss in this book,",
    "functions as a reinforcement signal, meaning that it",
]
rows = [
    [
    "is the main driver of learning. For example,",
    "is the critical factor in the TD model of",
],
    [
    "classical conditioning, and",
    "is the reinforcement signal for learning both a value function",
],
]

[[tables]]
page_num = 413
headers = [
    "to R?. Learning complete: the value function accurately predicts future reward,",
    "is positive at",
]
rows = [
    [
    "the earliest predictive state, and",
    "= 0 at the time of the non-zero reward. R? omitted: at the",
],
    [
    "time the predicted reward is omitted,",
    "becomes negative. See text for a complete explanation",
],
]

[[tables]]
page_num = 484
headers = [
    "kPr{Sk =s0, ⌧ =k",
    "S0 =s, A0:k1 ⇠⇡!, ⌧ ⇠!}. (17.3)",
]
rows = [
    [
    "Note that, because of the factor of k, this p(s0",
    "s, !) is no longer a transition probability",
],
    [
    "and no longer sums to one over all values of s0. (Nevertheless, we continue to use the ‘",
    "’",
],
]

[[figures]]
label = "fig:1"
page_num = 1
image_path = "images/image_p1_1.png"

[[figures]]
label = "fig:2"
page_num = 19
image_path = "images/image_p19_2.png"

[[figures]]
label = "fig:3"
page_num = 19
image_path = "images/image_p19_3.png"

[[figures]]
label = "fig:4"
page_num = 49
image_path = "images/image_p49_4.png"

[[figures]]
label = "fig:5"
page_num = 49
image_path = "images/image_p49_5.png"

[[figures]]
label = "fig:6"
page_num = 49
image_path = "images/image_p49_6.png"

[[figures]]
label = "fig:7"
page_num = 58
image_path = "images/image_p58_7.png"

[[figures]]
label = "fig:8"
page_num = 60
image_path = "images/image_p60_8.png"

[[figures]]
label = "fig:9"
page_num = 60
image_path = "images/image_p60_9.png"

[[figures]]
label = "fig:10"
page_num = 61
image_path = "images/image_p61_10.png"

[[figures]]
label = "fig:11"
page_num = 61
image_path = "images/image_p61_11.png"

[[figures]]
label = "fig:12"
page_num = 61
image_path = "images/image_p61_12.png"

[[figures]]
label = "fig:13"
page_num = 61
image_path = "images/image_p61_13.png"

[[figures]]
label = "fig:14"
page_num = 61
image_path = "images/image_p61_14.png"

[[figures]]
label = "fig:15"
page_num = 62
image_path = "images/image_p62_15.png"

[[figures]]
label = "fig:16"
page_num = 62
image_path = "images/image_p62_16.png"

[[figures]]
label = "fig:17"
page_num = 62
image_path = "images/image_p62_17.png"

[[figures]]
label = "fig:18"
page_num = 62
image_path = "images/image_p62_18.png"

[[figures]]
label = "fig:19"
page_num = 62
image_path = "images/image_p62_19.png"

[[figures]]
label = "fig:20"
page_num = 117
image_path = "images/image_p117_20.png"

[[figures]]
label = "fig:21"
page_num = 117
image_path = "images/image_p117_21.png"

[[figures]]
label = "fig:22"
page_num = 128
image_path = "images/image_p128_22.png"

[[figures]]
label = "fig:23"
page_num = 129
image_path = "images/image_p129_23.png"

[[figures]]
label = "fig:24"
page_num = 157
image_path = "images/image_p157_24.png"

[[figures]]
label = "fig:25"
page_num = 157
image_path = "images/image_p157_25.png"

[[figures]]
label = "fig:26"
page_num = 157
image_path = "images/image_p157_26.png"

[[figures]]
label = "fig:27"
page_num = 167
image_path = "images/image_p167_27.png"

[[figures]]
label = "fig:28"
page_num = 212
image_path = "images/image_p212_28.png"

[[figures]]
label = "fig:29"
page_num = 212
image_path = "images/image_p212_29.png"

[[figures]]
label = "fig:30"
page_num = 212
image_path = "images/image_p212_30.png"

[[figures]]
label = "fig:31"
page_num = 212
image_path = "images/image_p212_31.png"

[[figures]]
label = "fig:32"
page_num = 212
image_path = "images/image_p212_32.png"

[[figures]]
label = "fig:33"
page_num = 226
image_path = "images/image_p226_33.png"

[[figures]]
label = "fig:34"
page_num = 230
image_path = "images/image_p230_34.png"

[[figures]]
label = "fig:35"
page_num = 230
image_path = "images/image_p230_35.png"

[[figures]]
label = "fig:36"
page_num = 235
image_path = "images/image_p235_36.png"

[[figures]]
label = "fig:37"
page_num = 235
image_path = "images/image_p235_37.png"

[[figures]]
label = "fig:38"
page_num = 235
image_path = "images/image_p235_38.png"

[[figures]]
label = "fig:39"
page_num = 235
image_path = "images/image_p235_39.png"

[[figures]]
label = "fig:40"
page_num = 235
image_path = "images/image_p235_40.png"

[[figures]]
label = "fig:41"
page_num = 235
image_path = "images/image_p235_41.png"

[[figures]]
label = "fig:42"
page_num = 235
image_path = "images/image_p235_42.png"

[[figures]]
label = "fig:43"
page_num = 236
image_path = "images/image_p236_43.png"

[[figures]]
label = "fig:44"
page_num = 240
image_path = "images/image_p240_44.png"

[[figures]]
label = "fig:45"
page_num = 249
image_path = "images/image_p249_45.png"

[[figures]]
label = "fig:46"
page_num = 267
image_path = "images/image_p267_46.png"

[[figures]]
label = "fig:47"
page_num = 268
image_path = "images/image_p268_47.png"

[[figures]]
label = "fig:48"
page_num = 270
image_path = "images/image_p270_48.png"

[[figures]]
label = "fig:49"
page_num = 270
image_path = "images/image_p270_49.png"

[[figures]]
label = "fig:50"
page_num = 274
image_path = "images/image_p274_50.png"

[[figures]]
label = "fig:51"
page_num = 284
image_path = "images/image_p284_51.png"

[[figures]]
label = "fig:52"
page_num = 284
image_path = "images/image_p284_52.png"

[[figures]]
label = "fig:53"
page_num = 302
image_path = "images/image_p302_53.png"

[[figures]]
label = "fig:54"
page_num = 302
image_path = "images/image_p302_54.png"

[[figures]]
label = "fig:55"
page_num = 305
image_path = "images/image_p305_55.png"

[[figures]]
label = "fig:56"
page_num = 313
image_path = "images/image_p313_56.png"

[[figures]]
label = "fig:57"
page_num = 313
image_path = "images/image_p313_57.png"

[[figures]]
label = "fig:58"
page_num = 317
image_path = "images/image_p317_58.png"

[[figures]]
label = "fig:59"
page_num = 317
image_path = "images/image_p317_59.png"

[[figures]]
label = "fig:60"
page_num = 321
image_path = "images/image_p321_60.png"

[[figures]]
label = "fig:61"
page_num = 321
image_path = "images/image_p321_61.png"

[[figures]]
label = "fig:62"
page_num = 328
image_path = "images/image_p328_62.png"

[[figures]]
label = "fig:63"
page_num = 328
image_path = "images/image_p328_63.png"

[[figures]]
label = "fig:64"
page_num = 328
image_path = "images/image_p328_64.png"

[[figures]]
label = "fig:65"
page_num = 345
image_path = "images/image_p345_65.png"

[[figures]]
label = "fig:66"
page_num = 345
image_path = "images/image_p345_66.png"

[[figures]]
label = "fig:67"
page_num = 345
image_path = "images/image_p345_67.png"

[[figures]]
label = "fig:68"
page_num = 345
image_path = "images/image_p345_68.png"

[[figures]]
label = "fig:69"
page_num = 345
image_path = "images/image_p345_69.png"

[[figures]]
label = "fig:70"
page_num = 350
image_path = "images/image_p350_70.png"

[[figures]]
label = "fig:71"
page_num = 352
image_path = "images/image_p352_71.png"

[[figures]]
label = "fig:72"
page_num = 375
image_path = "images/image_p375_72.png"

[[figures]]
label = "fig:73"
page_num = 375
image_path = "images/image_p375_73.png"

[[figures]]
label = "fig:74"
page_num = 376
image_path = "images/image_p376_74.png"

[[figures]]
label = "fig:75"
page_num = 377
image_path = "images/image_p377_75.png"

[[figures]]
label = "fig:76"
page_num = 377
image_path = "images/image_p377_76.png"

[[figures]]
label = "fig:77"
page_num = 378
image_path = "images/image_p378_77.png"

[[figures]]
label = "fig:78"
page_num = 378
image_path = "images/image_p378_78.png"

[[figures]]
label = "fig:79"
page_num = 378
image_path = "images/image_p378_79.png"

[[figures]]
label = "fig:80"
page_num = 380
image_path = "images/image_p380_80.png"

[[figures]]
label = "fig:81"
page_num = 387
image_path = "images/image_p387_81.png"

[[figures]]
label = "fig:82"
page_num = 387
image_path = "images/image_p387_82.png"

[[figures]]
label = "fig:83"
page_num = 387
image_path = "images/image_p387_83.png"

[[figures]]
label = "fig:84"
page_num = 387
image_path = "images/image_p387_84.png"

[[figures]]
label = "fig:85"
page_num = 387
image_path = "images/image_p387_85.png"

[[figures]]
label = "fig:86"
page_num = 387
image_path = "images/image_p387_86.png"

[[figures]]
label = "fig:87"
page_num = 387
image_path = "images/image_p387_87.png"

[[figures]]
label = "fig:88"
page_num = 387
image_path = "images/image_p387_88.png"

[[figures]]
label = "fig:89"
page_num = 387
image_path = "images/image_p387_89.png"

[[figures]]
label = "fig:90"
page_num = 387
image_path = "images/image_p387_90.png"

[[figures]]
label = "fig:91"
page_num = 387
image_path = "images/image_p387_91.png"

[[figures]]
label = "fig:92"
page_num = 387
image_path = "images/image_p387_92.png"

[[figures]]
label = "fig:93"
page_num = 387
image_path = "images/image_p387_93.png"

[[figures]]
label = "fig:94"
page_num = 387
image_path = "images/image_p387_94.png"

[[figures]]
label = "fig:95"
page_num = 387
image_path = "images/image_p387_95.png"

[[figures]]
label = "fig:96"
page_num = 407
image_path = "images/image_p407_96.png"

[[figures]]
label = "fig:97"
page_num = 408
image_path = "images/image_p408_97.png"

[[figures]]
label = "fig:98"
page_num = 410
image_path = "images/image_p410_98.png"

[[figures]]
label = "fig:99"
page_num = 411
image_path = "images/image_p411_99.png"

[[figures]]
label = "fig:100"
page_num = 413
image_path = "images/image_p413_100.png"

[[figures]]
label = "fig:101"
page_num = 418
image_path = "images/image_p418_101.png"

[[figures]]
label = "fig:102"
page_num = 418
image_path = "images/image_p418_102.png"

[[figures]]
label = "fig:103"
page_num = 418
image_path = "images/image_p418_103.png"

[[figures]]
label = "fig:104"
page_num = 418
image_path = "images/image_p418_104.png"

[[figures]]
label = "fig:105"
page_num = 418
image_path = "images/image_p418_105.png"

[[figures]]
label = "fig:106"
page_num = 418
image_path = "images/image_p418_106.png"

[[figures]]
label = "fig:107"
page_num = 418
image_path = "images/image_p418_107.png"

[[figures]]
label = "fig:108"
page_num = 418
image_path = "images/image_p418_108.png"

[[figures]]
label = "fig:109"
page_num = 418
image_path = "images/image_p418_109.png"

[[figures]]
label = "fig:110"
page_num = 418
image_path = "images/image_p418_110.png"

[[figures]]
label = "fig:111"
page_num = 455
image_path = "images/image_p455_111.png"

[[figures]]
label = "fig:112"
page_num = 457
image_path = "images/image_p457_112.png"

[[figures]]
label = "fig:113"
page_num = 467
image_path = "images/image_p467_113.png"

[[figures]]
label = "fig:114"
page_num = 467
image_path = "images/image_p467_114.png"

[[figures]]
label = "fig:115"
page_num = 467
image_path = "images/image_p467_115.png"

[[figures]]
label = "fig:116"
page_num = 467
image_path = "images/image_p467_116.png"

[[figures]]
label = "fig:117"
page_num = 467
image_path = "images/image_p467_117.png"

[[figures]]
label = "fig:118"
page_num = 467
image_path = "images/image_p467_118.png"

[[figures]]
label = "fig:119"
page_num = 467
image_path = "images/image_p467_119.png"

[[figures]]
label = "fig:120"
page_num = 467
image_path = "images/image_p467_120.png"

[[figures]]
label = "fig:121"
page_num = 467
image_path = "images/image_p467_121.png"

[[figures]]
label = "fig:122"
page_num = 467
image_path = "images/image_p467_122.png"

[[figures]]
label = "fig:123"
page_num = 467
image_path = "images/image_p467_123.png"

[[figures]]
label = "fig:124"
page_num = 467
image_path = "images/image_p467_124.png"

[[figures]]
label = "fig:125"
page_num = 467
image_path = "images/image_p467_125.png"

[[figures]]
label = "fig:126"
page_num = 467
image_path = "images/image_p467_126.png"

[[figures]]
label = "fig:127"
page_num = 467
image_path = "images/image_p467_127.png"

[[figures]]
label = "fig:128"
page_num = 467
image_path = "images/image_p467_128.png"

[[figures]]
label = "fig:129"
page_num = 467
image_path = "images/image_p467_129.png"

[[figures]]
label = "fig:130"
page_num = 467
image_path = "images/image_p467_130.png"

[[figures]]
label = "fig:131"
page_num = 467
image_path = "images/image_p467_131.png"

[[figures]]
label = "fig:132"
page_num = 467
image_path = "images/image_p467_132.png"

[[figures]]
label = "fig:133"
page_num = 467
image_path = "images/image_p467_133.png"

[[figures]]
label = "fig:134"
page_num = 467
image_path = "images/image_p467_134.png"

[[figures]]
label = "fig:135"
page_num = 467
image_path = "images/image_p467_135.png"

[[figures]]
label = "fig:136"
page_num = 467
image_path = "images/image_p467_136.png"

[[figures]]
label = "fig:137"
page_num = 467
image_path = "images/image_p467_137.png"

[[figures]]
label = "fig:138"
page_num = 467
image_path = "images/image_p467_138.png"

[[figures]]
label = "fig:139"
page_num = 467
image_path = "images/image_p467_139.png"

[[figures]]
label = "fig:140"
page_num = 467
image_path = "images/image_p467_140.png"

[[figures]]
label = "fig:141"
page_num = 467
image_path = "images/image_p467_141.png"

[[figures]]
label = "fig:142"
page_num = 467
image_path = "images/image_p467_142.png"

[[figures]]
label = "fig:143"
page_num = 467
image_path = "images/image_p467_143.png"

[[figures]]
label = "fig:144"
page_num = 467
image_path = "images/image_p467_144.png"

[[figures]]
label = "fig:145"
page_num = 470
image_path = "images/image_p470_145.png"

[[figures]]
label = "fig:146"
page_num = 475
image_path = "images/image_p475_146.png"

[[figures]]
label = "fig:147"
page_num = 476
image_path = "images/image_p476_147.png"

[[figures]]
label = "fig:148"
page_num = 476
image_path = "images/image_p476_148.png"

[[figures]]
label = "fig:149"
page_num = 476
image_path = "images/image_p476_149.png"

[[figures]]
label = "fig:150"
page_num = 476
image_path = "images/image_p476_150.png"

[[figures]]
label = "fig:151"
page_num = 478
image_path = "images/image_p478_151.png"

[[figures]]
label = "fig:152"
page_num = 485
image_path = "images/image_p485_152.png"

[[figures]]
label = "fig:153"
page_num = 488
image_path = "images/image_p488_153.png"

[[figures]]
label = "fig:154"
page_num = 488
image_path = "images/image_p488_154.png"

[[figures]]
label = "fig:155"
page_num = 488
image_path = "images/image_p488_155.png"

[[figures]]
label = "fig:156"
page_num = 488
image_path = "images/image_p488_156.png"

[[figures]]
label = "fig:157"
page_num = 488
image_path = "images/image_p488_157.png"
