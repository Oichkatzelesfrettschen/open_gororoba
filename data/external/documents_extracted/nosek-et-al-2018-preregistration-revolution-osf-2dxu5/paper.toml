equations = []
tables = []
figures = []
full_text = """
1\r
The Preregistration Revolution\r
Brian A. Noseka,b ORCID: 0000-0001-6797-5476\r
Charles R. Ebersoleb ORCID: 0000-0002-8607-2579\r
Alexander C. DeHavena ORCID: 0000-0002-2241-7259\r
David T. Mellora ORCID: 0000-0002-3125-5888\r
a Center for Open Science, 210 Ridge McIntire Road, Suite 500, Charlottesville, VA 22903\r
b University of Virginia, Department of Psychology, 102 Gilmer Hall, Charlottesville, VA 22904\r
Corresponding Author: Brian Nosek, 210 Ridge McIntire Road, Suite 500, Charlottesville, VA \r
22903, nosek@virginia.edu\r
Keywords: prediction, postdiction, methodology, rigor, reproducibility, preregistration, \r
transparency, science policy, philosophy of science \r
Abstract\r
Progress in science relies in part on generating hypotheses with existing observations and testing \r
hypotheses with new observations. This distinction between postdiction and prediction is \r
appreciated conceptually, but is not respected in practice. Mistaking generation of postdictions \r
with testing of predictions reduces the credibility of research findings. However, ordinary biases \r
in human reasoning such as hindsight bias make it hard to avoid this mistake. An effective \r
solution is to define the research questions and analysis plan prior to observing the research \r
outcomes–a process called preregistration. Preregistration distinguishes analyses and outcomes \r
that result from predictions from those that result from postdictions. A variety of practical \r
strategies are available to make the best possible use of preregistration in circumstances that fall \r
short of the ideal application, such as when the data are pre-existing. Services are now available \r
for preregistration across all disciplines facilitating a rapid increase in the practice. Widespread \r
adoption of preregistration will increase distinctiveness between hypothesis generation and \r
hypothesis testing and will improve the credibility of research findings. \r
Classification: Social Science, Psychological and Cognitive Sciences

2\r
The Preregistration Revolution\r
Progress in science is marked by reducing uncertainty about nature. Scientists generate \r
models that may explain prior observations and predict future observations. Those models are \r
approximations and simplifications of reality. Models are iteratively improved and replaced by \r
reducing the amount of prediction error. As prediction error decreases, certainty about what will \r
occur in the future increases. This view of research progress is captured by George Box’s \r
aphorism “All models are wrong but some are useful” (1,2).\r
Scientists improve models by generating hypotheses based on existing observations and \r
testing those hypotheses by obtaining new observations. These distinct modes of research are \r
discussed by philosophers and methodologists as hypothesis-generating versus hypothesis\u0002testing, the context of discovery versus the context of justification, data-independent versus data\u0002contingent analysis, and exploratory versus confirmatory research (e.g., 3,4,5,6). We use the \r
more general terms––postdiction and prediction––to capture this important distinction. \r
A common thread among epistemologies of science is that postdiction is characterized by \r
the use of data to generate hypotheses about why something occurred, and prediction is \r
characterized by the acquisition of data to test ideas about what will occur. In prediction, data are \r
used to confront the possibility that the prediction is wrong. In postdiction, the data are already \r
known and the postdiction is generated to explain why they occurred. \r
Testing predictions is vital for establishing diagnostic evidence for explanatory claims. \r
Testing predictions assesses the uncertainty of scientific models by observing how well the \r
predictions account for new data. Generating postdictions is vital for discovery of possibilities \r
not yet considered. In many cases, researchers have very little basis to generate predictions or \r
evidence can reveal that initial expectations were wrong. Progress in science often proceeds via \r
unexpected discovery––a study reveals an inexplicable pattern of results that sends the \r
investigation on a new trajectory. \r
Why does the distinction between prediction and postdiction matter? Failing to appreciate \r
the difference can lead to overconfidence in post hoc explanations (postdictions) and inflate the \r
likelihood of believing that there is evidence for a finding when there is not. Presenting \r
postdictions as predictions can increase the attractiveness and publishability of findings by \r
falsely reducing uncertainty. Ultimately, this decreases reproducibility (6,7,8,9,10,11). \r
Mental constraints on distinguishing predictions and postdictions \r
It is common for researchers to alternate between postdiction and prediction. Ideas are \r
generated and observed data modifies those ideas. Over time and iteration, researchers develop \r
understanding of the phenomenon under study. That understanding might result in a model, \r
hypothesis, or theory. The dynamism of the research enterprise and limits of human reasoning \r
make it easy to mistake postdiction as prediction. The problem with this is understood as post \r
hoc theorizing or hypothesizing after the results are known (12). It is an example of circular \r
reasoning––generating a hypothesis based on observing data, and then evaluating the validity of \r
the hypothesis based on the same data. 

3\r
Hindsight bias, also known as the I-knew-it-all-along effect, is the tendency to see \r
outcomes as more predictable after the fact compared to before they were observed (13,14). With \r
hindsight bias, the observer uses the data to generate an explanation, a postdiction, and \r
simultaneously perceives that they would have anticipated that explanation in advance, a \r
prediction. A common case is when the researcher’s prediction is vague so that many possible \r
outcomes can be rationalized after the fact as supporting the prediction. For example, a \r
biomedical researcher might predict that a treatment will improve health, and postdictively \r
identify the one of five health outcomes that showed a positive benefit as the one most relevant \r
for testing the prediction. A political scientist might arrive at a model using a collection of \r
covariates and exclusion criteria that can be rationalized after the fact, but would not have been \r
anticipated as relevant beforehand. A chemist may have random variation occurring across a \r
number of results and nevertheless be able to construct a narrative post facto that imbues \r
meaning in the randomness. To an audience of historians (15), Amos Tversky provided a cogent \r
explanation of the power of hindsight for considering evidence:\r
“All too often, we find ourselves unable to predict what will happen; yet after the \r
fact we explain what did happen with a great deal of confidence. This ‘ability’ to \r
explain that which we cannot predict, even in the absence of any additional \r
information, represents an important, though subtle, flaw in our reasoning. It leads \r
us to believe that there is a less uncertain world than there actually is...”\r
Mistaking postdiction as prediction underestimates the uncertainty of outcomes and can produce \r
psychological overconfidence in the resulting findings. \r
The values of impartiality and objectivity are pervasive (16), particularly for scientists, \r
but human reasoning is not reliably impartial or objective (17,18). Scientists are motivated to \r
advance knowledge; scientists are also motivated to obtain job security, awards, publications, \r
and grants. In the present research culture, these rewards are more likely to be secured by \r
obtaining certain kinds of research outcomes over others. Novel results are rewarded more than \r
redundant or incremental additions to existing knowledge. Positive results––finding a \r
relationship between variables or an effect of treatments on outcomes––are rewarded more than \r
negative results––failing to find a relationship or effect; clean results that provide a strong \r
narrative are rewarded more than outcomes that show uncertainty or exceptions to the favored \r
narrative (19,20,21,9). Novel, positive, clean results are better results both for reward and for \r
launching science into new domains of inquiry. However, achieving novel, positive, clean results \r
is a rare event. Progress in research is halting, messy, and uncertain. The incentives for such \r
results combined with their infrequency creates a potential conflict of interest for the researcher. \r
If certain kinds of results are more rewarded than others, then researchers are motivated to obtain \r
results that are more likely to be rewarded regardless of the accuracy of those results. \r
Lack of clarity between postdiction and prediction provides the opportunity to select, \r
rationalize, and report tests that maximize reward over accuracy. Moreover, good intentions are \r
not sufficient to overcome the fallibility of memory, motivated reasoning, and cognitive biases \r
that can occur outside of conscious awareness or control (22,23,24,25,26). Researchers may 

4\r
design a study to investigate one question and, upon observing the outcomes, misremember the \r
original purposes as more aligned with what was observed. Researchers may genuinely believe \r
that they would have predicted, or even that they did predict, the outcomes as observed (22). \r
Researchers may employ confirmation bias by seeking evidence consistent with their \r
expectations and finding fault or ignoring evidence that is inconsistent with their expectations \r
(24). These reasoning challenges are exacerbated by the misuse of common tools of statistical \r
inference to provide false comfort about the reliability of evidence. \r
Standard tools of statistical inference assume prediction\r
Null Hypothesis Significance Testing (NHST) is designed for prediction––testing \r
hypotheses––not for postdiction––generating hypotheses (6,27). The pervasiveness in many \r
disciplines of NHST and its primary statistic, the p-value, implies either that most research is \r
prediction or that postdiction is frequently mistaken as prediction with errant application of \r
NHST.*\r
In NHST, one usually compares a null hypothesis of no relationships among the variables \r
and an alternate hypothesis in which the variables are related. Data are then observed that lead to \r
rejection or not of the null hypothesis. Rejection of the null hypothesis at p < .05 is a claim \r
about the likelihood that data as extreme or more extreme than the observed data would have \r
occurred if the null hypothesis were true. It is underappreciated that the presence of “hypothesis \r
testing” in the name NHST is consequential for constraining its appropriate use to testing \r
predictions. The diagnosticity of a p-value is partly contingent on knowing how many tests were \r
performed (27). Deciding that a given p < .05 result is unlikely✝, and therefore evidence against \r
the null hypothesis, is very different if it was the only test conducted versus one of 20, 200, or \r
2000 tests. \r
If there were only one inference test to perform and only one way to conduct that test, \r
then the p-value is diagnostic about its intended likelihood. It is not hyperbole to say that this \r
almost never occurs. Even in the simplest studies, there is more than one way to perform the \r
statistical inference test. For example, researchers must decide whether any observations should \r
be excluded from the analysis, whether any measures should be transformed or combined, and \r
whether any other variables should be included in the model as covariates. \r
Correcting diagnosticity of p-values for the number of tests that were actually conducted \r
is relatively straightforward (33,34), though inconsistently––even rarely––applied in practice \r
(35,36). However, counting the literal performance of statistical tests is not sufficient to account \r
for how observing the data can influence the selection of tests to conduct. Gelman and Loken \r
(37) refer to the problem as the garden of forking paths. There are a vast number of choices for \r
analyzing data that could be made. If those choices are made during analysis, observing the data \r
may make selecting some paths more likely and others less likely. By the end, it may be \r
impossible to estimate the paths that could have been selected if the data had looked different or \r
if analytic decisions were influenced by hindsight, confirmation, and outcome biases. This leaves 

5\r
the observed p-values with unknown diagnosticity rendering them uninterpretable. In other \r
words, NHST cannot be used with confidence for postdiction.‡\r
In prediction, the problem of forking paths is avoided because the analytic pipeline is \r
specified prior to observing the data. As such, with correction for the number of tests conducted, \r
p-values retain their diagnosticity. In postdiction, analytic decisions are influenced by the \r
observed data creating the forking paths. The researcher is exploring the data to discover what is \r
possible. The data helps generate, not test, new questions and hypotheses. \r
The problem of failing to distinguish between postdiction and prediction is vastly \r
underestimated in practice. Researchers may conduct lots of studies and test many possible \r
relationships. Even if there are no relationships to find, some of those tests will elicit apparent \r
evidence––positive results––by chance (27). If researchers selectively report positive results \r
more frequently than negative results, then the likelihood of false positives will increase \r
(38,39,40). Moreover, researchers have substantial degrees-of-freedom to conduct many \r
different tests, and selection of those that yield positive results over those that yield negative \r
results will increase likelihood of attractive results at the expense of accuracy (41,42,30). \r
If researchers are clear about when they are in prediction and postdiction modes of \r
research, then the benefits (and limits) of statistical inference will be preserved. But, with means, \r
motive, and opportunity to misperceive postdiction as prediction and to selectively rationalize \r
and report a biased subset of outcomes, researchers are prone to false confidence in evidence. \r
Preregistration is a solution that helps researchers maintain clarity between prediction and \r
postdiction, and preserve accurate calibration of evidence.\r
Preregistration distinguishes prediction and postdiction\r
Preregistration of an analysis plan is committing to analytic steps without advance \r
knowledge of the research outcomes. That commitment is usually accomplished by posting the \r
analysis plan to an independent registry such as http://clinicaltrials.gov/ or http://osf.io/. The \r
registry preserves the preregistration and makes it discoverable, sometimes after an embargo \r
period. With preregistration, prediction is achieved because selection of tests is not influenced by \r
the observed data, and all conducted tests are knowable. The analysis plan provides constraint to \r
specify how the data will be used to confront the research questions. \r
In principle, inferences from preregistered analyses will be more reproducible than NHST \r
analyses that were not preregistered because the relation between the analysis choices and \r
findings cannot be influenced by motivation, memory, or reasoning biases. We say “in principle” \r
because the case for preregistration is theoretically strong as a matter of inductive inference and \r
empirically bolstered by some correlational evidence. However, there is not yet sufficient \r
experimental evidence establishing its superiority for reproducibility. Correlational evidence \r
suggests that hypothesizing in advance relates to increased replicability (11). Further, \r
preregistration is correlated with outcomes that suggest reduced publication or reporting biases. \r
For example, Kaplan and Irvin (43) observed a dramatic drop in the rate of positive results \r
following the requirement to preregister primary outcomes in a sample of clinical trials. The 

6\r
benefits of preregistration are lost if researchers do not follow the preregistrations (44,45). \r
However, there is evidence that preregistration makes it possible to detect and possibly correct \r
selection and reporting biases (e.g., www.COMPare-trials.org). Franco, Malhotra, and \r
Simonovits (38) observed that 40% of published papers in their sample of preregistered studies \r
failed to report one or more of the experimental manipulations (treatment conditions), and 70% \r
of published papers failed to report one or more of the outcome variables. Moreover, there was \r
substantial selection bias in outcomes that were reported in the paper (96% of consistently \r
significant findings included in published articles) versus those that were left out (65% of null \r
effects not included in published articles). \r
Formally speaking, analyses conducted on the data that are not part of the preregistration \r
inform postdiction. In principle, preregistration can establish a bright line between prediction and \r
postdiction. This preserves the diagnosticity of NHST inference for predictions, and clarifies the \r
role of postdiction for generating possible explanations to test as predictions in the future. In \r
practice, there are challenges for implementing preregistration and maintaining a clear distinction \r
between prediction and postdiction. Nevertheless, there are opportunities to benefit from \r
preregistration even when the idealistic bright line cannot be achieved.\r
Preregistration in practice\r
Preregistration does not favor prediction over postdiction; its purpose is to make clear \r
which is which. There are practical challenges for effective integration of preregistration in many \r
areas of research. We first describe the ideal of preregistration and then address some of the \r
practical challenges. \r
The ideal. The idealized scenario for preregistration follows the simplified model of \r
research taught in elementary school. A scientist makes observations in the world and generates \r
a research question or hypothesis from those observations. A study design and analysis plan are \r
created to evaluate that question. Then data are collected according to the design and analyzed \r
according to the analysis plan. This confronts the hypothesis by testing whether it predicts the \r
outcomes of the experiment. Following that, the researcher might explore the data for potential \r
discoveries that generate hypotheses or potential explanations after the fact. The most interesting \r
postdictions are then converted into predictions for designing the next study and the cycle \r
repeats. In this idealized model, preregistration adds very little burden––the researcher just posts \r
the study design and analysis plan to an independent registry before observing the data, and then \r
reports the outcomes of the analysis according to that plan. However, the idealized model is a \r
simplification of how most research actually occurs. \r
Challenge #1: Changes to procedure during study administration. Sometimes the \r
best laid plans are difficult to achieve. Jolene preregisters an experimental design using human \r
infants as participants. She plans to collect 100 observations. Data collection is difficult. She can \r
only get 60 parents to bring their infants to her lab. She also discovers that some infants fall \r
asleep during study administration. She had not thought of this in advance; the preregistered \r
analysis plan does not exclude sleeping babies. 

7\r
Deviations from data collection and analysis plans are common, even in the most \r
predictable investigations. Deviations do not necessarily rule out testing predictions effectively. \r
If the outcomes have not yet been observed, Jolene can document the changes to her \r
preregistration without undermining diagnosticity. But, even if the data have been observed, \r
preregistration provides substantial benefit. Jolene can transparently report changes that were \r
made and why. Most of the design and analysis plan is still preserved and deviations are reported \r
transparently making it possible to assess their impact. Compared to the situation in which Jolene \r
did not preregister at all, preregistration with reported deviations provides substantially greater \r
confidence in the resulting statistical inferences. \r
There is certainly increased risk of bias with deviations from analysis plans after \r
observing the data, even when changes are reported transparently. For example, under NHST, if \r
Jolene uses the observed results to help decide whether to continue data collection, the likelihood \r
of misleading results may increase (30,46). With transparent reporting, observers can assess the \r
deviations and their rationale. The only way to achieve that transparency is with preregistration.\r
Challenge #2: Discovery of assumption violations during analysis. During analysis, \r
Courtney discovers that the distribution of one of her variables has a ceiling effect and another is \r
not normally distributed. These violate the assumptions of her preregistered tests. Violations like \r
these cannot be identified until observing the data. Nevertheless, multiple strategies are available \r
to address contingencies in data analytic methods without undermining diagnosticity of statistical \r
inference. \r
For some kinds of analysis, it is possible to define stages and preregister incrementally. \r
For example, a researcher could define a preregistration that evaluates distributional forms of \r
variables to determine data exclusions, transformations, and appropriate model assumptions that \r
do not reveal anything about the research outcomes. After that, the researcher preregisters the \r
model most appropriate for testing the outcomes of interest. Effective application of sequential \r
preregistration is difficult in many research applications. If an earlier stage reveals information \r
about outcomes to be tested at a subsequent stage, then the preregistration is compromised. \r
A more robust option is to blind the dataset by scrambling some of the observations so \r
that distributional forms are still retained but there is no way to know the actual outcomes until \r
the dataset is unblinded (47,48). Researchers can then address outliers and modeling assumptions \r
without revealing the outcomes. Blinding can be difficult to achieve in practice depending on the \r
nature of the dataset and outcomes of interest. \r
Another method is to preregister a decision tree. The decision tree defines the sequence \r
of tests and decision rules at each stage of the sequence. For example, the decision tree might \r
specify testing a normality assumption and, depending on the outcome, selection of either a \r
parametric or nonparametric test. A decision tree is particularly useful when the range of \r
possible analyses is easily described. However, it is possible to preregister biases into decision \r
trees. For example, one could preregister testing a sequence of exclusion rules and stopping \r
when one achieves p < .05. On the positive side, this misbehavior is highly detectable; on the 

8\r
negative side, it invalidates the diagnosticity of statistical inference. Preregistration does not \r
eliminate the possibility of poor statistical practices, but it does make them detectable. \r
A final option is to establish standard operating procedures (SOPs) that accompany one \r
or many preregistrations. SOPs describe decision rules for handling observed data (49), and have \r
more general application than a decision tree. SOPs are likely to be effective for areas of \r
research with common modeling approaches with many data treatment decisions. Also, SOPs \r
can be shared across many investigations to promote standards for data analysis. SOPs have the \r
same risks as decision trees of building in biasing influences, but those are detectable and \r
avoidable. SOPs sometimes emerge as community norms or evidence-based best practices such \r
as in the standards of evidence to claim autophagy (50) or the analysis pipeline for the Implicit \r
Association Test (51). Development of community norms requires deliberate effort and \r
consensus building, but the benefits for fostering de facto preregistration are substantial if the \r
community adheres to the standards.\r
Challenge #3: Data are pre-existing. Ian uses data provided by others to conduct his \r
research. In most cases, he does not know what variables and data are available to analyze until \r
after data collection is complete. This makes it difficult to preregister according to the idealized \r
model.\r
The extent to which testing predictions is possible on pre-existing data depends on \r
whether decisions about the analysis plan are blind to the data. “Pure” preregistration is still \r
possible if no one has observed the data. For example, a paleontologist can test predictions about \r
what will be observed from fossils yet to be discovered, and an economist can create predictions \r
of government data that exist but have not been released. However, once the data have been \r
observed, there are inevitable risks for blinding. Questions to ask include: “Who has observed \r
the data?” and “What observations, summaries, or findings have been communicated, and to \r
whom?” A researcher could test predictions using a dataset that has been examined by hundreds \r
of others if the new analyst is entirely blind to what others have observed and reported. But, \r
there are lots of ways––direct and indirect––to be influenced by observed data. If the new analyst \r
read a summary report of the dataset or receives advice on how to approach the dataset by prior \r
analysts, decisions might be undesirably influenced. Likewise, knowing some outcomes might \r
influence decisions, even if the analysis is on different variables from the dataset. For example, a \r
political scientist might preregister an analysis examining the relationship between religiosity \r
and volunteerism using an existing dataset. She has never observed data for the variables of \r
interest, but she has previously observed a relationship between political ideology and charitable \r
giving. Even though she is blind to data from her selected variables, the likelihood of positive \r
correlations between ideology and religiosity and between charitable giving and volunteerism \r
damages blinding. \r
This highlights how partial blinding creates a gray area between prediction and \r
postdiction. Once definitive blindness is sacrificed, the diagnosticity of statistical inference is \r
maximized by registering analysis plans and transparently reporting what was and was not \r
known in advance about the dataset. This transparency provides insight about potential biasing 

9\r
influences for, at minimum, subjective assessment of credibility. Otherwise nothing is \r
preregistered and there is no basis to assess credibility. An effective preregistration will account \r
for any loss of blinding and what impact that could have on the reported results. \r
Challenge #4: Longitudinal studies and large, multivariate datasets. Lily leads a \r
massive project that makes yearly observations of many variables over a 20-year period. \r
Members of her lab group conduct dozens of investigations with this large dataset producing a \r
few papers each year. Lily could not have preregistered the entire design and analysis plan for all \r
future papers at project onset. Moreover, the longitudinal design amplifies the challenges of pre\u0002existing data and changes to protocols after preregistration (52,53).\r
Solutions to the first three challenges also apply to this scenario, but longitudinal data \r
collection provides some additional opportunities. Each year, some variables are newly observed. \r
Preregistrations in advance of the new observations gain some benefits of blinding. The \r
limitations of this are the same as with correlated variables in large, multivariate datasets. If \r
variables at unobserved time t+1 are highly likely to be correlated with variables at observed \r
time t, then blinding could be weakened. Likewise, the effective blinding of a particular \r
statistical test depends, in part, on what proportion of the relevant data have been observed. \r
Nevertheless, partial blinding via preregistration offers more protection than no blinding at all.\r
Challenge #5: Many experiments. Natalie’s laboratory acquires data quickly, \r
sometimes running multiple experiments per week. The notion of preregistering every \r
experiment seems highly burdensome for their efficient workflow. \r
Teams that run many experiments are often doing so in the context of a methodological \r
paradigm in which each experiment varies some key aspects of a common procedure. In this \r
situation, preregistration can be as efficient as the design of the experiments themselves. A \r
preregistration template defines the variables and parameters for the protocol, and the \r
preregistrations document which parameters will be changed or manipulated for each successive \r
experiment. \r
In some cases, data acquisition is so simple that any documentation process interferes \r
with efficiency. In such scenarios, researchers can achieve confirmatory research via replication. \r
All initial experiments are treated as exploratory research. When something of interest is \r
observed, then the initial design and analysis script become the preregistration for testing a \r
prediction by running the experiment again. Easy data acquisition is a gift for rapidly \r
establishing the reproducibility of findings.\r
Challenge #6: A program of research. Brandon’s area of research is high risk and most \r
research outcomes are null results. Every once in awhile he gets a positive result, and the \r
implications are huge. As long as he preregisters everything, Brandon can be confident in his \r
statistical inference, right? Not necessarily. Imagine, for example, that Brandon gets a positive \r
result once every 20 tries. Even though every experiment is preregistered, given the aggregate \r
program of research, it is plausible that the positive results are occurring by chance. \r
This example illustrates another key element of preregistration. Not only is it essential \r
that the analysis plan be defined blind to the research outcomes, all outcomes of that analysis 

10\r
plan must be reported in order to avoid the problem of selective reporting. Transparent reporting \r
that 1 in 20 experiments or 1 in 20 analyses yielded a positive result will help researchers \r
identify the one as a likely false positive. If the one hit is tantalizing, replication facilitates \r
confidence in the observed effect. Preregistration does not eliminate the challenge of multiple \r
comparisons or selective reporting across studies, but it does make it possible to effectively \r
correct for multiple comparisons with full reporting. Achieving this benefit requires that \r
preregistrations and the results of the analysis plans are permanently preserved and accessible for \r
review.\r
Challenge #7: Few a priori expectations. Matt does not perceive that preregistration is \r
of use to him because he considers his research to be discovery science. In most cases, his group \r
wades into new problems with very little idea of what direction it will go and the outcomes they \r
observe send them in new directions. \r
It is common to begin a research program with few predictions. It is less common for \r
research to remain entirely exploratory through a sequence of studies and authoring of a paper. \r
If the data are used to generate hypotheses rather than claiming evidence for those hypotheses, \r
then the paper may be appropriately embracing post hoc explanation to open and test new areas \r
of inquiry. However, there are reasons to believe that sometimes postdiction is recast–wittingly \r
or unwittingly–as prediction. Indeed, the ubiquity of NHST in some fields implies either that \r
researchers are mostly testing predictions, or they are misusing NHST to develop support for \r
postdictions. In exploratory or discovery research, p-values have unknown diagnosticity and \r
their use can falsely imply testing rather than generating hypotheses. Preserving diagnosticity of \r
p-values means reporting them only when testing predictions. \r
Part of the problem is that researchers are incentivized to present postdictions and \r
exploratory results as if they had expected them in advance. Hindsight bias illustrates the folly \r
of this approach. Discovery science is vitally important for identifying new avenues of what is \r
possible. But, dressing up discovery as tests of theoretical predictions undermines the credibility \r
of all science by making it impossible to distinguish hypothesis generation from hypothesis \r
testing and, consequently, calibrate the uncertainty of available evidence. \r
Preregistration benefits both exploratory research and testing predictions during the \r
iterative research process. Following tests of predictions, data can be explored without constraint \r
for discoveries that might guide planning for the next experiment. Some discoveries will result in \r
predictions worth testing. This iteration can occur between studies. A first study is preregistered \r
with a simple analysis plan, and is then mostly used for exploratory analysis to generate \r
predictions that form the basis of a preregistration for a second study. \r
It is also possible to embrace discovery in a single large study and have some hypothesis \r
testing of interesting possibilities. In a process known as cross-validation, the dataset is split in \r
two. One part is used for exploratory analysis to develop the models and predictions, the other \r
part is sealed until exploration is complete (54). Sealing a dataset and preregistering the \r
outcomes of the discovery process before unsealing converts postdictions from the initial dataset \r
to predictions for the holdout dataset. 

11\r
Some research scenarios involve few clear predictions, and it is difficult to collect enough \r
data for splitting the dataset. Unfortunately, there is no magical solution. The rules of statistical \r
inference have no empathy for how hard it is to acquire the data. When data collection is \r
difficult, progress will be slower. For some domains, the questions are important enough to \r
pursue despite the slower progress.\r
Challenge #8: Competing predictions. Rusty and Melanie are collaborating on a project \r
in which they agree on the study design and analysis plan, but they have competing predictions \r
about what will occur because of their distinct theoretical orientations. This situation is not \r
actually a challenge for preregistration. In fact, it has desirable characteristics that can lead to \r
strong inference for favoring one theory over another (55). Prediction research can hold multiple\r
predictions simultaneously. The key for effective classical inference is to have well-defined \r
questions and an analysis plan that tests those questions. \r
Challenge #9: Narrative inferences and conclusions. Alexandra preregistered her \r
study, reported all the preregistered outcomes, and clearly distinguished the outcomes of tested \r
predictions and the postdictions generated from the data. Some of her tested predictions yielded \r
more interesting or notable results than others. Naturally, her narrative focused on the more \r
interesting results. \r
One can follow all of the ideals of preregistration and still leverage chance in the \r
interpretation of results. If one conducts 10 analyses and the narrative implications of the paper \r
focus on just two of them, inferential error can increase in how the paper is applied and cited. \r
Essentially this is a circumstance of failing to correct for multiple comparisons (35). This can be \r
corrected statistically by applying alpha corrections like Bonferroni (56) such that narrative focus \r
on just positive results is not associated with inflating likelihood of false positives. But selective \r
attention and interpretation can occur, and it is difficult to address this statistically. \r
Preregistration does not prevent authors or readers taking narrative license to deviate \r
from what is justified from the evidence. How a paper is used as evidence for a phenomenon \r
may be influenced by its qualitative interpretations and conclusions beyond the quantitative \r
evidence. An unwise solution would remove narrative structure and interpretation from \r
scientific papers. Interpretation and narrative conclusions are an important stimulus in the \r
development of theory. The originator of evidence and interpretation has one view of the data \r
and its implications, but with transparency, other views and interpretations can be applied. Those \r
distinct interpretations of the same statistical evidence are a feature of science as a decentralized \r
community activity of independent observers. The influence of selective inference is detectable \r
and addressable only with transparency of the research process. \r
Making preregistration the norm\r
Despite its value for transparency, rigor, and reproducibility, the prevalence of \r
preregistration is only just starting to emerge in basic, preclinical, and applied research. \r
However, just as the present culture provides means (reasoning biases and misuse of statistics), \r
motive (publication), and opportunity (no a priori commitments to predictions) for dysfunctional 

12\r
research practices, the culture is shifting to provide means, motive, and opportunity for rigor and \r
robustness of research practices via preregistration.\r
Advancing the means for preregistration. A substantial barrier to preregistration is \r
insufficient or ineffective training of good statistical and methodological practices. Most \r
researchers embrace the norms of science and aim to do the most rigorous work that they can \r
(57). Those values are advanced with education and resources for effective preregistration in \r
one’s research domain. The reference list for this review provides a starting point, and there are \r
some education modules available online to facilitate preregistration planning. For example, \r
online courses (https://www.coursera.org/specializations/statistics, \r
https://www.coursera.org/learn/statistical-inferences), instructional guides \r
(http://help.osf.io/m/registrations/l/546603-enter-the-preregistration-challenge), criteria \r
established for preregistration badge credentials (https://osf.io/6q7nm/), and collections of \r
preregistration templates (https://osf.io/zab38/wiki/). \r
Researchers are familiar with many aspects of preregistration already because they occur \r
in other common research practices. For example, grant applications sometimes require \r
specification of the proposed methodology. Funding agencies are recognizing the value of \r
requiring more rigorous specification of the design and analysis plans––potentially achieving \r
sufficient detail to become a preregistration. Also, research domains that require submission for \r
ethics review for research on humans or animals must specify some of the research methodology \r
before conducting the research. It is only a few additional steps to incorporate analysis plans to \r
achieve an effective preregistration. Finally, thesis proposals for students often require \r
comprehensive design and analysis plans that can easily become preregistrations. Extending \r
these common practices will enable many researchers to preregister their work with small steps \r
from existing practices.\r
Advancing the motive for preregistration. If researchers behave exclusively according \r
to their ideals, then education about the value and appropriate use of preregistration might be \r
sufficient for adoption. But, relying on ideals is not sufficient. Researchers are sensitive to the \r
incentives that increase their likelihood of obtaining jobs, grants, publications, and awards. The \r
existing culture has had relatively weak incentives for research rigor and reproducibility, but this \r
is changing. Preregistration is required by U.S. law for clinical trials, and is necessary to be \r
published in International Committee of Medical Journal Editors policy§, which specifies only \r
rare exceptions for work that was not preregistered.¶ Beyond clinical trials, thousands of journals \r
and a number of funders are signatories to the Transparency and Openness Promotion (TOP) \r
Guidelines (http://cos.io/top/) that define standards for transparency and reproducibility, \r
including preregistration. As journals and funders begin to adopt expectations for preregistration, \r
researchers’ behavior will follow. Also, some journals have adopted badges for preregistration as \r
incentives for authors to get credit for having preregistered with explicit designation on the \r
published article. It is possible that such incentives will become nearly as effective as badges for \r
open data, which were associated with more than a 10-fold increase in data sharing in an initial \r
test (58). 

13\r
Other efforts incorporate incentives for preregistration into the publishing process. The \r
Preregistration Challenge (http://cos.io/prereg/) offers one thousand $1000 awards to researchers \r
that publish the results of a preregistered study (see also https://www.erpc2016.com/). A \r
publishing model called Registered Reports (http://cos.io/rr/) is offered by dozens of journals to \r
facilitate preregistration (59,60). With Registered Reports, authors submit their research question \r
and methodology to the journal for peer review prior to observing the outcomes of the research. \r
If reviewers agree that the question is sufficiently important and the methodology to test it is of \r
sufficiently high quality, then the paper is given in-principle acceptance. The researchers then \r
carry out the study and submit the final report to the journal. At second-stage review, reviewers \r
do not evaluate the perceived importance of the outcomes. Rather, they evaluate the quality of \r
study execution and adherence to the preregistered plan. In addition to the benefits of \r
preregistration, this workflow addresses selective reporting of results and facilitates improving \r
research designs during the peer review process. \r
Beyond the intrinsic value of preregistration for the quality of research, these initiatives \r
are shifting the incentives for researchers’ career interests to be more aligned with preregistration \r
as a standard activity. Already, there is evidence of some cultural shift. For example, there are \r
more than 7,000 preregistrations on the Open Science Framework (http://osf.io/) for research \r
across all areas of science.\r
Advancing the opportunity for preregistration. Existing domain-specific and domain\u0002general registries make it possible for researchers in any discipline to preregister their research. \r
The World Health Organization maintains a list of registries by nation or region \r
(http://www.who.int/ictrp/network/primary/en/) such as the largest existing registry \r
http://clinicaltrials.gov/. While focused on clinical trials in biomedicine, many of these registries \r
offer flexibility to register other kinds of research. The American Economics Association (AEA \r
RCT Registry https://www.socialscienceregistry.org/), the Registry for International \r
Development Impact Evaluations (RIDIE Registry http://ridie.3ieimpact.org/), and the Evidence \r
in Governance and Politics registry (EGAP Registry http://egap.org/content/registration) are \r
registries for economics and political science research. The Open Science Framework (OSF; \r
https://osf.io) is a domain-general registry service that provides multiple formats for \r
preregistration (http://osf.io/registries/), including the flexible and relatively comprehensive \r
Preregistration Challenge format (http://osf.io/prereg/). Finally, the website \r
http://AsPredicted.org/ provides a simple form for preregistration, but it is not itself a registry \r
because users can keep their completed forms private forever and selectively report \r
preregistrations. However, researchers can post completed forms to a registry to meet the \r
preservation and transparency standards. \r
These steps on education, incentives, and services above anticipate growth in \r
preregistration, and emergence of a research literature about preregistration to identify its \r
strengths, weaknesses, and opportunities for improvement.

14\r
What scientific research looks like when preregistration is pervasive\r
Pervasive preregistration is distant but achievable. When it occurs, generating possible \r
discoveries and testing clear predictions will both be valued and distinctly labeled. Exploratory \r
analyses and postdiction will be understood as generative events to identify what is possible, \r
encouraging follow-up research testing predictions to identify what is likely. The decline of \r
selective reporting across and within studies will increase the credibility of research evidence. To \r
get there, the research community must solve the challenge of coordinated action in a \r
decentralized system. All stakeholders in science must embrace their role in shaping the research \r
incentives for career advancement, and nudge those incentives so that what is good for science \r
and what is good for the scientist are the same thing. \r
Conclusion\r
Sometimes researchers use existing observations of nature to generate ideas about how \r
the world works. This is called postdiction. Other times, researchers have an idea about how the \r
world works and make new observations to test whether that idea is a reasonable explanation. \r
This is called prediction. To make confident inferences, it is important to know which is which. \r
Preregistration solves this challenge by requiring researchers to state how they will analyze the \r
data before they observe it, allowing them to confront a prediction with the possibility of being \r
wrong. Preregistration improves the interpretability and credibility of research findings.\r
Footnotes\r
*\r
This paper focuses on NHST because of its pervasive use (e.g., 28, 29). The opportunities and \r
challenges discussed are somewhat different with other statistical approaches such as Bayesian \r
methods. However, no statistical method on its own avoids researcher opportunity for flexibility \r
in analytical decisions, such as exclusion criteria, the creation of variables from specific \r
measurements (30).\r
✝Notably, p-values near 0.05 are not actually very unlikely in typical research practices (31), \r
leading some researchers to recommend .005 as a more stringent criterion for claiming \r
"significance" (32).\r
‡\r
The challenges of reasoning biases and distinguishing postdiction and prediction are not limited \r
to NHST, but they can manifest somewhat differently in other forms of data analysis. We focus \r
on NHST because of its common use.\r
§\r
http://www.icmje.org/recommendations/browse/publishing-and-editorial-issues/clinical-trial\u0002registration.html\r
¶\r
The standards for preregistration in clinical trials do not yet require comprehensive specification \r
of analysis plans, though they do require identification of primary and secondary outcome \r
variables.\r
Acknowledgments: This work was supported by grants from the Laura and John Arnold \r
Foundation and the National Institute of Aging. 

15\r
References\r
1. Box GEP (1976) Science and Statistics. Journal of the American Statistical Association\r
71(356):791–799.\r
2. Box GEP (1979) Robustness in the Strategy of Scientific Model Building. Robustness in \r
Statistics, eds Launer RL, Wilkinson GN (Academic Press), pp 201–236.\r
3. de Groot AD (2014) The meaning of “significance” for different types of research [translated \r
and annotated by Eric-Jan Wagenmakers, Denny Borsboom, Josine Verhagen, Rogier Kievit, \r
Marjan Bakker, Angelique Cramer, Dora Matzke, Don Mellenbergh, and Han L. J. van der \r
Maas]. Acta Psychologica 148:188–194.\r
4. Hoyningen-Huene P (1987) Context of discovery and context of justification. Studies in \r
History and Philosophy of Science Part A 18(4):501–515.\r
5. Kuhn TS (1970) Logic of Discovery or Psychology of Research? Criticism and the Growth of \r
Knowledge, eds Lakatos I, Musgrave A (Cambridge University Press), pp 1-23.\r
6. Wagenmakers E-J, Wetzels R, Borsboom D, van der Maas HLJ, Kievit RA (2012) An \r
Agenda for Purely Confirmatory Research. Perspectives on Psychological Science 7(6):632–\r
638.\r
7. Forstmeier W, Wagenmakers E-J, Parker TH (2016) Detecting and avoiding likely false\u0002positive findings – a practical guide. Biol Rev, doi:10.1111/brv.12315.\r
8. Munafò MR, et al. (2017) A manifesto for reproducible science. Nature Human Behaviour\r
1(1):s41562-16-21–16.\r
9. Nosek BA, Spies JR, Motyl M (2012) Scientific Utopia: II. Restructuring Incentives and \r
Practices to Promote Truth Over Publishability. Perspectives on Psychological Science\r
7(6):615–631.\r
10. Open Science Collaboration (2015) Estimating the reproducibility of psychological science. \r
Science 349(6251):aac4716.\r
11. Swaen GG, Teggeler O, Amelsvoort V, Gpm L (2001) False positive outcomes and design \r
characteristics in occupational cancer epidemiology studies. Int J Epidemiol 30(5):948–954.\r
12. Kerr NL (1998) HARKing: Hypothesizing After the Results are Known. Personality and \r
Social Psychology Review 2(3):196–217.\r
13. Fischhoff B, Beyth R (1975) I knew it would happen. Organizational Behavior and Human \r
Performance 13(1):1–16.

16\r
14. Fischhoff B (2003) Hindsight ≠ foresight: the effect of outcome \r
knowledge on judgment under uncertainty. Quality and Safety in Health Care\r
12(4):304–311.\r
15. Lewis M (2016) The Undoing Project: A Friendship that Changed our Minds, (W. W. \r
Norton & Company, New York).\r
16. Hawkins CB, Nosek BA (2012) Motivated Independence? Implicit Party Identity Predicts \r
Political Judgments Among Self-Proclaimed Independents. Personality and Social Psychology \r
Bulletin 38(11):1437–1452.\r
17. Kahneman D, Slovic P, Tversky A (1982) Judgment Under Uncertainty: Heuristics and \r
Biases (Cambridge University Press).\r
18. Kahneman D (2011) Thinking, Fast and Slow (Macmillan).\r
19. Bakker M, van Dijk A, Wicherts JM (2012) The Rules of the Game Called Psychological \r
Science. Perspectives on Psychological Science 7(6):543–554.\r
20. Giner-Sorolla R (2012) Science or Art? How Aesthetic Standards Grease the Way Through \r
the Publication Bottleneck but Undermine Science. Perspectives on Psychological Science\r
7(6):562–571.\r
21. Mahoney MJ (1977) Publication prejudices: An experimental study of confirmatory bias in \r
the peer review system. Cogn Ther Res 1(2):161–175.\r
22. Christensen-Szalanski JJJ, Willham CF (1991) The hindsight bias: A meta-analysis. \r
Organizational Behavior and Human Decision Processes 48(1):147–168.\r
23. Kunda Z (1990) The case for motivated reasoning. Psychological Bulletin\r
108(3):480–498.\r
24. Nickerson RS (1998) Confirmation bias: A ubiquitous phenomenon in many guises. Review \r
of General Psychology 2(2):175–220.\r
25. Nosek BA, Riskind RG (2012) Policy Implications of Implicit Social Cognition. Social \r
Issues and Policy Review 6(1):113–147.\r
26. Pronin E, Kugler MB (2007) Valuing thoughts, ignoring behavior: The introspection illusion \r
as a source of the bias blind spot. Journal of Experimental Social Psychology 43(4):565–578.\r
27. Sellke T, Bayarri MJ, Berger JO (2001) Calibration of ρ Values for Testing Precise Null \r
Hypotheses. The American Statistician 55(1):62–71.

17\r
28. Hubbard R, Ryan PA (2000) Statistical Significance with Comments by Editors of Marketing \r
Journals: The Historical Growth of Statistical Significance Testing in Psychology—and its \r
Future Prospects. Educational and Psychological Measurement 60(5):661–681.\r
29. Stephens PA, Buskirk SW, del Rio CM (2007) Inference in ecology and evolution. Trends in \r
Ecology & Evolution 22(4):192–197.\r
30. Simmons JP, Nelson LD, Simonsohn U (2011) False-Positive Psychology: Undisclosed \r
Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant. \r
Psychological Science 22(11):1359–1366.\r
31. Wasserstein RL, Lazar NA (2016) The ASA’s Statement on p-Values: Context, Process, and \r
Purpose. The American Statistician 70(2):129–133.\r
32. Benjamin DJ, et al. (in press) Redefine statistical significance. Nature Human Behavior\r
doi:10.17605/OSF.IO/MKY9J.\r
33. Dunnett CW (1955) A Multiple Comparison Procedure for Comparing Several Treatments \r
with a Control. Journal of the American Statistical Association 50(272):1096–1121.\r
34. Tukey JW (1949) Comparing Individual Means in the Analysis of Variance. Biometrics\r
5(2):99–114.\r
35. Benjamini Y (2010) Simultaneous and selective inference: Current successes and future \r
challenges. Biom J 52(6):708–721.\r
36. Saxe R, Brett M, Kanwisher N (2006) Divide and conquer: A defense of functional \r
localizers. NeuroImage 30(4):1088–1096.\r
37. Gelman A, Loken E (2014) The Statistical Crisis in Science. American Scientist 102(6):460.\r
38. Franco A, Malhotra N, Simonovits G (2014) Publication bias in the social sciences: \r
Unlocking the file drawer. Science 345(6203):1502–1505.\r
39. Greenwald AG (1975) Consequences of prejudice against the null hypothesis. Psychological \r
Bulletin 82(1):1–20.\r
40. Rosenthal R (1979) The file drawer problem and tolerance for null results. Psychological \r
Bulletin 86(3):638–641.\r
41. Ioannidis JPA (2005) Why Most Published Research Findings Are False. PLOS Medicine\r
2(8):e124.

18\r
42. John LK, Loewenstein G, Prelec D (2012) Measuring the Prevalence of Questionable \r
Research Practices With Incentives for Truth Telling. Psychological Science 23(5):524–532.\r
43. Kaplan RM, Irvin VL (2015) Likelihood of Null Effects of Large NHLBI Clinical Trials Has \r
Increased over Time. PLOS ONE 10(8):e0132382.\r
44. Cybulski L, Mayo-Wilson E, Grant S (2016) Improving transparency and reproducibility \r
through registration: The status of intervention trials published in clinical psychology journals. \r
Journal of Consulting and Clinical Psychology 84(9):753–767.\r
45. Odutayo A, et al. (2017) Association between trial registration and positive study findings: \r
cross sectional study (Epidemiological Study of Randomized Trials—ESORT). BMJ 356:j917.\r
46. Armitage P, McPherson CK, Rowe BC (1969) Repeated Significance Tests on Accumulating \r
Data. Journal of the Royal Statistical Society Series A (General) 132(2):235–244.\r
47. Dutilh G, et al. (2017) A test of the diffusion model explanation for the worst performance \r
rule using preregistration and blinding. Atten Percept Psychophys 79(3):713–725.\r
48. MacCoun R, Perlmutter S (2015) Blind analysis: Hide results to seek the truth. Nature\r
526(7572):187–189.\r
49. Lin W, Green DP (2016) Standard Operating Procedures: A Safety Net for Pre-Analysis \r
Plans. PS: Political Science & Politics 49(3):495–500.\r
50. Klionsky DJ, et al. (2016) Guidelines for the use and interpretation of assays for monitoring \r
autophagy (3rd edition). Autophagy 12(1):1–222.\r
51. Greenwald AG, Nosek BA, Banaji MR (2003) Understanding and using the Implicit \r
Association Test: I. An improved scoring algorithm. Journal of Personality and Social \r
Psychology 85(2):197–216.\r
52. Campbell L, Loving TJ, Lebel EP (2014) Enhancing transparency of the research process to \r
increase accuracy of findings: A guide for relationship researchers. Pers Relationship 21(4):531–\r
545.\r
53. Cockburn A (2017) Long-term data as infrastructure: a comment on Ihle et al. Behav Ecol\r
28(2):357–357.\r
54. Fafchamps M, Labonne J (2016) Using Split Samples to Improve Inference about Causal \r
Effects (National Bureau of Economic Research) doi:10.3386/w21842.\r
55. Platt JR (1964) Strong Inference. Science 146(3642):347–353.

19\r
56. Holm S (1979) A Simple Sequentially Rejective Multiple Test Procedure. Scandinavian \r
Journal of Statistics 6(2):65–70.\r
57. Anderson MS, Martinson BC, De Vries R (2007) Normative Dissonance in Science: Results \r
from a National Survey of U.S. Scientists. Journal of Empirical Research on Human Research \r
Ethics 2(4):3–14.\r
58. Kidwell MC, et al. (2016) Badges to Acknowledge Open Practices: A Simple, Low-Cost, \r
Effective Method for Increasing Transparency. PLOS Biology 14(5):e1002456.\r
59. Chambers CD, Feredoes E, Muthukumaraswamy SD, Etchells P (2014) Instead of “playing \r
the game” it is time to change the rules: Registered Reports at AIMS Neuroscience and beyond. \r
AIMS Neuroscience 1(1):4–17.\r
60. Nosek BA, Lakens D (2014) Registered Reports. Social Psychology 45(3):137–141."""

[metadata]
title = "nosek et al 2018 preregistration revolution osf 2dxu5"
authors = ["Unknown"]
year = 2018

[[sections]]
number = "1"
title = "The Preregistration Revolution"
text = """
Brian A. Noseka,b ORCID: 0000-0001-6797-5476\r
Charles R. Ebersoleb ORCID: 0000-0002-8607-2579\r
Alexander C. DeHavena ORCID: 0000-0002-2241-7259\r
David T. Mellora ORCID: 0000-0002-3125-5888\r
a Center for Open Science, 210 Ridge McIntire Road, Suite 500, Charlottesville, VA 22903\r
b University of Virginia, Department of Psychology, 102 Gilmer Hall, Charlottesville, VA 22904\r
Corresponding Author: Brian Nosek, 210 Ridge McIntire Road, Suite 500, Charlottesville, VA \r
22903, nosek@virginia.edu\r
Keywords: prediction, postdiction, methodology, rigor, reproducibility, preregistration, \r
transparency, science policy, philosophy of science \r
Abstract\r
Progress in science relies in part on generating hypotheses with existing observations and testing \r
hypotheses with new observations. This distinction between postdiction and prediction is \r
appreciated conceptually, but is not respected in practice. Mistaking generation of postdictions \r
with testing of predictions reduces the credibility of research findings. However, ordinary biases \r
in human reasoning such as hindsight bias make it hard to avoid this mistake. An effective \r
solution is to define the research questions and analysis plan prior to observing the research \r
outcomes–a process called preregistration. Preregistration distinguishes analyses and outcomes \r
that result from predictions from those that result from postdictions. A variety of practical \r
strategies are available to make the best possible use of preregistration in circumstances that fall \r
short of the ideal application, such as when the data are pre-existing. Services are now available \r
for preregistration across all disciplines facilitating a rapid increase in the practice. Widespread \r
adoption of preregistration will increase distinctiveness between hypothesis generation and \r
hypothesis testing and will improve the credibility of research findings. \r
Classification: Social Science, Psychological and Cognitive Sciences"""

[[sections]]
number = "2"
title = "The Preregistration Revolution"
text = """
Progress in science is marked by reducing uncertainty about nature. Scientists generate \r
models that may explain prior observations and predict future observations. Those models are \r
approximations and simplifications of reality. Models are iteratively improved and replaced by \r
reducing the amount of prediction error. As prediction error decreases, certainty about what will \r
occur in the future increases. This view of research progress is captured by George Box’s \r
aphorism “All models are wrong but some are useful” (1,2).\r
Scientists improve models by generating hypotheses based on existing observations and \r
testing those hypotheses by obtaining new observations. These distinct modes of research are \r
discussed by philosophers and methodologists as hypothesis-generating versus hypothesis\u0002testing, the context of discovery versus the context of justification, data-independent versus data\u0002contingent analysis, and exploratory versus confirmatory research (e.g., 3,4,5,6). We use the \r
more general terms––postdiction and prediction––to capture this important distinction. \r
A common thread among epistemologies of science is that postdiction is characterized by \r
the use of data to generate hypotheses about why something occurred, and prediction is \r
characterized by the acquisition of data to test ideas about what will occur. In prediction, data are \r
used to confront the possibility that the prediction is wrong. In postdiction, the data are already \r
known and the postdiction is generated to explain why they occurred. \r
Testing predictions is vital for establishing diagnostic evidence for explanatory claims. \r
Testing predictions assesses the uncertainty of scientific models by observing how well the \r
predictions account for new data. Generating postdictions is vital for discovery of possibilities \r
not yet considered. In many cases, researchers have very little basis to generate predictions or \r
evidence can reveal that initial expectations were wrong. Progress in science often proceeds via \r
unexpected discovery––a study reveals an inexplicable pattern of results that sends the \r
investigation on a new trajectory. \r
Why does the distinction between prediction and postdiction matter? Failing to appreciate \r
the difference can lead to overconfidence in post hoc explanations (postdictions) and inflate the \r
likelihood of believing that there is evidence for a finding when there is not. Presenting \r
postdictions as predictions can increase the attractiveness and publishability of findings by \r
falsely reducing uncertainty. Ultimately, this decreases reproducibility (6,7,8,9,10,11). \r
Mental constraints on distinguishing predictions and postdictions \r
It is common for researchers to alternate between postdiction and prediction. Ideas are \r
generated and observed data modifies those ideas. Over time and iteration, researchers develop \r
understanding of the phenomenon under study. That understanding might result in a model, \r
hypothesis, or theory. The dynamism of the research enterprise and limits of human reasoning \r
make it easy to mistake postdiction as prediction. The problem with this is understood as post \r
hoc theorizing or hypothesizing after the results are known (12). It is an example of circular \r
reasoning––generating a hypothesis based on observing data, and then evaluating the validity of \r
the hypothesis based on the same data. 

3\r
Hindsight bias, also known as the I-knew-it-all-along effect, is the tendency to see \r
outcomes as more predictable after the fact compared to before they were observed (13,14). With \r
hindsight bias, the observer uses the data to generate an explanation, a postdiction, and \r
simultaneously perceives that they would have anticipated that explanation in advance, a \r
prediction. A common case is when the researcher’s prediction is vague so that many possible \r
outcomes can be rationalized after the fact as supporting the prediction. For example, a \r
biomedical researcher might predict that a treatment will improve health, and postdictively \r
identify the one of five health outcomes that showed a positive benefit as the one most relevant \r
for testing the prediction. A political scientist might arrive at a model using a collection of \r
covariates and exclusion criteria that can be rationalized after the fact, but would not have been \r
anticipated as relevant beforehand. A chemist may have random variation occurring across a \r
number of results and nevertheless be able to construct a narrative post facto that imbues \r
meaning in the randomness. To an audience of historians (15), Amos Tversky provided a cogent \r
explanation of the power of hindsight for considering evidence:\r
“All too often, we find ourselves unable to predict what will happen; yet after the \r
fact we explain what did happen with a great deal of confidence. This ‘ability’ to \r
explain that which we cannot predict, even in the absence of any additional \r
information, represents an important, though subtle, flaw in our reasoning. It leads \r
us to believe that there is a less uncertain world than there actually is...”\r
Mistaking postdiction as prediction underestimates the uncertainty of outcomes and can produce \r
psychological overconfidence in the resulting findings. \r
The values of impartiality and objectivity are pervasive (16), particularly for scientists, \r
but human reasoning is not reliably impartial or objective (17,18). Scientists are motivated to \r
advance knowledge; scientists are also motivated to obtain job security, awards, publications, \r
and grants. In the present research culture, these rewards are more likely to be secured by \r
obtaining certain kinds of research outcomes over others. Novel results are rewarded more than \r
redundant or incremental additions to existing knowledge. Positive results––finding a \r
relationship between variables or an effect of treatments on outcomes––are rewarded more than \r
negative results––failing to find a relationship or effect; clean results that provide a strong \r
narrative are rewarded more than outcomes that show uncertainty or exceptions to the favored \r
narrative (19,20,21,9). Novel, positive, clean results are better results both for reward and for \r
launching science into new domains of inquiry. However, achieving novel, positive, clean results \r
is a rare event. Progress in research is halting, messy, and uncertain. The incentives for such \r
results combined with their infrequency creates a potential conflict of interest for the researcher. \r
If certain kinds of results are more rewarded than others, then researchers are motivated to obtain \r
results that are more likely to be rewarded regardless of the accuracy of those results. \r
Lack of clarity between postdiction and prediction provides the opportunity to select, \r
rationalize, and report tests that maximize reward over accuracy. Moreover, good intentions are \r
not sufficient to overcome the fallibility of memory, motivated reasoning, and cognitive biases \r
that can occur outside of conscious awareness or control (22,23,24,25,26). Researchers may 

4\r
design a study to investigate one question and, upon observing the outcomes, misremember the \r
original purposes as more aligned with what was observed. Researchers may genuinely believe \r
that they would have predicted, or even that they did predict, the outcomes as observed (22). \r
Researchers may employ confirmation bias by seeking evidence consistent with their \r
expectations and finding fault or ignoring evidence that is inconsistent with their expectations \r
(24). These reasoning challenges are exacerbated by the misuse of common tools of statistical \r
inference to provide false comfort about the reliability of evidence. \r
Standard tools of statistical inference assume prediction\r
Null Hypothesis Significance Testing (NHST) is designed for prediction––testing \r
hypotheses––not for postdiction––generating hypotheses (6,27). The pervasiveness in many \r
disciplines of NHST and its primary statistic, the p-value, implies either that most research is \r
prediction or that postdiction is frequently mistaken as prediction with errant application of \r
NHST.*\r
In NHST, one usually compares a null hypothesis of no relationships among the variables \r
and an alternate hypothesis in which the variables are related. Data are then observed that lead to \r
rejection or not of the null hypothesis. Rejection of the null hypothesis at p < .05 is a claim \r
about the likelihood that data as extreme or more extreme than the observed data would have \r
occurred if the null hypothesis were true. It is underappreciated that the presence of “hypothesis \r
testing” in the name NHST is consequential for constraining its appropriate use to testing \r
predictions. The diagnosticity of a p-value is partly contingent on knowing how many tests were \r
performed (27). Deciding that a given p < .05 result is unlikely✝, and therefore evidence against \r
the null hypothesis, is very different if it was the only test conducted versus one of 20, 200, or \r
2000 tests. \r
If there were only one inference test to perform and only one way to conduct that test, \r
then the p-value is diagnostic about its intended likelihood. It is not hyperbole to say that this \r
almost never occurs. Even in the simplest studies, there is more than one way to perform the \r
statistical inference test. For example, researchers must decide whether any observations should \r
be excluded from the analysis, whether any measures should be transformed or combined, and \r
whether any other variables should be included in the model as covariates. \r
Correcting diagnosticity of p-values for the number of tests that were actually conducted \r
is relatively straightforward (33,34), though inconsistently––even rarely––applied in practice \r
(35,36). However, counting the literal performance of statistical tests is not sufficient to account \r
for how observing the data can influence the selection of tests to conduct. Gelman and Loken \r
(37) refer to the problem as the garden of forking paths. There are a vast number of choices for \r
analyzing data that could be made. If those choices are made during analysis, observing the data \r
may make selecting some paths more likely and others less likely. By the end, it may be \r
impossible to estimate the paths that could have been selected if the data had looked different or \r
if analytic decisions were influenced by hindsight, confirmation, and outcome biases. This leaves 

5\r
the observed p-values with unknown diagnosticity rendering them uninterpretable. In other \r
words, NHST cannot be used with confidence for postdiction.‡\r
In prediction, the problem of forking paths is avoided because the analytic pipeline is \r
specified prior to observing the data. As such, with correction for the number of tests conducted, \r
p-values retain their diagnosticity. In postdiction, analytic decisions are influenced by the \r
observed data creating the forking paths. The researcher is exploring the data to discover what is \r
possible. The data helps generate, not test, new questions and hypotheses. \r
The problem of failing to distinguish between postdiction and prediction is vastly \r
underestimated in practice. Researchers may conduct lots of studies and test many possible \r
relationships. Even if there are no relationships to find, some of those tests will elicit apparent \r
evidence––positive results––by chance (27). If researchers selectively report positive results \r
more frequently than negative results, then the likelihood of false positives will increase \r
(38,39,40). Moreover, researchers have substantial degrees-of-freedom to conduct many \r
different tests, and selection of those that yield positive results over those that yield negative \r
results will increase likelihood of attractive results at the expense of accuracy (41,42,30). \r
If researchers are clear about when they are in prediction and postdiction modes of \r
research, then the benefits (and limits) of statistical inference will be preserved. But, with means, \r
motive, and opportunity to misperceive postdiction as prediction and to selectively rationalize \r
and report a biased subset of outcomes, researchers are prone to false confidence in evidence. \r
Preregistration is a solution that helps researchers maintain clarity between prediction and \r
postdiction, and preserve accurate calibration of evidence.\r
Preregistration distinguishes prediction and postdiction\r
Preregistration of an analysis plan is committing to analytic steps without advance \r
knowledge of the research outcomes. That commitment is usually accomplished by posting the \r
analysis plan to an independent registry such as http://clinicaltrials.gov/ or http://osf.io/. The \r
registry preserves the preregistration and makes it discoverable, sometimes after an embargo \r
period. With preregistration, prediction is achieved because selection of tests is not influenced by \r
the observed data, and all conducted tests are knowable. The analysis plan provides constraint to \r
specify how the data will be used to confront the research questions. \r
In principle, inferences from preregistered analyses will be more reproducible than NHST \r
analyses that were not preregistered because the relation between the analysis choices and \r
findings cannot be influenced by motivation, memory, or reasoning biases. We say “in principle” \r
because the case for preregistration is theoretically strong as a matter of inductive inference and \r
empirically bolstered by some correlational evidence. However, there is not yet sufficient \r
experimental evidence establishing its superiority for reproducibility. Correlational evidence \r
suggests that hypothesizing in advance relates to increased replicability (11). Further, \r
preregistration is correlated with outcomes that suggest reduced publication or reporting biases. \r
For example, Kaplan and Irvin (43) observed a dramatic drop in the rate of positive results \r
following the requirement to preregister primary outcomes in a sample of clinical trials. The 

6\r
benefits of preregistration are lost if researchers do not follow the preregistrations (44,45). \r
However, there is evidence that preregistration makes it possible to detect and possibly correct \r
selection and reporting biases (e.g., www.COMPare-trials.org). Franco, Malhotra, and \r
Simonovits (38) observed that 40% of published papers in their sample of preregistered studies \r
failed to report one or more of the experimental manipulations (treatment conditions), and 70% \r
of published papers failed to report one or more of the outcome variables. Moreover, there was \r
substantial selection bias in outcomes that were reported in the paper (96% of consistently \r
significant findings included in published articles) versus those that were left out (65% of null \r
effects not included in published articles). \r
Formally speaking, analyses conducted on the data that are not part of the preregistration \r
inform postdiction. In principle, preregistration can establish a bright line between prediction and \r
postdiction. This preserves the diagnosticity of NHST inference for predictions, and clarifies the \r
role of postdiction for generating possible explanations to test as predictions in the future. In \r
practice, there are challenges for implementing preregistration and maintaining a clear distinction \r
between prediction and postdiction. Nevertheless, there are opportunities to benefit from \r
preregistration even when the idealistic bright line cannot be achieved.\r
Preregistration in practice\r
Preregistration does not favor prediction over postdiction; its purpose is to make clear \r
which is which. There are practical challenges for effective integration of preregistration in many \r
areas of research. We first describe the ideal of preregistration and then address some of the \r
practical challenges. \r
The ideal. The idealized scenario for preregistration follows the simplified model of \r
research taught in elementary school. A scientist makes observations in the world and generates \r
a research question or hypothesis from those observations. A study design and analysis plan are \r
created to evaluate that question. Then data are collected according to the design and analyzed \r
according to the analysis plan. This confronts the hypothesis by testing whether it predicts the \r
outcomes of the experiment. Following that, the researcher might explore the data for potential \r
discoveries that generate hypotheses or potential explanations after the fact. The most interesting \r
postdictions are then converted into predictions for designing the next study and the cycle \r
repeats. In this idealized model, preregistration adds very little burden––the researcher just posts \r
the study design and analysis plan to an independent registry before observing the data, and then \r
reports the outcomes of the analysis according to that plan. However, the idealized model is a \r
simplification of how most research actually occurs. \r
Challenge #1: Changes to procedure during study administration. Sometimes the \r
best laid plans are difficult to achieve. Jolene preregisters an experimental design using human \r
infants as participants. She plans to collect 100 observations. Data collection is difficult. She can \r
only get 60 parents to bring their infants to her lab. She also discovers that some infants fall \r
asleep during study administration. She had not thought of this in advance; the preregistered \r
analysis plan does not exclude sleeping babies."""

[[sections]]
number = "7"
title = "Deviations from data collection and analysis plans are common, even in the most"
text = """
predictable investigations. Deviations do not necessarily rule out testing predictions effectively. \r
If the outcomes have not yet been observed, Jolene can document the changes to her \r
preregistration without undermining diagnosticity. But, even if the data have been observed, \r
preregistration provides substantial benefit. Jolene can transparently report changes that were \r
made and why. Most of the design and analysis plan is still preserved and deviations are reported \r
transparently making it possible to assess their impact. Compared to the situation in which Jolene \r
did not preregister at all, preregistration with reported deviations provides substantially greater \r
confidence in the resulting statistical inferences. \r
There is certainly increased risk of bias with deviations from analysis plans after \r
observing the data, even when changes are reported transparently. For example, under NHST, if \r
Jolene uses the observed results to help decide whether to continue data collection, the likelihood \r
of misleading results may increase (30,46). With transparent reporting, observers can assess the \r
deviations and their rationale. The only way to achieve that transparency is with preregistration.\r
Challenge #2: Discovery of assumption violations during analysis. During analysis, \r
Courtney discovers that the distribution of one of her variables has a ceiling effect and another is \r
not normally distributed. These violate the assumptions of her preregistered tests. Violations like \r
these cannot be identified until observing the data. Nevertheless, multiple strategies are available \r
to address contingencies in data analytic methods without undermining diagnosticity of statistical \r
inference. \r
For some kinds of analysis, it is possible to define stages and preregister incrementally. \r
For example, a researcher could define a preregistration that evaluates distributional forms of \r
variables to determine data exclusions, transformations, and appropriate model assumptions that \r
do not reveal anything about the research outcomes. After that, the researcher preregisters the \r
model most appropriate for testing the outcomes of interest. Effective application of sequential \r
preregistration is difficult in many research applications. If an earlier stage reveals information \r
about outcomes to be tested at a subsequent stage, then the preregistration is compromised. \r
A more robust option is to blind the dataset by scrambling some of the observations so \r
that distributional forms are still retained but there is no way to know the actual outcomes until \r
the dataset is unblinded (47,48). Researchers can then address outliers and modeling assumptions \r
without revealing the outcomes. Blinding can be difficult to achieve in practice depending on the \r
nature of the dataset and outcomes of interest. \r
Another method is to preregister a decision tree. The decision tree defines the sequence \r
of tests and decision rules at each stage of the sequence. For example, the decision tree might \r
specify testing a normality assumption and, depending on the outcome, selection of either a \r
parametric or nonparametric test. A decision tree is particularly useful when the range of \r
possible analyses is easily described. However, it is possible to preregister biases into decision \r
trees. For example, one could preregister testing a sequence of exclusion rules and stopping \r
when one achieves p < .05. On the positive side, this misbehavior is highly detectable; on the 

8\r
negative side, it invalidates the diagnosticity of statistical inference. Preregistration does not \r
eliminate the possibility of poor statistical practices, but it does make them detectable. \r
A final option is to establish standard operating procedures (SOPs) that accompany one \r
or many preregistrations. SOPs describe decision rules for handling observed data (49), and have \r
more general application than a decision tree. SOPs are likely to be effective for areas of \r
research with common modeling approaches with many data treatment decisions. Also, SOPs \r
can be shared across many investigations to promote standards for data analysis. SOPs have the \r
same risks as decision trees of building in biasing influences, but those are detectable and \r
avoidable. SOPs sometimes emerge as community norms or evidence-based best practices such \r
as in the standards of evidence to claim autophagy (50) or the analysis pipeline for the Implicit \r
Association Test (51). Development of community norms requires deliberate effort and \r
consensus building, but the benefits for fostering de facto preregistration are substantial if the \r
community adheres to the standards.\r
Challenge #3: Data are pre-existing. Ian uses data provided by others to conduct his \r
research. In most cases, he does not know what variables and data are available to analyze until \r
after data collection is complete. This makes it difficult to preregister according to the idealized \r
model.\r
The extent to which testing predictions is possible on pre-existing data depends on \r
whether decisions about the analysis plan are blind to the data. “Pure” preregistration is still \r
possible if no one has observed the data. For example, a paleontologist can test predictions about \r
what will be observed from fossils yet to be discovered, and an economist can create predictions \r
of government data that exist but have not been released. However, once the data have been \r
observed, there are inevitable risks for blinding. Questions to ask include: “Who has observed \r
the data?” and “What observations, summaries, or findings have been communicated, and to \r
whom?” A researcher could test predictions using a dataset that has been examined by hundreds \r
of others if the new analyst is entirely blind to what others have observed and reported. But, \r
there are lots of ways––direct and indirect––to be influenced by observed data. If the new analyst \r
read a summary report of the dataset or receives advice on how to approach the dataset by prior \r
analysts, decisions might be undesirably influenced. Likewise, knowing some outcomes might \r
influence decisions, even if the analysis is on different variables from the dataset. For example, a \r
political scientist might preregister an analysis examining the relationship between religiosity \r
and volunteerism using an existing dataset. She has never observed data for the variables of \r
interest, but she has previously observed a relationship between political ideology and charitable \r
giving. Even though she is blind to data from her selected variables, the likelihood of positive \r
correlations between ideology and religiosity and between charitable giving and volunteerism \r
damages blinding. \r
This highlights how partial blinding creates a gray area between prediction and \r
postdiction. Once definitive blindness is sacrificed, the diagnosticity of statistical inference is \r
maximized by registering analysis plans and transparently reporting what was and was not \r
known in advance about the dataset. This transparency provides insight about potential biasing 

9\r
influences for, at minimum, subjective assessment of credibility. Otherwise nothing is \r
preregistered and there is no basis to assess credibility. An effective preregistration will account \r
for any loss of blinding and what impact that could have on the reported results. \r
Challenge #4: Longitudinal studies and large, multivariate datasets. Lily leads a \r
massive project that makes yearly observations of many variables over a 20-year period. \r
Members of her lab group conduct dozens of investigations with this large dataset producing a \r
few papers each year. Lily could not have preregistered the entire design and analysis plan for all \r
future papers at project onset. Moreover, the longitudinal design amplifies the challenges of pre\u0002existing data and changes to protocols after preregistration (52,53).\r
Solutions to the first three challenges also apply to this scenario, but longitudinal data \r
collection provides some additional opportunities. Each year, some variables are newly observed. \r
Preregistrations in advance of the new observations gain some benefits of blinding. The \r
limitations of this are the same as with correlated variables in large, multivariate datasets. If \r
variables at unobserved time t+1 are highly likely to be correlated with variables at observed \r
time t, then blinding could be weakened. Likewise, the effective blinding of a particular \r
statistical test depends, in part, on what proportion of the relevant data have been observed. \r
Nevertheless, partial blinding via preregistration offers more protection than no blinding at all.\r
Challenge #5: Many experiments. Natalie’s laboratory acquires data quickly, \r
sometimes running multiple experiments per week. The notion of preregistering every \r
experiment seems highly burdensome for their efficient workflow. \r
Teams that run many experiments are often doing so in the context of a methodological \r
paradigm in which each experiment varies some key aspects of a common procedure. In this \r
situation, preregistration can be as efficient as the design of the experiments themselves. A \r
preregistration template defines the variables and parameters for the protocol, and the \r
preregistrations document which parameters will be changed or manipulated for each successive \r
experiment. \r
In some cases, data acquisition is so simple that any documentation process interferes \r
with efficiency. In such scenarios, researchers can achieve confirmatory research via replication. \r
All initial experiments are treated as exploratory research. When something of interest is \r
observed, then the initial design and analysis script become the preregistration for testing a \r
prediction by running the experiment again. Easy data acquisition is a gift for rapidly \r
establishing the reproducibility of findings.\r
Challenge #6: A program of research. Brandon’s area of research is high risk and most \r
research outcomes are null results. Every once in awhile he gets a positive result, and the \r
implications are huge. As long as he preregisters everything, Brandon can be confident in his \r
statistical inference, right? Not necessarily. Imagine, for example, that Brandon gets a positive \r
result once every 20 tries. Even though every experiment is preregistered, given the aggregate \r
program of research, it is plausible that the positive results are occurring by chance. \r
This example illustrates another key element of preregistration. Not only is it essential \r
that the analysis plan be defined blind to the research outcomes, all outcomes of that analysis 

10\r
plan must be reported in order to avoid the problem of selective reporting. Transparent reporting \r
that 1 in 20 experiments or 1 in 20 analyses yielded a positive result will help researchers \r
identify the one as a likely false positive. If the one hit is tantalizing, replication facilitates \r
confidence in the observed effect. Preregistration does not eliminate the challenge of multiple \r
comparisons or selective reporting across studies, but it does make it possible to effectively \r
correct for multiple comparisons with full reporting. Achieving this benefit requires that \r
preregistrations and the results of the analysis plans are permanently preserved and accessible for \r
review.\r
Challenge #7: Few a priori expectations. Matt does not perceive that preregistration is \r
of use to him because he considers his research to be discovery science. In most cases, his group \r
wades into new problems with very little idea of what direction it will go and the outcomes they \r
observe send them in new directions. \r
It is common to begin a research program with few predictions. It is less common for \r
research to remain entirely exploratory through a sequence of studies and authoring of a paper. \r
If the data are used to generate hypotheses rather than claiming evidence for those hypotheses, \r
then the paper may be appropriately embracing post hoc explanation to open and test new areas \r
of inquiry. However, there are reasons to believe that sometimes postdiction is recast–wittingly \r
or unwittingly–as prediction. Indeed, the ubiquity of NHST in some fields implies either that \r
researchers are mostly testing predictions, or they are misusing NHST to develop support for \r
postdictions. In exploratory or discovery research, p-values have unknown diagnosticity and \r
their use can falsely imply testing rather than generating hypotheses. Preserving diagnosticity of \r
p-values means reporting them only when testing predictions. \r
Part of the problem is that researchers are incentivized to present postdictions and \r
exploratory results as if they had expected them in advance. Hindsight bias illustrates the folly \r
of this approach. Discovery science is vitally important for identifying new avenues of what is \r
possible. But, dressing up discovery as tests of theoretical predictions undermines the credibility \r
of all science by making it impossible to distinguish hypothesis generation from hypothesis \r
testing and, consequently, calibrate the uncertainty of available evidence. \r
Preregistration benefits both exploratory research and testing predictions during the \r
iterative research process. Following tests of predictions, data can be explored without constraint \r
for discoveries that might guide planning for the next experiment. Some discoveries will result in \r
predictions worth testing. This iteration can occur between studies. A first study is preregistered \r
with a simple analysis plan, and is then mostly used for exploratory analysis to generate \r
predictions that form the basis of a preregistration for a second study. \r
It is also possible to embrace discovery in a single large study and have some hypothesis \r
testing of interesting possibilities. In a process known as cross-validation, the dataset is split in \r
two. One part is used for exploratory analysis to develop the models and predictions, the other \r
part is sealed until exploration is complete (54). Sealing a dataset and preregistering the \r
outcomes of the discovery process before unsealing converts postdictions from the initial dataset \r
to predictions for the holdout dataset. 

11\r
Some research scenarios involve few clear predictions, and it is difficult to collect enough \r
data for splitting the dataset. Unfortunately, there is no magical solution. The rules of statistical \r
inference have no empathy for how hard it is to acquire the data. When data collection is \r
difficult, progress will be slower. For some domains, the questions are important enough to \r
pursue despite the slower progress.\r
Challenge #8: Competing predictions. Rusty and Melanie are collaborating on a project \r
in which they agree on the study design and analysis plan, but they have competing predictions \r
about what will occur because of their distinct theoretical orientations. This situation is not \r
actually a challenge for preregistration. In fact, it has desirable characteristics that can lead to \r
strong inference for favoring one theory over another (55). Prediction research can hold multiple\r
predictions simultaneously. The key for effective classical inference is to have well-defined \r
questions and an analysis plan that tests those questions. \r
Challenge #9: Narrative inferences and conclusions. Alexandra preregistered her \r
study, reported all the preregistered outcomes, and clearly distinguished the outcomes of tested \r
predictions and the postdictions generated from the data. Some of her tested predictions yielded \r
more interesting or notable results than others. Naturally, her narrative focused on the more \r
interesting results. \r
One can follow all of the ideals of preregistration and still leverage chance in the \r
interpretation of results. If one conducts 10 analyses and the narrative implications of the paper \r
focus on just two of them, inferential error can increase in how the paper is applied and cited. \r
Essentially this is a circumstance of failing to correct for multiple comparisons (35). This can be \r
corrected statistically by applying alpha corrections like Bonferroni (56) such that narrative focus \r
on just positive results is not associated with inflating likelihood of false positives. But selective \r
attention and interpretation can occur, and it is difficult to address this statistically. \r
Preregistration does not prevent authors or readers taking narrative license to deviate \r
from what is justified from the evidence. How a paper is used as evidence for a phenomenon \r
may be influenced by its qualitative interpretations and conclusions beyond the quantitative \r
evidence. An unwise solution would remove narrative structure and interpretation from \r
scientific papers. Interpretation and narrative conclusions are an important stimulus in the \r
development of theory. The originator of evidence and interpretation has one view of the data \r
and its implications, but with transparency, other views and interpretations can be applied. Those \r
distinct interpretations of the same statistical evidence are a feature of science as a decentralized \r
community activity of independent observers. The influence of selective inference is detectable \r
and addressable only with transparency of the research process. \r
Making preregistration the norm\r
Despite its value for transparency, rigor, and reproducibility, the prevalence of \r
preregistration is only just starting to emerge in basic, preclinical, and applied research. \r
However, just as the present culture provides means (reasoning biases and misuse of statistics), \r
motive (publication), and opportunity (no a priori commitments to predictions) for dysfunctional 

12\r
research practices, the culture is shifting to provide means, motive, and opportunity for rigor and \r
robustness of research practices via preregistration.\r
Advancing the means for preregistration. A substantial barrier to preregistration is \r
insufficient or ineffective training of good statistical and methodological practices. Most \r
researchers embrace the norms of science and aim to do the most rigorous work that they can \r
(57). Those values are advanced with education and resources for effective preregistration in \r
one’s research domain. The reference list for this review provides a starting point, and there are \r
some education modules available online to facilitate preregistration planning. For example, \r
online courses (https://www.coursera.org/specializations/statistics, \r
https://www.coursera.org/learn/statistical-inferences), instructional guides \r
(http://help.osf.io/m/registrations/l/546603-enter-the-preregistration-challenge), criteria \r
established for preregistration badge credentials (https://osf.io/6q7nm/), and collections of \r
preregistration templates (https://osf.io/zab38/wiki/). \r
Researchers are familiar with many aspects of preregistration already because they occur \r
in other common research practices. For example, grant applications sometimes require \r
specification of the proposed methodology. Funding agencies are recognizing the value of \r
requiring more rigorous specification of the design and analysis plans––potentially achieving \r
sufficient detail to become a preregistration. Also, research domains that require submission for \r
ethics review for research on humans or animals must specify some of the research methodology \r
before conducting the research. It is only a few additional steps to incorporate analysis plans to \r
achieve an effective preregistration. Finally, thesis proposals for students often require \r
comprehensive design and analysis plans that can easily become preregistrations. Extending \r
these common practices will enable many researchers to preregister their work with small steps \r
from existing practices.\r
Advancing the motive for preregistration. If researchers behave exclusively according \r
to their ideals, then education about the value and appropriate use of preregistration might be \r
sufficient for adoption. But, relying on ideals is not sufficient. Researchers are sensitive to the \r
incentives that increase their likelihood of obtaining jobs, grants, publications, and awards. The \r
existing culture has had relatively weak incentives for research rigor and reproducibility, but this \r
is changing. Preregistration is required by U.S. law for clinical trials, and is necessary to be \r
published in International Committee of Medical Journal Editors policy§, which specifies only \r
rare exceptions for work that was not preregistered.¶ Beyond clinical trials, thousands of journals \r
and a number of funders are signatories to the Transparency and Openness Promotion (TOP) \r
Guidelines (http://cos.io/top/) that define standards for transparency and reproducibility, \r
including preregistration. As journals and funders begin to adopt expectations for preregistration, \r
researchers’ behavior will follow. Also, some journals have adopted badges for preregistration as \r
incentives for authors to get credit for having preregistered with explicit designation on the \r
published article. It is possible that such incentives will become nearly as effective as badges for \r
open data, which were associated with more than a 10-fold increase in data sharing in an initial \r
test (58). 

13\r
Other efforts incorporate incentives for preregistration into the publishing process. The \r
Preregistration Challenge (http://cos.io/prereg/) offers one thousand $1000 awards to researchers \r
that publish the results of a preregistered study (see also https://www.erpc2016.com/). A \r
publishing model called Registered Reports (http://cos.io/rr/) is offered by dozens of journals to \r
facilitate preregistration (59,60). With Registered Reports, authors submit their research question \r
and methodology to the journal for peer review prior to observing the outcomes of the research. \r
If reviewers agree that the question is sufficiently important and the methodology to test it is of \r
sufficiently high quality, then the paper is given in-principle acceptance. The researchers then \r
carry out the study and submit the final report to the journal. At second-stage review, reviewers \r
do not evaluate the perceived importance of the outcomes. Rather, they evaluate the quality of \r
study execution and adherence to the preregistered plan. In addition to the benefits of \r
preregistration, this workflow addresses selective reporting of results and facilitates improving \r
research designs during the peer review process. \r
Beyond the intrinsic value of preregistration for the quality of research, these initiatives \r
are shifting the incentives for researchers’ career interests to be more aligned with preregistration \r
as a standard activity. Already, there is evidence of some cultural shift. For example, there are \r
more than 7,000 preregistrations on the Open Science Framework (http://osf.io/) for research \r
across all areas of science.\r
Advancing the opportunity for preregistration. Existing domain-specific and domain\u0002general registries make it possible for researchers in any discipline to preregister their research. \r
The World Health Organization maintains a list of registries by nation or region \r
(http://www.who.int/ictrp/network/primary/en/) such as the largest existing registry \r
http://clinicaltrials.gov/. While focused on clinical trials in biomedicine, many of these registries \r
offer flexibility to register other kinds of research. The American Economics Association (AEA \r
RCT Registry https://www.socialscienceregistry.org/), the Registry for International \r
Development Impact Evaluations (RIDIE Registry http://ridie.3ieimpact.org/), and the Evidence \r
in Governance and Politics registry (EGAP Registry http://egap.org/content/registration) are \r
registries for economics and political science research. The Open Science Framework (OSF; \r
https://osf.io) is a domain-general registry service that provides multiple formats for \r
preregistration (http://osf.io/registries/), including the flexible and relatively comprehensive \r
Preregistration Challenge format (http://osf.io/prereg/). Finally, the website \r
http://AsPredicted.org/ provides a simple form for preregistration, but it is not itself a registry \r
because users can keep their completed forms private forever and selectively report \r
preregistrations. However, researchers can post completed forms to a registry to meet the \r
preservation and transparency standards. \r
These steps on education, incentives, and services above anticipate growth in \r
preregistration, and emergence of a research literature about preregistration to identify its \r
strengths, weaknesses, and opportunities for improvement."""

[[sections]]
number = "14"
title = "What scientific research looks like when preregistration is pervasive"
text = """
Pervasive preregistration is distant but achievable. When it occurs, generating possible \r
discoveries and testing clear predictions will both be valued and distinctly labeled. Exploratory \r
analyses and postdiction will be understood as generative events to identify what is possible, \r
encouraging follow-up research testing predictions to identify what is likely. The decline of \r
selective reporting across and within studies will increase the credibility of research evidence. To \r
get there, the research community must solve the challenge of coordinated action in a \r
decentralized system. All stakeholders in science must embrace their role in shaping the research \r
incentives for career advancement, and nudge those incentives so that what is good for science \r
and what is good for the scientist are the same thing. \r
Conclusion\r
Sometimes researchers use existing observations of nature to generate ideas about how \r
the world works. This is called postdiction. Other times, researchers have an idea about how the \r
world works and make new observations to test whether that idea is a reasonable explanation. \r
This is called prediction. To make confident inferences, it is important to know which is which. \r
Preregistration solves this challenge by requiring researchers to state how they will analyze the \r
data before they observe it, allowing them to confront a prediction with the possibility of being \r
wrong. Preregistration improves the interpretability and credibility of research findings.\r
Footnotes\r
*\r
This paper focuses on NHST because of its pervasive use (e.g., 28, 29). The opportunities and \r
challenges discussed are somewhat different with other statistical approaches such as Bayesian \r
methods. However, no statistical method on its own avoids researcher opportunity for flexibility \r
in analytical decisions, such as exclusion criteria, the creation of variables from specific \r
measurements (30).\r
✝Notably, p-values near 0.05 are not actually very unlikely in typical research practices (31), \r
leading some researchers to recommend .005 as a more stringent criterion for claiming \r
"significance" (32).\r
‡\r
The challenges of reasoning biases and distinguishing postdiction and prediction are not limited \r
to NHST, but they can manifest somewhat differently in other forms of data analysis. We focus \r
on NHST because of its common use.\r
§\r
http://www.icmje.org/recommendations/browse/publishing-and-editorial-issues/clinical-trial\u0002registration.html\r
¶\r
The standards for preregistration in clinical trials do not yet require comprehensive specification \r
of analysis plans, though they do require identification of primary and secondary outcome \r
variables.\r
Acknowledgments: This work was supported by grants from the Laura and John Arnold \r
Foundation and the National Institute of Aging."""

[[sections]]
number = "15"
title = "References"
text = """
1. Box GEP (1976) Science and Statistics. Journal of the American Statistical Association\r
71(356):791–799.\r
2. Box GEP (1979) Robustness in the Strategy of Scientific Model Building. Robustness in \r
Statistics, eds Launer RL, Wilkinson GN (Academic Press), pp 201–236.\r
3. de Groot AD (2014) The meaning of “significance” for different types of research [translated \r
and annotated by Eric-Jan Wagenmakers, Denny Borsboom, Josine Verhagen, Rogier Kievit, \r
Marjan Bakker, Angelique Cramer, Dora Matzke, Don Mellenbergh, and Han L. J. van der \r
Maas]. Acta Psychologica 148:188–194.\r
4. Hoyningen-Huene P (1987) Context of discovery and context of justification. Studies in \r
History and Philosophy of Science Part A 18(4):501–515.\r
5. Kuhn TS (1970) Logic of Discovery or Psychology of Research? Criticism and the Growth of \r
Knowledge, eds Lakatos I, Musgrave A (Cambridge University Press), pp 1-23."""

[[sections]]
number = "6"
title = "Wagenmakers E-J, Wetzels R, Borsboom D, van der Maas HLJ, Kievit RA (2012) An"
text = """
Agenda for Purely Confirmatory Research. Perspectives on Psychological Science 7(6):632–\r
638.\r
7. Forstmeier W, Wagenmakers E-J, Parker TH (2016) Detecting and avoiding likely false\u0002positive findings – a practical guide. Biol Rev, doi:10.1111/brv.12315.\r
8. Munafò MR, et al. (2017) A manifesto for reproducible science. Nature Human Behaviour\r
1(1):s41562-16-21–16.\r
9. Nosek BA, Spies JR, Motyl M (2012) Scientific Utopia: II. Restructuring Incentives and \r
Practices to Promote Truth Over Publishability. Perspectives on Psychological Science\r
7(6):615–631.\r
10. Open Science Collaboration (2015) Estimating the reproducibility of psychological science. \r
Science 349(6251):aac4716.\r
11. Swaen GG, Teggeler O, Amelsvoort V, Gpm L (2001) False positive outcomes and design \r
characteristics in occupational cancer epidemiology studies. Int J Epidemiol 30(5):948–954.\r
12. Kerr NL (1998) HARKing: Hypothesizing After the Results are Known. Personality and \r
Social Psychology Review 2(3):196–217.\r
13. Fischhoff B, Beyth R (1975) I knew it would happen. Organizational Behavior and Human \r
Performance 13(1):1–16.

16"""

[[sections]]
number = "14"
title = "Fischhoff B (2003) Hindsight ≠ foresight: the effect of outcome"
text = """
knowledge on judgment under uncertainty. Quality and Safety in Health Care\r
12(4):304–311."""

[[sections]]
number = "15"
title = "Lewis M (2016) The Undoing Project: A Friendship that Changed our Minds, (W. W."
text = """
Norton & Company, New York).\r
16. Hawkins CB, Nosek BA (2012) Motivated Independence? Implicit Party Identity Predicts \r
Political Judgments Among Self-Proclaimed Independents. Personality and Social Psychology \r
Bulletin 38(11):1437–1452.\r
17. Kahneman D, Slovic P, Tversky A (1982) Judgment Under Uncertainty: Heuristics and \r
Biases (Cambridge University Press)."""

[[sections]]
number = "18"
title = "Kahneman D (2011) Thinking, Fast and Slow (Macmillan)."
text = """
19. Bakker M, van Dijk A, Wicherts JM (2012) The Rules of the Game Called Psychological \r
Science. Perspectives on Psychological Science 7(6):543–554.\r
20. Giner-Sorolla R (2012) Science or Art? How Aesthetic Standards Grease the Way Through \r
the Publication Bottleneck but Undermine Science. Perspectives on Psychological Science\r
7(6):562–571.\r
21. Mahoney MJ (1977) Publication prejudices: An experimental study of confirmatory bias in \r
the peer review system. Cogn Ther Res 1(2):161–175.\r
22. Christensen-Szalanski JJJ, Willham CF (1991) The hindsight bias: A meta-analysis. \r
Organizational Behavior and Human Decision Processes 48(1):147–168."""

[[sections]]
number = "23"
title = "Kunda Z (1990) The case for motivated reasoning. Psychological Bulletin"
text = """
108(3):480–498.\r
24. Nickerson RS (1998) Confirmation bias: A ubiquitous phenomenon in many guises. Review \r
of General Psychology 2(2):175–220.\r
25. Nosek BA, Riskind RG (2012) Policy Implications of Implicit Social Cognition. Social \r
Issues and Policy Review 6(1):113–147.\r
26. Pronin E, Kugler MB (2007) Valuing thoughts, ignoring behavior: The introspection illusion \r
as a source of the bias blind spot. Journal of Experimental Social Psychology 43(4):565–578.\r
27. Sellke T, Bayarri MJ, Berger JO (2001) Calibration of ρ Values for Testing Precise Null \r
Hypotheses. The American Statistician 55(1):62–71.

17\r
28. Hubbard R, Ryan PA (2000) Statistical Significance with Comments by Editors of Marketing \r
Journals: The Historical Growth of Statistical Significance Testing in Psychology—and its \r
Future Prospects. Educational and Psychological Measurement 60(5):661–681.\r
29. Stephens PA, Buskirk SW, del Rio CM (2007) Inference in ecology and evolution. Trends in \r
Ecology & Evolution 22(4):192–197.\r
30. Simmons JP, Nelson LD, Simonsohn U (2011) False-Positive Psychology: Undisclosed \r
Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant. \r
Psychological Science 22(11):1359–1366.\r
31. Wasserstein RL, Lazar NA (2016) The ASA’s Statement on p-Values: Context, Process, and \r
Purpose. The American Statistician 70(2):129–133.\r
32. Benjamin DJ, et al. (in press) Redefine statistical significance. Nature Human Behavior\r
doi:10.17605/OSF.IO/MKY9J.\r
33. Dunnett CW (1955) A Multiple Comparison Procedure for Comparing Several Treatments \r
with a Control. Journal of the American Statistical Association 50(272):1096–1121.\r
34. Tukey JW (1949) Comparing Individual Means in the Analysis of Variance. Biometrics\r
5(2):99–114.\r
35. Benjamini Y (2010) Simultaneous and selective inference: Current successes and future \r
challenges. Biom J 52(6):708–721."""

[[sections]]
number = "36"
title = "Saxe R, Brett M, Kanwisher N (2006) Divide and conquer: A defense of functional"
text = """
localizers. NeuroImage 30(4):1088–1096.\r
37. Gelman A, Loken E (2014) The Statistical Crisis in Science. American Scientist 102(6):460.\r
38. Franco A, Malhotra N, Simonovits G (2014) Publication bias in the social sciences: \r
Unlocking the file drawer. Science 345(6203):1502–1505.\r
39. Greenwald AG (1975) Consequences of prejudice against the null hypothesis. Psychological \r
Bulletin 82(1):1–20.\r
40. Rosenthal R (1979) The file drawer problem and tolerance for null results. Psychological \r
Bulletin 86(3):638–641.\r
41. Ioannidis JPA (2005) Why Most Published Research Findings Are False. PLOS Medicine\r
2(8):e124.

18\r
42. John LK, Loewenstein G, Prelec D (2012) Measuring the Prevalence of Questionable \r
Research Practices With Incentives for Truth Telling. Psychological Science 23(5):524–532.\r
43. Kaplan RM, Irvin VL (2015) Likelihood of Null Effects of Large NHLBI Clinical Trials Has \r
Increased over Time. PLOS ONE 10(8):e0132382.\r
44. Cybulski L, Mayo-Wilson E, Grant S (2016) Improving transparency and reproducibility \r
through registration: The status of intervention trials published in clinical psychology journals. \r
Journal of Consulting and Clinical Psychology 84(9):753–767.\r
45. Odutayo A, et al. (2017) Association between trial registration and positive study findings: \r
cross sectional study (Epidemiological Study of Randomized Trials—ESORT). BMJ 356:j917.\r
46. Armitage P, McPherson CK, Rowe BC (1969) Repeated Significance Tests on Accumulating \r
Data. Journal of the Royal Statistical Society Series A (General) 132(2):235–244.\r
47. Dutilh G, et al. (2017) A test of the diffusion model explanation for the worst performance \r
rule using preregistration and blinding. Atten Percept Psychophys 79(3):713–725.\r
48. MacCoun R, Perlmutter S (2015) Blind analysis: Hide results to seek the truth. Nature\r
526(7572):187–189.\r
49. Lin W, Green DP (2016) Standard Operating Procedures: A Safety Net for Pre-Analysis \r
Plans. PS: Political Science & Politics 49(3):495–500.\r
50. Klionsky DJ, et al. (2016) Guidelines for the use and interpretation of assays for monitoring \r
autophagy (3rd edition). Autophagy 12(1):1–222."""

[[sections]]
number = "51"
title = "Greenwald AG, Nosek BA, Banaji MR (2003) Understanding and using the Implicit"
text = """
Association Test: I. An improved scoring algorithm. Journal of Personality and Social \r
Psychology 85(2):197–216.\r
52. Campbell L, Loving TJ, Lebel EP (2014) Enhancing transparency of the research process to \r
increase accuracy of findings: A guide for relationship researchers. Pers Relationship 21(4):531–\r
545.\r
53. Cockburn A (2017) Long-term data as infrastructure: a comment on Ihle et al. Behav Ecol\r
28(2):357–357.\r
54. Fafchamps M, Labonne J (2016) Using Split Samples to Improve Inference about Causal \r
Effects (National Bureau of Economic Research) doi:10.3386/w21842."""

[[sections]]
number = "55"
title = "Platt JR (1964) Strong Inference. Science 146(3642):347–353."
text = """
19\r
56. Holm S (1979) A Simple Sequentially Rejective Multiple Test Procedure. Scandinavian \r
Journal of Statistics 6(2):65–70.\r
57. Anderson MS, Martinson BC, De Vries R (2007) Normative Dissonance in Science: Results \r
from a National Survey of U.S. Scientists. Journal of Empirical Research on Human Research \r
Ethics 2(4):3–14.\r
58. Kidwell MC, et al. (2016) Badges to Acknowledge Open Practices: A Simple, Low-Cost, \r
Effective Method for Increasing Transparency. PLOS Biology 14(5):e1002456.\r
59. Chambers CD, Feredoes E, Muthukumaraswamy SD, Etchells P (2014) Instead of “playing \r
the game” it is time to change the rules: Registered Reports at AIMS Neuroscience and beyond. \r
AIMS Neuroscience 1(1):4–17."""

[[sections]]
number = "60"
title = "Nosek BA, Lakens D (2014) Registered Reports. Social Psychology 45(3):137–141."
text = ""
