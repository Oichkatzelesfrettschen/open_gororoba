equations = []
tables = []
full_text = """
Elise van der Pol\r
Symmetry and Structure\r
in\r
Deep Reinforcement Learning\r
Universiteit van Amsterdam

This book was typeset by the author using LATEX and the Tufte ebook\r
template. The cover was designed by the author in Inkscape, adapted\r
from a generated image.\r
Printing: www.proefschriftmaken.nl.\r
Copyright © 2023 Elise van der Pol\r
ISBN: 978-94-6469-413-0

Symmetry and Structure in Deep Reinforcement Learning\r
ACADEMISCH PROEFSCHRIFT\r
ter verkrijging van de graad van doctor\r
aan de Universiteit van Amsterdam\r
op gezag van de Rector Magnificus\r
prof. dr. ir. P.P.C.C. Verbeek\r
ten overstaan van een door het College voor Promoties ingestelde commissie,\r
in het openbaar te verdedigen in de Agnietenkapel\r
op woensdag 12 juli 2023, te 16.00 uur\r
door Elise Esmeralda van der Pol\r
geboren te Haarlem

Promotiecommissie\r
Promotor: prof. dr. M. Welling Universiteit van Amsterdam\r
Copromotores: dr. H.C. van Hoof Universiteit van Amsterdam\r
dr. F.A. Oliehoek Technische Universiteit Delft\r
Overige leden: prof. dr. M. de Rijke Universiteit van Amsterdam\r
prof. dr. E. Kanoulas Universiteit van Amsterdam\r
dr. ir. E.J. Bekkers Universiteit van Amsterdam\r
dr. D. Precup McGill University\r
prof. dr. J. Peters Technische Universität Darmstadt\r
Faculteit der Natuurwetenschappen, Wiskunde en Informatica

Voor Bibi\r
and we pour bag after bag\r
of leaves on the lawn,\r
waiting for them to leap\r
onto the bare branches.\r
— Matt Rasmussen

vii\r
Contents\r
1 Introduction 1\r
1.1 List of Publications 6\r
Part I Symmetry 9\r
2 MDP Homomorphic Networks:\r
Group Symmetries in Reinforcement Learning 11\r
2.1 Introduction 11\r
2.2 Background 14\r
2.3 Method 18\r
2.4 Experiments 21\r
2.5 Related Work 25\r
2.6 Conclusion 26\r
2.7 Broader Impact Statement 26\r
Appendices 29\r
2.A The Symmetrizer 29\r
2.B Experimental Settings 32\r
2.C Breakout Experiments 39\r
2.D Cartpole-v1 Deeper Network Results 40\r
2.E Bellman Equations 40\r
3 Multi-Agent MDP Homomorphic Networks 41\r
3.1 Introduction 41

viii\r
3.2 Related Work 43\r
3.3 Background 44\r
3.4 Distributing Symmetries over Multiple Agents 45\r
3.5 Experiments 52\r
3.6 E(3) Equivariance 55\r
3.7 Conclusion 56\r
3.8 Ethics Statement 57\r
3.9 Reproducibility Statement 57\r
Appendices 59\r
3.A Message Passing Networks, Communication, and Distribution 59\r
3.B Equivariance of Proposed Message Passing Layers 60\r
3.C Discrete Rotations of Continuous Vectors 61\r
3.D Experimental Details 61\r
3.E Architectural details 62\r
Part II Structure 69\r
4 Plannable Approximations to MDP Homomorphisms 71\r
4.1 Introduction 71\r
4.2 Background 73\r
4.3 Learning MDP Homomorphisms 75\r
4.4 Experiments 80\r
4.5 Related Work 88\r
4.6 Relation to Group Equivariance 89\r
4.7 Conclusion 89\r
5 Learning Factored Representations of Markov Decision Processes 91\r
5.1 Introduction 91\r
5.2 Background 93\r
5.3 Structured World Models 93\r
5.4 Transition Model 95\r
5.5 Related Work 96\r
5.6 Experiments 97\r
5.7 Conclusions 101

ix\r
6 Conclusion 103\r
7 Acknowledgments 109\r
8 Summary 131\r
9 Samenvatting - Dutch Summary 133

1\r
1\r
Introduction

2\r
Symmetry and structure are everywhere in the world. When we walk,\r
the movement of our right leg mirrors that of our left leg. When\r
molecules are rotated, their molecular properties are unchanged. When\r
we navigate to a destination, we take the connectivity of different road\r
segments into account. When we talk, we can string words together to\r
form completely novel sentences. In every day life, we use information\r
about the symmetry and structure of our tasks to guide our decision\r
making.\r
In Artificial Intelligence, symmetries and structure are also ubiquitous.\r
Consider a robot that mirrors its left and right leg movements during\r
locomotion, automated chip design, a drone swarm tracking wildlife\r
movement, a bot playing Atari Pong where the top and bottom part\r
of the screen are reflections of each other, molecular design, a com\u0002puter player considering rotated board states in the game of Go, and\r
autonomous vehicles switching from the right side of the road in the\r
Netherlands to the left side of the road in the UK. These are all exam\u0002ples of tasks within AI that exhibit some form of symmetry or struc\u0002ture. Leveraging knowledge of inherent symmetry and structure is an\r
important step towards building systems that scale.\r
Reinforcement learning is a fundamental field of study in Artificial In\u0002telligence that encourages artificial agents to learn from positive and\r
negative feedback signals, which we call rewards. By trial-and-error,\r
the agent can learn to associate situations, actions, and feedback in\r
order to improve its decisions. For example, we can give a robot a\r
positive reward for walking fast and a negative reward for falling over.\r
Similarly, we can give a computer player a positive reward for winning\r
a game, and a negative reward for losing a game, or give a positive re\u0002ward to an agent that proposes a particularly efficient chip design.\r
Using concepts from the field of reinforcement learning, we can for\u0002malize the examples above in order to propose approaches that lead to\r
good decision making from an agent. In deep reinforcement learning, an\r
agent uses neural networks to decide on which action to take, where\r
the neural networks are adapted to the task using the received reward\r
signals. However, even tasks that require intelligence far below human\r
capabilities can present issues to artificial decision makers. Consider\r
any vision-based control system acting in the real world. The agent re\u0002ceives observations as camera input, and has to learn the best action to\r
take. The number of possible observations is prohibitively large, and it\r
is unlikely that the agent will encounter two states that are exactly the\r
same. As such, we would like to the agent to be able to re-use expe\u0002rience from earlier states to take good decisions in unseen states with\r
similar characteristics. For example, when deciding how to move its

3\r
left leg, the agent should mirror the movements it learned for moving\r
its right leg.\r
The examples above are a few of the cases where symmetry and struc\u0002ture appear in reinforcement learning problems. These can be formal\u0002ized by considering when taking an action in a state is equivalent to\r
taking another action in another state. In this thesis, we will study how\r
we can use symmetry and structure in reinforcement learning when it\r
is known, and how we can extract it if it is not.\r
An agent should not learn what is already known. Whether knowl\u0002edge is provided by a system designer as prior knowledge or obtained\r
by the agent itself through generalization should depend on the con\u0002text of the problem. By properly re-using knowledge, we can reduce\r
the number of times the agent needs to interact with the world, an\r
essential part of scaling to real world settings. In this thesis, we will\r
particularly look at symmetry and structure in reinforcement learning.\r
As such, our main research question is:\r
Main Research Question: How can we incorporate and extract\r
symmetry and structure in reinforcement learning?\r
We first consider the case where there is an obvious symmetry in the\r
problem we are trying to solve. Many problems exhibit symmetry,\r
since symmetry is a natural part of the physical world. To illustrate,\r
consider the classic pole balancing task. In pole balancing, the agent\r
controls a cart that can move left or right. Her goal is to move the cart\r
left and right to ensure that a pole standing upright on the cart does\r
not fall down. It does not matter if the pole is currently leaning to the\r
left, and the car is moving right, or the pole is leaning right and the\r
cart is moving left. Both cases are mirrored versions of the same un\u0002derlying situation. An agent moving east to reach a goal on the east,\r
or north to reach a goal in the north, are also two very similar situa\u0002tions. These problems and those mentioned above have something in\r
common: they exhibit equivalences between different pairs of states\r
and actions. The beauty of state-action equivalence is that it allows us\r
to consider similarity between taking one action in one state and tak\u0002ing another action in another state. This is what allows us to express\r
symmetry in reinforcement learning problems. In Chapter 2 we con\u0002sider the problem of constraining the class of neural network policies\r
to only those that are symmetric under certain transformations, and\r
answer the first research question:

4\r
Research Question 1: How can we leverage knowledge of symmetries\r
in deep reinforcement learning?\r
Our main contribution in Chapter 2 is proposing MDP Homomorphic\r
Networks, a class of neural networks that incorporate reinforcement\r
learning symmetries into neural networks. This approach bridges\r
deep reinforcement learning and equivariant networks, and shows a\r
substantial improvement in data efficiency compared to unstructured\r
baselines. Additionally, we propose a new method for constructing\r
equivariant neural network weights.\r
After considering the case of symmetric reinforcement learning tasks,\r
we investigate the use of similar methods in a more complex setting:\r
that of distributed cooperative multi-agent systems. In such settings,\r
the task at hand must be solved by agents taking local actions and\r
communicating locally with each other. In Chapter 3 we answer the\r
second research question:\r
Research Question 2. How can we leverage knowledge of global\r
symmetries in distributed cooperative multi-agent systems?\r
A straightforward approach to this problem would be a naive applica\u0002tion of single agent approaches to symmetry in reinforcement learning.\r
However, such a method would prohibit us from using distributed exe\u0002cution methods. Instead, we propose an equivariant distributed policy\r
network that allows the policy to be distributed at execution time, re\u0002quiring only local communication and computation to ensure global\r
symmetries. This approach results in improved data efficiency in sym\u0002metric cooperative multi-agent problems.\r
In the second part of this thesis, we consider that there is usually some\r
underlying structure in the unstructured information we receive. For\r
example, objects of the same type tend to behave similarly when force\r
is applied to them. Similarly, there are dynamics underlying a sim\u0002ple pole balancing system. An agent may observe only feature vec\u0002tors, and not the equations governing the system. Finally, the space\r
of possible images of 3 × 48 × 48 pixels is significantly larger than the\r
number of states in a simple grid world. However, an agent does not\r
a priori know the number of possible actual states. It only knows that\r
it receives images of a certain size. In the second part of this thesis,\r
we consider how to extract structure from interaction data. First, in\r
Chapter 4, we consider the problem of learning representations of de\u0002cision making problems that are plannable. By plannable, we mean

5\r
that if we run standard planning algorithms on the graph of learned\r
representations, we get good decision making strategies for the orig\u0002inal problem. In practice, this means that we want the dynamics of\r
the transition function in the original problem to be mirrored by the\r
dynamics of the latent transition function. Our 3rd research question\r
is therefore:\r
Research Question 3. How can we learn representations of the world\r
that capture the structure of the environment?\r
In Chapter 4, we take cues from the equivariance literature [34, 193,\r
187] and contrastive learning [135, 94] to learn the abstract graph un\u0002derlying a decision making problem. Noting that the effect of an action\r
in the decision making problem should be matched by the effect on the\r
abstract graph, we learn representations of the original problem and\r
show that they are indeed plannable for a variety of problems, includ\u0002ing continuous state spaces, and generalizing to unseen objects and\r
goals. This action-equivariant planning approach results in improved\r
data efficiency compared to model-free baselines and reconstruction\r
baselines.\r
In Chapter 5, we consider a further structure in the problem: if we have\r
a set of objects that can be acted upon individually, each state is itself\r
structured. If we can recover the individual objects and a factored\r
transition function from pixel observations, this can improve predic\u0002tion and generalization. As such, our 4th research question is:\r
Research Question 4. How can we learn representations of the world\r
that capture the structure in individual states?\r
In Chapter 5 we show that it is possible to find object-oriented repre\u0002sentations from pixels based on factored actions and a fixed dataset of\r
interactions. We propose an approach that uses contrastive learning\r
on environment interaction samples, resulting in a structured repre\u0002sentation that leverages a graph neural network transition function.\r
The approach we propose improves prediction performance in latent\r
space compared to unstructured and reconstruction baselines.\r
In this thesis, we explore symmetry and structure in deep reinforce\u0002ment learning. We leverage prior knowledge of symmetries in single\r
agent and multi-agent systems in Chapters 2- 3, and extract structure\r
from interactions with the world in Chapters 4-5. We will conclude in

6\r
Chapter 6 and provide suggestions for future work.\r
1.1 List of Publications\r
The following publications form the basis of this thesis:\r
• Elise van der Pol, Thomas Kipf, Frans A. Oliehoek, Max Welling\r
(2020). "Plannable Approximations to MDP Homomorphisms: Equiv\u0002ariance under Actions." In: International Conference on Autonomous\r
Agents and Multi-Agent Systems (AAMAS) [170]. Chapter 4.\r
• Elise van der Pol, Daniel E. Worrall, Herke van Hoof, Frans A.\r
Oliehoek, Max Welling (2020). "MDP Homomorphic Networks:\r
Group Symmetries in Reinforcement Learning." In: Advances in\r
Neural Information Processing Systems (NeurIPS) [173]. Chapter 2.\r
• Elise van der Pol, Herke van Hoof, Frans A. Oliehoek, Max Welling\r
(2021). "Multi-Agent MDP Homomorphic Networks." In: Interna\u0002tional Conference on Learning Representations (ICLR) [172]. Chap\u0002ter 3.\r
• Thomas Kipf, Elise van der Pol, Max Welling (2019). "Contrastive\r
Learning of Structured World Models." In: International Conference\r
on Learning Representations (ICLR) [94]. Chapter 5.\r
• Johannes Brandstetter, Rob Hesselink, Elise van der Pol, Erik J.\r
Bekkers, Max Welling (2021). "Geometric and Physical Quantities\r
improve E(3) Equivariant Message Passing." In: International Con\u0002ference on Learning Representations (ICLR) [23]. Chapter 3.\r
I have contributed in all aspects to all first author publications listed.\r
Max Welling, Frans Oliehoek, and Herke van Hoof provided supervi\u0002sion, guidance, insight, and technical advice. In "MDP Homomorphic\r
Networks: Group Symmetries in Reinforcement Learning." Daniel Wor\u0002rall proposed the Symmetrizer, which was implemented by us jointly.\r
In "Contrastive Learning of Structured World Models." Thomas Kipf\r
contributed in all aspects. I provided reinforcement learning insights,\r
proposed the use of action factorization and the evaluation method,\r
and ran baseline experiments. Figures and tables reproduced with per\u0002mission. In "Geometric and Physical Quantities improve E(3) Equivari\u0002ant Message Passing.", Johannes Brandstetter, Rob Hesselink, and Erik\r
Bekkers contributed in all aspects. I provided the original architectural

7\r
idea and took an advisory role. Figures reproduced with permission.\r
I have further contributed to:\r
• Tejaswi Kasarla, Gertjan J. Burghouts, Max van Spengler, Elise van\r
der Pol, Rita Cucchiara, Pascal Mettes (2022). "Maximum Class Sep\u0002aration as Inductive Bias in One Matrix." Accepted to: Neural Infor\u0002mation Processing Systems (NeurIPS) [87].\r
• Darius Muglich, Christian Schroeder de Witt, Elise van der Pol,\r
Shimon Whiteson, Jakob Foerster (2022). "Equivariant Networks for\r
Zero-Shot Coordination." Accepted to: Neural Information Process\u0002ing Systems (NeurIPS).\r
• Elise van der Pol, Ian Gemp, Yoram Bachrach, Richard Everett\r
(2022). "Stochastic Parallelizable Eigengap Dilation for Large Graph\r
Clustering". In: ICML 2022 Workshop on Topology, Algebra, and\r
Geometry in Machine Learning [169].\r
• Pascal Mettes, Elise van der Pol, Cees G.M. Snoek (2019). "Hyper\u0002spherical Prototype Networks." In: Advances in Neural Information\r
Processing Systems (NeurIPS) [120].\r
• Ondrej Biza, Elise van der Pol, Thomas Kipf (2021). "The Impact\r
of Negative Sampling on Contrastive Structured World Models." In:\r
ICML Workshop on Self-Supervised Learning for Reasoning and\r
Perception [17].\r
• Laurens Weitkamp, Elise van der Pol, Zeynep Akata (2018). "Visual\r
Rationalizations in Deep Reinforcement Learning for Atari Games."\r
In: Benelux Conference on Artificial intelligence (BNAIC) [190].\r
• Frans A. Oliehoek, Rahul Savani, Jose Gallego-Posada, Elise van\r
der Pol, Roderich Groß (2018). "Beyond Local Nash Equilibria for\r
Adversarial Networks." In: Annual Machine Learning Conference\r
of Belgium and the Netherlands (Benelearn) [134].

9\r
Part I\r
Symmetry

11\r
2\r
MDP Homomorphic Networks:\r
Group Symmetries in\r
Reinforcement Learning\r
2.1 Introduction\r
This part of the dissertation leverages knowledge of symmetries in sin\u0002gle and multi-agent reinforcement learning problems. In this Chapter,\r
we propose MDP homomorphic networks, which enforce symmetries\r
in reinforcement learning problems. In the following chapter we take\r
this approach beyond single-agent symmetries.\r
This chapter introduces MDP homomorphic networks for deep rein\u0002forcement learning. MDP homomorphic networks are neural networks\r
that are equivariant under symmetries in the joint state-action space of\r
an MDP. Current approaches to deep reinforcement learning do not\r
usually exploit knowledge about such structure. By building this prior\r
knowledge into policy and value networks using an equivariance con\u0002straint, we can reduce the size of the solution space. We specifically fo\u0002cus on group-structured symmetries (invertible transformations). Ad\u0002ditionally, we introduce an easy method for constructing equivariant\r
network layers numerically, so the system designer need not solve the\r
equivariance constraints by hand, as is typically done. We construct\r
MDP homomorphic MLPs and CNNs that are equivariant under ei-

12\r
ther a group of reflections or rotations. We show that such networks\r
converge faster than unstructured baselines on CartPole, a grid world\r
and Pong.\r
This chapter considers learning decision-making systems that exploit\r
symmetries in the structure of the world. Deep reinforcement learning\r
(DRL) is concerned with learning neural function approximators for\r
decision making strategies. While DRL algorithms have been shown\r
to solve complex, high-dimensional problems [156, 153, 125, 124], they\r
are often used in problems with large state-action spaces, and thus\r
require many samples before convergence. Many tasks exhibit sym\u0002metries, easily recognized by a designer of a reinforcement learning\r
system. Consider the classic control task of balancing a pole on a cart.\r
Balancing a pole that falls to the right requires an equivalent, but mir\u0002rored, strategy to one that falls to the left. See Figure 2.1. In this\r
chapter, we exploit knowledge of such symmetries in the state-action\r
space of Markov decision processes (MDPs) to reduce the size of the\r
solution space.\r
We use the notion of MDP homomorphisms [143, 141] to formalize these\r
symmetries. Intuitively, an MDP homomorphism is a map between\r
MDPs, preserving the essential structure of the original MDP, while\r
removing redundancies in the problem description, i.e., equivalent\r
state-action pairs. The removal of these redundancies results in a\r
smaller state-action space, upon which we may more easily build a\r
policy. While earlier work has been concerned with discovering an\r
MDP homomorphism for a given MDP [143, 141, 127, 142, 16, 170],\r
we are instead concerned with how to construct deep policies, satisfy\u0002ing the MDP homomorphism. We call these models MDP homomorphic\r
networks.\r
MDP homomorphic networks use experience from one state-action\r
pair to improve the policy for all ‘equivalent’ pairs. See Section 2.2\r
for a definition. They do this by tying the weights for two states if\r
they are equivalent under a transformation chosen by the designer,\r
such as s and L[s] in Figure 2.1.\r
Such weight-tying follows a similar principle to the use of convo\u0002lutional networks [109], which are equivariant to translations of the\r
input [34]. In particular, when equivalent state-action pairs can be\r
related by an invertible transformation, which we refer to as group\u0002structured, we show that the policy network belongs to the class of\r
group-equivariant neural networks [34, 193]. Equivariant neural networks\r
are a class of neural network, which have built-in symmetries [34,

13\r
Figure 2.1: Example state-action\r
space symmetry, where L is a\r
horizontal reflection. The pairs\r
(s, ←) and (L[s], →) (and by ex\u0002tension (s, →) and (L[s], ←)) are\r
symmetric under a horizontal\r
flip. Constraining the set of poli\u0002cies to those where π(s, ←) =\r
π(L[s], →) reduces the size of\r
the solution space.\r
35, 193, 188, 186]. They are a generalization of convolutional neu\u0002ral networks—which exhibit translation symmetry—to transformation\r
groups (group-structured equivariance) and transformation semigroups\r
[194] (semigroup-structured equivariance). They have been shown to\r
reduce sample complexity for classification tasks [193, 191] and also to\r
be universal approximators of symmetric functions1[198]. We borrow\r
1 Specifically group equivariant net\u0002works are universal approximators to\r
functions symmetric under linear repre\u0002sentations of compact groups.\r
from the literature on group equivariant networks to design policies\r
that tie weights for state-action pairs given their equivalence classes,\r
with the goal of reducing the number of samples needed to find good\r
policies. Furthermore, we can use the MDP homomorphism property\r
to design not just policy networks, but also value networks and even\r
environment models. MDP homomorphic networks are agnostic to the\r
type of model-free DRL algorithm, as long as an appropriate transfor\u0002mation on the output is given. In this chapter we focus on equivariant\r
policy and invariant value networks. See Figure 2.1 for an example\r
policy.\r
An additional contribution of this chapter is a novel numerical way\r
of finding equivariant layers for arbitrary transformation groups. The\r
design of equivariant networks imposes a system of linear constraint\r
equations on the linear/convolutional layers [35, 34, 193, 188]. Solving\r
these equations has typically been done analytically by hand, which\r
is a time-consuming and intricate process, barring rapid prototyp\u0002ing. Rather than requiring analytical derivation, our method only re\u0002quires that the system designer specify input and output transforma\u0002tion groups of the form {state transformation, policy transformation}.

14\r
We provide Pytorch [138] implementations of our equivariant network\r
layers, and implementations of the transformations used in this chap\u0002ter. We also experimentally demonstrate that exploiting equivalences\r
in MDPs leads to faster learning of policies for DRL.\r
Our contributions are two-fold:\r
• We draw a connection between MDP homomorphisms and group\r
equivariant networks, proposing MDP homomorphic networks to\r
exploit symmetries in decision-making problems;\r
• We introduce a numerical algorithm for the automated construction\r
of equivariant layers.\r
2.2 Background\r
Here we outline the basics of the theory behind MDP homomorphisms\r
and equivariance. We begin with a brief outline of the concepts of\r
equivalence, invariance, and equivariance, followed by a review of the\r
Markov decision process (MDP). We then review the MDP homomor\u0002phism, which builds a map between ‘equivalent’ MDPs.\r
Equivalence, Invariance, and Equivariance\r
Equivalence If a function f : X → Y maps two inputs x, x\r
0 ∈ X\r
to the same value, that is f(x) = f(x\r
0\r
), then we say that x and x\r
0\r
are f-equivalent. For instance, two states s,s\r
0\r
leading to the same\r
optimal value V\r
∗\r
(s) = V\r
∗\r
(s\r
0\r
) would be V\r
∗\r
-equivalent or optimal value\r
equivalent [141]. An example of two optimal value equivalent states\r
would be states s and L[s] in the CartPole example of Figure 2.1. The\r
set of all points f-equivalent to x is called the equivalence class of x.\r
Groups A group (G, ·) is a set G, together with a binary operator ·,\r
which is closed, i.e. ∀g1, g2 ∈ G, g1 · g2 ∈ G. The group (G, ·) obeys\r
the group axioms:\r
• Associativity: For all g1, g2, g3 ∈ G, (g1 · g2) · g3 = g1 · (g2 · g3);\r
• Identity: There is an element e ∈ G such that for all g1 ∈ G, e · g1 =\r
g1 · e = g1;\r
• Invertibility: For all g1 ∈ G, there exists an element g\r
−1\r
1 ∈ G such

15\r
that g1 · g\r
−1\r
1 = g\r
−1\r
1\r
· g1 = e.\r
Invariance and Symmetries Typically there exist very intuitive rela\u0002tionships between the points in an equivalence class. In the Cart\u0002Pole example of Figure 2.1 this relationship is a horizontal flip about\r
the vertical axis. This is formalized with the transformation operator\r
Lg : X → X , where g ∈ G and G is a mathematical group. If Lg\r
satisfies\r
f(x) = f(Lg[x]), for all g ∈ G, x ∈ X , (2.1)\r
then we say that f is invariant or symmetric to Lg and that {Lg}g∈G is a\r
set of symmetries of f . We can see that for the invariance equation to be\r
satisfied, it must be that Lg can only map x to points in its equivalence\r
class. Note that in abstract algebra for Lg to be a true transformation\r
operator, G must contain an identity operation; that is Lg[x] = x for\r
some g and all x. An interesting property of transformation operators\r
which leave f invariant, is that they can be composed and still leave\r
f invariant, so Lg ◦ Lhis also a symmetry of f for all g, h ∈ G. In\r
abstract algebra, this property is known as a semigroup property. If\r
Lg is always invertible, this is called a group property. In this work, we\r
experiment with group-structured transformation operators. For more\r
information, see [42]. Another helpful concept is that of orbits. If f is\r
invariant to Lg, then it is invariant along the orbits of G. The orbit\r
Ox of point x is the set of points reachable from x via transformation\r
operator Lg:\r
Ox , {Lg[x] ∈ X |g ∈ G}. (2.2)\r
Equivariance A related notion to invariance is equivariance. Given a\r
transformation operator Lg : X → X and a mapping f : X → Y, we\r
say that f is equivariant [34, 193] to the transformation if there exists\r
a second transformation operator Kg : Y → Y in the output space of f\r
such that\r
Kg[ f(x)] = f(Lg[x]), for all g ∈ G, x ∈ X . (2.3)\r
The operators Lg and Kg can be seen to describe the same transforma\u0002tion, but in different spaces. In fact, an equivariant map can be seen\r
to map orbits to orbits. We also see that invariance is a special case\r
of equivariance, if we set Kg to the identity operator for all g. Given\r
Lg and Kg, we can solve for the collection of equivariant functions f\r
satisfying the equivariance constraint. Moreover, for linear transfor\u0002mation operators and linear f a rich theory already exists in which\r
f is referred to as an intertwiner [35]. In the equivariant deep learn\u0002ing literature, neural networks are built from interleaving intertwiners

16\r
and equivariant nonlinearities. As far as we are aware, most of these\r
methods are hand-designed per pair of transformation operators, with\r
the exception of [40]. In this chapter, we introduce a computational\r
method to solve for intertwiners given a pair of transformation opera\u0002tors.\r
Markov Decision Processes\r
A Markov decision process (MDP) is a tuple (S, A, R, T, γ), with state\r
space S, action space A, immediate reward function R : S × A → R, tran\u0002sition function T : S × A × S → R≥0, and discount factor γ ∈ [0, 1]. The\r
goal of solving an MDP is to find a policy π ∈ Π, π : S × A → R≥0\r
(written π(a|s)), where π normalizes to unity over the action space,\r
that maximizes the expected return Rt = Eπ[∑\r
T\r
k=0\r
γ\r
k\r
rt+k+1]. The ex\u0002pected return from a state s under a policy π is given by the value\r
function Vπ. A related object is the Q-value Qπ, the expected return\r
from a state s after taking action a under π. V\r
π and Qπ are governed\r
by the well-known Bellman equations [15] (see Supplementary). In an\r
MDP, optimal policies π\r
∗ attain an optimal value V∗ and correspond\u0002ing Q-value given by V\r
∗\r
(s) = max\r
π∈Π\r
V\r
π(s) and Q∗\r
(s) = max\r
π∈Π\r
Qπ(s).\r
Figure 2.2: Example of a re\u0002duction in an MDP’s state-action\r
space under an MDP homomor\u0002phism h. Here ‘equivalence’\r
is represented by a reflection\r
of the dynamics in the vertical\r
axis. This equivalence class is\r
encoded by h by mapping all\r
equivalent state-action pairs to\r
the same abstract state-actions.\r
MDP with Symmetries Symmetries can appear in MDPs. For instance,\r
in Figure 2.2 CartPole has a reflection symmetry about the vertical axis.\r
Here we define an MDP with symmetries. In an MDP with symmetries\r
there is a set of transformations on the state-action space, which leaves\r
the reward function and transition operator invariant. We define a\r
state transformation and a state-dependent action transformation as\r
Lg : S → S and K\r
s\r
g\r
: A → A respectively. Invariance of the reward\r
function and transition function is then characterized as\r
R(s, a) = R(Lg[s], K\r
s\r
g\r
[a]) for all g ∈ G,s ∈ S, a ∈ A (2.4)\r
T(s\r
0\r
|s, a) = T(Lg[s\r
0\r
]|Lg[s], K\r
s\r
g\r
[a]) for all g ∈ G,s ∈ S, a ∈ A. (2.5)\r
Written like this, we see that in an MDP with symmetries the reward\r
function and transition operator are invariant along orbits defined by\r
the transformations (Lg, K\r
s\r
g\r
).\r
MDP Homomorphisms MDPs with symmetries are closely related to\r
MDP homomorphisms, as we explain below. First we define the lat-

17\r
ter. An MDP homomorphism h [143, 141] is a mapping from one MDP\r
M = (S, A, R, T, γ) to another M¯ = (S¯, A¯, R¯, T¯, γ) defined by a sur\u0002jective map from the state-action space S × A to an abstract state-action\r
space S ×¯ A¯. In particular, h consists of a tuple of surjective maps\r
(σ, {αs|s ∈ S}), where we have the state map σ : S → S¯ and the state\u0002dependent action map αs\r
: A → A¯. These maps are built to satisfy the\r
following conditions\r
R¯(σ(s), αs(a)) , R(s, a) for all s ∈ S, a ∈ A,\r
(2.6)\r
T¯(σ(s\r
0\r
)|σ(s), αs(a)) , ∑\r
s\r
00∈σ−1(s0)\r
T(s\r
00|s, a) for all s,s0 ∈ S, a ∈ A.\r
(2.7)\r
An exact MDP homomorphism provides a model equivalence abstrac\u0002tion [113]. Given an MDP homomorphism h, two state-action pairs\r
(s, a) and (s\r
0\r
, a\r
0\r
) are called h-equivalent if σ(s) = σ(s\r
0\r
) and αs(a) =\r
αs\r
0(a\r
0\r
). Symmetries and MDP homomorphisms are connected in a\r
natural way: If an MDP has symmetries Lg and Kg, the above equa\u0002tions 2.4 and 2.5 hold. This means that we can define a corresponding\r
MDP homomorphism, which we define next.\r
Group-structured MDP Homomorphisms Specifically, for an MDP with\r
symmetries, we can define an abstract state-action space, by map\u0002ping (s, a) pairs to (a representative point of) their equivalence class\r
(σ(s), αs(a)). That is, state-action pairs and their transformed version\r
are mapped to the same abstract state in the reduced MDP:\r
(σ(s), αs(a)) =\r
\u0010\r
σ(Lg[s]), αLg[s](K\r
s\r
g\r
[a])\u0011∀g ∈ G,s ∈ S, a ∈ A (2.8)\r
In this case, we call the resulting MDP homomorphism group struc\u0002tured. In other words, all the state-action pairs in an orbit defined by\r
a group transformation are mapped to the same abstract state by a\r
group-structured MDP homomorphism.\r
Optimal Value Equivalence and Lifted Policies h-equivalent state\u0002action pairs share the same optimal Q-value and optimal value func\u0002tion [141]. There exists an abstract optimal Q-value Q¯ ∗ and abstract\r
optimal value function V¯ ∗, such that Q∗(s, a) = Q¯ ∗(σ(s), αs(a)) and\r
V\r
∗\r
(s) = V¯ ∗(σ(s)). This is known as optimal value equivalence [141].\r
Policies can thus be optimized in the simpler abstract MDP. The opti\u0002mal abstract policy π¯(a¯|σ(s)) can then be pulled back to the original\r
MDP using a procedure called lifting 2. The lifted policy is given in\r
2 Note that we use the terminology lifting\r
to stay consistent with [141].\r
Equation 2.9. A lifted optimal abstract policy is also an optimal policy

18\r
in the original MDP [141]. Note that while other lifted policies ex\u0002ist, we follow [141, 143] and choose the lifting that divides probability\r
mass uniformly over the preimage:\r
π\r
↑\r
(a|s) ,\r
π¯(a¯|σ(s))\r
|{a ∈ α\r
−1\r
s (a¯)}|\r
, for any s ∈ S and a ∈ α\r
−1\r
s\r
(a¯). (2.9)\r
2.3 Method\r
The focus of the next section is on the design of MDP homomorphic\r
networks—policy networks and value networks obeying the MDP ho\u0002momorphism. In the first section of the method, we show that any pol\u0002icy network satisfying the MDP homomorphism property must be an\r
equivariant neural network. In the second part of the method, we in\u0002troduce a novel numerical technique for constructing group-equivariant\r
networks, based on the transformation operators defining the equiva\u0002lence state-action pairs under the MDP homomorphism.\r
Lifted Policies Are Invariant\r
Lifted policies in symmetric MDPs with group-structured symmetries\r
are invariant under the group of symmetries. Consider the following:\r
Take an MDP with symmetries defined by transformation operators\r
(Lg, K\r
s\r
g\r
) for g ∈ G. Now, if we take s\r
0 = Lg[s] and a0 = Ks\r
g\r
[a] for\r
any g ∈ G, (s\r
0\r
, a\r
0\r
) and (s, a) are h-equivalent under the corresponding\r
MDP homomorphism h = (σ, {αs|s ∈ S}). So\r
π\r
↑\r
(a|s) = π¯(αs(a)|σ(s))\r
|{a ∈ α\r
−1\r
s (a¯)}|\r
=\r
π¯(αs\r
0(a\r
0\r
)|σ(s\r
0\r
))\r
|{a\r
0 ∈ α\r
−1\r
s\r
0 (a¯)}|\r
= π\r
↑\r
(a\r
0\r
|s\r
0\r
), (2.10)\r
for all s ∈ S, a ∈ A and g ∈ G. In the first equality we have used\r
the definition of the lifted policy. In the second equality, we have used\r
the definition of h-equivalent state-action pairs, where σ(s) = σ(Lg(s))\r
and αs(a) = αs\r
0(a\r
0\r
). In the third equality, we have reused the definition\r
of the lifted policy. Thus we see that, written in this way, the lifted pol\u0002icy is invariant under state-action transformations (Lg, K\r
s\r
g\r
). This equa\u0002tion is very general and applies for all group-structured state-action\r
transformations. For a finite action space, this statement of invariance\r
can be re-expressed as a statement of equivariance, by considering the\r
vectorized policy.\r
Invariant Policies On Finite Action Spaces Are Equivariant Vector\u0002ized Policies For convenience we introduce a vector of probabilities

19\r
for each of the discrete actions under the policy\r
π(s) ,\r
h\r
π(a1|s), π(a2|s), ..., π(aN|s)\r
i>\r
, (2.11)\r
where a1, ..., aN are the N possible discrete actions in action space A.\r
The action transformation K\r
s\r
g maps actions to actions invertibly. Thus\r
applying an action transformation to the vectorized policy permutes\r
the elements. We write the corresponding permutation matrix as Kg.\r
Note that\r
K\r
−1\r
g π(s) ,\r
h\r
π(K\r
s\r
g\r
[a1]|s), π(K\r
s\r
g\r
[a2]|s), ..., π(K\r
s\r
g\r
[aN]|s)\r
i>\r
, (2.12)\r
where writing the inverse K−1\r
g\r
instead of Kg is required to maintain\r
the property KgKh = Kgh. The invariance of the lifted policy can then\r
be written as π↑(s) = K−1\r
g π↑\r
(Lg[s]), which can be rearranged to the\r
equivariance equation\r
Kgπ\r
↑\r
(s) = π\r
↑\r
(Lg[s]) for all g ∈ G,s ∈ S, a ∈ A. (2.13)\r
This equation shows that the lifted policy must satisfy an equivariance\r
constraint. In deep learning, this has already been well-explored in the\r
context of supervised learning [34, 35, 193, 194, 188]. Next, we present\r
a novel way to construct such networks.\r
Building MDP Homomorphic Networks\r
Our goal is to build neural networks that follow Eq. 2.13; that is, we\r
wish to find neural networks that are equivariant under a set of state\r
and policy transformations. Equivariant networks are common in su\u0002pervised learning [34, 35, 193, 194, 188, 186]. For instance, in semantic\r
segmentation shifts and rotations of the input image result in shifts\r
and rotations in the segmentation. A neural network consisting of\r
only equivariant layers and non-linearities is equivariant as a whole,\r
too3[34]. Thus, once we know how to build a single equivariant layer,\r
3\r
we can simply stack such layers together. Note that this is true re- See Appendix 2.B for more details.\r
gardless of the representation of the group, i.e. this works for spatial\r
transformations of the input, feature map permutations in interme\u0002diate layers, and policy transformations in the output layer. For the\r
experiments presented in this chapter, we use the same group repre\u0002sentations for the intermediate layers as for the output, i.e. permuta\u0002tions. For finite groups, such as cyclic groups or permutations, point\u0002wise nonlinearities preserve equivariance [34], allowing the use of e.g.\r
rectified linear units without losing equivariance.\r
In the past, learnable equivariant layers were designed by hand for\r
each transformation group individually [34, 35, 193, 194, 191, 188, 186].

20\r
This is time-consuming and laborious. Here we present a novel way\r
to build learnable linear layers that satisfy equivariance automatically.\r
Equivariant Layers We begin with a single linear layer z\r
0 = Wz +\r
b, where W ∈ RDout×Din and b ∈ RDout is a bias. To simplify the\r
math, we merge the bias into the weights so W 7→ [W, b] and z 7→\r
[z, 1]\r
>. We denote the space of the augmented weights as Wtotal. For\r
a given pair of linear group transformation operators in matrix form\r
(Lg, Kg), where Lg is the input transformation and Kg is the output\r
transformation, we then have to solve the equation\r
KgWz = WLgz, for all g ∈ G, z ∈ R\r
Din+1\r
. (2.14)\r
Since this equation is true for all z we can in fact drop z entirely. Our\r
task now is to find all weights W which satisfy Equation 2.14. We label\r
this space of equivariant weights as W, defined as\r
W , {W ∈ Wtotal | KgW = WLg, for all g ∈ G}, (2.15)\r
again noting that we have dropped z. To find the space W notice that\r
for each g ∈ G the constraint KgW = WLg is in fact linear in W. Thus,\r
to find W we need to solve a set of linear equations in W. For this\r
we introduce a construction, which we call a symmetrizer S(W). The\r
symmetrizer is\r
S(W) ,\r
1\r
|G| ∑\r
g∈G\r
K\r
−1\r
g WLg. (2.16)\r
S has three important properties, of which proofs are provided in Ap\u0002pendix A. First, S(W) is symmetric (S(W) ∈ W). Second, S fixes any\r
symmetric W: (W ∈ W =⇒ S(W) = W). Third, S is idempotent,\r
S(S(W)) = S(W). These properties show that S projects arbitrary\r
W ∈ Wtotal to the equivariant subspace W.\r
Since W is the solution set for a set of simultaneous linear equa\u0002tions, W is a linear subspace of the space of all possible weights\r
Algorithm 1: Equivariant layer construction\r
1: Sample N weight matrices W1,W2, ...,WN ∼ N (W; 0,I) for N ≥\r
dim(Wtotal)\r
2: Symmetrize samples: W¯\r
i = S(Wi) for i = 1, ..., N\r
3: Vectorize samples and stack as W¯ = [vec(W¯\r
1), vec(W¯\r
2), ...]\r
4: Apply SVD: W¯ = UΣV>\r
5: Keep first r = rank(W¯ ) right-singular vectors (columns of V) and\r
unvectorize to shape of Wi

21\r
Wtotal. Thus each W ∈ W can be parametrized as a linear combina\u0002tion of basis weights {Vi}\r
r\r
i=1\r
, where r is the rank of the subspace and\r
span({Vi}\r
r\r
i=1\r
) = W. To find as basis for W, we take a Gram-Schmidt\r
orthogonalization approach. We first sample weights in the total space\r
Wtotal and then project them into the equivariant subspace with the\r
symmetrizer. We do this for multiple weight matrices, which we then\r
stack and feed through a singular value decomposition to find a basis\r
for the equivariant space. This procedure is outlined in Algorithm 1.\r
Any equivariant layer can then be written as a linear combination of\r
bases\r
W =\r
r\r
∑\r
i=1\r
ciVi, (2.17)\r
where the ci’s are learnable scalar coefficients, r is the rank of the\r
equivariant space, and the matrices Vi are the basis vectors, formed\r
from the reshaped right-singular vectors in the SVD. An example is\r
shown in Figure 2.3. To run this procedure, all that is needed are the\r
Figure 2.3: Example of 4-way ro\u0002tationally symmetric filters.\r
transformation operators Lg and Kg. Note we do not need to know\r
the explicit transformation matrices, but just to be able to perform the\r
mappings W 7→ WLg and W 7→ K−1\r
g W. For instance, some matrix Lg\r
rotates an image patch, but we could equally implement WLg using a\r
built-in rotation function. Code is available 4.\r
4 https://github.com/ElisevanderPol/\r
symmetrizer/\r
2.4 Experiments\r
We evaluated three flavors of MDP homomorphic network—an MLP,\r
a CNN, and an equivariant feature extractor—on three RL tasks that\r
exhibit group symmetry: CartPole, a grid world, and Pong. We use\r
RLPYT [159] for the algorithms. Hyperparameters (and the range con\u0002sidered), architectures, and group implementation details are in the\r
Supplementary Material. Code is available 5.\r
5 https://github.com/ElisevanderPol/\r
mdp-homomorphic-networks

22\r
Environment Space Transformations\r
CartPole S (x, θ, x˙,\r
˙θ) (x, θ, x˙,˙θ),(−x, −θ, −x˙, − ˙θ)\r
A (←, →) (←, →), (→, ←)\r
Grid World S {0, 1}\r
21×21 Identity, y 90◦\r
, y 180◦, y 270◦\r
A (∅, ↑, →, ↓, ←) (∅, ↑, →, ↓, ←),(∅, →, ↓, ←, ↑),(∅, ↓, ←, ↑, →),(∅, ←, ↑, →, ↓)\r
Pong S {0, ..., 255}\r
4×80×80 Identity, reflect\r
A (∅, ∅, ↑, ↓, ↑, ↓) (∅, ∅, ↑, ↓, ↑, ↓), (∅, ∅, ↓, ↑, ↓, ↑)\r
Table 2.1: Environments and\r
Symmetries: We showcase a\r
visual guide of the state and\r
action spaces for each environ\u0002ment along with the effect of the\r
transformations. Note, the sym\u0002bols should not be taken to be\r
hard mathematical statements,\r
they are merely a visual guide\r
for communication.\r
Environments\r
For each environment we show S and A with respective representa\u0002tions of the group transformations.\r
CartPole In the classic pole balancing task [10], we used a two-element\r
group of reflections about the y-axis. We used OpenAI’s Cartpole\u0002v1 [24] implementation, which has a 4-dimensional observation vector:\r
(cart position x, pole angle θ, cart velocity x˙, pole velocity ˙θ). The\r
(discrete) action space consists of applying a force left and right (←\r
, →). We chose this example for its simple symmetries.\r
Grid world We evaluated on a toroidal 7-by-7 predator-prey grid world\r
with agent-centered coordinates. The prey and predator are randomly\r
placed at the start of each episode, lasting a maximum of 100 time\r
steps. The agent’s goal is to catch the prey, which takes a step in a\r
random compass direction with probability 0.15 and stands still oth\u0002erwise. Upon catching the prey, the agent receives a reward of +1,\r
and -0.1 otherwise. The observation is a 21 × 21 binary image iden\u0002tifying the position of the agent in the center and the prey in relative\r
coordinates. See Figure 2.6a. This environment was chosen due to its\r
four-fold rotational symmetry.\r
Pong We evaluated on the RLPYT [159] implementation of Pong. In\r
our experiments, the observation consisted of the 4 last observed frames,\r
with upper and lower margins cut off and downscaled to an 80 × 80\r
grayscale image. In this setting, there is a flip symmetry over the hor\u0002izontal axis: if we flip the observations, the up and down actions also\r
flip. A curious artifact of Pong is that it has duplicate (up, down)\r
actions, which means that to simplify matters, we mask out the pol\u0002icy values for the second pair of (up, down) actions. We chose Pong\r
because of its higher dimensional state space. Finally, for Pong we\r
additionally compare to two data augmentation baselines: stochas\u0002tic data augmentation, where for each state, action pair we randomly\r
transform them or not before feeding them to the network, and the\r
second an equivariant version of [99] and similar to [156], where both\r
state and transformed state are input to the network. The output of\r
the transformed state is appropriately transformed, and both policies\r
are averaged.

23\r
0 20 40 60 80 100 120 140\r
Time steps (x 500) \r
0\r
100\r
200\r
300\r
400\r
500\r
Average Return\r
Nullspace\r
Random\r
Equivariant\r
(a) Cartpole-v1: Bases\r
0 20 40 60 80 100 120 140\r
Time steps (x 500) \r
0\r
100\r
200\r
300\r
400\r
500\r
Average Return\r
MLP, 4 64 128 2\r
MLP, 4 128 128 2\r
Equivariant, 4 64 64 2\r
(b) Cartpole-v1: MLPs\r
0 100 200 300 400 500 600\r
Time steps (x 25000) \r
20\r
15\r
10\r
5\r
0\r
5\r
10\r
15\r
20\r
Average Return\r
Nullspace\r
Random\r
Convolutional\r
Equivariant\r
(c) Pong\r
Figure 2.4: Cartpole: Trained\r
with PPO, all networks fine\u0002tuned over 7 learning rates. 25%,\r
50% and 75% quantiles over 25\r
random seeds shown. a) Equiv\u0002ariant, random, and nullspace\r
bases. b) Equivariant basis, and\r
two MLPs with different degrees\r
of freedom. Pong: Trained\r
with A2C, all networks tuned\r
over 3 learning rates. 25%,\r
50% and 75% quantiles over 15\r
random seeds shown c) Equiv\u0002ariant, nullspace, and random\r
bases, and regular CNN for\r
Pong.\r
Models\r
We implemented MDP homomorphic networks on top of two base\r
architectures: MLP and CNN (exact architectures in Supplementary).\r
We further experimented with an equivariant feature extractor, ap\u0002pended by a non-equivariant network, to isolate where equivariance\r
made the greatest impact.\r
Basis Networks We call networks whose weights are linear combi\u0002nations of basis weights basis networks. As an ablation study on all\r
equivariant networks, we sought to measure the effects of the basis\r
training dynamics. We compared an equivariant basis against a pure\r
nullspace basis, i.e. an explicitly non-symmetric basis using the right\u0002null vectors from the equivariant layer construction, and a random ba\u0002sis, where we skip the symmetrization step in the layer construction\r
and use the full rank basis. Unless stated otherwise, we reduce the\r
number of ‘channels’ in the basis networks compared to the regular\r
networks by dividing by the square root of the group size, ending up\r
with a comparable number of trainable parameters.\r
Results and Discussion\r
We show training curves for CartPole in Figures 2.4a-b, Pong in Fig\u0002ure 2.4c and for the grid world in Figure 2.6. Across all experiments\r
we observed that the MDP homomorphic network outperforms both\r
the non-equivariant basis networks and the standard architectures, in\r
terms of convergence speed.\r
This confirms our motivations that building symmetry-preserving pol\u0002icy networks leads to faster convergence. Additionally, when com\u0002pared to the data augmentation baselines in Figure 2.5, using equiv\u0002ariant networks is more beneficial. This is consistent with other results\r
in the equivariance literature [14, 187, 191, 193]. While data augmenta\u0002tion can be used to create a larger dataset by exploiting symmetries, it\r
does not directly lead to effective parameter sharing (as our approach\r
does). Note, in Pong we only train the first 15 million frames to high-

24\r
0 100 200 300 400 500 600\r
Time steps (x 25000) \r
20\r
15\r
10\r
5\r
0\r
5\r
10\r
15\r
20\r
Average Return\r
Stoch. Data Aug.\r
Full Data Aug.\r
Convolutional\r
Equivariant\r
Figure 2.5: Data augmentation\r
comparison on Pong.\r
light the difference in the beginning; in constrast, a typical training\r
duration is 50-200 million frames [124, 159].\r
For our ablation experiment, we wanted to control for the introduc\u0002tion of bases. It is not clear a priori that a network with a basis has\r
the same gradient descent dynamics as an equivalent ‘basisless’ net\u0002work. We compared equivariant, non-equivariant, and random bases,\r
as mentioned above. We found the equivariant basis led to the fastest\r
convergence. Figures 2.4a and 2.4c show that for CartPole and Pong\r
the nullspace basis converged faster than the random basis. In the grid\r
world there was no clear winner between the two. This is a curious\r
result, requiring deeper investigation in a follow-up.\r
For a third experiment, we investigated what happens if we sacrifice\r
strict equivariance of the policy. This is attractive because it removes\r
the need to find a transformation operator for a flattened output fea\u0002ture map. Instead, we only maintained an equivariant feature extrac\u0002tor, compared against a basic CNN feature extractor. The networks\r
built on top of these extractors were MLPs. The results, in Figure 2.4c,\r
are two-fold: 1) Basis feature extractors converge faster than standard\r
CNNs, and 2) the equivariant feature extractor has fastest convergence.\r
We hypothesize the equivariant feature extractor is fastest as it is easi\u0002est to learn an equivariant policy from equivariant features.\r
We have additionally compared an equivariant feature extractor to a\r
regular convolutional network on the Atari game Breakout, where the\r
difference between the equivariant network and the regular network\r
is much less pronounced. For details, see Appendix 2.C.

25\r
(a) Symmetries\r
0 25 50 75 100 125 150 175 200\r
Time steps (x 10000) \r
10\r
1\r
10\r
0\r
0\r
Average Return\r
Nullspace\r
Random\r
Equivariant\r
(b) Grid World: Bases\r
0 25 50 75 100 125 150 175 200\r
Time steps (x 10000) \r
101\r
100\r
0\r
Average Return\r
Convolutional\r
Equivariant\r
(c) Grid World: CNNs\r
Figure 2.6: Grid World:\r
Trained with A2C, all networks\r
fine-tuned over 6 learning rates.\r
25%, 50% and 75% quantiles\r
over 20 random seeds shown.\r
a) showcase of symmetries, b)\r
Equivariant, nullspace, and ran\u0002dom bases c) plain CNN and\r
equivariant CNN.\r
2.5 Related Work\r
Past work on MDP homomorphisms has often aimed at discovering\r
the map itself based on knowledge of the transition and reward func\u0002tion, and under the assumption of enumerable state spaces [141, 142,\r
143, 165]. Other work relies on learning the map from sampled expe\u0002rience from the MDP [170, 16, 118]. Exactly computing symmetries in\r
MDPs is graph isomorphism complete [127] even with full knowledge\r
of the MDP dynamics. Rather than assuming knowledge of the tran\u0002sition and reward function, and small and enumerable state spaces, in\r
this work we take the inverse view: we assume that we have an eas\u0002ily identifiable transformation of the joint state–action space and ex\u0002ploit this knowledge to learn more efficiently. Exploiting symmetries\r
in deep RL has been previously explored in the game of Go, in the\r
form of symmetric filter weights [151, 30] or data augmentation [156].\r
Other work on data augmentation increases sample efficiency and gen\u0002eralization on well-known benchmarks by augmenting existing data\r
points state transformations such as random translations, cutout, color\r
jitter and random convolutions [99, 31, 107, 111]. In contrast, we en\u0002code symmetries into the neural network weights, leading to more\r
parameter sharing. Additionally, such data augmentation approaches\r
tend to take the invariance view, augmenting existing data with state\r
transformations that leave the state’s Q-values intact [99, 31, 107, 111]\r
(the exception being [115] and [119], who augment trajectories rather\r
than just states). Similarly, permutation invariant networks are com\u0002monly used in approaches to multi-agent RL [160, 117, 81]. We instead\r
take the equivariance view, which accommodates a much larger class of\r
symmetries that includes transformations on the action space. Abdol\u0002hosseini et al. [1] have previously manually constructed an equivari\u0002ant network for a single group of symmetries in a single RL problem,\r
namely reflections in a bipedal locomotion task. Our MDP homomor\u0002phic networks allow for automated construction of networks that are\r
equivariant under arbitrary discrete groups and are therefore applica\u0002ble to a wide variety of problems.

26\r
From an equivariance point-of-view, the automatic construction of equiv\u0002ariant layers is new. [35] comes close to specifying a procedure, outlin\u0002ing the system of equations to solve, but does not specify an algorithm.\r
The basic theory of group equivariant networks was outlined in [34,\r
35] and [33], with notable implementations to 2D roto-translations on\r
grids [193, 188, 186] and 3D roto-translations on grids [192, 191, 187].\r
All of these works have relied on hand-constructed equivariant layers.\r
2.6 Conclusion\r
This chapter introduced MDP homomorphic networks, a family of\r
deep architectures for reinforcement learning problems where symme\u0002tries have been identified. MDP homomorphic networks tie weights\r
over symmetric state-action pairs. This weight-tying leads to fewer\r
degrees-of-freedom and in our experiments we found that this trans\u0002lates into faster convergence. We used the established theory of MDP\r
homomorphisms to motivate the use of equivariant networks in sym\u0002metric MDPs, thus formalizing the connection between equivariant\r
networks and symmetries in reinforcement learning. As an innova\u0002tion, we also introduced the first method to automatically construct\r
equivariant network layers, given a specification of the symmetries in\r
question, thus removing a significant implementational obstacle. For\r
future work, we want to further understand the symmetrizer and its\r
effect on learning dynamics, as well as generalizing to problems that\r
are not fully symmetric.\r
2.7 Broader Impact Statement\r
The goal of this chapter is to make (deep) reinforcement learning tech\u0002niques more efficient at solving Markov decision processes (MDPs)\r
by making use of prior knowledge about symmetries. We do not ex\u0002pect the particular algorithm we develop to lead to immediate societal\r
risks. However, Markov decision processes are very general, and can\r
e.g. be used to model problems in autonomous driving, smart grids,\r
and scheduling. Thus, solving such problems more efficiently can in\r
the long run cause positive or negative societal impact.\r
For example, making transportation or power grids more efficient,\r
thereby making better use of scarce resources, would be a significantly\r
positive impact. Other potential applications, such as in autonomous

27\r
weapons, pose a societal risk [131]. Like many AI technologies, when\r
used in automation, our technology can have a positive impact (in\u0002creased productivity) and a negative impact (decreased demand) on\r
labor markets.\r
More immediately, control strategies learned using RL techniques are\r
hard to verify and validate. Without proper precaution (e.g. [177]),\r
employing such control strategies on physical systems thus run the\r
risk of causing accidents involving people, e.g. due to reward mis\u0002specification, unsafe exploration, or distributional shift [5].

29\r
Chapter Appendix\r
2.A The Symmetrizer\r
In this section we prove three properties of the symmetrizer: the sym\u0002metric property (S(W) ∈ W for all W ∈ Wtotal ), the fixing prop\u0002erty (W ∈ W =⇒ S(W) = W) , and the idempotence property\r
(S(S(W)) = S(W) for all W ∈ Wtotal).\r
The Symmetric Property Here we show that the symmetrizer S maps\r
matrices W ∈ Wtotal to equivariant matrices S(W) ∈ W. For this,\r
we show that a symmetrized weight matrix S(W) from Equation 16\r
satisfies the equivariance constraint of Equation 14.\r
Proof 1 (The symmetric property) We begin by recalling the equivari\u0002ance constraint\r
KgWz = WLgz, for all g ∈ G, z ∈ R\r
Din+1\r
. (2.18)\r
Now note that we can drop the dependence on z, since this equation is true for\r
all z. At the same time, we left-multiply both sides of this equation by Kg\r
−1\r
,\r
which is possible because group representations are invertible. This results in\r
the following set of equations\r
W = K\r
−1\r
g WLg, for all g ∈ G. (2.19)\r
Any W satisfying this equation satisfies Equation 2.18 and is thus a member\r
of W. To show that S(W) is a member of W, we thus would need show that\r
S(W) = K\r
−1\r
g S(W)Lg for all W ∈ Wtotal and g ∈ G. This can be shown as

30\r
follows:\r
K\r
−1\r
g S(W)Lg = K\r
−1\r
g\r
 \r
1\r
|G| ∑\r
h∈G\r
K\r
−1\r
h WLh\r
!\r
Lg substitute S(W) = K\r
−1\r
g S(W)Lg\r
(2.20)\r
=\r
1\r
|G| ∑\r
h∈G\r
K\r
−1\r
g K\r
−1\r
h WLhLg (2.21)\r
=\r
1\r
|G| ∑\r
h∈G\r
K\r
−1\r
hg WLhg representation definition: LhLg = Lhg\r
(2.22)\r
=\r
1\r
|G| ∑\r
g\r
0g−1∈G\r
K\r
−1\r
g\r
0 WLg\r
0 change of variables g0 = hg, h = g\r
0\r
g\r
−1\r
(2.23)\r
=\r
1\r
|G| ∑\r
g\r
0∈Gg\r
K\r
−1\r
g\r
0 WLg\r
0 g\r
0\r
g\r
−1 ∈ G ⇐⇒ g0 ∈ Gg\r
(2.24)\r
=\r
1\r
|G| ∑\r
g\r
0∈G\r
K\r
−1\r
g\r
0 WLg\r
0 G = Gg (2.25)\r
= S(W) definition of symmetrizer.\r
(2.26)\r
Thus we see that S(W) satisfies the equivariance constraint, which implies\r
that S(W) ∈ W.\r
The Fixing Property For the symmetrizer to be useful, we need to\r
make sure that its range covers the equivariant subspace W, and not\r
just a subset of it; that is, we need to show that\r
W = {S(W) ∈ W|W ∈ Wtotal}. (2.27)\r
We show this by picking a matrix W ∈ W and showing that W ∈\r
W =⇒ S(W) = W.\r
Proof 2 (The fixing property) We begin by assuming that W ∈ W, then\r
S(W) = 1\r
|G| ∑\r
g∈G\r
K\r
−1\r
g WLg definition (2.28)\r
=\r
1\r
|G| ∑\r
g∈G\r
K\r
−1\r
g KgW W ∈ W ⇐⇒ KgW = WLg, ∀g ∈ G\r
(2.29)\r
=\r
1\r
|G| ∑\r
g∈G\r
W (2.30)\r
= W (2.31)

31\r
This means that the symmetrizer leaves the equivariant subspace invariant.\r
In fact, the statement we just showed is stronger in saying that each point in\r
the equivariant subspace is unaltered by the symmetrizer. In the language of\r
group theory we say that subspace W is fixed under G. Since S : Wtotal →\r
W and there exist matrices W such that for every W ∈ W, S(W) = W, we\r
have shown that\r
W = {S(W) ∈ W|W ∈ Wtotal}. (2.32)\r
The Idempotence Property Here we show that the symmetrizer S(W)\r
from Equation 16 is idempotent, S(S(W)) = S(W).\r
Proof 3 (The idempotence property) Recall the definition of the symmetrizer\r
S(W) = 1\r
|G| ∑\r
g∈G\r
K\r
−1\r
g WLg. (2.33)\r
Now let’s expand S(S(W)):\r
S(S(W)) = S\r
 \r
1\r
|G| ∑\r
h∈G\r
K\r
−1\r
h WLh\r
!\r
(2.34)\r
=\r
1\r
|G| ∑\r
g∈G\r
K\r
−1\r
g\r
 \r
1\r
|G| ∑\r
h∈G\r
K\r
−1\r
h WLh\r
!\r
Lg (2.35)\r
=\r
1\r
|G| ∑\r
g∈G\r
 \r
1\r
|G| ∑\r
h∈G\r
K\r
−1\r
g K\r
−1\r
h WLhLg\r
!\r
linearity of sum\r
(2.36)\r
=\r
1\r
|G| ∑\r
g∈G\r
 \r
1\r
|G| ∑\r
h∈G\r
K\r
−1\r
hg WLhg!\r
definition of group representations\r
(2.37)\r
=\r
1\r
|G| ∑\r
g∈G\r
\r
\r
1\r
|G| ∑\r
g\r
0g−1∈G\r
K\r
−1\r
g\r
0 WLg\r
0\r
\r
 change of variables g0 = hg\r
(2.38)\r
=\r
1\r
|G| ∑\r
g∈G\r
\r
\r
1\r
|G| ∑\r
g\r
0∈Gg\r
K\r
−1\r
g\r
0 WLg\r
0\r
\r
 g\r
0\r
g\r
−1 ∈ G ⇐⇒ g0 ∈ Gg\r
(2.39)\r
=\r
1\r
|G| ∑\r
g∈G\r
\r
\r
1\r
|G| ∑\r
g\r
0∈G\r
K\r
−1\r
g\r
0 WLg\r
0\r
\r
 Gg = G (2.40)\r
=\r
1\r
|G| ∑\r
g\r
0∈G\r
K\r
−1\r
g\r
0 WLg\r
0 sum over constant\r
(2.41)\r
= S(W) (2.42)

32\r
Thus we see that S(W) satisfies the equivariance constraint, which implies\r
that S(W) ∈ W.\r
2.B Experimental Settings\r
Designing representations\r
In the main text we presented a method to construct a space of in\u0002tertwiners W using the symmetrizer. This relies on us already hav\u0002ing chosen specific representations/transformation operators for the\r
input, the output, and for every intermediate layer of the MDP homo\u0002morphic networks. While for the input space (state space) and output\r
space (policy space), these transformation operators are easy to define,\r
it is an open question how to design a transformation operator for the inter\u0002mediate layers of our networks. Here we give some rules of thumb that\r
we used, followed by the specific transformation operators we used in\r
our experiments.\r
For each experiment we first identified the group G of transforma\u0002tions. In every case, this was a finite group of size |G|, where the size\r
is the number of elements in the group (number of distinct transfor\u0002mation operators). For example, a simple flip group as in Pong has\r
two elements, so |G| = 2. Note that the group size |G| does not nec\u0002essarily equal the size of the transformation operators, whose size is\r
determined by the dimensionality of the input/activation layer/policy.\r
Stacking Equivariant Layers If we stack equivariant layers, the result\u0002ing network is equivariant as a whole too [34]. To see that this is\r
the case, consider the following example. Assume we have network f ,\r
consisting of layers f1 and f2, which satisfy the layer-wise equivariance\r
constraints:\r
Pg[ f1(x)] = f1(Lg[x]) (2.43)\r
Kg[ f2(x)] = f2(Pg[x]) (2.44)\r
With Kg the output transformation of the network, Lg the input trans\u0002formation, and Pg the intermediate transformation. Now,\r
Kg[ f(x)] = Kg[ f2(f1(x))] (2.45)\r
= f2(Pg[ f1(x)] (f2 equivariance constraint) (2.46)\r
= f2(f1(Lg[x])) (f1 equivariance constraint) (2.47)\r
= f(Lg[x]) (2.48)

33\r
and so the whole network f is equivariant with regards to the input\r
transformation Lg and the output transformation Kg. Note that this\r
depends on the intermediate representation Pg being shared between\r
layers, i.e. f1’s output transformation is the same as f2’s input trans\u0002formation.\r
MLP-structured networks For MLP-structured networks (CartPole), typ\u0002ically the activations have shape [batch_size, nc], with nc the num\u0002ber of channels. Instead we used a shape of [batch_size, nc,\r
representation_size], where for the intermediate layers\r
representation_size=|G|+1 (we have a +1 because of the bias). The\r
transformation operators we then apply to the activations is the set of\r
permutations for group size |G| appended with a 1 on the diagonal for\r
the bias, acting on this last ‘representation dimension’. Thus a forward\r
pass of a layer is computed as\r
yb,cout,rout=\r
nc\r
∑\r
cin=1\r
|G|+1\r
∑\r
rin=1\r
zb,cin,rinWcout,rout,cin,rin (2.49)\r
where\r
Wcout,rout,cin,rin =\r
rank(W)\r
∑\r
i=1\r
ci,cout,cinVi,rout,rin . (2.50)\r
CNN-structured networks For CNN-structured networks (Pong and Grid\r
World), typically the activations have shape [batch_size,\r
nc, height, width]. Instead we used a shape of\r
[batch_size, nc, representation_size, height, width], where for\r
the intermediate layers representation_size=|G|+1. The transforma\u0002tion operators we apply to the input of the layer is a spatial transfor\u0002mation on the height, width dimensions and a permutation on the\r
representation dimension. This is because in the intermediate layers\r
of the network the activations do not only transform in space, but also\r
along the representation dimensions of the tensor. The transformation\r
operators we apply to the output of the layer is just a permutation\r
on the representation dimension. Thus a forward pass of a layer is\r
computed as\r
yb,cout,rout,hout,wout=\r
nc\r
∑\r
cin=1\r
|G|+1\r
∑\r
rin=1\r
∑\r
hin,win\r
zb,cin,rin,hout+hin,wout+winWcout,rout,cin,rin,hin,win\r
(2.51)\r
where\r
Wcout,rout,cin,rin,hin,win =\r
rank(W)\r
∑\r
i=1\r
ci,cout,cinVi,rout,rin,hin,win . (2.52)

34\r
Equivariant Nullspace Random MLP\r
0.01 0.005 0.001 0.001\r
Table 2.B.1: Final learning\r
rates used in CartPole-v1 exper\u0002iments.\r
Cartpole-v1\r
Group Representations For states:\r
Lge =\r
\r
\r
1 0 0 0\r
0 1 0 0\r
0 0 1 0\r
0 0 0 1\r
\r
\r
, Lg1 =\r
\r
\r
−1 0 0 0\r
0 −1 0 0\r
0 0 −1 0\r
0 0 0 −1\r
\r
\r
For intermediate layers and policies:\r
K\r
π\r
ge =\r
 \r
1 0\r
0 1!\r
, K\r
π\r
g1 =\r
 \r
0 1\r
1 0!\r
For values we require an invariant rather than equivariant output. This\r
invariance is implemented by defining the output representations to be\r
|G| identity matrices of the desired output dimensionality. For predict\u0002ing state values we required a 1-dimensional output, and we thus used\r
|G| 1-dimensional identity matrices, i.e. for value output V:\r
K\r
V\r
ge =\r
\u0010\r
1\r
\u0011\r
, K\r
V\r
g1 =\r
\u0010\r
1\r
\u0011\r
Hyperparameters For both the basis networks and the MLP, we used\r
Xavier initialization. We trained PPO using ADAM on 16 parallel envi\u0002ronments and fine-tuned over the learning rates {0.01, 0.05, 0.001, 0.005,\r
0.0001, 0.0003, 0.0005} by running 25 random seeds for each setting,\r
and report the best curve. The final learning rates used are shown\r
in Table 2.B.1. Other hyperparameters were defaults in RLPYT [159],\r
except that we turn off learning rate decay.\r
Architecture\r
Basis networks:\r
Listing 2.1: Basis Networks Architecture for CartPole-v1\r
1 BasisLinear(repr_in=4, channels_in=1, repr_out=2, channels_out=64)\r
2 ReLU()\r
3 BasisLinear(repr_in=2, channels_in=64, repr_out=2, channels_out=64)\r
4 ReLU()\r
5 BasisLinear(repr_in=2, channels_in=64, repr_out=2, channels_out=1)\r
6 BasisLinear(repr_in=2, channels_in=64, repr_out=1, channels_out=1)\r
First MLP variant:

35\r
Listing 2.2: First MLP Architecture for CartPole-v1\r
1 Linear(channels_in=1, channels_out=64)\r
2 ReLU()\r
3 Linear(channels_in=64, channels_out=128)\r
4 ReLU()\r
5 Linear(channels_in=128, channels_out=1)\r
6 Linear(channels_in=128, channels_out=1)\r
Second MLP variant:\r
Listing 2.3: Second MLP Architecture for CartPole-v1\r
1 Linear(channels_in=1, channels_out=128)\r
2 ReLU()\r
3 Linear(channels_in=128, channels_out=128)\r
4 ReLU()\r
5 Linear(channels_in=128, channels_out=1)\r
6 Linear(channels_in=128, channels_out=1)\r
GridWorld\r
Group Representations For states we use numpy.rot90. The stack of\r
weights is rolled.\r
For the intermediate representations:\r
Lge =\r
\r
\r
1 0 0 0\r
0 1 0 0\r
0 0 1 0\r
0 0 0 1\r
\r
\r
, Lg1 =\r
\r
\r
0 0 0 1\r
1 0 0 0\r
0 1 0 0\r
0 0 1 0\r
\r
\r
,\r
Lg2 =\r
\r
\r
0 0 1 0\r
0 0 0 1\r
1 0 0 0\r
0 1 0 0\r
\r
\r
, Lg3 =\r
\r
\r
0 1 0 0\r
0 0 1 0\r
0 0 0 1\r
1 0 0 0\r
\r
\r
For the policies:\r
K\r
π\r
ge =\r
\r
\r
1 0 0 0 0\r
0 1 0 0 0\r
0 0 1 0 0\r
0 0 0 1 0\r
0 0 0 0 1\r
\r
\r
, K\r
π\r
g1 =\r
\r
\r
1 0 0 0 0\r
0 0 0 0 1\r
0 1 0 0 0\r
0 0 1 0 0\r
0 0 0 1 0\r
\r
\r
,\r
K\r
π\r
g2 =\r
\r
\r
1 0 0 0 0\r
0 0 0 1 0\r
0 0 0 0 1\r
0 1 0 0 0\r
0 0 1 0 0\r
\r
\r
, K\r
π\r
g3 =\r
\r
\r
1 0 0 0 0\r
0 0 1 0 0\r
0 0 0 1 0\r
0 0 0 0 1\r
0 1 0 0 0\r
\r


36\r
For the values:\r
K\r
V\r
ge =\r
\u0010\r
1\r
\u0011\r
, K\r
V\r
g1 =\r
\u0010\r
1\r
\u0011\r
, K\r
V\r
g2 =\r
\u0010\r
1\r
\u0011\r
, K\r
V\r
g3 =\r
\u0010\r
1\r
\u0011\r
Hyperparameters For both the basis networks and the CNN, we used\r
He initialization. We trained A2C using ADAM on 16 parallel environ\u0002ments and fine-tuned over the learning rates {0.00001, 0.00003, 0.0001,\r
0.0003, 0.001, 0.003} on 20 random seeds for each setting, and reporting\r
the best curve. The final learning rates used are shown in Table 2.B.2.\r
Other hyperparameters were defaults in RLPYT [159].\r
Equivariant Nullspace Random CNN\r
0.001 0.003 0.001 0.003\r
Table 2.B.2: Final learning rates\r
used in grid world experiments.\r
Architecture\r
Basis networks:\r
Listing 2.4: Basis Networks Architecture for GridWorld\r
1 BasisConv2d(repr_in=1, channels_in=1, repr_out=4, channels_out=b √\r
16\r
4\r
c,\r
2 filter_size=(7, 7), stride=2, padding=0)\r
3 ReLU()\r
4 BasisConv2d(repr_in=4, channels_in=b √\r
16\r
4\r
c, repr_out=4, channels_out=b √\r
32\r
4\r
c,\r
5 filter_size=(5, 5), stride=1, padding=0)\r
6 ReLU()\r
7 GlobalMaxPool()\r
8 BasisLinear(repr_in=4, channels_in=b √\r
32\r
4\r
c, repr_out=4, channels_out=b\r
512\r
√\r
4\r
c)\r
9 ReLU()\r
10 BasisLinear(repr_in=4, channels_in=b\r
512\r
√\r
4\r
c, repr_out=5, channels_out=1)\r
11 BasisLinear(repr_in=4, channels_in=b\r
512\r
√\r
4\r
c, repr_out=1, channels_out=1)\r
CNN:\r
Listing 2.5: CNN Architecture for GridWorld\r
1 Conv2d(channels_in=1, channels_out=16,\r
2 filter_size=(7, 7), stride=2, padding=0)\r
3 ReLU()\r
4 Conv2d(channels_in=16,channels_out=32,\r
5 filter_size=(5, 5), stride=1, padding=0)\r
6 ReLU()\r
7 GlobalMaxPool()\r
8 Linear(channels_in=32, channels_out=512)\r
9 ReLU()\r
10 Linear(channels_in=512, channels_out=5)\r
11 Linear(channels_in=512, channels_out=1)

37\r
Pong\r
Group Representations For the states we use numpy’s indexing to flip\r
the input, i.e.\r
w = w[..., ::-1, :], then the permutation on the representation\r
dimension of the weights is a numpy.roll, since the group is cyclic.\r
For the intermediate layers:\r
Lge =\r
 \r
1 0\r
0 1!\r
, Lg1 =\r
 \r
0 1\r
1 0!\r
Hyperparameters For both the basis networks and the CNN, we used\r
He initialization. We trained A2C using ADAM on 4 parallel environ\u0002ments and fine-tuned over the learning rates {0.0001, 0.0002, 0.0003}\r
on 15 random seeds for each setting, and reporting the best curve. The\r
learning rates to fine-tune over were selected to be close to where the\r
baseline performed well in preliminary experiments. The final learn\u0002ing rates used are shown in Table 2.B.3. Other hyperparameters were\r
defaults in RLPYT [159].\r
Equivariant Nullspace Random CNN\r
0.0002 0.0002 0.0002 0.0001\r
Table 2.B.3: Learning rates used\r
in Pong experiments.\r
Architecture\r
Basis Networks:\r
Listing 2.6: Basis Networks Architecture for Pong\r
1 BasisConv2d(repr_in=1, channels_in=4, repr_out=2, channels_out=b √\r
16\r
2\r
c,\r
2 filter_size=(8, 8), stride=4, padding=0)\r
3 ReLU()\r
4 BasisConv2d(repr_in=2, channels_in=b √\r
16\r
2\r
c, repr_out=2, channels_out=b √\r
32\r
2\r
c,\r
5 filter_size=(5, 5), stride=2, padding=0)\r
6 ReLU()\r
7 Linear(channels_in=2816, channels_out=b\r
512\r
√\r
2\r
c)\r
8 ReLU()\r
9 Linear(channels_in=b\r
512\r
√\r
2\r
c, channels_out=6)\r
10 Linear(channels_in=b\r
512\r
√\r
2\r
c, channels_out=1)\r
CNN:\r
Listing 2.7: CNN Architecture for Pong\r
1 Conv2d(channels_in=4, channels_out=16, filter_size=(8, 8), stride=4, padding=0)\r
2 ReLU()

38\r
3 Conv2d(channels_in=16,channels_out=32, filter_size=(5, 5), stride=2, padding=0)\r
4 ReLU()\r
5 Linear(channels_in=2048, channels_out=512)\r
6 ReLU()\r
7 Linear(channels_in=512, channels_out=6)\r
8 Linear(channels_in=512, channels_out=1)

39\r
2.C Breakout Experiments\r
We evaluated the effect of an equivariant basis extractor on Breakout,\r
compared to a baseline convolutional network. The hyperparameter\r
settings and architecture were largely the same as those of Pong, ex\u0002cept for the input group representation, a longer training time, and\r
that we considered a larger range of learning rates. To ensure sym\u0002metric states, we remove the two small decorative blocks in the bottom\r
corners.\r
Group Representations For the states we use numpy’s indexing to flip\r
the input, i.e.\r
w = w[..., :, ::-1] (note the different axis than in Pong), then the\r
permutation on the representation dimension of the weights is a\r
numpy.roll, since the group is cyclic.\r
For the intermediate layers:\r
Lge =\r
 \r
1 0\r
0 1!\r
, Lg1 =\r
 \r
0 1\r
1 0!\r
Hyperparameters We used He initialization. We trained A2C using\r
ADAM on 4 parallel environments and fine-tuned over the learning\r
rates {0.001, 0.005, 0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.00001, 0.00005}\r
on 15 random seeds for each setting, and reporting the best curve. The\r
final learning rates used are shown in Table 2.C.1. Other hyperparam\u0002eters were defaults in RLPYT [159].\r
Equivariant CNN\r
0.0002 0.0002\r
Table 2.C.1: Learning rates used\r
in Breakout experiments.\r
0 250 500 750 1000 1250 1500 1750 2000\r
Time steps (x 25000) \r
0\r
20\r
40\r
60\r
80\r
100\r
Average Return\r
Equivariant\r
Convolutional\r
Figure 2.C.1: Breakout:\r
Trained with A2C, all networks\r
fine-tuned over 9 learning rates.\r
25%, 50% and 75% quantiles\r
over 14 random seeds shown.

40\r
Results Figure 2.C.1 shows the result of the equivariant feature ex\u0002tractor versus the convolutional baseline. While we again see an im\u0002provement over the standard convolutional approach, the difference is\r
much less pronounced than in CartPole, Pong or the grid world. It\r
is not straightforward why. One factor could be that the equivariant\r
feature extractor is not end-to-end MDP homomorphic. It instead out\u0002puts a type of MDP homomorphic state representations and learns a\r
regular policy on top. As a result, the unconstrained final layers may\r
negate some of the advantages of the equivariant feature extractor.\r
This may be more of an issue for Breakout than Pong, since Breakout\r
is a more complex game.\r
2.D Cartpole-v1 Deeper Network Results\r
We show the effect of training a deeper network – 4 layers instead of\r
2 – for CartPole-v1 in Figure 2.D.1. The performance of the regular\r
depth networks in Figure 4b and the deeper networks in Figure 2.D.1\r
is comparable, except that for the regular MLP, the variance is much\r
higher when using deeper networks.\r
0 20 40 60 80 100 120 140\r
Time steps (x 500) \r
0\r
100\r
200\r
300\r
400\r
500\r
Average Return\r
Nullspace\r
Random\r
Equivariant\r
(a) Cartpole-v1: Bases\r
0 20 40 60 80 100 120 140\r
Time steps (x 500) \r
0\r
100\r
200\r
300\r
400\r
500\r
Average Return\r
MLP, 4 64 64 128 128 2\r
MLP, 4 128 128 128 128 2\r
Equivariant, 4 64 64 64 64 2\r
(b) Cartpole-v1: MLPs\r
Figure 2.D.1: Cartpole:\r
Trained with PPO, all networks\r
fine-tuned over 7 learning rates.\r
25%, 50% and 75% quantiles\r
over 25 random seeds shown.\r
a) Equivariant, random, and\r
nullspace bases. b) Equivariant\r
basis, and two MLPs with\r
different degrees of freedom.\r
2.E Bellman Equations\r
V\r
π\r
(s) = ∑\r
a∈A\r
π(s, a)\r
"\r
R(s, a) + γ ∑\r
s\r
0∈S\r
T(s, a,s\r
0\r
)V\r
π\r
(s\r
0\r
)\r
#\r
(2.53)\r
Q\r
π\r
(s, a) = R(s, a) + γ ∑\r
s\r
0∈S\r
T(s, a,s\r
0\r
)V\r
π\r
(s\r
0\r
). (2.54)

41\r
3\r
Multi-Agent MDP\r
Homomorphic Networks\r
3.1 Introduction\r
In the previous Chapter, we have shown that enforcing group symme\u0002tries in single agent reinforcement learning improves data efficiency.\r
In this Chapter, we will go beyond the single agent case and consider\r
global symmetries in distributed cooperative multi-agent learning.\r
Equivariant and geometric deep learning have gained traction in re\u0002cent years, showing promising results in supervised learning [34, 191,\r
187, 186, 193, 52, 166], unsupervised learning [39] and reinforcement\r
learning [173, 157]. In single agent reinforcement learning, incorpo\u0002rating symmetry information has been a successful approach, for ex\u0002ample using MDP homomorphic networks [173], trajectory augmen\u0002tation [115, 119], or symmetric locomotion policies [1]. Equivariance\r
enables an agent to learn policies more efficiently within its environ\u0002ment by sharing weights between state-action pairs that are equivalent\r
under a transformation. As a result of this weight sharing, the agent\r
implicitly learns a policy in a reduced version of the MDP. We are inter\u0002ested in cooperative multi-agent reinforcement learning, where sym\u0002metries exist both in the global environment, and between individual\r
agents in the larger multi-agent system. Existing work on symmetries\r
in single agent reinforcement learning can only be generalized to the\r
fully centralized multi-agent setting, because such approaches rely on

42\r
the global symmetry in the full state-action spaces and these can result\r
in correspondences across agents, as shown in Figure 3.1.1.\r
Figure 3.1.1: Example of a global\r
symmetry in a traffic light con\u0002trol problem, where each agent,\r
locally controlling the traffic\r
lights of a single intersection,\r
has to decide to give a green\r
light to either the horizontal\r
or vertical lanes. When a ro\u0002tated state occurs in the envi\u0002ronment the optimal policy is\r
permuted, both between and\r
within agents. Local policies\r
are color coded. Multi-Agent\r
MDP Homomorphic Networks\r
are equivariant to global sym\u0002metries while still allowing dis\u0002tributed execution based on lo\u0002cal observations and local com\u0002munications only. This means such approaches cannot be used in distributed multi-agent\r
systems with communication constraints. Here, we seek to be equiv\u0002ariant to global symmetries of cooperative multi-agent systems while\r
still being able to execute policies in a distributed manner.\r
Existing work in deep multi-agent reinforcement learning has shown\r
the potential of using permutation symmetries and invariance between\r
agents [117, 81, 163, 144, 18, 160, 171]. Such work takes an anonymity\r
view of homogeneous agents, where the agent’s observations matter\r
for the policy but not its identity. Using permutation symmetries en\u0002sures extensive weight sharing between agents, resulting in improved\r
data efficiency. Here, we go beyond such permutation symmetries,\r
and consider more general symmetries of global multi-agent systems,\r
such as rotational symmetries.\r
In this chapter, we propose Multi-Agent MDP Homomorphic Net\u0002works, a class of distributed policy networks which are equivariant\r
to global symmetries of the multi-agent system, as well as to standard\r
permutation symmetries. Our contributions are as follows. (i) We\r
propose a factorization of global symmetries in the joint state-action\r
space of cooperative multi-agent systems. (ii) We introduce a multi\u0002agent equivariant policy network based on this factorization. (iii) Our\r
main contribution is an approach to cooperative multi-agent reinforce-

43\r
ment learning that is globally equivariant while requiring only local\r
agent computation and local communication between agents at execu\u0002tion time. We evaluate Multi-Agent MDP Homomorphic Networks on\r
symmetric multi-agent problems and show improved data efficiency\r
compared to non-equivariant baselines.\r
3.2 Related Work\r
Symmetries in single agent reinforcement learning Symmetries in Markov\r
Decision Processes have been formalized by [204, 141]. Recent work\r
on symmetries in single agent deep reinforcement learning has shown\r
improvements in terms of data efficiency. Such work revolves around\r
symmetries in policy networks [173, 157], symmetric filters [30], in\u0002variant data augmentation [107, 99] or equivariant trajectory augmen\u0002tation [115, 119, 122] These approaches are only suitable for single\r
agent problems or centralized multi-agent controllers. Here, we solve\r
the problem of enforcing global equivariance while still allowing dis\u0002tributed execution.\r
Graphs and permutation symmetries in multi agent reinforcement learning\r
Graph-based methods in cooperative multiagent reinforcement learn\u0002ing are well-explored. Much work is based around coordination graphs\r
[64, 62, 97], including approaches that approximate local Q-functions\r
with neural networks and use max-plus to find a joint policy [171, 18],\r
and approaches that use graph-structured networks to find joint poli\u0002cies or value functions [81, 160]. In deep learning for multi-agent sys\u0002tems, the use of permutation symmetries is common, either through\r
explicit formulations [163, 18] or through the use of graph or mes\u0002sage passing networks [117, 81, 160]. Policies in multi-agent systems\r
with permutation symmetries between agents are also known as func\u0002tionally homogeneous policies [204] or policies with agent anonymity\r
[144, 175]. Here, we move beyond permutation symmetries to a broader\r
group of symmetries in multiagent reinforcement learning.\r
Symmetries in multi agent reinforcement learning Recently, [74] used\r
knowledge of symmetries to improve zero-shot coordination in games\r
which require symmetry-breaking. Here, we instead use symmetries\r
in cooperative multi-agent systems to improve data efficiency by pa\u0002rameter sharing between different transformations of the global sys\u0002tem.

44\r
3.3 Background\r
In this section we introduce the necessary definitions and notation\r
used in the rest of the chapter.\r
Multi-Agent MDPs\r
We will start from the definition of Multi-Agent MDPs, a class of fully\r
observable cooperative multi-agent systems. Full observability implies\r
that each agent can execute the same centralized policy. Later on we\r
will define a distributed variant of this type of decision making prob\u0002lem.\r
Definition 1 A Multi-Agent Markov Decision Process (MMDP) [21] is\r
a tuple (N , S, A, T, R) where N is a set of m agents, S is the state space, A =\r
A1 × · · · × Am is the joint action space of the MMDP, T : S × A1 × · · · ×\r
Am × S → [0, 1] is the transition function, and R : S × A1 × · · · × Am → R\r
is the immediate reward function.\r
The goal of an MMDP, as in the single agent case, is to find a joint\r
policy that maps states to probability distributions over joint actions,\r
π : S → ∆(A), (with ∆(A) the space of such distributions) to maximize\r
the expected discounted return of the system, Rt = E[∑\r
T\r
k=0\r
γ\r
k\r
rt+k+1]\r
with γ ∈ [0, 1] a discount factor. An MMDP can be viewed as a single\u0002agent MDP where the agent takes joint actions.\r
Groups and Transformations\r
In this chapter we will refer extensively to group symmetries. Here,\r
we will briefly introduce these concepts and explain their significance\r
to discussing equivalences of decision making problems.\r
A group G is a set with a binary operator · that obeys the group ax\u0002ioms: identity, inverse, closure, and associativity. Consider as a run\u0002ning example the set of 90 degree rotations {0\r
◦\r
, 90◦, 180◦, 270◦}, which\r
we can write as rotation matrices:\r
R(θ) = "\r
cos θ − sin θ\r
sin θ cos θ\r
#\r
(3.1)\r
with θ ∈ {0, π\r
2\r
, π,\r
3π\r
2\r
}. Composing any two matrices in this set results\r
in another matrix in the set, meaning the set is closed under com\u0002position. For example, composing R(\r
π\r
2\r
) and R(π) results in another\r
member of the set, in this case R(\r
3π\r
2\r
). Similarly, each member of the\r
set has an inverse that is also in the set, and R(0) is an identity ele-

45\r
ment. Since matrix multiplication is associative, the group axioms are\r
satisfied and the set is a group under composition.\r
A group action is a function G × X → X that satisfies ex = x (where e\r
is the identity element) and (g · h)x = g · (hx). For example, the group\r
of 90 degree rotation matrices acts on vectors to rotate them. Similar to\r
the action of this group on vectors, we can define an action of the same\r
group on image space: e.g., the NumPy [68] function np.rot90 acts on\r
the set of images. We will consider group actions on the set of states\r
represented as image observations. We match these with group actions\r
on policies. Since we consider discrete action spaces, a group element\r
g acting on a policy π will be represented as a matrix multiplication\r
of the policy with a permutation matrix.\r
Figure 3.3.1: The orbit of a traffic\r
light state under the group of 90 de\u0002gree rotations.\r
When discussing symmetries in decision making problems, we iden\u0002tify sets of state-action pairs that are equivalent: if the state is trans\u0002formed, the policy should be transformed as well, but potentially\r
with a different representation of the transformation. See Figure 3.1.1.\r
We are interested in the case where the reward and transition func\u0002tions are invariant in the orbit of state-action pairs under a symmetry\r
group. The orbit of a point v ∈ V, with V a vector space, is the set\r
of all its transformations (e.g. all rotations of the point), defined as\r
O(v) = {gv|∀g ∈ G}. The orbit of a point under a group forms an\r
equivalence class. See Figure 3.3.1 for an example of an orbit of a\r
traffic light state.\r
3.4 Distributing Symmetries over Multiple Agents\r
Consider the cooperative traffic light control system in Figure 3.1.1\r
that contains transformation-equivalent global state-action pairs. We\r
first formalize global symmetries of the system similarly to symmetries\r
in a single agent MDP. Then we will discuss how we can formulate\r
distributed symmetries in a distributed MMDP. Finally, we introduce\r
Multi-Agent MDP Homomorphic Networks.

46\r
Symmetries in MMDPs\r
We define symmetries in an MMDP similar to an MDP with symme\u0002tries [173].\r
Definition 2 An MMDP is an MMDP with symmetries if reward and tran\u0002sition functions are invariant under a transformation group G. That is, the\r
MMDP has symmetries if there is at least one non-trivial group G of trans\u0002formations Lg : S → S and for every s, Ks\r
g\r
: A → A such that\r
R(s, a) = R(Lg[s], K\r
s\r
g\r
[a]) ∀g ∈ G,s ∈ S, a ∈ A, (3.2)\r
T(s, a,s\r
0\r
) = T(Lg[s], K\r
s\r
g\r
[a], Lg[s\r
0\r
]) ∀g ∈ G,s,s\r
0 ∈ S, a ∈ A. (3.3)\r
If two state-action pairs s, a and Lg[s], K\r
s\r
g\r
[a] obey Eq. 3.2 and 3.3, then\r
they are equivalent [173]. Consider as an example the symmetries in\r
Figure 3.1.1. These symmetries can result in correspondences across\r
agents, for example when the observation of agent i is mapped by\r
the symmetry to another agent j that is arbitrarily far away and with\r
which there is no communication channel. In the next section, we will\r
resolve this problem by defining distributed symmetries in terms of\r
local observations and the communication graph defined by the state.\r
If we have an MMDP with symmetries, that means that there are sym\u0002metric optimal policies, i.e. if the state of the MMDP transforms, the\r
policy transforms accordingly. The above definition of an MMDP with\r
symmetries is only applicable to the centralized setting. If we want\r
to be able to execute policies in a distributed manner, we will need to\r
enforce equivariance in a distributed manner.\r
Distributed Multi-Agent Symmetries\r
In a distributed MMDP, agents make decisions based on local infor\u0002mation only, i.e. the local states they observe, and the communications\r
they receive from neighbors, defined as follows:\r
Definition 3 A Distributed Multiagent Markov Decision Process (Dis\u0002tributed MMDP) (N , S, A, T, R) is an MMDP where agents can communi\u0002cate as specified by a graph G = (V, E) with one node vi ∈ V per agent\r
and an edge (i, j) ∈ E if agents i and j can communicate. Thus, S =\r
({Si}i∈N , {Eij}(i,j)∈E ), with Sithe set of state features observable by agent\r
i, which may include shared global features, and Eij the set of edge features\r
between i and j. In a distributed MMDP, each agent’s action can only depend\r
on the local state and the communications it receives1.\r
1 Communication is constrained, i.e.\r
agents cannot simply share their full ob\u0002servations with each other.

47\r
Here, we focus on Distributed MMDPs which have a spatial compo\u0002nent, i.e. each agent has a coordinate in some space, and the attributes\r
of the edges between the agents in the communication graph contain\r
spatial information as well. For instance, the attributes eij ∈ E for edge\r
(i, j) might be the difference vector between agent i and agent j’s co\u0002ordinates. Since both agent observations and interaction edges have\r
spatial features, a global symmetry will affect both the agent observa\u0002tions, the agent locations, and the features on the interaction edges.\r
See Figure 3.4.1.\r
permute observations transform edges rotate observations\r
original state symmetric state\r
Figure 3.4.1: Example of how\r
a global transformation of a dis\u0002tributed traffic light control state\r
can be viewed as 1) a permutation\r
of the observations over the agents,\r
2) a permutation of the interaction\r
edges, 3) a transformation of the lo\u0002cal observations.\r
To allow a globally equivariant policy network with distributed exe\u0002cution, we might naively decide to restrict each agent’s local policy\r
network to be equivariant to local transformations. However, this does\r
not give us the correct global transformation, as joining the local trans\u0002formations does not give us the same as the global transformations, as\r
illustrated in Figure 3.4.2.\r
Instead, to get the correct transformation as shown in the left side\r
of Figure 3.4.2, the local state is transformed, but also its position is\r
changed, which can be seen as a permutation of the agents and their\r
neighbors. To give an example of the equivariance constraint that we\r
want to impose: the lower left agent (before transformation) should\r
select an action based on its local state and communication received\r
from its northern and eastern neighbor, while the top left agent (after\r
transformation) should select the transformed version of the action\r
based on its rotated local state and communication from its eastern\r
and southern neighbor.\r
Since the agent has no other information about the system, if the lo\u0002cal observations are transformed (e.g. rotated), and the messages it\r
receives are transformed similarly, then from a local perspective the\r
agent is in an equivalent state and should execute the same policy, but\r
with an equivalently transformed action.\r
From the perspective of our agent and all its neighbors, the equiva\u0002lence holds for this local subgraph as well: if the observations and

48\r
Figure 3.4.2: Example of the dif\u0002ference between a global transfor\u0002mation on the global state, and a\r
set of local transformations on lo\u0002cal states. On the left we rotate\r
the entire world by 90 degrees clock\u0002wise, which involves rotating cross\u0002ings and streets. On the right we\r
perform local uncoordinated trans\u0002formations only at the street level.\r
The latter is not a symmetry of the\r
problem. local interactions rotate relative to each other, then the whole subgraph\r
rotates. See Figure 3.4.1. Thus, as long as the transformations are\r
applied to the full set of observations and the full set of communica\u0002tions, we have a global symmetry. We therefore propose the following\r
definition of a Distributed MMDP with Symmetries.\r
Definition 4 Let s = ({si}i∈N , {eij}(i,j)∈E ). Then a Distributed MMDP\r
with symmetries is a Distributed MMDP for which the following equations\r
hold for at least one non-trivial set of group transformations Lg : S → S and\r
for every s, Ks\r
g\r
: A → A such that\r
R(s, a) = R(Lg[s], K\r
s\r
g\r
[a]) ∀g ∈ G, s ∈ S, a ∈ A (3.4)\r
T(s, a, s\r
0\r
) = T(Lg[s], K\r
s\r
g\r
[a], Lg[s\r
0\r
]) ∀g ∈ G, s, s\r
0 ∈ S, a ∈ A (3.5)\r
where equivalently to acting on s with Lg, we can act on the interaction and\r
agent features separately with L˜\r
g and Ug, to end up in the same global state:\r
Lg[s] = (P\r
s\r
g\r
[{L˜\r
g[si\r
]}i∈N ], P\r
e\r
g\r
[{Ug[eij]}(i,j)∈E ]) (3.6)\r
for L˜\r
g : Si → Si\r
, Ug : E → E, and Ps\r
g\r
and Pe\r
g\r
the state and edge permuta\u0002tions.\r
Here, E is the set of edge features. The symmetries acting on the agents\r
and agent interactions in the Distributed MMDP are a class of symme\u0002tries we call distributable symmetries. We have now defined a class of\r
Distributed MMDPs with symmetries for which we can distribute a\r
global symmetry into a set of symmetries on agents and agent inter\u0002actions. This distribution allows us to define distributed policies that\r
respect the global symmetry.\r
Multi-Agent MDP Homomorphic Networks\r
We have shown above how distributable global symmetries can be de\u0002composed into local symmetries on agent observations and agent inter-

49\r
actions. Here, we discuss how to implement distributable symmetries\r
in multi-agent systems in practice.\r
General Formulation\r
We want to build a neural network that1) allows distributed execution,\r
so that we can compute policies without a centralized controller2) al\u0002lows us to pass communications between agents (agent interactions),\r
to enable coordination and3) exhibits the following global equivari\u0002ance constraint: π~ θ (Lg[s]) = Hsg[π~ θ (s)] (3.7)\r
withH\r
sg\r
:Π→Π. Thus, the policy networkπ~ that outputs the joint\r
policy must be equivariant under group transformations of the global\r
state. To satisfy1) and2), i.e. allowing distributed execution and\r
agent-to-agent communication, as well as permutation equivariance,\r
we formulate the network as a message passing network (MPN), but\r
with global equivariance constraints. Hsg [π~ ] = MPN\r
θ\r
(L\r
g\r
[s])(3.8)\r
Since a network is end-to-end equivariant if all its layers are equivari\u0002ant with matching representations [34], we require layer-wise equiv\u0002ariance constraints on the layers. A single layer in an MPN is given by\r
a set of node updates, i.e.f\r
(l+1)\r
i\r
=φ\r
u\r
\u0010\r
f\r
(l)\r
i\r
,\r
∑\r
|Ni| j=1\r
φ\r
m\r
\u0010\r
eij,f\r
(l)\r
j\r
\u0011\u0011, with\r
f\r
(l)\r
j\r
the current encoding of agentj in layerl,φ\r
m the message function\r
that computesm\r
j→i based on the edge\r
eij and the current encoding\r
of agentj, andφ\r
u the node update function that updates agent\r
i’s\r
current encoding based onf\r
(l)\r
i\r
and the aggregated received message\r
m\r
(l)\r
i\r
. Since the layer is given by a set of node updates, the equivari\u0002ance constraint is on the node updates. In other words, φu must be\r
an equivariant function of local encodingf\r
(l)\r
i\r
and aggregated message\r
m\r
(l)\r
i\r
:\r
Pg\r
h\r
φ\r
u\r
(f\r
(l)\r
i\r
,m\r
(l)\r
i\r
)\r
i\r
=φ\r
u\r
\u0010\r
L\r
g\r
[f\r
(l)\r
i\r
],L\r
g\r
[m\r
(l)\r
i\r
]\r
\u0011\r
(3.9)\r
Thus, the node update functionφ\r
u is constrained to be equivariant to\r
transformations of inputsf\r
(l)\r
i\r
andm\r
(l)\r
i\r
. Therefore, to conclude that\r
the outputs ofφ\r
u transform according to\r
Pg, we only need to enforce\r
that its inputsfi andm\r
i\r
transform according toL\r
g. Thus, the sub\u0002function φm that computes the messages m(l) i must be constrained to\r
be equivariant as well. Sinceφ\r
m takes as input the previous layer’s\r
encodings as well as the edgeseij, this means that1) the encodings\r
must contain geometric information about the state, e.g. which rota\u0002tion the local state is in and 2) the edge attributes contain geometric

50\r
information as well, i.e. they transform when the global state trans\u0002forms (Appendix 3.B).\r
Lg\r
h\r
m\r
(l)\r
i\r
i\r
=\r
|Ni|\r
∑\r
j=1\r
φm\r
\u0010\r
Ug\r
\u0002\r
eij\u0003\r
, Lg[ f\r
(l)\r
j\r
]\r
\u0011\r
(3.10)\r
Note that this constraint is satisfied when φm is equivariant, since lin\u0002ear combinations of equivariant functions are also equivariant [35].\r
Putting this all together, the local encoding f\r
(l)\r
i\r
for each agent is equiv\u0002ariant to the set of edge rotations and the set of rotations of encodings\r
in the previous layer. For more details, see Appendix 3.B. Thus, we\r
now have the general formulation of Multi-Agent MDP Homomor\u0002phic Networks. At execution time, the distributed nature of Multi\u0002Agent MDP Homomorphic Networks allows them to be copied onto\r
different devices and messages exchanged between agents only locally,\r
while still enforcing the global symmetries.\r
Multi-Agent MDP Homomorphic Network Architecture\r
Multi-Agent MDP Homomorphic Networks consist of equivariant lo\u0002cal observation encoders φe\r
: Si → R|G|×D, where G is the group,\r
|G| is its size, and D the dimension of the encoding, equivariant lo\u0002cal message functions φm : E × R|G|×D → R|G|×F where F is di\u0002mension of the message encoding, equivariant local update functions\r
φu : R|G|×D × R|G|×F → R|G|×D, and equivariant local policy predic\u0002tors φπ : R|G|×D → Π(Ai). Take the example of multi-agent traffic\r
light control with 90 degree rotation symmetries, which we evaluate\r
on in Section 3.5. In this setting, we wish to constrain φe to be equiv\u0002ariant to rotations Rg of the local observations. We will require the\r
outputs of φe to permute according to L\r
−1\r
g whenever the input rotates\r
by Rg.\r
Lg [φe(si)] = φe(Rg [si\r
]) ∀g ∈ G (3.11)\r
This has the form of a standard equivariance constraint, which allows\r
conventional approaches to enforcing group equivariance, e.g. [34].\r
In this chapter, we will enforce group equivariance using the Sym\u0002metrizer [173]. Before training, the Symmetrizer enforces group (e.g.\r
rotational) symmetries by projecting neural network weights onto an\r
equivariant subspace, and then uses SVD to find a basis for the equiv\u0002ariant subspace. Then, during and after training, the weight matrix of\r
the neural network is realised as a linear combination of equivariant\r
basis weights, and the coefficients of the linear combination are up\u0002dated during training with PPO [153]. We use ReLU non-linearities\r
and regular representations. For more details on the Symmetrizer, we\r
refer the reader to [173].

51\r
After encoding the input observations with the equivariant encoding\r
functionφ\r
e\r
, we have an equivariant encoding of the local states that\r
has a compositional form: the rotation of the state is represented by the\r
ordering of group channels (see [173]) and the other state features are\r
represented by the information in those channels. Similarly, we con\u0002strain the message update functions to be equivariant to the permuta\u0002tion Lg in the group channels of the state encodings and the rotation Ug of a difference vector eij, representing the edge (i, j): Lg hφm \u0010eij, f (l) j\r
\u0011i\r
=φ\r
m\r
\u0010\r
Ug\r
\u0002\r
eij\r
\u0003\r
,L\r
g\r
h\r
f\r
(l)\r
j\r
i\u0011\r
(3.12)\r
Since linear combinations of equivariant functions are equivariant as\r
well [35], the aggregated messagem\r
(l)\r
i\r
=\r
∑\r
|Ni| j\r
m\r
j→i\r
is equivariant too.\r
Whileeij andf\r
(l)\r
j\r
transform under the same groupG, they do not\r
transform under the same group action:eij is a vector that transforms\r
with a rotation matrix, whereasf\r
(l)\r
j\r
transforms with a permutation of\r
group channels. The question arises how to build group equivariant\r
layers that transform both the edge and the agent features appropri\u0002ately. The method we use is to build equivariant layers using direct\r
sum representations, where the representationsUg andL\r
g are com\u0002bined as follows: Tg = Ug ⊕ Lg = "Ug 0 0 Lg# (3.13)\r
where 0 represents a zero-matrix of the appropriate size. Consider a\r
weight matrixW\r
l acting on\r
"\r
eij f (l\r
)\r
j\r
#\r
. The equivariance constraint then\r
becomesW\r
(l)\r
Tg=L\r
g\r
W\r
(l)\r
.\r
To preserve the geometric information coming from the messages, the\r
node update function is similarly constrained to be equivariant. Im\u0002portantly, the permutation on the outputs of φu must match the per\u0002mutation on the inputs of the next layer’s φm (i.e. the output of one\r
layer must use the same group representation as the input of the next\r
layer). This ensures that we can add multiple layers together while\r
preserving the geometric information. In practice, it is convenient to\r
use a single representationL\r
g for all permutation representations.\r
L\r
g\r
h\r
φ\r
u\r
(f\r
(l)\r
i\r
,m\r
(l)\r
i\r
)\r
i\r
=φ\r
u\r
\u0010\r
L\r
g\r
[f\r
(l)\r
i\r
],L\r
g\r
[m\r
(l)\r
i\r
]\r
\u0011\r
(3.14)\r
Finally, afterM layers (M message passing rounds), we output local\r
equivariant policies based on the state encodings at layerM using local\r
policy networkπ:\r
π\r
i\r
\u0010\r
L\r
g\r
h\r
f\r
(M)\r
i\r
i\u0011\r
=Pg\r
h\r
π\r
i\r
\u0010\r
f\r
(M)\r
i\r
\u0011i\r
(3.15)

52\r
Here, Pg is the permutation representation on the actions of the indi\u0002vidual agent, e.g. if in a grid world the state is flipped, Pg is the matrix\r
that permutes the left and right actions accordingly.\r
3.5 Experiments\r
The evaluation of Multi Agent MDP Homomorphic networks has a\r
singular goal: to investigate and quantify the effect of distributed ver\u0002sions of global equivariance in symmetric cooperative multi-agent re\u0002inforcement learning. We compare to three baselines. The first is a\r
non-homomorphic variant of our network. This is a standard MPN,\r
which is a permutation equivariant multi-agent graph network but\r
not equivariant to global rotations. The other two are variants with\r
symmetric data augmentation, in the spirit of [107, 99]. For a stochas\u0002tic data augmentation baseline, on each forward pass one of the group\r
elements is sampled and used to transform the input, and appropri\u0002ately transform the output as well. For a full augmentation baseline,\r
every state and policy is augmented with all its rotations in the group.\r
For evaluation, we use the common centralized training, decentralized\r
execution paradigm [100, 133] (see Appendix 3.A for more details).\r
We train in a centralized fashion, with PPO [153], which will adjust\r
the coefficients of the weight matrices in the network. The informa\u0002tion available to the actors and critics is their local information and the\r
information received from neighbors. We first evaluate on a wildlife\r
monitoring task, a variant of predator-prey type problems with pixel\u0002based inputs where agents can have information none of the other\r
agents have. Additionally, we evaluate the networks on the more com\u0002plex coordination problem of traffic light control, with pixel-based in\u0002puts. We focus on C4 as the discrete group to investigate whether\r
equivariance improves multi-agent systems, as C4 has been shown to\r
be effective in supervised learning and single-agent settings.\r
Wildlife Monitoring\r
0 200 400 600 800 1000\r
Time steps (x 500) \r
3\r
2\r
1\r
0\r
1\r
2\r
3\r
Average Return\r
Standard MPN\r
Augmented MPN\r
Equivariant MPN\r
(a) 3 agents.\r
0 200 400 600 800 1000\r
Time steps (x 500) \r
3\r
2\r
1\r
0\r
1\r
2\r
3\r
Average Return\r
Standard MPN\r
Augmented MPN\r
Equivariant MPN\r
(b) 4 agents.\r
Figure 3.5.1: Results for the dis\u0002tributed drone wildlife monitoring\r
task. 25%, 50% and 75% quan\u0002tiles shown over 15 random seeds.\r
All approaches tuned over 6 learn\u0002ing rates.

53\r
Setup We evaluate on a distributed wildlife monitoring setup, where\r
a set of drones has to coordinate to trap poachers. To trap a poacher,\r
one drone has to hover above them while the other assists from the\r
side, and for each drone that assists the team receives +1 reward. Two\r
drones cannot be in the same location at the same time. Since the\r
drones have only cameras mounted at the bottom, they cannot see\r
each other. The episode ends when the poacher is trapped by at least 2\r
drones, or 100 time steps have passed. On each time step the team gets\r
-0.05 reward. All agents (including the poacher) can stand still or move\r
in the compass directions. The poacher samples actions uniformly. We\r
train for 500k time steps. The drones can send communications to\r
drones within a 3 by 3 radius around their current location, meaning\r
that the problem is a distributed MMDP. Due to changing agent loca\u0002tions and the limited communication radius, the communication graph\r
is dynamic and can change between time steps. The observations are\r
21 by 21 images representing a agent-centric view of a 7 by 7 toroidal\r
grid environment that shows where the target is relative to the drone.\r
While the grid is toroidal, the communication distance is not: at the\r
edges of the grid, communication is blocked. This problem exhibits 90\r
degree rotations: when the global state rotates, the agents’ local poli\u0002cies should permute, and so should the probabilities assigned to the\r
actions in the local policies.\r
Results Results for this task are shown in Figure 3.5.1, with on the y\u0002axis the average return and on the x-axis the number of time steps. In\r
both the 3-agent and 4-agent case, using a Multi-Agent MDP Homo\u0002morphic Network improves compared to using MPNs without sym\u0002metry information, and compared to using symmetric data augmen\u0002tation. We conclude that in the proposed task, our approach learns\r
effective joint policies in fewer environment interactions compared to\r
the baselines.\r
Traffic Light Control\r
For a second experiment, we focus on a more complex coordination\r
problem: reducing vehicle wait times in traffic light control. Traffic\r
light control constitutes a longstanding and open problem (see [185]\r
for an overview): not only is the optimal coordination strategy non\u0002obvious, traffic light control is a problem where wrong decisions can\r
quickly lead to highly suboptimal states due to congestion. We use this\r
setting to answer the following question: does enforcing symmetries\r
help in complex coordination problems?

54\r
0 200 400 600 800 1000\r
Time steps (x 500) \r
8\r
9\r
10\r
11\r
12\r
13\r
14\r
15\r
16\r
17\r
Average Wait Time\r
Standard MPN\r
Stoch. Augmented MPN\r
Full Augmented MPN\r
Equivariant MPN\r
Figure 3.5.2: Average vehicle wait\r
times for distributed settings of the\r
traffic light control task. Graphs\r
show 25%, 50% and 75% quantiles\r
over 20 independent training runs.\r
All approaches tuned over 6 learn\u0002ing rates.\r
Setup We use a traffic simulator with four traffic lights. On each of\r
eight entry roads, for 100 time steps, a vehicle enters the simulation on\r
each step with probability 0.1. Each agent controls the lights of a single\r
intersection and has a local action space of (grgr, rgrg), indicating\r
which two of its four lanes get a red or green light. Vehicles move\r
at a rate of one unit per step, unless they are blocked by a red light\r
or a vehicle. If blocked, the vehicle needs one step to restart. The\r
goal is reducing the average vehicle waiting time. The simulation ends\r
after all vehicles have exited the system, or after 500 steps. The team\r
reward is − 1\r
1000\r
1\r
C ∑c∈C w(c), with C the vehicles in the system and\r
w(c) vehicle c’s cumulative waiting time.\r
Results We show results in Figure 3.5.2. While the standard MPN\r
architecture had reasonable performance on the toy problem, it takes\r
many environment interactions to improve the policy in the more com\u0002plex coordination problem presented by traffic light control. Adding\r
data augmentation helps slightly. However, we see that enforcing\r
the global symmetry helps the network find an effective policy much\r
faster. In this setting, the coordination problem is hard to solve: in ex\u0002periments with centralized controllers, the standard baseline performs\r
better, though it is still slower to converge than the equivariant cen\u0002tralized controller. Overall, enforcing global symmetries in distributed\r
traffic light control leads to effective policies in fewer environment in\u0002teractions.

55\r
3.6 E(3) Equivariance\r
Many tasks can benefit from making use of their inherent symmetries\r
and structure. We saw in this Chapter that we can construct graph\r
networks that are equivariant to global transformations while still al\u0002lowing for distributed execution. Such networks make use of both the\r
structure and the symmetry of cooperative multi-agent problems.\r
In this Chapter and Chapter 2, we consider symmetries in decision\r
making problems, looking in particular at discrete groups of 2 or 4\r
elements using regular representations. However, many real world\r
problems exhibit symmetries that are continuous, for example as is\r
the case in robotics [203, 179, 136] or molecular design [157]. For that\r
reason, later work has looked at a broader class of symmetries in single\r
agent learning, such as the SE(2) [203], SO(2) [179], SO(3) [136, 157]\r
groups. In this Section, we discuss Steerable E(3) Equivariant graph\r
networks (SEGNNs), which take the ideas presented in this Chapter a\r
step further: we consider the E(3) group, the group of all isometries of\r
3d Euclidean space (rotations, reflections, and translations). SEGNNs\r
are a class of E(3) equivariant graph neural networks that use non\u0002linear convolutions. SEGNNs show promising results on physics and\r
chemistry tasks, where for example molecular data tends to exhibit\r
both graph structure and E(3) symmetry. See Figure 3.6.1.\r
Figure 3.6.1: Equivariance di\u0002agram for equivariant operator\r
φ for 3D molecular graph with\r
steerable node features. If the\r
molecule rotates, the node fea\u0002tures do as well.\r
Earlier work considered equivariant convolutional neural networks [34,\r
35, 193, 32, 98, 187, 14, 13, 186]. Similarly, there is earlier work on\r
equivariant GNN architectures [195, 101, 166, 7, 52, 50]. It turns out\r
that where early work tends to linearly transform the node and edge\r
features2followed by a non-linearity, Multi-Agent MDP Homomor\u00022 Pseudo-linearly for methods using at\u0002tention [52, 7]\r
phic Networks and SEGNNs use non-linear message aggregation. SEG\u0002NNs in particular also use steerable messages (rather than invariant or\r
discrete equivariant messages). SEGNNs use non-linear group convo\u0002lutions, and can be seen as a generalization of EGNNs [147], which\r
sends invariant messages, where SEGNNs send equivariant messages.\r
SEGNNs are able to perform non-linear convolutions with rotation-

56\r
ally invariant scalars or covariant vectors or tensors. Since molecu\u0002lar data sets can include node information such as velocity, force or\r
atomic spin, SEGNNs are evaluated on n-Body toy datasets, QM9, and\r
OC20, where they are able to use the geometric and physical attributes\r
present in the data. SEGNNs set a new state of the art on n-Body toy\r
datasets. Additionally, SEGNNs are a new state of the art in ISRE of\r
OC20 and competitive on QM9. For more details, see [23]. Seeing the\r
potential of SEGNNs on molecular prediction tasks, it is likely they can\r
provide possible benefits when adapted into Multi-agent MDP Homo\u0002morphic Networks for E(3) Equivariant multi-agent policy networks\r
following the ideas in this Chapter, for example for applications in\r
robotics or molecular design.\r
3.7 Conclusion\r
We consider distributed cooperative multi-agent systems that exhibit\r
global symmetries. In particular, we propose a factorization of global\r
symmetries into symmetries on local observations and local interac\u0002tions. On this basis, we propose Multi-Agent MDP Homomorphic\r
Networks, a class of policy networks that allows distributed execution\r
while being equivariant to global symmetries. We compare to non\u0002equivariant distributed networks, and show that global equivariance\r
improves data efficiency on both a predator-prey variant, and on the\r
complex coordination problem of traffic light control.\r
Scope We focus on discrete groups. For future work, this could be\r
generalized by using steerable representations, at the cost of not being\r
able to use pointwise nonlinearities. We also focus on discrete actions.\r
This might be generalized by going beyond action permutations, e.g.,\r
for 2D continuous worlds a continuous rotation of the actions. Fur\u0002thermore, our approach uses group channels for each layer (regular\r
representations). For small groups this is not an issue, but for much\r
larger groups this would require infeasible computational resources.\r
Finally, this work has focused on exact symmetries and considers im\u0002perfect symmetries and violations of symmetry constraints a promis\u0002ing future topic.

57\r
3.8 Ethics Statement\r
Our work has several potential future applications, e.g. in autonomous\r
driving, decentralized smart grids or robotics. Such applications hope\u0002fully have a positive societal impact, but there are also risks of nega\u0002tive societal impact: through the application itself (e.g. military), labor\r
market impact, or by use in safety-critical applications without proper\r
verification and validation. These factors should be taken into account\r
when developing such applications.\r
3.9 Reproducibility Statement\r
To ensure reproducibility, we describe our setup in the Experiments\r
section and we include hyperparameters, group actions, and architec\u0002ture details in the Appendix. We will also make our code publicly\r
available.

59\r
Chapter Appendix\r
3.A Message Passing Networks, Communication, and Dis\u0002tribution\r
Message passing algorithms are commonly used to coordinate be\u0002tween agents while allowing for factorization of the global decision\r
making function [64, 105, 171, 18]. Message passing networks approx\u0002imate such message passing algorithms [199, 148]. Thus, we can view\r
message passing networks as a type of learned communication be\u0002tween coordinating agents. Message passing networks can be executed\r
using only local communication and computation. To see that this is\r
the case, consider the equations that determine the message passing\r
networks in this paper:\r
f\r
(0)\r
i = φe(si) (3.16)\r
m\r
(l)\r
j→i = φm(eij, f\r
(l)\r
j\r
) (3.17)\r
m\r
(l)\r
i =\r
|Ni|\r
∑\r
j\r
mj→i(3.18)\r
f\r
(l+1)\r
i = φu(f\r
(l)\r
i\r
, m\r
(l)\r
i\r
) (3.19)\r
for all agents i and layers (message passing rounds) l. In Eq. 3.16,\r
each agent encodes its local observation siinto a local feature vector\r
f\r
(0)\r
i\r
. In Eq. 3.17, each agent j computes its message to agent i using its\r
own local feature vector f\r
(l)\r
j\r
and its shared edge features eij = xi − xj.\r
After agent i receives its neighbors’ messages, it aggregates them in\r
Eq. 3.18. Finally, in Eq. 3.19 agent i updates its local feature vector\r
using the aggregated message and its local feature vector. Clearly,\r
every step in this network requires only local computation and local\r
communication, therefore allowing the network to be distributed over\r
agents at execution time.

60\r
3.B Equivariance of Proposed Message Passing Layers\r
Recall that eij = xi − xj(the difference between the locations of agent i\r
and agent j). Therefore,\r
Ug[eij] = Ug[xi − xj] = Ug[xi] − Ug[xj] (3.20)\r
So, when both agent i and agent j are moved to a location transformed\r
by Ug, the edge features eij are transformed by Ug as well. If all agent\r
positions and observations rotate by the same underlying group, this\r
means the full system has rotated. In this paper, we place equivariance\r
constraints on the message function:\r
Kg[∑\r
j\r
φm(eij, fj)] = ∑\r
j\r
φm(Ug[eij], Lg[ fj]) (3.21)\r
This means that the messages are only permuted by Kg if both the\r
local features fj and the edge features eij are transformed (by Lg and\r
Ug respectively). To see that the proposed message passing layers are\r
equivariant to transformations on agent features and edge features,\r
consider the following example. Assume we have a message passing\r
layer φ consisting of node update function φu and message update\r
function φm, such that with agent features { fi}, and edge features {eij},\r
the output for agent i is given by\r
φ\r
(i)\r
\r
{eij}, { fj}\r
\u0001\r
= φu (fi\r
, mi) (3.22)\r
= φu\r
 \r
fi,∑\r
j\r
φm\r
\r
eij, fj\r
\u0001\r
!\r
(3.23)\r
Assume φu and φm are constrained to be equivariant in the following\r
way:\r
Pg[φu(fi, mi)] = φu(Lg[ fi], Kg[mi]) (3.24)\r
Kg[∑\r
j\r
φm(eij, fj)] = ∑\r
j\r
φm(Ug[eij], Lg[ fj]) (3.25)

61\r
for group elements g ∈ G. Then, for layer φ:\r
Pg\r
h\r
φ\r
(i)\r
\r
{eij}, { fj}\r
\u0001\r
i\r
= Pg [φu (fi\r
, mi)] (3.26)\r
= Pg\r
"\r
φu\r
 \r
fi,∑\r
j\r
φm\r
\r
eij, fj\r
\u0001\r
!# (3.27)\r
= φu\r
 \r
Lg [ fi\r
] , Kg\r
"\r
∑\r
j\r
φm\r
\r
eij, fj\r
\u0001\r
#! (3.28)\r
(using Eq. 3.24) (3.29)\r
= φu\r
 \r
Lg [ fi\r
] ,∑\r
j\r
φm\r
\r
Ug\r
\u0002\r
eij\u0003\r
, Lg\r
\u0002\r
fj\r
\u0003\u0001!\r
(3.30)\r
(using Eq. 3.25) (3.31)\r
= φ\r
(i)\r
\r
{Ug\r
\u0002\r
eij\u0003\r
}, {Lg\r
\u0002\r
fj\r
\u0003\r
}\r
\u0001\r
(3.32)\r
3.C Discrete Rotations of Continuous Vectors\r
Here we outline how to build weight matrices equivariant to discrete\r
rotations of continuous vectors. Let eij = xj − xi be an arbitrary, con\u0002tinuous difference vector between the coordinates of agent j and the\r
coordinates of agent i. Then this difference vector transforms under 90\r
degree rotations using the group of standard 2D rotation matrices of\r
the form\r
Rg = R(θ) = "\r
cos θ − sin θ\r
sin θ cos θ\r
#\r
(3.33)\r
for θ ∈ [0, π\r
2\r
, π,\r
3π\r
2\r
]. A weight matrix W is now equivariant to 90\r
degree rotations of eij if\r
KgWeij = WRgeij (3.34)\r
with {Kg}g∈G e.g. a permutation matrix representation of the same\r
group. So,\r
W = K\r
−1\r
g WRg (3.35)\r
which we can solve using standard approaches.\r
3.D Experimental Details\r
For all approaches, including baselines, we run at least 15 random\r
seeds for 6 different learning rates, {0.001, 0.003, 0.0001, 0.0003, 0.00001,

62\r
0.00003}, and report the best learning rate for each. Other hyperpa\u0002rameters are taken as default in the codebase [159, 173].\r
Learning Rates Reported\r
After tuning the learning rate, we report the best one for each ap\u0002proach. See Table 3.D.1.\r
Distributed Settings Standard Augmented Equivariant\r
Drones, 3 agents 0.001 0.0003 0.001\r
Drones, 4 agents 0.0003 0.001 0.001\r
Traffic, 4 agents 0.0001 0.0001 0.0001\r
Table 3.D.1: Best learning rates\r
for distributed settings for\r
MPNs.\r
Assets Used\r
• Numpy [68] 1.19.2: BSD 3-Clause "New" or "Revised" License;\r
• PyTorch [137] 1.2.0: Modified BSD license;\r
• RLPYT [159]: MIT license;\r
• MDP Homomorphic Networks & Symmetrizer [173]: MIT license.\r
3.E Architectural details\r
Architectures are given below and were chosen to be as similar as\r
possible between different approaches, keeping the number of train\u0002able parameters comparable between approaches. We chose 2 message\r
passing layers to allow for 2 message passing hops. For the message\r
passing networks, we use L1-normalization of the adjacency matrix.\r
Architectural Overview\r
The global structure of our network is given in Figure 3.E.1.\r
G-CNN\r
z3\r
Equivariant \r
Message \r
Passing\r
ⲡN\r
ⲡ3\r
ⲡ2\r
ⲡ1\r
G-CNN\r
G-CNN\r
G-CNN\r
z2\r
z1\r
zN\r
xNx2\r
x1\r
x3\r
... ... ...\r
vN\r
v3\r
v2\r
v1...\r
Figure 3.E.1: General overview\r
of Multi-Agent MDP Homo\u0002morphic Networks. G-CNN\r
refers to a group-equivariant\r
CNN encoder. Equivariant\r
message passing refers to the\r
proposed equivariant message\r
passing networks. Encoding lo\u0002cal states with group-CNNs en\u0002sure the state encodings zi are\r
group-equivariant. The loca\u0002tions xi are input to the equiv\u0002ariant message passing network.

63\r
Architectures\r
Wildlife Monitoring\r
Listing 3.1: Equivariant Network Architecture for Centralized Drones\r
1 EqConv2d(repr_in=1, channels_in=m, repr_out=4, channels_out=b √\r
16\r
4\r
c,\r
2 filter_size=(7, 7), stride=2, padding=0)\r
3 ReLU()\r
4 EqConv2d(repr_in=4, channels_in=b √\r
16\r
4\r
c, repr_out=4, channels_out=b √\r
32\r
4\r
c,\r
5 filter_size=(5, 5), stride=1, padding=0)\r
6 ReLU()\r
7 GlobalMaxPool()\r
8 EqLinear(repr_in=4, channels_in=b √\r
32\r
4\r
c, repr_out=4, channels_out=b\r
128\r
√\r
4\r
c)\r
9 ReLU()\r
10 EqLinear(repr_in=4, channels_in=b\r
128\r
√\r
4\r
c, repr_out=5, channels_out=b √\r
64\r
4\r
c)\r
11 ReLU()\r
12 ModuleList([EqLinear(repr_in=4, channels_in=b √\r
64\r
4\r
c, repr_out=5,\r
13 channels_out=1) for i in range(m)])\r
14 EqLinear(repr_in=4, channels_in=b √\r
64\r
4\r
c, repr_out=1, channels_out=1)\r
Listing 3.2: CNN Architecture for Centralized Drones\r
1 Conv2d(channels_in=m, channels_out=16,\r
2 filter_size=(7, 7), stride=2, padding=0)\r
3 ReLU()\r
4 Conv2d(channels_in=16,channels_out=32,\r
5 filter_size=(5, 5), stride=1, padding=0)\r
6 ReLU()\r
7 GlobalMaxPool()\r
8 Linear(channels_in=32, channels_out=256)\r
9 ReLU()\r
10 ModuleList([Linear(channels_in=256,\r
11 channels_out=5) for i in range(m)])\r
12 Linear(channels_in=256, channels_out=1)\r
Listing 3.3: Equivariant Network Architecture for Distributed Drones\r
1 EqConv2d(repr_in=1, channels_in=1, repr_out=4, channels_out=b √\r
16\r
4\r
c,\r
2 filter_size=(7, 7), stride=2, padding=0)\r
3 ReLU()\r
4 EqConv2d(repr_in=4, channels_in=b √\r
16\r
4\r
c, repr_out=4, channels_out=b √\r
32\r
4\r
c,\r
5 filter_size=(5, 5), stride=1, padding=0)\r
6 ReLU()\r
7 GlobalMaxPool()\r
8 EqMessagePassingLayer(repr_in=4+4+2, channels_in=b √\r
32\r
4\r
c, repr_out=4,\r
9 channels_out=b √\r
64\r
4\r
c)\r
10 ReLU()\r
11 EqMessagePassingLayer(repr_in=4+4+2, channels_in=b √\r
64\r
4\r
c, repr_out=4,\r
12 channels_out=b √\r
64\r
4\r
c)

64\r
13 ReLU()\r
14 EqMessagePassingLayer(repr_in=4, channels_in=b √\r
64\r
4\r
c, repr_out=5,\r
15 channels_out=1)\r
16 EqMessagePassingLayer(repr_in=4, channels_in=b √\r
64\r
4\r
c, repr_out=1,\r
17 channels_out=1)\r
Listing 3.4: MPN Architecture for Distributed Drones\r
1 Conv2d(channels_in=1, channels_out=16,\r
2 filter_size=(7, 7), stride=2, padding=0)\r
3 ReLU()\r
4 Conv2d(channels_in=16, channels_out=32,\r
5 filter_size=(5, 5), stride=1, padding=0)\r
6 ReLU()\r
7 GlobalMaxPool()\r
8 MessagePassingLayer(channels_in=32+32+2, channels_out=64)\r
9 ReLU()\r
10 MessagePassingLayer(channels_in=64+64+2, channels_out=64)\r
11 ReLU()\r
12 Linear(channels_in=64, channels_out=5)\r
13 Linear(channels_in=64, channels_out=1)\r
Traffic Light Control\r
Listing 3.5: Equivariant Network Architecture for Centralized Traffic\r
1 EqConv2d(repr_in=1, channels_in=3, repr_out=4, channels_out=b √\r
16\r
4\r
c,\r
2 filter_size=(7, 7), stride=2, padding=0)\r
3 ReLU()\r
4 EqConv2d(repr_in=4, channels_in=b √\r
16\r
4\r
c, repr_out=4, channels_out=b √\r
32\r
4\r
c,\r
5 filter_size=(5, 5), stride=1, padding=0)\r
6 ReLU()\r
7 GlobalMaxPool()\r
8 EqLinear(repr_in=4, channels_in=b √\r
32\r
4\r
c, repr_out=4, channels_out=b\r
128\r
√\r
4\r
c)\r
9 ReLU()\r
10 EqLinear(repr_in=4, channels_in=b\r
128\r
√\r
4\r
c, repr_out=5, channels_out=b √\r
64\r
4\r
c)\r
11 ReLU()\r
12 EqLinear(repr_in=4, channels_in=b √\r
64\r
4\r
c, repr_out=8, channels_out=1)\r
13 EqLinear(repr_in=4, channels_in=b √\r
64\r
4\r
c, repr_out=1, channels_out=1)\r
Listing 3.6: CNN Architecture for Centralized Traffic\r
1 Conv2d(channels_in=3, channels_out=16,\r
2 filter_size=(7, 7), stride=2, padding=0)\r
3 ReLU()\r
4 Conv2d(channels_in=16,channels_out=32,\r
5 filter_size=(5, 5), stride=1, padding=0)\r
6 ReLU()\r
7 GlobalMaxPool()\r
8 Linear(channels_in=32, channels_out=256)

65\r
9 ReLU()\r
10 Linear(channels_in=256, channels_out=8)\r
11 Linear(channels_in=256, channels_out=1)\r
Listing 3.7: Equivariant Network Architecture for Distributed Traffic\r
1 EqConv2d(repr_in=1, channels_in=3, repr_out=4, channels_out=b √\r
16\r
4\r
c,\r
2 filter_size=(7, 7), stride=2, padding=0)\r
3 ReLU()\r
4 EqConv2d(repr_in=4, channels_in=b √\r
16\r
4\r
c, repr_out=4, channels_out=b √\r
32\r
4\r
c,\r
5 filter_size=(5, 5), stride=1, padding=0)\r
6 ReLU()\r
7 GlobalMaxPool()\r
8 EqMessagePassingLayer(repr_in=4+4+2, channels_in=b √\r
32\r
4\r
c, repr_out=4,\r
9 channels_out=b √\r
64\r
4\r
c)\r
10 ReLU()\r
11 EqMessagePassingLayer(repr_in=4+4+2, channels_in=b √\r
64\r
4\r
c, repr_out=4,\r
12 channels_out=b √\r
64\r
4\r
c)\r
13 ReLU()\r
14 EqMessagePassingLayer(repr_in=4, channels_in=b √\r
64\r
4\r
c, repr_out=2,\r
15 channels_out=1)\r
16 EqMessagePassingLayer(repr_in=4, channels_in=b √\r
64\r
4\r
c, repr_out=1,\r
17 channels_out=1)\r
Listing 3.8: MPN Architecture for Distributed Traffic\r
1 Conv2d(channels_in=3, channels_out=16,\r
2 filter_size=(7, 7), stride=2, padding=0)\r
3 ReLU()\r
4 Conv2d(channels_in=16, channels_out=32,\r
5 filter_size=(5, 5), stride=1, padding=0)\r
6 ReLU()\r
7 GlobalMaxPool()\r
8 MessagePassingLayer(channels_in=32+32+2, channels_out=64)\r
9 ReLU()\r
10 MessagePassingLayer(channels_in=64+64+2, channels_out=64)\r
11 ReLU()\r
12 Linear(channels_in=64, channels_out=2)\r
13 Linear(channels_in=64, channels_out=1)\r
Group Actions\r
Here we list the group actions used in different equivariant layers\r
throughout our experiments. For all equivariant layers, we use the\r
Symmetrizer [173] to find equivariant weight bases.\r
Rotation-equivariant Filters For all equivariant encoder networks, we\r
create 90 degree rotation-equivariant filters using np.rot90.

66\r
Group Actions for Wildlife Monitoring\r
Linear layers Permutation matrices representing the following permu\u0002tations:\r
e = [0, 1, 2, 3]\r
g1 = [3, 0, 1, 2]\r
g2 = [2, 3, 0, 1]\r
g3 = [1, 2, 3, 0]\r
Policy layers, centralized Permutation matrices representing the fol\u0002lowing permutations:\r
e = [0, 1, 2, 3, 4]\r
g1 = [0, 2, 3, 4, 1]\r
g2 = [0, 3, 4, 1, 2]\r
g3 = [0, 4, 1, 2, 3]\r
Value layers, centralized Permutation matrices representing the follow\u0002ing permutations:\r
e = [1]\r
g1 = [1]\r
g2 = [1]\r
g3 = [1]\r
Message Passing Layers Acting on state features, permutation matrices\r
representing the following permutations:\r
e = [0, 1, 2, 3]\r
g1 = [3, 0, 1, 2]\r
g2 = [2, 3, 0, 1]\r
g3 = [1, 2, 3, 0]\r
Acting on edge features, the following rotation matrices:\r
e=np.eye(2)\r
g1=np.array([[0, -1], [1, 0]])\r
g2=np.array([[-1, 0], [0, -1]])\r
g3=np.array([[0, 1], [-1, 0]])\r
Policy layers, distributed Permutation matrices representing the fol\u0002lowing permutations:\r
e = [0, 1, 2, 3, 4]\r
g1 = [0, 2, 3, 4, 1]\r
g2 = [0, 3, 4, 1, 2]

67\r
g3 = [0, 4, 1, 2, 3]\r
Value layers, distributed Permutation matrices representing the follow\u0002ing permutations:\r
e = [1]\r
g1 = [1]\r
g2 = [1]\r
g3 = [1]\r
Group Actions for Traffic Light Control\r
Linear layers Permutation matrices representing the following permu\u0002tations:\r
e = [0, 1, 2, 3]\r
g1 = [3, 0, 1, 2]\r
g2 = [2, 3, 0, 1]\r
g3 = [1, 2, 3, 0]\r
Policy layers, centralized Permutation matrices representing the fol\u0002lowing permutations:\r
e = [0, 1, 2, 3, 4, 5, 6, 7]\r
g1 = [5, 4, 1, 0, 7, 6, 3, 2]\r
g2 = [6, 7, 4, 5, 2, 3, 0, 1]\r
g3 = [3, 2, 7, 6, 1, 0, 5, 4]\r
Value layers, centralized Permutation matrices representing the follow\u0002ing permutations:\r
e = [1]\r
g1 = [1]\r
g2 = [1]\r
g3 = [1]\r
Message Passing Layers Acting on state features, permutation matrices\r
representing the following permutations:\r
e = [0, 1, 2, 3]\r
g1 = [3, 0, 1, 2]\r
g2 = [2, 3, 0, 1]\r
g3 = [1, 2, 3, 0]\r
Acting on edge features, the following rotation matrices:

68\r
e =np.eye(2)\r
g1 =np.array([[0, -1], [1, 0]])\r
g2 =np.array([[-1, 0], [0, -1]])\r
g3 =np.array([[0, 1], [-1, 0]])\r
Policy layers, distributed Permutation matrices representing the fol\u0002lowing permutations:\r
e = [0, 1]\r
g1 = [1, 0]\r
g2 = [0, 1]\r
g3 = [1, 0]\r
Value layers, distributed Permutation matrices representing the follow\u0002ing permutations:\r
e = [1]\r
g1 = [1]\r
g2 = [1]\r
g3 = [1]

69\r
Part II\r
Structure

71\r
4\r
Plannable Approximations to\r
MDP Homomorphisms\r
4.1 Introduction\r
For this part of the dissertation, we turn our attention to the problem of\r
learning representations for decision making problems. In this Chap\u0002ter, we learn the structure of the environment, and propose the concept\r
of action-equivariance as a generalization of group-equivariance. In the\r
following Chapter we will capture the structure in individual states.\r
Dealing with high dimensional state spaces and unknown environ\u0002mental dynamics presents an open problem in decision making [70].\r
Classical dynamic programming approaches require knowledge of en\u0002vironmental dynamics and low dimensional, tabular state spaces [140].\r
Recent deep reinforcement learning methods on the other hand offer\r
good performance, but often at the cost of being unstable and sample\u0002hungry [70, 124, 77, 125]. The deep model-based reinforcement learn\u0002ing literature aims to fill this gap, for example by finding policies after\r
learning models based on input reconstruction [103, 66, 202, 36], by us\u0002ing environmental models in auxiliary losses [45, 77], or by forcing net\u0002work architectures to resemble planning algorithms [164, 132]. While\r
effective in learning end-to-end policies, these types of approaches\r
are not forced to learn good representations and may thus not build\r
proper environmental models. In this work, we focus on learning rep\u0002resentations of the world that are suitable for exact planning methods.

72\r
To combine dynamic programming with the representational power of\r
deep networks, we factorize the online decision-making problem into\r
a self-supervised model learning stage and a dynamic programming\r
stage.\r
We do this under the assumption that good representations minimize\r
MDP metrics [57, 46, 113, 165]. While such metrics have desirable theo\u0002retical guarantees, they require an enumerable state space and knowl\u0002edge of the environmental dynamics, and are thus not usable in many\r
problems. To resolve this issue, we propose to learn representations\r
using the more flexible notion of action equivariant mappings, where\r
the effects of actions in input space are matched by equivalent action\r
effects in the latent space. See Figure 4.1.1.\r
Figure 4.1.1: Visualization of\r
the notion of equivariance un\u0002der actions. We say Z is an\r
action equivariant mapping if\r
Z(T(s, a)) = K\r
s\r
g\r
(Z(s), A¯\r
s(a)).\r
We make the following contributions. First, we propose learning an\r
equivariant map and corresponding action embeddings. This corre\u0002sponds to using MDP homomorphism metrics [165] of deterministic\r
MDPs, enabling planning in the homomorphic image of the original\r
MDP. Second, we prove that for deterministic MDPs, when our loss\r
is zero, we have an MDP homomorphism [143]. This means that the\r
resulting policy can be lifted to the original MDP. Third, we provide\r
experimental evaluation in a variety of settings to show 1) that we can\r
recover the graph structure of the input MDP, 2) that planning in this

73\r
abstract space results in good policies for the original space, 3) that\r
we can change to arbitrary new goal states without further gradient\r
descent updates and 4) that this works even when the input states\r
are continuous, or when generalizing to new instances with the same\r
dynamics.\r
4.2 Background\r
Markov Decision Processes An infinite horizon Markov Decision Pro\u0002cess (MDP) is a tuple M = (S, A, R, T, γ), where s ∈ S is a Markov\r
state, a ∈ A is an action that an agent can take, R : S × A → R is\r
a reward function that returns a scalar signal r defining the desirabil\u0002ity of some observed transition, 0 ≤ γ ≤ 1 is a discount factor that\r
discounts future rewards exponentially and T : S × A × S → [0, 1]\r
is a transition function, that for a pair of states and an action assigns\r
a probability of transitioning from the first to the second state. The\r
goal of an agent in an MDP is to find a policy π : S × A → [0, 1],\r
a function assigning probabilities to actions in states, that maximizes\r
the return Gt = ∑\r
∞\r
k=0\r
γ\r
k\r
rt+k+1. The expected return of a state, action\r
pair under a policy π is given by a Q-value function Qπ : S × A → R\r
where Qπ(s, a) = Eπ [Gt\r
|st = s, at = a]. The value of a state under an\r
optimal policy π\r
∗\r
is given by the value function V\r
∗\r
: S → R, defined\r
as V\r
∗ = maxa Q∗\r
(s, a) under the Bellman optimality equation.\r
Value Iteration Value Iteration (VI) is a dynamic programming algo\u0002rithm that finds Q-values in MDPs, by iteratively applying the Bellman\r
optimality operator. This can be viewed as a graph diffusion where\r
each state is a vertex and transition probabilities define weighted edges.\r
VI is guaranteed to find the optimal policy in an MDP. For more de\u0002tails, see [140].\r
Bisimulation Metrics To enable computing optimal policies in MDPs\r
with very large or continuous state spaces, one approach is aggregat\u0002ing states based on their similarity in terms of environmental dynam\u0002ics [38, 113]. A key concept is the notion of stochastic bisimulations for\r
MDPs, which was first introduced by [38]. Stochastic bisimulation de\u0002fines an equivalence relation on MDP states based on matching reward\r
and transition functions, allowing states to be compared to each other.\r
Later work [46] observes that the notion of stochastic bisimulation is\r
too stringent (the dynamics must match exactly) and proposes using a

74\r
more general bisimulation metric instead, with the general form\r
d(s,s\r
0\r
) = max\r
a\r
\u0010\r
cR|R(s, a) − R(s\r
0\r
, a)| + cTdP(T(s, a), T(s\r
0\r
, a))\u0011(4.1)\r
where cR and cT are weighting constants, T(·, a) is a distribution over\r
next states and dP is some probability metric, such as the Kantorovich\r
(Wasserstein) metric. Such probability metrics are recursively com\u0002puted. For more details, see [46]. The bisimulation metric provides\r
a distance between states that is not based on input features but on\r
environmental dynamics.\r
MDP Homomorphism A generalization of the mapping induced by\r
bisimulations is the notion of MDP homomorphisms [143]. MDP ho\u0002momorphisms were introduced by [141] as an extension of [38]. An\r
MDP homomorphism (σ, {αs|s ∈ S}) is a tuple of functions 
Z,\r
\b\r
A¯\r
s\r
\t\u000B\r
with Z : S → Z a function that maps states to abstract states, and\r
each A¯\r
s\r
: A → A¯ a state-dependent function that maps actions to ab\u0002stract actions, that preserves the structure of the input MDP. We use\r
the definition given by [143]:\r
Definition 5 (Stochastic MDP Homomorphism) A Stochastic MDP ho\u0002momorphism from a stochastic MDP M = hS, A, T, Ri to an MDP M¯ = D\r
Z, A¯, K\r
s\r
g\r
, R¯\r
E\r
is a tuple (σ, {αs|s ∈ S}) = 
Z,\r
\b\r
A¯\r
s\r
\t\u000B , with\r
• Z : S → Z the state embedding function, and\r
• A¯\r
s\r
: A → A¯ the action embedding functions,\r
such that the following identities hold:\r
∀s,s\r
0∈S,a∈A K\r
s\r
g\r
(Z(s\r
0\r
)|Z(s), A¯\r
s(a)) = ∑\r
s\r
00∈[s0\r
]Z\r
T(s\r
00|s, a) (4.2)\r
∀s∈S,a∈A R¯(Z(s), A¯\r
s(a)) = R(s, a) (4.3)\r
Here, [s\r
0\r
]Z = Z\r
−1\r
(Z(s\r
0\r
)) is the equivalence class of s0 under Z.\r
We specifically consider deterministic MDPs. In that case:\r
Definition 6 (Deterministic MDP Homomorphism) A Deterministic MDP\r
homomorphism from a deterministic MDP M = hS, A, T, Ri to an MDP\r
M¯ =\r
D\r
Z, A¯, K\r
s\r
g\r
, R¯\r
E\r
is a tuple (σ, {αs|s ∈ S}) = 
Z,\r
\b\r
A¯\r
s\r
\t\u000B , with\r
• Z : S → Z the state embedding function, and\r
• A¯\r
s\r
: A → A¯ the action embedding functions,

75\r
such that the following identities hold:\r
∀s,s\r
0∈S,a∈A T(s, a) = s\r
0 =⇒ Ks\r
g\r
(Z(s), A¯\r
s(a)) = Z(s\r
0\r
) (4.4)\r
∀s∈S,a∈A R¯(Z(s), A¯\r
s(a)) = R(s, a) (4.5)\r
The states s are organized into equivalence classes under Z if they fol\u0002low the same dynamics in z-space. The MDP M¯ is referred to as the\r
homomorphic image of M under h [143]. An important property of MDP\r
homomorphisms is that a policy optimal in homomorphic image M¯\r
can be lifted to an optimal policy in M [143, 69]. Looking at these def\u0002initions, it may be clear that MDP homomorphisms and bisimulation\r
metrics are closely related. The difference is that the latter measures\r
distances between two MDP states, while the former is a map from\r
one MDP to another. However, the idea of forming a distance metric\r
by taking a sum of the distances can be extended to homomorphisms,\r
as proposed by [165]:\r
d((s, a),(Z(s), A¯\r
s(a))) = cR|R(s, a) − R¯(Z(s), A¯s(a))|\r
+ cTdP(ZT(s, a), K\r
s\r
g\r
(Z(s), A¯\r
s(a))), (4.6)\r
with dP a suitable measure of the difference between distributions (e.g.,\r
Kantorovich metric), and ZT(s, a) shorthand for projecting the distri\u0002bution over next states into the space of Z (see [54] for details). We\r
refer to this as the MDP homomorphism metric.\r
Action-Equivariance We define a mapping Z : S → Z to be action\u0002equivariant if Z(T(s, a)) = T¯(Z(s), A¯\r
s(a)) and R(s, a) = R¯(Z(s), A¯s(a)),\r
i.e. when the constraints in Eq. 4.4 and Eq. 4.5 hold.\r
4.3 Learning MDP Homomorphisms\r
We are interested in learning compact, plannable representations of\r
MDPs. We call MDP representations plannable if the optimal policy\r
found by planning algorithms such as VI can be lifted to the original\r
MDP and still be close to optimal. This is the case when the rep\u0002resentation respects the original MDP’s dynamics, such as when the\r
equivariance constraints in Eq. 4.4 and Eq. 4.5 hold. In this chapter we\r
leverage MDP homomorphism metrics to find such representations.\r
In particular, we introduce a loss function that enforces these equivari-

76\r
ance constraints, then construct an abstract MDP in the learned repre\u0002sentation space. We compute a policy in the abstract MDP M¯ using\r
VI, and lift the abstract policy to the original space. To keep things\r
simple, we focus on deterministic MDPs, but in preliminary experi\u0002ments our method performed well out of the box on stochastic MDPs.\r
Additionally, the framework we outline here can be extended to the\r
stochastic case, as [54] does for bisimulation metrics.\r
Learning State Representations\r
Here we show how to learn state representations that respect action\u0002equivariance. We embed the states in S into Euclidean space using a\r
contrastive loss based on MDP homomorphism metrics. Similar losses\r
have often been used in related work [54, 94, 6, 135, 51], which we\r
compare in Section 4.5. We represent the mapping Z using a neural\r
network parameterized by θ, whose output will be denoted Zθ. This\r
function maps a state s ∈ S to a latent representation z ∈ Z ⊆ RD.\r
We additionally approximate the abstract transition K\r
s\r
g by a function\r
T¯\r
φ : Z × A → Z ¯ parameterized by φ, and the abstract rewards R¯\r
by a neural network R¯\r
ζ\r
: Z → R, parameterized by ζ, that predicts\r
the reward for an abstract state. From Eq. 4.5 we simplify to a state\u0002dependent reward using R(s) = R¯ (Z(s)) where R(s) is the reward\r
function that outputs a scalar value for an s ∈ S, and R¯ is its equiv\u0002alent in M¯ . During training, we first sample a set of experience tuples\r
D = {(st, at,rt,st+1)}\r
N\r
n=1\r
by rolling out an exploration policy πe for\r
K trajectories. To learn representations that respect Eq. 4.4 and 4.5,\r
we minimize the distance between the result of transitioning in ob\u0002servation space, and then mapping to Z, or first mapping to Z and\r
then transitioning in latent space (see Figure 4.1.1). Additionally, the\r
distance between the observed reward R(s) and the predicted reward\r
R¯\r
ζ (Zθ (s)) is minimized. We thus include a general reward loss term.\r
We write s\r
0\r
n = T(sn, an), zn = Zθ (sn), and minimize\r
L(θ, φ, ζ) = 1\r
N\r
N\r
∑\r
n=1\r
\u0014\r
d\r
\r
Zθ (s\r
0\r
n\r
), T¯\r
φ(zn, A¯φ(zn, an))\u0001\r
+d\r
\r
R(sn), R¯\r
ζ (zn)\r
\u0001\r
\u0015\r
(4.7)\r
by randomly sampling batches of experience tuples from D. In this\r
chapter, we use d(z, z\r
0\r
) = 1\r
2\r
(z − z\r
0\r
)\r
2\r
to model distances in Z ⊆ RD.\r
Here, T¯\r
φ is a function that maps a point in latent space z ∈ Z to a\r
new state z\r
0 ∈ Z by predicting an action-effect that acts on z. We adopt\r
earlier approaches of letting T¯\r
φ be of the form T¯φ(z, a¯) = z + A¯φ(z, a),\r
where A¯\r
φ(z, a) is a simple feedforward network [94, 51]. Thus A¯φ :\r
Z × A → A¯ is a function mapping from the original action space to

77\r
an abstract action space, and A¯\r
φ(z, a) approximates A¯s(a) (Eq. 4.4).\r
The resulting transition loss is a variant of the loss proposed in [94].\r
The function R¯\r
ζ\r
: Z → R predicts the reward from z. Since Z, K\r
s\r
g\r
and R¯ are neural networks optimized with SGD, Eq. 4.7 has a triv\u0002ial solution where all states are mapped to the same point, especially\r
in the sparse reward case. When the reward function is informative,\r
minimizing Eq. 4.7 can suffice, as is empirically demonstrated in [54].\r
However, when rewards are sparse, the representations may collapse\r
to the trivial embedding, and for more complex tasks [54] requires a\r
pixel reconstruction term. In practice, earlier works use a variety of\r
solutions to prevent the trivial map. Approaches based on pixel recon\u0002structions are common [182, 183, 36, 104, 67, 85, 66, 202, 54], but there\r
are also approaches based on self-supervision that use alternatives to\r
reconstruction of input states [6, 94, 51, 135, 3, 9, 200].\r
To prevent trivial solutions, we use a contrastive loss, maximizing the\r
distance between the latent next state and the embeddings of a set of\r
random other states, S¬ = {sj}\r
J\r
j=1\r
sampled from the same trajectory\r
on every epoch. Thus, the complete loss is\r
L(θ, φ, ζ) = 1\r
N\r
N\r
∑\r
n=1\r
\u0014\r
d\r
\r
Zθ (s\r
0\r
n\r
), T¯\r
φ(zn, A¯φ(zn, an))\u0001\r
+d\r
\r
R(sn), R¯\r
ζ (zn)\r
\u0001\r
+ ∑\r
s¬∈S¬\r
d¬\r
\r
Zθ (s¬), T¯\r
φ(zn, A¯φ(zn, an))\u0001\r
\u0015\r
(4.8)\r
where d¬ is a negative distance function. Similar to [94], we use the\r
hinge loss d¬(z, z\r
0\r
) = max(0, e − d(z, z\r
0\r
)) to prevent the negative dis\u0002tance from growing indefinitely. Here, e is a parameter that controls\r
the scale of the embeddings. To limit the scope of this chapter, we con\u0002sider domains where we can find a reasonable data set of transitions\r
without considering exploration. Changing the sampling policy will\r
introduce bias in the data set, influencing the representations. Here\r
we evaluate if we can find plannable MDP homomorphisms and leave\r
the exploration problem to future work.\r
Constructing the Abstract MDP\r
After learning a structured latent space, we find abstract MDP M¯ by\r
constructing reward and transition functions from Zθ, T¯\r
φ, R¯\r
ζ\r
.\r
Abstract States\r
Core to our approach is the idea that exploiting action-equivariance\r
constraints leads to nicely structured abstract spaces that can be planned

78\r
Figure 4.3.1: Schematic\r
overview of our method.\r
We learn the map Z from S to\r
Z and discretize Z to obtain\r
X . We plan in X and use\r
interpolated Q-values to obtain\r
a policy in S.\r
in. Of course the space Z is still continuous, which requires either\r
more complex planning methods, or state discretization. In this chap\u0002ter we aim for the latter, simpler, option, by constructing a discrete\r
set X of (‘prototype’) latent states in Z over which we can perform\r
standard dynamic programming techniques. We will denote such pro\u0002totype states as x ∈ X , cf. Figure 4.3.1. Of course, we then also need\r
to construct discrete transition Tˆ\r
φ and reward Rˆ\r
ζ\r
functions. The next\r
sub-sections will outline methods to obtain these from Z, T¯\r
φ and R¯\r
ζ\r
.\r
To find a ‘plannable’ set of states, the abstract state space should be\r
sufficiently covered. To construct the set, we sample L states from\r
the replay memory and encode them, i.e. X = {Zθ (sl)|sl ∼ D}L\r
l=1\r
,\r
pruning duplicates.\r
Reward Function\r
In Eq. 4.8 we use a reward prediction loss to encourage the latent states\r
to contain information about the rewards. This helps separate distinct\r
states with comparable transition functions. During planning, we can\r
use this predicted reward R¯\r
ζ\r
. When the reward depends on a chang\u0002ing goal state, such as in the goal-conditioned tasks in Section 3.5,\r
Rˆ\r
ζ (Zθ (s)) = 1 if Zθ (s) = Zθ (sg) and 0 otherwise. We use this reward\r
function in planning, i.e. Rˆ\r
ζ (x) = 1 if x = Zθ (sg) and 0 otherwise.\r
Transition Function\r
We model the transitions on the basis of similarity in the abstract space.\r
We follow earlier work [53, 92] and assume that if two states are con\u0002nected by an action in the state space, they should be close after apply\u0002ing the latent action transition. The transition function is a distribu\u0002tion over next latent states. Therefore, we use a temperature softmax\r
to model transition probabilities between representations of abstract\r
states in X :\r
Tˆ\r
0\r
φ\r
(zj|zi, α) = e\r
−d(zj,zi+A¯\r
φ(zi\r
,α))/τ\r
∑k∈X e\r
−d(zk,zi+A¯\r
φ(zi\r
,α))/τ\r
(4.9)

79\r
Thus, for the transitions between abstract states:\r
Tˆ\r
φ(x = j|x\r
0 = i, aˆ = α) = Tˆ0\r
φ\r
(zj|zi, α) (4.10)\r
where τ is a temperature parameter that determines how ‘soft’ the\r
edges are, and zjis the representation of abstract state j. Intuitively,\r
this means that if an action moves two states closer together, the weight\r
of their connection increases, and if it moves two states away from each\r
other, the weight of their connection decreases. For very small τ, the\r
transitions are deterministic.\r
Convergence to an MDP homomorphism\r
We now show that when combining optimization of our proposed loss\r
fuction equation 4.8 with the construction of an abstract MDP as de\u0002tailed in this subsection, we can approximate an MDP homomorphism.\r
Specifically, for deterministic MDPs, we show that when the loss func\u0002tion in Eq. 4.8 reaches zero, we have an MDP homomorphism of M.\r
Theorem 1 In a deterministic MDP M, assuming a training set that con\u0002tains all state, action pairs, and an exhaustively sampled set of abstract states\r
X we consider a sequence of losses in a successful training run, i.e. the losses\r
converge to 0. In the limit of the loss L in Eq. 4.8 approaching 0, i.e. L → 0\r
and 0 < τ \u001C 1, τ \u001C e, (σ, {αs|s ∈ S}) = (Zθ, A¯\r
φ) is an MDP homomor\u0002phism of M.\r
Proof 4 Fix 0 < τ \u001C 1 and write z = Zθ (s) and a¯ = A¯\r
φ(z, a). Consider\r
that learning converges, i.e. L → 0. This implies that the individual loss\r
terms d(T¯\r
φ(z, a¯), z\r
0\r
), d¬(T¯\r
φ(z, a¯), z¬) and d(R(s), R¯\r
ζ (z)) also go to zero for\r
all (s, a,r,s\r
0\r
,s¬) ∼ D.\r
Positive samples: As the distance for positive samples d+ =\r
d(T¯\r
φ(z, a¯), z\r
0\r
) → 0, then d+ \u001C τ. Since d+ \u001C τ, then e−d+/τ ≈ 1.\r
Negative samples: Because the negative distance d¬(T¯\r
φ(z, a¯), z¬) → 0,\r
d¬ ≤ e. This, in turn, implies that the distance to all negative samples\r
d− = d(T¯\r
φ(z, a¯), z¬) ≥ e and thus τ \u001C e ≤ d−, meaning that 1 \u001C d−\r
τ\r
and\r
thus e−d−/τ ≈ 0.\r
This means that when the loss approaches 0, Tˆ0\r
φ\r
(z\r
0\r
|z, a¯) = 1 where\r
T(s\r
0\r
|s, a) = 1 and Tˆ0\r
φ\r
(z¬|z, a¯) = 0 when T(s¬|s, a) = 0. Since M\r
is deterministic, T(s\r
0\r
|s, a) transitions to one state with probability 1, and\r
probability 0 for the others. Therefore, Tˆ0\r
φ\r
(Zθ (s\r
0\r
)|Zθ (s), A¯\r
φ(Zθ (s), a)) =\r
∑s\r
00∈[s0\r
]Z\r
T(s\r
00|s, a) and Eq. 4.4 holds. As the distance for rewards\r
d(R(s), R¯\r
ζ (z)) → 0, we have that R¯ζ (z) = R(s) and Eq. 4.5 holds. There\u0002fore, when the loss reaches zero we have an MDP homomorphism of M.\r
Note that Eq. 4.8 will not completely reach zero: negative samples are\r
drawn uniformly. Thus, a positive sample may occasionally be treated

80\r
as a negative sample. Refining the negative sampling can further im\u0002prove this approach.\r
Planning and Acting\r
After constructing the abstract MDP we plan with VI [140] and lift\r
the found policy to the original space by interpolating between Q\u0002value embeddings. Given Mˆ = (X , Aˆ, Tˆ\r
φ, Rˆ\r
ζ\r
, γ), VI finds a policy\r
πˆ that is optimal in Mˆ . For a new state s\r
∗ ∈ S, we embed it in\r
the representation space Z as z\r
∗ = Zθ (s∗\r
) and use a softmax over its\r
distance to each x ∈ X to interpolate between their Q-values, i.e.\r
Q(z\r
∗\r
, a) = ∑\r
x∈X\r
w(z\r
∗\r
, x)Q(x, a) (4.11)\r
w(z\r
∗\r
, x) = e\r
−d(zx,z\r
∗\r
)/η\r
∑k∈X e\r
−d(zk,z\r
∗)/η\r
(4.12)\r
where η is a temperature parameter that sets the ‘softness’ of the inter\u0002polation. We use the interpolated Q-values for greedy action selection\r
for s\r
∗\r
, transition to s\r
∗∗ and iterate until the episode ends.\r
4.4 Experiments\r
Here we show that in simple domains, our approach 1) succeeds at\r
finding plannable MDP homomorphisms for discrete and continuous\r
problems 2) requires less data than model-free approaches, 3) gener\u0002alizes to new reward functions and data and 4) trains faster than ap\u0002proaches based on reconstructions. We focus on deterministic MDPs.\r
While preliminary results on stochastic domains were promising, an\r
in-depth discussion is beyond the scope of this chapter.\r
Baselines\r
To evaluate our approach, we compare to a number of baselines:\r
1. WM-AE: An auto-encoder approach inspired by World Models [66].\r
We follow their approach of training representations using a recon\u0002struction loss, then learning latent dynamics on fixed representa\u0002tions. We experimented with a VAE [91], which did not perform\r
well (see [94] for similar results). We thus use an auto-encoder to\r
learn an embedding, then train an MLP to predict the next state\r
from embedding and action.\r
2. LD-AE: An auto-encoder with latent dynamics. We train an auto-

81\r
encoder to reconstruct the input, and predict the next latent state.\r
We experimented with reconstructing the next state, but this re\u0002sulted in the model placing the next state embeddings in a different\r
location than the latent transitions.\r
3. DMDP-H: We evaluate the effectiveness of training without nega\u0002tive sampling. This is similar to DeepMDP [54]. However, unlike\r
DeepMDP, DMDP-H uses action-embeddings, for a fairer compari\u0002son.\r
4. GC-Model-Free: Finally, we compare to a goal-conditioned model\u0002free baseline (REINFORCE with state-value baseline), to contrast\r
our approach with directly optimizing the policy1. We include the\r
1 Deep reinforcement learning algo\u0002rithms such as our baseline may fail\r
catastrophically depending on the ran\u0002dom seed [70]. For a fair compari\u0002son, we train the baseline on 6 random\r
seeds, then remove those seeds where\r
the method fails to converge for the train\r
setting.\r
goal state as input for a fair comparison.\r
To fairly compare the planning approaches, we perform a grid search\r
over the softness of the transition function by evaluating performance\r
on the train goals in τ ∈ [1, 0.1, 0.001, 0.0001, 0.00001, 1e − 20]. Unless\r
otherwise stated, the planning approaches are all trained on datasets\r
of 1000 trajectories, sampled with a random policy. The learning rate\r
is set to 0.001 and we use Adam [90]. For the hinge loss, we use e = 1.\r
The latent dimensionality is set to 50 everywhere. Our approach is\r
trained for 100 epochs. WM-AE is trained for 1000 epochs in total: 500\r
for the auto-encoder and 500 for the dynamics. LD-AE is trained for\r
1000 epochs. For constructing the abstract MDP we sample 1024 states\r
from D, project unto Z and prune duplicates. For planning we use VI\r
with discount factor γ = 0.9, 500 backups and interpolation parameter\r
(Eq. 4.12) η = 1e − 20. The learning rate for the model-free baseline\r
was chosen by fine-tuning on the training goals. For the model-free\r
baseline, we use a learning rate of 5e − 4 and we train for 500k steps\r
(more than five times the number of samples the planning approaches\r
use). Network Zθ has 2 convolutional layers (both 16 channels, 3 × 3\r
filters) and 3 fully connected layers (input→ 64 → 32 → |z|). Net\u0002works Tφ and Rξ each have 2 fully connected layers. We use ReLU\r
non-linearities between layers.\r
Object Collection\r
We test our approach on an object collection task inspired by the key\r
task in [51], with major differences: rather than searching for three\r
keys in a labyrinth, the agent is placed in a room with some objects.\r
Its task is to collect the key. On every time step, the agent receives a\r
state—a 3 × 48 × 48 pixel image (a channel per object, including the\r
agent), as shown in Figure 4.4.1—and a goal state of the same size.

82\r
At train time, the agent receives reward of 1 on collection of the key\r
object, and a reward of −1 if it grabs the wrong object, and a reward\r
of −0.1 on every time step. The episode ends if the agent picks up\r
one (or more) of the objects and delivers it to one of the four corners\r
(randomly sampled at episode start), receiving an additional delivery\r
reward of 1. At test time, the agent is tasked with retrieving one of\r
the objects chosen at random, and delivering to a randomly chosen\r
location, encoded as a desired goal state. This task will evaluate how\r
easily the trained agent adapts to new goals/reward functions. The\r
agent can interact with the environment until it reaches the goal or\r
100 steps have passed. For both tasks, we compare to the model-free\r
baseline. We also compare to the DMDP-H, WD-AE and LD-AE base\u0002lines. We additionally perform a grid search over the hinge, number of\r
Figure 4.4.1: Example states in\r
the object collection domain for\r
the single object and double ob\u0002ject tasks.\r
state samples for discretization and η hyperparameters for insight in\r
how these influence the performance. This showed that our approach\r
is robust with respect to the hinge parameter, but it influences the scale\r
of the embeddings. The results decrease only when using 256 or fewer\r
state samples. Lastly, η is robust for values lower than 1. We opt for\r
a low value of η, to assign most weight to the Q-value of the closest\r
state.\r
Single Object Task\r
We first evaluate a simple task with only one object (a key). The agent’s\r
task is to retrieve the key, and move to one of four delivery locations in\r
the corners of the room. The delivery location is knowledge supplied\r
to the agent in the form of a goal state that places the agent in the cor\u0002rect corner and shows that there is no key. These goal states are also\r
supplied to the baseline, during training and testing. Additionally, we\r
perform an ablation study on the effect of the reward loss. The average\r
episode lengths are shown in Table 4.4.1. Our approach outperforms

83\r
Avg. ep. length ↓\r
Task Single Object Double Object\r
Goal Set Train Test Train Test\r
GC-Model-free 10.00 ± 0.11 67.25 ± 6.81 10.10 ± 0.69 38.25 ± 15.30\r
WM-AE 12.96 ± 8.93 10.03 ± 5.56 29.61 ± 19.42 22.53 ± 22.12\r
LD-AE 23.46 ± 27.10 21.04 ± 21.71 60.26 ± 29.14 52.72 ± 27.32\r
DMDP-H (J = 0) 82.88 ± 11.62 85.69 ± 7.98 81.24 ± 2.45 81.17 ± 2.69\r
Ours, J = 1, 8.61 ± 0.35 7.53 ± 0.24 8.53 ± 0.36 8.38 ± 0.07\r
Ours, J = 3 8.68 ± 0.27 7.63 ± 0.19 8.61 ± 0.38 8.95 ± 0.63\r
Ours, J = 5 8.57 ± 0.48 7.74 ± 0.22 8.26 ± 0.84 8.96 ± 1.15\r
Table 4.4.1: Comparing average\r
episode length of 100 episodes\r
on the object collection domain.\r
Reporting mean and standard\r
deviation over 5 random seeds\r
for the planning approaches.\r
The model free approach is av\u0002eraged over 4 random seeds for\r
the single object domain, 3 ran\u0002dom seeds for the double object\r
domain. all baselines, both at train and at test time. There is no clear prefer\u0002ence in terms of the number of negative samples — as long as J > 0\r
— the result for all values of J are quite close together. The DMDP-H\r
approach fails to find a reasonable policy, possibly due to the sparse\r
rewards in this task providing little pull against state collapse. Out of\r
the planning baselines, WM-AE performs best, probably because visu\u0002ally salient features are aligned with decision making features in this\r
task. Finally, the model-free approach is the best performing baseline\r
on the training goals, but does not generalize to test goals.\r
The results of the reward ablation are shown in Table 4.4.2. While\r
75\r
50\r
25\r
0\r
25\r
50\r
75\r
100\r
60 40 20 0 20 40 60 80\r
80\r
60\r
40\r
20\r
0\r
20\r
40\r
60\r
80\r
4\r
5\r
6\r
7\r
8\r
9\r
Value\r
(a) WM-AE Base\u0002line\r
20 15 10 5\r
0\r
5\r
10 15 20\r
10\r
5\r
0\r
5\r
10\r
8\r
6\r
4\r
2\r
0\r
2\r
4\r
6\r
8\r
0\r
1\r
2\r
3\r
4\r
5\r
Value\r
(b) LD-AE Base\u0002line\r
0.02\r
0.00\r
0.02\r
0.04\r
0.06\r
0.08\r
0.10 0.02 0.00 0.02 0.04 0.06\r
0.03\r
0.02\r
0.01\r
0.00\r
0.01\r
0.02\r
0.03\r
0.04\r
0\r
2\r
4\r
6\r
8\r
Value\r
(c) DMDP-H Base\u0002line\r
6\r
4\r
2\r
0\r
2\r
4\r
6\r
4\r
2\r
0\r
2\r
4\r
0.75\r
0.50\r
0.25\r
0.00\r
0.25\r
0.50\r
0.75\r
1.00\r
3\r
4\r
5\r
6\r
7\r
8\r
9\r
Value\r
(d) This chapter\r
Figure 4.4.2: Abstract MDP for\r
three approaches in the single\r
object room domain. Nodes\r
are PCA projections of abstract\r
states, edges are predicted T¯\r
φ,\r
colors are predicted values.\r
removing the reward loss does not influence performance much for\r
J = 0, J = 3 and J = 5, when J = 1 the reward prediction is needed\r
to separate the states. Without the reward, the single negative sample\r
does not provide enough pull for complete separation.\r
Avg. ep. length ↓\r
Reward Loss No Reward Loss\r
Goal Set Train Test Train Test\r
DMDP-H (J = 0) 82.88 ± 11.62 85.69 ± 7.98 87.03 ± 3.08 84.08 ± 3.02\r
Ours, J = 1 8.61 ± 0.35 7.53 ± 0.24 74.32 ± 19.90 68.54 ± 17.29\r
Ours, J = 3 8.68 ± 0.27 7.63 ± 0.19 8.54 ± 0.36 7.44 ± 0.21\r
Ours, J = 5 8.57 ± 0.48 7.74 ± 0.22 8.52 ± 0.19 7.53 ± 0.20\r
Table 4.4.2: Ablation study of\r
the effect of the reward loss.\r
Comparing average episode\r
length of 100 episodes for the\r
single object room domain.\r
Reporting mean and standard\r
deviation over 5 random seeds.\r
We show the latent spaces found for the baselines and our approach in\r
Figure 4.4.2. Our approach has found a double grid structure - repre-

84\r
senting the grid world before, and after picking up the key. The base\u0002lines are reasonably plannable after training for long enough, but the\r
latent spaces aren’t as nicely structured as our approach. This mirrors\r
results in earlier work [94]. Thus, while pixel reconstruction losses\r
may be able to find reasonable representations for certain problems,\r
these rely on arbitrarily complex transition functions. Moreover, due\r
to their need to train a pixel reconstruction loss they take much longer\r
to find useable representations. This is shown in Figure 4.4.3b, where\r
0 20 40 60 80 100\r
Training epoch\r
20\r
40\r
60\r
80\r
100\r
Average episode length\r
J = 0\r
J = 1\r
J = 3\r
J = 5\r
(a) Comparison of different values of J.\r
0 200 400 600 800 1000\r
Training epoch\r
20\r
40\r
60\r
80\r
100\r
Average episode length\r
LD-AE\r
WM-AE\r
This paper\r
(b) Comparison of this chapter and the\r
WM-AE and LD-AE baselines. WM\u0002AE can not be evaluated until the\r
auto-encoder has finished training and\r
training of the dynamics model begins.\r
Figure 4.4.3: Average episode\r
length per training epoch for the\r
single object domain. Reported\r
mean and standard error over 5\r
random seeds.\r
the performance after planning for each training epoch is plotted and\r
compared. Additionally, we observe state collapse for DMDP-H in Fig\u0002ure 4.4.2c, and this is reflected in a high average episode length after\r
planning.\r
Double Object Task\r
We now extend the task to two objects: a key and an envelope. The\r
agent’s task at train time is still to retrieve the key. At test time, the\r
agent has to pick up the key or the envelope (randomly chosen) and\r
deliver it to one of the corners. We show results in Table 4.4.1. Again,\r
our method performs well on both train and test set, having clearly\r
learned a useful abstract representation, that generalizes to new goals.\r
The WM-AE baseline again fares better than the LD-AE baseline, and\r
DMDP-H fails to find a plannable representation. The model-free base\u0002line performs slightly worse than our method on this task, even after\r
seeing much more data. Additionally, even though it performs rea\u0002sonably well on the training goals, it does not generalize to new goals\r
at all. The WM-AE performs worse on this task than our approach,\r
but generalizes much better than the model-free baseline, due to its\r
planning, while the LD-AE baseline does not find plannable represen\u0002tations of this task.

85\r
Continuous State Spaces\r
0\r
2\r
4\r
6\r
8\r
1.5\r
1.0\r
0.5\r
0.0\r
0.5\r
1.0\r
1.5\r
1\r
0\r
1\r
2\r
3\r
0.0\r
0.2\r
0.4\r
0.6\r
0.8\r
1.0\r
Value\r
(a) WM-AE Baseline\r
2 1 0 1 2\r
0.2\r
0.0\r
0.2\r
0.4\r
0.6\r
0.8\r
1.0\r
1.2\r
1.0\r
0.5\r
0.0\r
0.5\r
1.0\r
0.0\r
0.2\r
0.4\r
0.6\r
0.8\r
1.0\r
Value\r
(b) LD-AE Baseline\r
0.020.010.000.010.020.03 0.005\r
0.000\r
0.005\r
0.010\r
0.015\r
0.005\r
0.000\r
0.005\r
0.010\r
0.015\r
0.2\r
0.4\r
0.6\r
0.8\r
1.0\r
Value\r
(c) DMDP-H Baseline\r
10\r
30 20 10 0 10 20 30 40\r
5\r
0\r
5\r
10\r
0.5\r
0.0\r
0.5\r
1.0\r
1.5\r
2.0\r
2.5\r
0.0\r
0.2\r
0.4\r
0.6\r
0.8\r
1.0\r
Value\r
(d) This chapter\r
Figure 4.4.4: Abstract MDP for\r
four approaches in CartPole.\r
Nodes are PCA projections of\r
abstract states, edges are pre\u0002dicted T¯\r
φ, colors are predicted\r
values.\r
We evaluate whether we can use our method to learn plannable rep\u0002resentations for continuous state spaces. We use OpenAI’s CartPole\u0002v0 environment [24]. We include again a model-free baseline that is\r
trained until completion as a reference for the performance of a good\r
policy. We also compare DMDP-H, WD-AE and LD-AE. We expect\r
that the latter two would perform well here; after all, the representa\u0002tion that they reconstruct is already quite compact. We additionally\r
evaluate performance when the amount of data is limited to only 100\r
trajectories (and we limit the number of training epochs for all plan\u0002ning approaches to 100 epochs). We plot the found latent space for our\r
approach and the baselines in Figure 4.4.4. The goal in this problem is\r
to reach the all-zero reward vector, which we set as the goal state with\r
reward 1, and all other states to reward 0. For our approach and both\r
auto-encoder baselines, the latent space forms a bowl with the goal in\r
its center. The DMDP-H again shows a shrunk latent space, and does\r
not have this bowl structure.\r
Results are shown in Table 4.4.3. Our approach performs best out of\r
all planning approaches. When trained fully, the model-free approach\r
performs better. However, when we limit the number of environmen\u0002tal interactions to 100 trajectories, we see that the planning approach\r
still finds a reasonable policy, while the model-free approach fails com-

86\r
Average episode length ↑ Standard Only 100 trajectories\r
GC-Model-free 197.85 ± 2.16 23.84 ± 0.88\r
WM-AE 150.61 ± 30.48 114.47 ± 17.32\r
LD-AE 157.10 ± 11.14 154.73 ± 50.49\r
DMDP-H (J = 0) 39.32 ± 9.02 72.81 ± 20.16\r
Ours, J = 1, 174.64 ± 22.43 127.37 ± 44.02\r
Ours, J = 3 166.05 ± 24.73 148.30 ± 67.27\r
Ours, J = 5 186.31 ± 12.28 171.53 ± 34.18\r
Table 4.4.3: CartPole results.\r
Comparing average episode\r
length over 100 episodes, re\u0002porting mean and standard\r
deviation over 5 random seeds.\r
The left column has standard\r
settings, in the right column\r
only 100 trajectories are encoun\u0002tered, and planning models are\r
trained for only 100 epochs.\r
pletely. This indicates that our approach is more data efficient.\r
Generalizing over Goals and Objects\r
Figure 4.4.5: Transitions in the\r
image manipulation task.\r
In many tasks we need to be able to generalize not only over goals, but\r
also object instances. We evaluate if our abstract state space generalizes\r
to unseen objects in a problem class. For this we construct an object\r
manipulation task. On each episode, an image of a piece of cloth\u0002ing is sampled from a set of training images in Fashion MNIST [196],\r
and a goal translation of the image is sampled from a set of train\r
goals (translations with negative x-offset: (−3, ·) up to and including\r
(−1, ·)). Thus, the underlying state space is a 7 × 7 grid. The trans\u0002lated image is provided to the agent as a goal state. The agent receives\r
a reward of +1 if she moves the clothing to the correct translation. See\r
Figure 4.4.5.\r
At test time, we evaluate performance on test goals (translations with\r
positive x-offset: (1, ·) up to and including (3, ·), seen before as states\r
for training images but never as goals) and test images. The latent\r
spaces for each of the four representation learning approaches are\r
shown in Figure 4.4.6. For DMDP-H, the latent space collapses to all\r
but a few points. For WD-AE and LD-AE, the latent space does not\r
exhibit clear structure. For our approach, there is a clear square grid\r
structure present in the latent space. However, the underlying trans-

87\r
lations for the images do not neatly align across images. Clustering\r
such states together is interesting future work.\r
20\r
10\r
0\r
10\r
20\r
30\r
40\r
30\r
20\r
10\r
0\r
10\r
20\r
30\r
40\r
30210\r
0010203040\r
0\r
2\r
4\r
6\r
8\r
Value\r
(a) WM-AE Baseline\r
0.2\r
0.1\r
0.0\r
0.1\r
0.2\r
0.3\r
0.4\r
0.5\r
0.4\r
0.2\r
0.0\r
0.2\r
0.4\r
0.3 0.2 0.1 0.0 0.1 0.20.30.4\r
0\r
2\r
4\r
6\r
8\r
Value\r
(b) LD-AE Baseline\r
0.00005 0.00000 0.00005 0.00010\r
0.00015\r
0.000200.00002\r
0.00001\r
0.00000\r
0.00001\r
0.00002\r
0.00003\r
0.000002\r
0.000001\r
0.000000\r
0.000001\r
0.000002\r
9.00\r
9.25\r
9.50\r
9.75\r
10.00\r
10.25\r
10.50\r
10.75\r
Value\r
(c) DMDP-H Baseline\r
7.5\r
5.0\r
2.5\r
0.0\r
2.5\r
5.0\r
7.5\r
10.0\r
4\r
2\r
0\r
2\r
4\r
6\r
0.3 0. 0\r
0.1 0.2.100.20.30.4\r
1.5\r
2.0\r
2.5\r
3.0\r
3.5\r
4.0\r
4.5\r
5.0\r
Value\r
(d) This chapter\r
Figure 4.4.6: Abstract MDP for\r
four approaches in planning in\r
Fashion MNIST. Nodes are PCA\r
projections of abstract states,\r
edges are predicted T¯\r
φ, colors\r
are predicted values.\r
Results are shown in Table 4.4.4. The goal-conditioned model-free\r
baseline has an easy time finding a good policy for the training set\u0002ting. It also generalizes well to unseen images. However, it has trouble\r
generalizing to new goal locations for both train and test images. Our\r
planning approach, on the other hand, loses some performance on\r
the training setting, but easily generalizes to both test images and test\r
goals. Neither WM-AE nor LD-AE find good policies in this problem.\r
They have a difficult time learning plannable representations because\r
their focus is on reconstructing individual images.\r
Avg. ep. length ↓\r
Dataset Train Test\r
Goal Set Train Test Train Test\r
GC-Model-free 4.82 ± 0.33 9.67 ± 5.01 4.75 ± 0.12 8.17 ± 2.67\r
WM-AE 59.95 ± 4.06 63.27 ± 3.36 64.27 ± 5.33 63.41 ± 2.04\r
LD-AE 56.39 ± 7.07 49.35 ± 4.05 51.45 ± 6.79 51.70 ± 3.97\r
DMDP-H (J = 0) 62.86 ± 3.87 66.68 ± 4.40 65.93 ± 4.98 64.86 ± 1.57\r
Ours, J = 1, 5.07 ± 0.87 5.27 ± 0.56 5.69 ± 0.93 5.63 ± 0.96\r
Ours, J = 3 5.60 ± 0.97 5.46 ± 0.97 6.44 ± 1.12 5.42 ± 0.89\r
Ours, J = 5 5.36 ± 0.71 5.67 ± 1.20 6.36 ± 1.21 5.34 ± 0.93\r
Table 4.4.4: Comparing average\r
episode length of 100 episodes\r
for planning in Fashion MNIST.\r
Reporting mean and standard\r
deviation over 5 random seeds.

88\r
4.5 Related Work\r
This chapter proposes a method for learning action equivariant map\u0002pings of MDPs, and using these mappings for constructing plannable\r
abstract MDPs. We learn action equivariant maps by minimizing MDP\r
homomorphism metrics [165]. As a result, when the loss reaches zero\r
the learned mapping is an MDP homomorphism [143]. MDP homo\u0002morphism metrics are a generalization of bisimulation metrics [46,\r
113]. Other works [71, 34, 193] consider equivariance to symmetry\r
group actions in learning. Here, we use a more general version of\r
equivariance under MDP actions for learning representations of MDPs.\r
We learn representations of MDPs by 1) predicting the next latent state,\r
2) predicting the reward and 3) using negative sampling to prevent\r
state collapse. Much recent work has considered self-supervised rep\u0002resentation learning for MDPs. Certain works focus on predicting the\r
next state using a contrastive loss [94, 6, 135], disregarding the re\u0002ward function. However, certain states may be erronously grouped\r
together without a reward function to distinguish them. [54] include\r
both rewards and transitions to propose an objective based on stochas\u0002tic bisimulation metrics [57, 46, 113]. However, at training time they\r
focus on deterministically predicting the next latent state. Their pro\u0002posed objective does not account for the possibility of latent space col\u0002lapse, and for complex tasks they require a pixel reconstruction term.\r
This phenomenon is also observed by [51], who prevent it with two\r
entropy maximization losses.\r
Many approaches to representation learning in MDPs depend (par\u0002tially) on learning to reconstruct the input state [36, 182, 66, 76, 67,\r
168, 104, 202, 85, 183, 178, 8]. A disadvantage of reconstruction losses\r
is training a decoder, which is time consuming and usually not re\u0002quired for decision making tasks. Additionally, such losses emphasize\r
visually salient features over features relevant to decision making.\r
Other approaches that side-step the pixel reconstruction loss include\r
predicting which action caused the transition between two states [3],\r
predicting the number of time steps between two states [9] or predict\u0002ing objects in an MDP state using supervised learning [200].\r
[82] identify a set of priors about the world and uses them to formulate\r
self-supervised objectives. In [55], the similarity between two states is\r
the difference in goal-conditioned policies needed to reach them from\r
another state. [152] learn representations for tree-based search that\r
must predict among others a policy and value function, and are thus\r
not policy-independent. Earlier work on decoupling representation\r
learning and planning exists [36, 182, 200]. However, these works use\r
objectives that include a pixel reconstruction term [36, 182] or require

89\r
labeling of objects in states for use in supervised learning [200].\r
Other work on planning algorithms in deep learning either assumes\r
knowledge of the state graph [164, 130, 112, 86], builds a graph out of\r
observed transitions [96] or structures the neural network architecture\r
as a planner [132, 45, 51], which limits the search depth.\r
4.6 Relation to Group Equivariance\r
Action-equivariance is a generalization of the older notion of group\r
equivariance [34]. In group equivariance, we require that for a function\r
Z : X → Z, and a group G acting on a space X , the following holds:\r
Z(g · x) = g¯ · Z(x) ∀g ∈ G, x ∈ X (4.13)\r
where g¯ indicates an equivalent group element acting on Z. If we view\r
the original point x ∈ X as a state, and the transformed point x\r
0 = gx\r
as a next state, we can define g as a special case of an MDP action and\r
write:\r
Z(T(x, g)) = T¯(Z(x), g¯) ∀g ∈ G, x ∈ X (4.14)\r
which we immediately recognize as the equation in Figure 4.1.1. We\r
can further generalize this by letting T(·, ·) and T¯(·, ·) be stochastic\r
functions, and matching their distributions. This generalization sug\u0002gests that the notion of a group action and an MDP action are more\r
similar than they at first glance look. Additionally, we can view equiv\u0002ariance in the general case as a body of work that finds or defines\r
homomorphisms between spaces that respect some notion of acting\r
on a point (a state, or e.g. an image) in order to bring it to another\r
point (a next state, or a rotated image).\r
4.7 Conclusion\r
This chapter proposes the use of ‘equivariance under actions’ for learn\u0002ing representations in deterministic MDPs. Action equivariance is en\u0002forced by the use of MDP homomorphism metrics in defining a loss\r
function. We also propose a method of constructing plannable abstract\r
MDPs from continuous latent spaces. We prove that for determinis\u0002tic MDPs, when our objective function is zero and our method for\r
constructing abstract MDP is used, the map we learn is an MDP ho\u0002momorphism. Additionally, we show empirically that our approach\r
is data-efficient and fast to train, and generalizes well to new goal

90\r
states and instances with the same environmental dynamics. Finally,\r
we show that action-equivariance is a generalization of group equivari\u0002ance. Potential future work includes an extension to stochastic MDPs\r
and clustering states on the basis of MDP metrics. Using a clustering\r
approach as part of model training, we can learn the prototypical states\r
rather than sampling them. This comes at the cost of having to back\u0002propagate through a discretization step, which in early experiments\r
(using Gumbel-Softmax [78]) led to instability.

91\r
5\r
Learning Factored\r
Representations of Markov\r
Decision Processes\r
5.1 Introduction\r
In the previous Chapter we presented work that learns to recover the\r
structure in an environment by using action-equivariance. In this Chap\u0002ter, we will learn the structure in individual states in order to improve\r
decision making in object-oriented decision making problems.\r
Many decision-making problems are inherently structured: manip\u0002ulating objects in a scene, steering agents in a multi-agent system,\r
placing chip blocks on a chip and attaching atoms in molecule de\u0002sign are all examples of problems where the full problem state is\r
decomposed into different factors, which are sometimes independent\r
of each other. By making use of this structure, we can often reduce\r
solving an exponentially complex problem to solving a series of sim\u0002pler problems. For example, if we have a structured representation\r
of states in a reinforcement learning problem, we can use factored\r
approaches to decision making [65, 63, 154, 88] or use similarly struc\u0002tured graph neural network architectures in deep reinforcement learn\u0002ing [102, 180, 75, 132, 130, 37]. In Chapter 3 we have assumed to\r
know the factorization of the environment. In this Chapter, we in-

92\r
vestigate the case where this assumption is loosened: we consider\r
a set of decision making problems with unknown structure. This\r
is an important research question, as structure is not always easily\r
given a priori. Oftentimes, observations of the world are unstructured\r
streams of e.g. image data, and structure in the state of the world\r
must be inferred by the agent itself. As such, there is a large body\r
of work on learning structured representations of scenes, resulting in\r
object-based representations [28, 11, 184, 174, 92, 161, 162, 197, 200].\r
Most works in this area require some form of human supervision,\r
but several works consider the fully self-supervised or unsupervised\r
setting [59, 128, 174, 79, 197, 25, 58, 44]. Such self-supervised meth\u0002ods are usually based on reconstructing the visual inputs from the\r
learned representations. There are a few issues with such approaches:\r
reconstruction-based methods need to be able to properly reconstruct\r
visually important yet potentially irrelevant features such as back\u0002grounds. This means a lot of training time and model capacity is\r
wasted on learning to accurately represent those features. Addition\u0002ally, one needs to spend training time and compute to learn a decoder\r
model which is not needed for the downstream task. Finally, such ap\u0002proaches tend to ignore visual features that are small but potentially\r
important to decision-making, such as a small ball in certain Atari\r
games. We therefore propose using a contrastive learning method\r
based on graph embedding approaches [19, 181], where states that\r
transition into each other are placed close together in latent space, and\r
random state pairs are pushed further from each other.\r
We introduce a method for learning factored representations of object\u0002oriented MDPs, where each state consists of a set of latent state vari\u0002ables, one per object in a scene. We model the latent transition model\r
as a graph neural network [149, 114, 95, 56, 12], with the nodes the la\u0002tent state variables. Due to the graph network transition model, we au\u0002tomatically obtain permutation equivariance on the object transitions.\r
We call this method Contrastively-trained Structured World Models\r
(C-SWMs). We also introduce a factored contrastive loss based on\r
learning translational graph embeddings [19, 181], and connect con\u0002trastive learning for state representations to relational graph embed\u0002dings [129]. Finally, we introduce a novel ranking-based evaluation\r
strategy which we use to demonstrate that C-SWMs learn object-level\r
state representations, combinatorially generalize to unseen states and\r
can identify objects from scenes without supervision in object manip\u0002ulation, 3D physics, and Atari settings.

93\r
5.2 Background\r
A Markov Decision Process (MDP) is a tuple M = (S, A, R, T, γ) with\r
S the set of states, A the set of actions, R : S × A → R the reward\r
function, T : S × A × S → [0, 1] the transition function and γ ∈ [0, 1] a\r
discount factor. An agent acts in an MDP with a policy π : S × A →\r
[0, 1]. In this Chapter, we will not consider the reward function, which\r
we will leave out going forward.\r
A factored MDP is an MDP whose state is a multivariate random vari\u0002able X = (X1, · · · , Xn) and state instances are x = (x1, · · · , xn) with\r
for every i, xi ∈ Dom(Xi). In a factored MDP, the transition function\r
P(x\r
0\r
|x, a) can be written as a set of Dynamic Bayesian Networks (one\r
per action) [22, 88, 154, 63]. We can write the transition function using\r
xi’s parent set, xa,i.\r
P(x\r
0\r
|x, a) = ∏\r
i\r
P(x\r
0\r
i\r
|xa,i) (5.1)\r
We base our method on the graph embedding method TransE [19].\r
Consider a knowledge base K of entity-relation-entity triples K =\r
{(et,rt, ot)}\r
T\r
t=1\r
, with et ∈ E the subject, rt ∈ R the relation (not to\r
be confused with the reward in an MDP) and ot ∈ E the object of the\r
knowledge base fact. We can draw a parallel between such fact triples\r
and state transitions (st, at,st+1) in an MDP without rewards. In a\r
sense, the MDP’s action can be viewed as the relation between a state\r
st and next state st+1.\r
TransE embeds knowledge facts with maps F : E → RD and G : R →\r
RD and computes the energy of a triple as H = d(F(et) + G(rt), F(ot))\r
with d(·, ·) the squared Euclidean distance and F (and G) are embed\u0002ding functions that map discrete entities (and relations) to RD, where\r
D is the dimensionality of the embedding space. During training, an\r
energy-based hinge loss [110] is used.\r
5.3 Structured World Models\r
We wish to learn structured abstract representations of MDP states\r
and permutation equivariant transition functions which are consistent\r
with the factored nature of the problem.

94\r
Learning Abstract Representations\r
Assume we have a dataset of experience B = {(st, at,st+1)}\r
T\r
t=1\r
sam\u0002pled from for example an exploration policy π, with T the number of\r
tuples in the data set. We wish to train an encoder E : S → Z that maps\r
states st ∈ S to abstract representations zt ∈ Z = Z1 × . . . × ZK, with K\r
the number of object slots. We set Zk = RD for each k, with D a hyper\u0002parameter. The abstract representations zt should contain only infor\u0002mation needed for modeling the transitions in the environment, and\r
no superfluous information (such as for example background color in\r
a video game). Note that while in this Chapter we do not consider the\r
reward, we can include reward in learning representations, as we did\r
in Chapter 4.\r
Since we wish to find a structured representation of the state, we re\u0002quire that our encoder maps from unstructured (usually image-based)\r
inputs to a factored, object-oriented representation. Our encoder thus\r
consists of two modules: an object extractor Eext that maps from ob\u0002servations to K feature maps, and an object encoder Eenc that maps\r
from feature maps to object representations. The feature maps mk\r
t =\r
[Eext(st)]k are flattened and used to predict abstract representations\r
z\r
k\r
t = Eenc(mkt\r
) with z\r
k\r
t ∈ Zk\r
. The feature maps can be viewed as object\r
masks corresponding to an object slot. The object encoder Eenc shares\r
weights between objects.\r
In this chapter we will assume a factored action space A = A1 × . . . ×\r
AK, which provides an independent action for each object in the scene.\r
This is a strong inductive bias for learning factored representations.\r
The full architecture is shown in Figure 5.3.1.\r
Contrastive Learning\r
We can use contrastive coding on the transitions in an MDP without\r
rewards: (st, at,st+1). However, the same action can have different\r
outcomes in different states. We therefore base the "relation effect" on\r
both the state and the action, resulting in the latent transition model\r
T(zt, at). In essence, we therefore constrain the latent transition func\u0002tion to model the effects of actions as translations in latent space. The\r
energy is then H = d(zt + T(zt, at), zt+1).\r
For a single (st, at,st+1) with a negative sample z˜t = E(s˜t), and s˜t\r
randomly sampled from the dataset, the energy-based hinge loss is\r
L = d(zt + T(zt, at), zt+1) + max(0, e − d(z˜t, zt+1)), (5.2)\r
where e is the margin for which e = 1 was used in our experiments.

95\r
The full loss is an expectation of Eq. 5.2 over samples from the dataset.\r
CNN\r
Object \r
extractor\r
MLP\r
Object \r
encoder\r
GNN\r
Transition\r
model\r
Contrastive\r
loss\r
st mt zt zt\r
+Δzt zt+1\r
Figure 5.3.1: Visualization of\r
general C-SWM architecture: a\r
CNN object extractor, MLP ob\u0002ject encoder and GNN transi\u0002tion model that uses the actions\r
to compute ∆zt, together with\r
an object-factorized contrastive\r
loss.\r
5.4 Transition Model\r
We model the transition function as a graph neural network (GNN) [149,\r
114, 95, 11, 56, 12]. When using a GNN, the nodes are the different ob\u0002jects in the scene and the model is able to learn different pairwise\r
interactions based on the object features, while being equivariant to\r
the order in which the objects are assigned to slots. The input for the\r
GNN are a set of nodes, and a set of edges. In this chapter we model\r
the graph as a fully connected graph, so that long range dependencies\r
do not require multiple message passing steps. The node inputs are\r
the object representations {z\r
t\r
k\r
}\r
K\r
k=1\r
extracted by the encoder and the ac\u0002tions at = (a\r
1\r
t\r
, · · · , a\r
K\r
t\r
). The actions are encoded as one-hot vectors for\r
discrete action spaces, but can be replaced by zero vectors in case of\r
no actions or continuous vectors in case of continuous action spaces.\r
The transition GNN T(·, ·) predicts the effect of taking at from zt:\r
∆zt = T(zt, at) = GNN({(z\r
k\r
t\r
, a\r
k\r
t\r
)}\r
K\r
k=1\r
). (5.3)\r
where ∆zt = (∆z\r
1\r
t\r
, · · · , ∆z\r
K\r
t\r
). The predicted next latent state is then\r
given by\r
zt+1 = (z\r
1\r
t + ∆z\r
1\r
t\r
, · · · , z\r
K\r
t + ∆z\r
K\r
t\r
). (5.4)\r
Thus, the transition function can also be viewed as predicting the in\u0002dividual translational effects of the action on each object in the state.\r
Message Passing The GNN consists of MLP node update functions\r
fnode and MLP edge update functions fedge which share parameters\r
between nodes and edges. A message passing round is given by\r
e\r
(i,j)\r
t = fedge([z\r
i\r
t\r
, z\r
j\r
t\r
]) (5.5)\r
∆z\r
j\r
t = fnode([z\r
j\r
t\r
, a\r
j\r
t\r
, ∑i6=je\r
(i,j)\r
t\r
]), (5.6)\r
where e\r
(i,j)\r
t\r
is a predicted edge representation between nodes i and j.\r
Multiple rounds of message passing are possible, but were not found\r
to be necessary, possibly due to the use of a fully connected graph.

96\r
Message passing in a fully connected graph is O(K\r
2\r
), but this may\r
be reduced to linear complexity if messages are only sent to nearest\r
neighbors in latent space. We leave this for future work.\r
Factored Contrastive Loss We now adapt the original contrastive loss\r
function in Eq. 5.2 to a factored loss, which is computed independently\r
for the different objects. Write the predicted effect of the action on the\r
k-th object as ∆z\r
k\r
t = T\r
k\r
(zt, at). Then, the energy H for positive samples\r
is\r
H =\r
1\r
K\r
K\r
∑\r
k=1\r
d(z\r
k\r
t + T\r
k\r
(zt, at), z\r
k\r
t+1\r
), (5.7)\r
and the energy H˜ for negative samples is\r
H˜ =\r
1\r
K\r
K\r
∑\r
k=1\r
d(z˜\r
k\r
t\r
, z\r
k\r
t+1\r
). (5.8)\r
Here, z˜t = E(s˜t) is the representation of the negative sample, and z˜\r
k\r
t\r
the representation of the k-th object. The full contrastive loss for a\r
single sample is then given by\r
L = H + max(0, e − H˜ ). (5.9)\r
5.5 Related Work\r
Here we review related work on state representation learning.\r
State Representation Learning State representation learning is a very\r
active field. The general goal is to find representations of states where\r
similar states are close together in latent space. Stochastic bisimula\u0002tion [38, 57], lax bisimulation [165] or MDP homomorphisms [141, 142]\r
are formalisms on which much of the work on state similarity is built1.\r
1 For an overview, see [113]. For example, there is much work on bisimulation and other similarity\r
metrics [46, 26, 106, 49, 47, 48, 27] and on using bisimulation (or MDP\r
homomorphism) metrics to learn or evaluate representations [201, 170,\r
54, 89, 4, 20]. It is also very common to use reconstruction-based\r
losses [36, 182, 66, 67, 108, 54, 76, 168, 104, 202, 85, 183, 178, 8]. Other\r
self-supervised learning approaches are also common [167, 51, 82, 43,\r
3, 9, 55, 152].\r
Contrastive Learning Contrastive approaches are common in learn\u0002ing graph representations [19, 139, 61, 19, 150, 176], word represen\u0002tations [123, 121], and image representations [135, 41, 72, 29]. They are

97\r
also becoming more common in state representation learning [94, 170,\r
158, 2, 116, 6, 135]. Most of these works on state representation do not\r
focus on recovering the structure in individual states.\r
Structured Models of Environments Graph networks have been used to\r
take advantage of the structure in an environment [160, 11, 73, 180, 92,\r
146]. Such approaches usually assume that the structured nature of the\r
environment is already known. For problems where this structure is\r
unknown, there is a body of literature (see [60] for a review) focusing\r
on recovering objects from scenes directly from pixels [59, 128, 174,\r
79, 197, 25, 58, 44], using pixel-based losses. Recently, other forms\r
of structure have been gaining ground as well, for example by taking\r
symmetries into account while learning representations [126, 136, 189,\r
155].\r
5.6 Experiments\r
We evaluated C-SWMs on different environments to see if they can\r
recover objects, predict transitions accurately, and generalize to new\r
combinations of objects in scenes. Code can be found at https://\r
github.com/tkipf/c-swm.\r
Evaluation and Training\r
We evaluate using rankings and compare to baselines. Settings, base\u0002lines, and evaluation metrics are described below. For more details\r
and experiments, see [94].\r
Evaluation Metrics Different models result in different latent spaces.\r
To compare their trajectory predictions in latent space, we compare\r
the different approaches based on ranking. This is done by encoding\r
the starting observation into latent space, followed by a prediction of\r
the next step(s). Then the target observation is encoded, as well as a\r
set of reference observations. The latent states are ranked based on\r
how close they are to the predicted latent target state. This allows\r
us to compare methods which learn very different latent spaces and\r
removes the need for comparing based on reconstruction error or to\r
do downstream planning (for planning performance of a related ap\u0002proach, see Chapter 4). We compare on Hits at Rank 1 (H@1) and\r
Mean Reciprocal Rank (MRR) (in %) after encoding the original state,\r
taking steps in latent space, and comparing to the encoding of the tar\u0002get state. We report mean and standard error on 4 runs of hold-out

98\r
environment instances.\r
Training and Evaluation We sample a training dataset by taking uni\u0002formly random actions in the environment and storing the interac\u0002tions. We similarly sample a separate evaluation data set for each\r
environment. We use Adam [90] with a learning rate of 5 · 10−4 and\r
a batch size of 1024. Details of architectures and hyperparameters are\r
included in the sections below and in [94].\r
Baselines For baselines we compare to auto-encoder based world mod\u0002els [66] (both AE and VAE [91]). For the 3-body physics environment\r
we additionally compare to Physics as Inverse Graphics (PAIG) [80].\r
Additionally, for 3D shapes we perform an ablation study where we\r
compare the effect of removing the latent GNN, the state factorization,\r
or the contrastive loss. For AE and VAE baselines we use a batch size\r
of 512 (due to higher memory demands) and for PAIG we use a batch\r
size of 100, as recommended by the authors.\r
(a) 3D blocks (b) Space Invaders (c) 3 Body Physics\r
Figure 5.6.1: Example observa\u0002tions from a) 3D shapes block\r
pushing world, b) Atari Space\r
Invaders, and c) 3 Body Physics.\r
3D Shapes\r
We evaluate on a novel 3D shapes environment, where each block is\r
moved by its own action. The shapes move in a 2D grid, but are\r
represented in the image as blocks in 3D, making it more difficult to\r
extract the underlying objects from visual information. The observa\u0002tions are 50 × 50 × 3 color images. Additional details in [94]. Exam\u0002ple observations are shown in Figure 5.6.1a. We sample 1000 training\r
episodes with 100 environment steps. We sample 10,000 evaluation\r
episodes with 10 steps each. Since the number of possible states is\r
large (approximately 6.4M unique states), a complete trajectory over\u0002lap between test and train data is unlikely. We train for 100 epochs.\r
Qualitative Results We visualize discovered object masks on held-out\r
test data in Figure 5.6.2a, which shows that the objects are masked out\r
separately. In Figure 5.6.2b we show the learned abstract transition\r
graph for one of the blocks. These results show a that the model is\r
able to recover the underlying grid structure of a given object’s transi\u0002tion graph. Note that no transitions are predicted where the block is

99\r
obstructed by another, even though the transition model does not have\r
access to the image inputs.\r
(a) Discovered object masks in a scene\r
from the 3D Cubes environment.\r
(b) Learned abstract state transition\r
graph of the yellow cube, others fixed.\r
Figure 5.6.2: Discovered object\r
masks (left) and abstract state\r
transition graphs for a single\r
object (right) in the 3D shapes\r
block pushing task. Each node\r
is an encoded state from a held\r
out test set, and each edge (color\r
coded by action type) is a transi\u0002tion predicted by the model.\r
Quantitative Results See Table 5.6.1 for quantitative results. C-SWMs\r
predict almost perfectly for short term (1 step), mid-term (5 steps), and\r
long term (10 steps) predictions, in terms of both H@1 and MRR. In\r
comparison, both the auto-encoder world model and the VAE world\r
model do quite well in the short term, but perform a lot worse on\r
mid-term and long term prediction. In terms of ablations of C-SWMs,\r
removing the latent GNN (replacing it by an object based MLP that\r
ignores interactions) slightly hurts long term prediction. Removing the\r
factored latent space hurts all predictions, and long term predictions\r
most. Removing the contrastive loss hurts all predictions, resulting in\r
a very low score for long term prediction especially.\r
Space Invaders\r
We also evaluate on Space Invaders, an Atari 2600 game which has\r
a lot of moving parts: a gun (the agent), the shields, and the aliens.\r
The observations are two consecutive frames, given as 50 × 50 × 6 ten\u0002sors. Example observations are shown in Figure 5.6.1b. We sample\r
1000 training episodes with 10 steps each. We sample 100 evaluation\r
episodes with 10 environment steps. To minimize train/test overlap,\r
we warm-start the data set by first taking random actions for 50 inter\u0002actions, discard them, and then start filling the data set. We addition\u0002ally take care that no trajectories from the test and train set overlap\r
completely. We train for 200 epochs.\r
Qualitative Results See Figure 5.6.3 for object-specific filters and a\r
state transition graph for Space Invaders. The transition graphs are\r
much harder to interpret than those for the 3D shapes grid world.\r
This can have multiple reasons. For one, actions are less indepen\u0002dent compared to the grid worlds. For example, shooting the bullet

100\r
now influences the aliens in the future. Additionally, objects cannot be\r
swapped out the way the objects can in the block world (i.e. the bullet\r
behaves differently than the aliens). Finally, there are many objects\r
with identical visual features (the aliens).\r
(a) Object-specific filters.\r
2 0 2\r
0.05\r
0.00\r
0.05\r
0.10\r
(b) Object slot 1.\r
2.5 0.0 2.5\r
0.0\r
0.5\r
1.0\r
(c) Object slot 2.\r
5 0 5\r
2\r
0\r
2\r
(d) Object slot 3.\r
Figure 5.6.3: Object filters (top)\r
and abstract state transition\r
graphs per object slot (bottom)\r
in Space Invaders. Each node\r
is an encoded state from an un\u0002seen test instance of the environ\u0002ment. Predictions from a trained\r
C-SWM model with K = 3 ob\u0002ject slots.\r
Quantitative Results See Table 5.6.1 for quantitative results. The pre\u0002diction is not as good as it was in the block world, possibly for similar\r
reasons as those listed above. Additionally, results can have a higher\r
variance. Compared to the AE and VAE baselines, C-SWMs perform\r
better at short, mid and long term prediction than the baselines, as\u0002suming we use the right number of slots (performance drops for K = 1\r
and is best for K = 5).\r
(a) Observations from 3-body gravitational\r
physics simulation (top) and learned filter\r
for one object (bottom).\r
2.5 0.0 2.5\r
1\r
0\r
1\r
2\r
(b) Abstract state transition\r
graph.\r
Figure 5.6.4: Object-specific fil\u0002ter (left) and embedded trajec\u0002tories (right) in 3-body physics.\r
Embeddings are projected from\r
four to two dimensions with\r
PCA. Orange nodes are starting\r
states, green edges are ground\r
truth transitions, purple edges\r
are predicted transitions.\r
3-Body Physics\r
Finally, we evaluate on 3-body physics, where the transitions of a sys\u0002tem of 3 objects have to be predicted. Notably, this environment does\r
not contain any actions. The observations are two consecutive frames,\r
given as 50 × 50 × 6 tensors. Example observations are shown in Fig\u0002ure 5.6.1c. We sample 5000 training episodes with 10 steps. We sample

101\r
1000 evaluation episodes with 10 steps. Since this environment has a\r
continuous state space, a full trajectory overlap between test and train\r
set is unlikely. We train for 100 epochs.\r
Qualitative Results We show learned filters for one of the objects, and\r
an abstract state graph for multiple trajectories in Figure 5.6.4. The\r
learned filters are able to separately encode each of the objects. In\r
the abstract state transition graph, we see that the model is able to\r
encode the relevant information about the object and predict smooth\r
trajectories, with the exception of one of the trajectories (in the center),\r
which deviates.\r
Quantitative Results See Table 5.6.1. Both C-SWMs and the AE and\r
VAE baselines are good at short and mid term prediction. C-SWM, as\r
well as the autoencoder baseline are also good at longer term predic\u0002tion. The PAIG model does not perform as well2\r
.\r
2 Using the hyperparameter settings rec\u0002ommended by the authors.\r
Summary C-SWMs predict almost perfectly in the block pushing task,\r
and are able to recover the grid structure of the transition function\r
almost perfectly. The ablation study shows that all components of\r
C-SWMs, but particularly the contrastive loss, contribute to a greater\r
predictive performance, both for short and long term prediction. The\r
object factorization is an important component as well, with the in\u0002teraction (GNN) component providing a small final boost. For Space\r
Invaders, C-SWMs and baseline models have a harder time represent\u0002ing the states well. While C-SWMs perform better than the baselines,\r
the performance is not as good as in the 3D shapes environment. Ad\u0002ditionally, the number of slots (a hyperparameter) has a non-trivial in\u0002fluence on the performance of the model, and should be chosen with\r
care. In the Physics environment, C-SWMs and the strongest baseline\r
(AE world model) perform well, with C-SWMs being slightly better at\r
long term prediction.\r
5.7 Conclusions\r
We proposed C-SWMs, a method for learning structured representa\u0002tions of states in object-oriented MDPs that uses contrastive coding\r
and learns a graph neural network transition model. C-SWMs make\r
use of the learned structure, improving on multi-step prediction and\r
providing better generalization to unseen environment configurations\r
than models that use decoders, unstructured transitions, or unstruc\u0002tured representations.

102\r
1 Step 5 Steps 10 Steps\r
Model H@1 MRR H@1 MRR H@1 MRR\r
3D BLOCKS\r
C-SWM 99.9±0.0 100±0.0 99.9±0.0 100±0.0 99.9±0.0 99.9±0.0\r
– latent GNN 99.9±0.0 99.9±0.0 96.3±0.4 97.7±0.3 86.0±1.8 90.2±1.5\r
– factored states 74.2±9.3 82.5±8.3 48.7±12.9 62.6±13.0 65.8±14.0 49.6±11.0\r
– contrastive loss 48.9±16.8 52.5±17.8 12.2±5.8 16.3±7.1 3.1±1.9 5.3±2.8\r
World Model (AE) 93.5±0.8 95.6±0.6 26.7±0.7 35.6±0.8 4.0±0.2 7.6±0.3\r
World Model (VAE) 90.9±0.7 94.2±0.6 31.3±2.3 41.8±2.3 7.2±0.9 12.9±1.3\r
SPACE\r
INVADERS\r
C-SWM (K = 5) 48.5±7.0 66.1±6.6 16.8±2.7 35.7±3.7 11.8±3.0 26.0±4.1\r
C-SWM (K = 3) 46.2±13.0 62.3±11.5 10.8±3.7 28.5±5.8 6.0±0.4 20.9±0.9\r
C-SWM (K = 1) 31.5±13.1 48.6±11.8 10.0±2.3 23.9±3.6 6.0±1.7 19.8±3.3\r
World Model (AE) 40.2±3.6 59.6±3.5 5.2±1.1 14.1±2.0 3.8±0.8 10.4±1.3\r
World Model (VAE) 1.0±0.0 5.3±0.1 0.8±0.2 5.2±0.0 1.0±0.0 5.2±0.0\r
3-BODY\r
PHYSICS\r
C-SWM 100±0.0 100±0.0 97.2±0.9 98.5±0.5 75.5±4.7 85.2±3.1\r
World Model (AE) 100±0.0 100±0.0 97.7±0.3 98.8±0.2 67.9±2.4 78.4±1.8\r
World Model (VAE) 100±0.0 100±0.0 83.1±2.5 90.3±1.6 23.6±4.2 37.5±4.8\r
Physics WM (PAIG) 89.2±3.5 90.7±3.4 57.7±12.0 63.1±11.1 25.1±13.0 33.1±13.4\r
Table 5.6.1: Ranking results for\r
multi-step prediction in latent\r
space. Reported mean and\r
standard error of scores over 4\r
runs on hold-out environment\r
instances. Highest (mean) scores\r
in bold.\r
Scope Our approach focuses on learning representations for deter\u0002ministic environments under the Markov assumption. We consider an\r
extension to stochastic transition functions and partially observable en\u0002vironments a useful avenue for future work. Additionally, C-SWMs in\r
their current form do not model the reward function, nor do they take\r
reward into account when learning representations. This results in\r
representations that may not contain all necessary information about\r
a given state. Another limitation is that we do not evaluate the repre\u0002sentations on downstream decision making tasks: their usefulness is\r
only evaluated in terms of predictive power. In Chapter 4 we looked at\r
unstructured representations learned in a similar way and show that\r
they are also plannable. Finally, C-SWMs are not able to tell different\r
instances of visually identical objects in a scene apart. For instance\r
disambiguation in a task such as Space Invaders, an approach based\r
on assigning objects into slots [83, 84, 145, 58, 44, 25, 93] could be a\r
useful future avenue.

103\r
6\r
Conclusion\r
This thesis studied symmetry and structure in deep reinforcement\r
learning. In particular, we focused on the main research question:\r
How can we incorporate and extract symmetry and structure in reinforce\u0002ment learning?\r
We divided the thesis up into two main parts, symmetry and struc\u0002ture. Part 1 considered symmetries in the state-action space of deci\u0002sion making problems. We considered symmetries in single agent and\r
multi-agent reinforcement learning problems. In Part 2, we discussed\r
structure in environments and states. We proposed learning action\u0002equivariant representations of MDPs, and recovering object-oriented\r
structure from environment interactions.\r
We start from the first research question, How can we leverage knowl\u0002edge of symmetries in deep reinforcement learning? To answer this ques\u0002tion, Chapter 2 introduces MDP Homomorphic networks for deep re\u0002inforcement learning. MDP Homomorphic networks are neural net\u0002works that are equivariant under symmetries in the joint state-action\r
space of an MDP. We make explicit the connection between MDP Ho\u0002momorphisms and equivariant networks. Current approaches to deep\r
reinforcement learning do not usually exploit knowledge about such\r
structure. By building this prior knowledge into policy and value\r
networks using an equivariance constraint, we can reduce the size of\r
the solution space. We specifically focus on group-structured symme\u0002tries (invertible transformations). Additionally, we introduce an easy\r
method for constructing equivariant network layers numerically, so

104\r
the system designer need not solve the constraints by hand, as is typ\u0002ically done. We construct MDP homomorphic MLPs and CNNs that\r
are equivariant under either a group of reflections or rotations. We\r
show that such networks converge faster than unstructured baselines\r
on CartPole, a grid world and Pong.\r
After showing the improvement in data efficiency using symmetry\r
constraints in single agent policies, we next investigate the effect of\r
global symmetry constraints in distributed cooperative multi agent\r
systems. The second research question, How can we leverage knowledge\r
of global symmetries in distributed cooperative multi-agent systems?, can\r
now be discussed. Chapter 3 introduces Multi-Agent MDP Homomor\u0002phic Networks, a class of networks that allows distributed execution\r
using only local information, yet is able to share experience between\r
global symmetries in the joint state-action space of cooperative multi\u0002agent systems. In cooperative multi-agent systems, complex symme\u0002tries arise between different configurations of the agents and their local\r
observations. For example, consider a group of agents navigating: ro\u0002tating the state globally results in a permutation of the optimal joint\r
policy. Existing work on symmetries in single agent reinforcement\r
learning can only be generalized to the fully centralized setting, be\u0002cause such approaches rely on the global symmetry in the full state\u0002action spaces, and these can result in correspondences across agents.\r
To encode such symmetries while still allowing distributed execution\r
we propose a factorization that decomposes global symmetries into\r
local transformations. Our proposed factorization allows for distribut\u0002ing the computation that enforces global symmetries over local agents\r
and local interactions. We introduce a multi-agent equivariant policy\r
network based on this factorization. We show empirically on symmet\u0002ric multi-agent problems that globally symmetric distributable policies\r
improve data efficiency compared to non-equivariant baselines.\r
In Part 2, we considered structure in learning representations of en\u0002vironments and states. We considered learning plannable represen\u0002tations that are equivariant to actions taken in the original problem\r
using contrastive learning. We dicuss the third research question, How\r
can we learn representations of the world that capture the structure of the\r
environment? In Chapter 4, we exploit action equivariance for repre\u0002sentation learning in reinforcement learning. Equivariance under ac\u0002tions states that transitions in the input space are mirrored by equiva\u0002lent transitions in latent space, while the map and transition functions\r
should also commute. We introduce a contrastive loss function that\r
enforces action equivariance on the learned representations. We prove\r
that when our loss is zero, we have a homomorphism of a determinis-

105\r
tic Markov Decision Process (MDP). Learning equivariant maps leads\r
to structured latent spaces, allowing us to build a model on which we\r
plan through value iteration. We show experimentally that for deter\u0002ministic MDPs, the optimal policy in the abstract MDP can be success\u0002fully lifted to the original MDP. Moreover, the approach easily adapts\r
to changes in the goal states. Empirically, we show that in such MDPs,\r
we obtain better representations in fewer epochs compared to repre\u0002sentation learning approaches using reconstructions, while generaliz\u0002ing better to new goals than model-free approaches. Additionally, we\r
show that action-equivariance is a generalization of the older notion\r
of group equivariance, which suggests that work on equivariance can\r
be viewed from the perspective of homomorphisms that act to move\r
from one state (point in a space) to another state (point in a space).\r
Thus, learned representations that are action-equivariant are plannable\r
with exact planning methods. We next consider the problem of ex\u0002tracting structure within a state, in the form of object-oriented repre\u0002sentation learning. We can now discuss the final research question:\r
How can we learn representations of the world that capture the structure in\r
individual states? Learning structured representations of the world is\r
an important step towards better generalization in downstream tasks.\r
Chapter 5 discusses Contrastively-trained Structured World Models\r
(C-SWMs), an approach to representation learning that uses a con\u0002trastive method to learn compositional representations of world states.\r
We show that C-SWMs can recover structured representations from\r
unstructured image inputs, and can generalize to new scenes better\r
than reconstruction-based baselines and ablated versions.\r
General Conclusions\r
The focus of this thesis was to leverage symmetry and structure in\r
deep reinforcement learning problems. We considered two views on\r
this problem: incorporating prior information in the form of sym\u0002metries, and finding representations that capture the structure in the\r
world. We have shown that using symmetries in single and multi\r
agent systems can leverage outside knowledge to reduce the amount\r
of environment interactions necessary to finding solutions. We have\r
also shown that it is possible to recover plannable representations by\r
focusing on learning action-equivariant representations, and that in\r
object-oriented problems with factored action spaces, we are able to\r
recover the object structure, and use this to better predict future states.

106\r
Limitations\r
The work presented in this thesis is only the first step in this research\r
area. Many open challenges remain. In certain problems, we might\r
not be able to identify the symmetries a priori. In general, comput\u0002ing symmetries in MDPs is isomorphism complete (complexity class\r
of testing whether two graphs are isomorphic) when the dynamics of\r
the system are known [127]. Developing methods for learning these\r
symmetries from data is a promising emerging research area [189]. It\r
is important to correctly identify symmetries, as the approaches pre\u0002sented here are not suitable for asymmetric problems. For example,\r
consider a cartpole problem where the right side of the tracks has more\r
friction than the left side of the tracks. In such a case, the dynamics\r
of the problem are not symmetric, even if they appear that way to a\r
system designer. Another example is playing a game with symmetric\r
dynamics with an asymmetric opponent. In football one might expect\r
a symmetry to exist between the top and the bottom half of the field.\r
However, if the opposing team has a bias to playing through the top\r
half, the resulting dynamics are asymmetric. For problems that are not\r
symmetric, using a symmetric equivariant policy network such as the\r
methods presented here is suboptimal; we force the agent to use sym\u0002metric policies only. If the optimal policy is asymmetric, this means\r
we exclude the optimal policy from the set of possible policies.\r
Enforcing symmetries in neural networks tends to introduce additional\r
computational overhead. In practice, we need to balance computa\u0002tional speed with sample efficiency. Problems with a high cost per\r
sample will benefit from using equivariance, whereas in problems with\r
cheap samples and a necessity for fast decision making it might be\r
preferable to use faster neural network architecturess combined with\r
large numbers of samples.\r
In partially observable problems, symmetries can depend on the whole\r
history of a trajectory [74]. This means that any network that uses such\r
information needs to propagate geometry information through pro\u0002cessing the whole trajectory. In practice, this means developing meth\u0002ods that propagate equivariant information through recurrent networks.\r
Throughout this work, we have focused on exact symmetries. In many\r
problems, the symmetries may only be approximate. For example, in\r
continuously rotating a robot arm we may wish to treat rotations of\r
−90◦ and 90.5◦ as symmetric. Some notion of e-symmetry may be\r
useful in problems like these. Additionally, some symmetries may be\r
partial: opening the door on the left is symmetric to opening the door

107\r
on the right, except the door on the right has a different color handle.\r
Dealing with situations like these will require us to find state-action\r
representations that abstract away irrelevant differences, for example\r
through learning MDP homomorphisms (see Chapter 4).\r
For planning in learned representations, there are limits to using dy\u0002namic programming, as we cannot always consider the full state space\r
before acting. Additionally, as the world is dynamic, there is value in\r
developing methods that are dynamic as well, changing representa\u0002tions on the fly as the agent encounters changes in the environmental\r
dynamics. Finally, our representation learning methods have focused\r
primarily on deterministic and Markov problems. Generalizing these\r
methods to stochastic and partially observable problems is an impor\u0002tant step to later applications.

109\r
7\r
Acknowledgments\r
I’d like to take this section to thank everyone who so generously gave\r
to me of their time, their support, or their insight. First of all, I’m\r
extremely grateful to my promotor, Max Welling, who beyond his bril\u0002liant intellect is a kind and creative person. Max, thank you for your\r
support, guidance, and the trust and freedom you gave me to pur\u0002sue the research that inspired me. I am furthermore deeply indebted\r
to my co-promotor Frans Oliehoek, who besides his sharp mind and\r
encyclopedic knowledge of the pre-deep learning RL literature, has\r
championed for me since before the PhD. Frans, I cannot express how\r
much your support has meant to me in finding my way as a researcher.\r
Further, I would like to express my deepest appreciation to my co\u0002promotor Herke van Hoof, whose insightful questions and conscien\u0002tious attitude have made my work stronger and my life easier. Herke,\r
thank you for being a supervisor I could always fall back on.\r
I’d like to extend my gratitude to the esteemed committee for my\r
PhD: Dr. Doina Precup, Dr. Jan Peters, Dr. Evangelos Kanoulas, Dr.\r
Maarten de Rijke, Dr. Erik Bekkers.\r
Thanks Yuge Shi and Karen Ullrich. Yuge, over the past few years you\r
have become one of my closest friends. It’s hard to describe what your\r
friendship means to me. Karen, you have been an incredible support\r
and cheerleader for me and I’m grateful for your wonderful friendship.\r
I furthermore want to extend my thanks to Rianne van den Berg and\r
Zeynep Akata for their time, advice, and friendship through the years.

110\r
I’d also like to thank Bert Bredeweg, for encouraging me to continue\r
my studies after undergrad, and Leo Dorst, for always being kind,\r
honest, and a wonderful teacher. Many thanks also to Cees Snoek,\r
Katja Hofmann, Jan-Willem van de Meent, Diederik Roijers, and Sam\r
Devlin for their time, advice, and support.\r
I would like to express my appreciation to my close collaborators\r
Daniel Worrall and Thomas Kipf. Thank you for being brilliant and a\r
delight to work with. Thanks as well to my other co-authors: Johannes\r
Brandstetter, Rob Hesselink, Ondrej Biza, Jose Gallego-Posada, Lau\u0002rens Weitkamp, Jakob Foerster, Darius Muglich, Christian Schroeder\r
de Witt, Shimon Whiteson, Tejaswi Kasarla, Gertjan Burghouts, Max\r
van Spengler, Rita Cucchiara, Rahul Savani, Roderich Groß.\r
Doing a PhD in AMLAB has been an incredible experience. A heart\u0002felt thanks to the many insightful, kind, creative, smart, and funny lab\u0002mates from AMLAB and beyond who I had the good fortune of doing a\r
PhD with. Thank you Sadaf, Maxi, Bas, Victor, Tim, Ivan, Sindy, Taco,\r
Jakub, Marco, Qi, Maartje, Gabriele, Maurice, Artem, Emiel, Tessa,\r
Matthias, Christos, Wendy, Sara, Sharvaree, Putri. Thank you as well\r
to Felice and Virginie for taking many tasks out of our hands, and to\r
Dennis for solving all of our computer problems.\r
Many thanks to the good people at DeepMind, who put a lot of effort\r
into turning a remote internship into a wonderful experience. Thanks\r
to my supervisors Ian Gemp, Richard Everett, and Yoram Bachrach.\r
Many thanks also to Danielle Belgrave, Feryal Behbahani, Luisa Zint\u0002graf and all the other kind folks at DeepMind. Additionally, I’d also\r
like to thank Michael Herman, Christian Daniel, and everyone else at\r
the Bosch Center for AI.\r
Finally, I’d like to thank my family and friends. Pascal, je bent mijn\r
steun en toeverlaat. Bedankt voor je grenzeloze enthousiasme, alle\r
middernachtelijke peptalks, en de cuba libres. Mam, bedankt voor je\r
standvastige vertrouwen in mijn kunnen, en bedankt dat je me hebt\r
opgevoed met het idee dat ik alles kan waar ik mijn best voor doe.\r
Emma, ik ben je zeer dankbaar voor je vriendschap van de afgelopen\r
16 jaar. Ik kan mijn leven niet voorstellen zonder jou. Ik ben heel\r
dankbaar voor mijn andere lieve vrienden, die een bron waren van\r
energie, comfort, en afleiding tijdens de PhD: Eszter, Bas, Tessa, Chiel,\r
Sander, Jelte, Willemijn, Jorn, Fabien. Bedankt familie Mazier, dat jullie\r
mij lang geleden opgevangen hebben. Bedankt familie Mettes, dat\r
jullie mij zo volledig in de familie opgenomen hebben.

111\r
Bibliography\r
[1] Farzad Abdolhosseini, Hung Yu Ling, Zhaoming Xie, Xue Bin\r
Peng, and Michiel van de Panne. On learning symmetric lo\u0002comotion. In ACM SIGGRAPH Motion, Interaction, and Games.\r
2019.\r
[2] Rishabh Agarwal, Marlos C. Machado, Pablo Samuel Castro,\r
and Marc G. Bellemare. Contrastive behavioral similarity em\u0002beddings for generalization in reinforcement learning. In Inter\u0002national Conference on Learning Representations, 2021.\r
[3] Pulkit Agrawal, Ashvin Nair, Pieter Abbeel, Jitendra Malik, and\r
Sergey Levine. Learning to poke by poking: Experiential learn\u0002ing of intuitive physics. In Advances in Neural Information Pro\u0002cessing Systems, 2016.\r
[4] Nele Albers, Miguel Suau, and Frans A. Oliehoek. Using bisim\u0002ulation metrics to analyze and evaluate latent state representa\u0002tions. In Belgian-Dutch Conference on Machine Learning, 2021.\r
[5] Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano,\r
John Schulman, and Dan Mané. Concrete problems in AI safety.\r
arXiv:1606.06565, 2016.\r
[6] Ankesh Anand, Evan Racah, Sherjil Ozair, Yoshua Bengio, Marc\u0002Alexandre Cote, and R. Devon Hjelm. Unsupervised state rep\u0002resentation learning in Atari. In Advances in Neural Information\r
Processing Systems, 2019.\r
[7] Brandon Anderson, Truong Son Hy, and Risi Kondor. Cor\u0002morant: Covariant molecular neural networks. In Advances in\r
Neural Information Processing Systems, 2019.\r
[8] Masataro Asai. Unsupervised grounding of plannable first\u0002order logic representation from images. In International Confer\u0002ence on Automated Planning and Scheduling, 2019.

112\r
[9] Yusuf Aytar, Tobias Pfaff, David Budden, Thomas Paine, Ziyu\r
Wang, and Nando de Freitas. Playing hard exploration games by\r
watching youtube. In Advances in Neural Information Processing\r
Systems, 2018.\r
[10] Andrew G. Barto, Richard S. Sutton, and Charles W. Anderson.\r
Neuronlike adaptive elements that can solve difficult learning\r
control problems. IEEE transactions on systems, man, and cybernet\u0002ics, 1983.\r
[11] Peter Battaglia, Razvan Pascanu, Matthew Lai, Danilo Jimenez\r
Rezende, and Koray Kavukcuoglu. Interaction networks for\r
learning about objects, relations and physics. In Advances in Neu\u0002ral Information Processing, 2016.\r
[12] Peter W. Battaglia, Jessica B. Hamrick, Victor Bapst, Alvaro\r
Sanchez-Gonzalez, Vinicius Zambaldi, Mateusz Malinowski,\r
Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner,\r
et al. Relational inductive biases, deep learning, and graph net\u0002works. arXiv preprint arXiv:1806.01261, 2018.\r
[13] Erik J. Bekkers. B-spline CNNs on Lie groups. In International\r
Conference on Learning Representations, 2019.\r
[14] Erik J. Bekkers, Maxime W. Lafarge, Mitko Veta, Koen A.J. Ep\u0002penhof, Josien P.W. Pluim, and Remco Duits. Roto-translation\r
covariant convolutional networks for medical image analy\u0002sis. In International Conference on Medical Image Computing and\r
Computer-Assisted Intervention, 2018.\r
[15] Richard E. Bellman. Dynamic Programming. 1957.\r
[16] Ondrej Biza and Robert Platt. Online abstraction with MDP ho\u0002momorphisms for deep learning. In International Conference on\r
Autonomous Agents and Multi-Agent Systems, 2019.\r
[17] Ondrej Biza, Elise van der Pol, and Thomas Kipf. The impact\r
of negative sampling on contrastive structured world models.\r
In ICML Workshop on Self-Supervised Learning for Reasoning and\r
Perception, 2021.\r
[18] Wendelin Böhmer, Vitaly Kurin, and Shimon Whiteson. Deep\r
coordination graphs. In International Conference on Machine Learn\u0002ing, 2020.\r
[19] Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason\r
Weston, and Oksana Yakhnenko. Translating embeddings for\r
modeling multi-relational data. In Advances in Neural Information\r
Processing, 2013.

113\r
[20] Nicolò Botteghi, Mannes Poel, Beril Sirmacek, and Christoph\r
Brune. Low-dimensional state and action representation\r
learning with MDP homomorphism metrics. arXiv preprint\r
arXiv:2107.01677, 2021.\r
[21] Craig Boutilier. Planning, learning and coordination in multi\u0002agent decision processes. In Conference on Theoretical aspects of\r
rationality and knowledge, 1996.\r
[22] Craig Boutilier, Richard Dearden, Moisés Goldszmidt, et al. Ex\u0002ploiting structure in policy construction. In International Joint\r
Conference on Artificial Intelligence, 1995.\r
[23] Johannes Brandstetter, Rob Hesselink, Elise van der Pol, Erik J.\r
Bekkers, and Max Welling. Geometric and physical quantities\r
improve E(3) equivariant message passing. In International Con\u0002ference on Learning Representations, 2021.\r
[24] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas\r
Schneider, John Schulman, Jie Tang, and Wojciech Zaremba.\r
OpenAI gym. arXiv preprint arXiv:1606.01540, 2016.\r
[25] Christopher P Burgess, Loic Matthey, Nicholas Watters, Rishabh\r
Kabra, Irina Higgins, Matt Botvinick, and Alexander Lerchner.\r
Monet: Unsupervised scene decomposition and representation.\r
arXiv preprint arXiv:1901.11390, 2019.\r
[26] Pablo Samuel Castro. Scalable methods for computing state sim\u0002ilarity in deterministic Markov decision processes. In AAAI Con\u0002ference on Artificial Intelligence, 2020.\r
[27] Pablo Samuel Castro, Tyler Kastner, Prakash Panangaden, and\r
Mark Rowland. MICo: Improved representations via sampling\u0002based state similarity for Markov decision processes. In Advances\r
in Neural Information Processing Systems, 2021.\r
[28] Michael B. Chang, Tomer Ullman, Antonio Torralba, and\r
Joshua B. Tenenbaum. A compositional object-based approach to\r
learning physical dynamics. In International Conference on Learn\u0002ing Representations, 2017.\r
[29] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geof\u0002frey Hinton. A simple framework for contrastive learning of vi\u0002sual representations. In International Conference on Machine Learn\u0002ing, 2020.\r
[30] Christopher Clark and Amos Storkey. Teaching deep convolu\u0002tional neural networks to play Go. In International Conference on\r
Machine Learning, 2015.

114\r
[31] Karl Cobbe, Oleg Klimov, Chris Hesse, Taehoon Kim, and John\r
Schulman. Quantifying generalization in reinforcement learn\u0002ing. In International Conference on Machine Learning, 2019.\r
[32] Taco S. Cohen, Mario Geiger, Jonas Koehler, and Max Welling.\r
Spherical CNNs. In International Conference on Learning Represen\u0002tations, 2018.\r
[33] Taco S. Cohen, Mario Geiger, and Maurice Weiler. A general the\u0002ory of equivariant CNNs on homogeneous spaces. In Advances\r
in Neural Information Processing Systems. 2019.\r
[34] Taco S. Cohen and Max Welling. Group equivariant convolu\u0002tional networks. In International Conference on Machine Learning,\r
2016.\r
[35] Taco S. Cohen and Max Welling. Steerable CNNs. In International\r
Conference on Learning Representations, 2017.\r
[36] Dane Corneil, Wulfram Gerstner, and Johanni Brea. Efficient\r
model-based deep reinforcement learning with variational state\r
tabulation. In International Conference on Machine Learning, 2018.\r
[37] Andreea Deac, Petar Veliˇckovi´c, Ognjen Milinkovi´c, Pierre-Luc\r
Bacon, Jian Tang, and Mladen Nikoli´c. XLVIN: executed latent\r
value iteration nets. arXiv preprint arXiv:2010.13146, 2020.\r
[38] Thomas Dean and Robert Givan. Model minimization in\r
Markov decision processes. In AAAI Conference on Artifical In\u0002telligence/IAAI Conference on Innovative Applications of Artificial In\u0002telligence, 1997.\r
[39] Neel Dey, Antong Chen, and Soheil Ghafurian. Group equivari\u0002ant generative adversarial networks. In International Conference\r
on Learning Representations, 2021.\r
[40] Nichita Diaconu and Daniel E. Worrall. Learning to convolve:\r
A generalized weight-tying approach. In International Conference\r
on Machine Learning, 2019.\r
[41] Alexey Dosovitskiy, Jost Tobias Springenberg, Martin Ried\u0002miller, and Thomas Brox. Discriminative unsupervised feature\r
learning with convolutional neural networks. In Advances in\r
Neural Information Processing Systems, 2014.\r
[42] David Steven Dummit and Richard M. Foote. Abstract Algebra.\r
Wiley, 2004.

115\r
[43] Sebastien Ehrhardt, Aron Monszpart, Niloy Mitra, and Andrea\r
Vedaldi. Unsupervised intuitive physics from visual observa\u0002tions. In Asian Conference on Computer Vision, 2018.\r
[44] Martin Engelcke, Adam R Kosiorek, Oiwi Parker Jones, and Ing\u0002mar Posner. GENESIS: Generative scene inference and sampling\r
with object-centric latent representations. International Conference\r
on Learning Representations, 2020.\r
[45] Gregory Farquhar, Tim Rocktäschel, Maximilian Igl, and\r
SA Whiteson. TreeQN and ATreeC: Differentiable tree\u0002structured models for deep reinforcement learning. In Interna\u0002tional Conference on Learning Representations, 2018.\r
[46] Norm Ferns, Prakash Panangaden, and Doina Precup. Metrics\r
for finite Markov decision processes. In Conference on Uncertainty\r
in Artificial Intelligence, 2004.\r
[47] Norm Ferns, Prakash Panangaden, and Doina Precup. Bisimu\u0002lation metrics for continuous Markov decision processes. SIAM\r
Journal on Computing, 2011.\r
[48] Norman Ferns, Pablo Samuel Castro, Doina Precup, and\r
Prakash Panangaden. Methods for computing state similarity\r
in Markov decision processes. In Conference on Uncertainty in\r
Artificial Intelligence, 2012.\r
[49] Norman Ferns and Doina Precup. Bisimulation metrics are op\u0002timal value functions. In Conference on Uncertainty in Artificial\r
Intelligence, 2014.\r
[50] Marc Finzi, Samuel Stanton, Pavel Izmailov, and Andrew Gor\u0002don Wilson. Generalizing convolutional neural networks for\r
equivariance to Lie groups on arbitrary continuous data. In In\u0002ternational Conference on Machine Learning, 2020.\r
[51] Vincent François-Lavet, Yoshua Bengio, Doina Precup, and Joelle\r
Pineau. Combined reinforcement learning via abstract represen\u0002tations. In AAAI Conference on Artificial Intelligence, 2019.\r
[52] Fabian B Fuchs, Daniel E Worrall, Volker Fischer, and Max\r
Welling. SE (3)-transformers: 3D roto-translation equivariant at\u0002tention networks. Advances in Neural Information Processing Sys\u0002tems, 2020.\r
[53] Victor Garcia Satorras and Joan Bruna. Few-shot learning with\r
graph neural networks. In International Conference on Learning\r
Representations, 2018.

116\r
[54] Carles Gelada, Saurabh Kumar, Jacob Buckman, Ofir Nachum,\r
and Marc G. Bellemare. DeepMDP: Learning continuous latent\r
space models for representation learning. In International Con\u0002ference on Machine Learning, 2019.\r
[55] Dibya Ghosh, Abhishek Gupta, and Sergey Levine. Learning\r
actionable representations with goal conditioned policies. In In\u0002ternational Conference on Learning Representations, 2019.\r
[56] Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol\r
Vinyals, and George E Dahl. Neural message passing for quan\u0002tum chemistry. In International Conference on Machine Learning,\r
2017.\r
[57] Robert Givan, Thomas Dean, and Matthew Greig. Equivalence\r
notions and model minimization in Markov decision processes.\r
In Artificial Intelligence, 2003.\r
[58] Klaus Greff, Raphaël Lopez Kaufman, Rishabh Kabra, Nick Wat\u0002ters, Christopher Burgess, Daniel Zoran, Loic Matthey, Matthew\r
Botvinick, and Alexander Lerchner. Multi-object representation\r
learning with iterative variational inference. In International Con\u0002ference on Machine Learning, 2019.\r
[59] Klaus Greff, Sjoerd van Steenkiste, and Jürgen Schmidhuber.\r
Neural expectation maximization. In Advances in Neural Infor\u0002mation Processing, 2017.\r
[60] Klaus Greff, Sjoerd van Steenkiste, and Jürgen Schmidhuber. On\r
the binding problem in artificial neural networks. arXiv preprint\r
arXiv:2012.05208, 2020.\r
[61] Aditya Grover and Jure Leskovec. node2vec: Scalable feature\r
learning for networks. In International Conference on Knowledge\r
Discovery and Data Mining, 2016.\r
[62] Carlos Guestrin, Daphne Koller, and Ronald Parr. Multiagent\r
planning with factored MDPs. In Advances in Neural Information\r
Processing Systems, 2002.\r
[63] Carlos Guestrin, Daphne Koller, Ronald Parr, and Shobha\r
Venkataraman. Efficient solution algorithms for factored MDPs.\r
Journal of Artificial Intelligence Research, 2003.\r
[64] Carlos Guestrin, Michail Lagoudakis, and Ronald Parr. Coordi\u0002nated reinforcement learning. In International Conference on Ma\u0002chine Learning, 2002.

117\r
[65] Carlos Guestrin, Shobha Venkataraman, and Daphne Koller.\r
Context-specific multiagent coordination and planning with fac\u0002tored MDPs. In AAAI Conference on Artifical Intelligence/IAAI\r
Conference on Innovative Applications of Artificial Intelligence, 2002.\r
[66] David Ha and Jürgen Schmidhuber. World models. arxiv preprint\r
arXiv:1803.10122, 2018.\r
[67] Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas,\r
David Ha, Honglak Lee, and James Davidson. Learning latent\r
dynamics for planning from pixels. In International Conference on\r
Machine Learning, 2019.\r
[68] Charles R. Harris, K. Jarrod Millman, Stéfan J van der Walt,\r
Ralf Gommers, Pauli Virtanen, David Cournapeau, Eric Wieser,\r
Julian Taylor, Sebastian Berg, Nathaniel J. Smith, Robert Kern,\r
Matti Picus, Stephan Hoyer, Marten H. van Kerkwijk, Matthew\r
Brett, Allan Haldane, Jaime Fernández del Río, Mark Wiebe,\r
Pearu Peterson, Pierre Gérard-Marchant, Kevin Sheppard, Tyler\r
Reddy, Warren Weckesser, Hameer Abbasi, Christoph Gohlke,\r
and Travis E. Oliphant. Array programming with NumPy. Na\u0002ture, 2020.\r
[69] Juris Hartmanis and R. E. Stearns. Algebraic Structure Theory Of\r
Sequential Machines. Prentice-Hall, Inc., 1966.\r
[70] Peter Henderson, Riashat Islam, Joelle Pineau, David Meger,\r
Doina Precup, and Philip Bachman. Deep reinforcement learn\u0002ing that matters. In AAAI Conference on Artificial Intelligence,\r
2018.\r
[71] Irina Higgins, David Amos, David Pfau, Sebastien Racaniere,\r
Loic Matthey, Danilo Rezende, and Alexander Lerchner. To\u0002wards a definition of disentangled representations. arxiv preprint\r
arXiv:1812.02230, 2018.\r
[72] R. Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon,\r
Karan Grewal, Adam Trischler, and Yoshua Bengio. Learn\u0002ing deep representations by mutual information estimation and\r
maximization. In International Conference on Learning Representa\u0002tions, 2019.\r
[73] Yedid Hoshen. VAIN: Attentional multi-agent predictive mod\u0002eling. In Advances in Neural Information Processing, 2017.\r
[74] Hengyuan Hu, Adam Lerer, Alex Peysakhovich, and Jakob Fo\u0002erster. "Other-play" for zero-shot coordination. In International\r
Conference on Machine Learning, 2020.

118\r
[75] Wenlong Huang, Igor Mordatch, and Deepak Pathak. One\r
policy to control them all: Shared modular policies for agent\u0002agnostic control. In International Conference on Machine Learning,\r
2020.\r
[76] Maximilian Igl, Luisa Zintgraf, Tuan Anh Le, Frank Wood, and\r
Shimon Whiteson. Deep variational reinforcement learning for\r
POMDPs. In International Conference on Machine Learning, 2018.\r
[77] Max Jaderberg, Volodymyr Mnih, Wojciech Marian Czar\u0002necki, Tom Schaul, Joel Z. Leibo, David Silver, and Koray\r
Kavukcuoglu. Reinforcement learning with unsupervised auxil\u0002iary tasks. In International Conference on Learning Representations,\r
2017.\r
[78] Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparame\u0002terization with Gumbel-Softmax. In International Conference on\r
Learning Representations, 2017.\r
[79] Michael Janner, Sergey Levine, William T Freeman, Joshua B\r
Tenenbaum, Chelsea Finn, and Jiajun Wu. Reasoning about\r
physical interactions with object-oriented prediction and plan\u0002ning. In International Conference on Learning Representations, 2019.\r
[80] Miguel Jaques, Michael Burke, and Timothy Hospedales.\r
Physics-as-inverse-graphics: Joint unsupervised learning of ob\u0002jects and physics from video. arXiv preprint arXiv:1905.11169,\r
2019.\r
[81] Jiechuan Jiang, Chen Dun, Tiejun Huang, and Zongqing Lu.\r
Graph convolutional reinforcement learning. In International\r
Conference on Learning Representations, 2020.\r
[82] Rico Jonschkowski and Oliver Brock. Learning state representa\u0002tions with robotic priors. In Autonomous Robots, 2015.\r
[83] Daniel Kahneman and Anne Treisman. Changing views of Atten\u0002tion and Automaticity. Academic Press, Inc., 1984.\r
[84] Daniel Kahneman, Anne Treisman, and Brian J Gibbs. The re\u0002viewing of object files: Object-specific integration of informa\u0002tion. Cognitive psychology, 1992.\r
[85] Lukasz Kaiser, Mohammad Babaeizadeh, Piotr Milos, Blazej Os\u0002inski, Roy H. Campbell, Konrad Czechowski, Dumitru Erhan,\r
Chelsea Finn, Piotr Kozakowski, Sergey Levine, et al. Model\u0002based reinforcement learning for Atari. In International Confer\u0002ence on Learning Representations, 2020.

119\r
[86] Peter Karkus, David Hsu, and Wee Sun Lee. QMDP-net: Deep\r
learning for planning under partial observability. In Advances in\r
Neural Information Processing Systems, 2017.\r
[87] Tejaswi Kasarla, Gertjan J Burghouts, Max van Spengler, Elise\r
van der Pol, Rita Cucchiara, and Pascal Mettes. Maximum\r
class separation as inductive bias in one matrix. arXiv preprint\r
arXiv:2206.08704, 2022.\r
[88] Michael Kearns and Daphne Koller. Efficient reinforcement\r
learning in factored MDPs. In International Joint Conference on\r
Artificial Intelligence, 1999.\r
[89] Mete Kemertas and Tristan Aumentado-Armstrong. Towards ro\u0002bust bisimulation metric learning. Advances in Neural Information\r
Processing Systems, 2021.\r
[90] Diederik P. Kingma and Jimmy Lei Ba. Adam: A method for\r
stochastic optimization. In International Conference on Learning\r
Representations, 2015.\r
[91] Diederik P. Kingma and Max Welling. Auto-encoding varia\u0002tional bayes. In International Conference on Learning Representa\u0002tions, 2014.\r
[92] Thomas Kipf, Ethan Fetaya, Kuan-Chieh Wang, Max Welling,\r
and Richard Zemel. Neural relational inference for interacting\r
systems. In International Conference on Machine Learning, 2018.\r
[93] Thomas Kipf, Yujia Li, Hanjun Dai, Vinicius Zambaldi, Alvaro\r
Sanchez-Gonzalez, Edward Grefenstette, Pushmeet Kohli, and\r
Peter Battaglia. Compile: Compositional imitation learning and\r
execution. In International Conference on Machine Learning, 2019.\r
[94] Thomas Kipf, Elise van der Pol, and Max Welling. Contrastive\r
learning of structured world models. In International Conference\r
on Learning Representations, 2020.\r
[95] Thomas N. Kipf and Max Welling. Semi-supervised classifica\u0002tion with graph convolutional networks. In International Confer\u0002ence on Learning Representations, 2017.\r
[96] Martin Klissarov and Doina Precup. Diffusion-based approxi\u0002mate value functions. In ICML Workshop on Efficient Credit As\u0002signment in Deep Learning and Deep Reinforcement Learning, 2018.\r
[97] Jelle R Kok and Nikos Vlassis. Collaborative multiagent rein\u0002forcement learning by payoff propagation. Journal of Machine\r
Learning Research, 2006.

120\r
[98] Risi Kondor and Shubhendu Trivedi. On the generalization of\r
equivariance and convolution in neural networks to the action of\r
compact groups. In International Conference on Machine Learning,\r
2018.\r
[99] Ilya Kostrikov, Denis Yarats, and Rob Fergus. Image augmenta\u0002tion is all you need: Regularizing deep reinforcement learning\r
from pixels. In International Conference on Learning Representa\u0002tions, 2021.\r
[100] Landon Kraemer and Bikramjit Banerjee. Multi-agent reinforce\u0002ment learning as a rehearsal for decentralized planning. Neuro\u0002computing, 2016.\r
[101] Schütt Kristof, Kindermans Pieter-Jan, Sauceda Huziel, Chmiela\r
Stefan, Tkatchenko Alexandre, and Klaus-Robert Müller. Schnet:\r
a continuous-filter convolutional neural network for modeling\r
quantum interactions. In Advances in Neural Information Process\u0002ing Systems, 2017.\r
[102] Vitaly Kurin, Maximilian Igl, Tim Rocktäschel, Wendelin\r
Boehmer, and Shimon Whiteson. My body is a cage: the role\r
of morphology in graph-based incompatible control. In Interna\u0002tional Conference on Learning Representations, 2021.\r
[103] Thanard Kurutach, Ignasi Clavera, Yan Duan, Aviv Tamar, and\r
Pieter Abbeel. Model-ensemble trust-region policy optimization.\r
In International Conference on Learning Representations, 2018.\r
[104] Thanard Kurutach, Aviv Tamar, Ge Yang, Stuart Russell, and\r
Pieter Abbeel. Learning plannable representations with causal\r
infogan. In Advances in Neural Information Processing Systems,\r
2018.\r
[105] Lior Kuyer, Shimon Whiteson, Bram Bakker, and Nikos Vlassis.\r
Multiagent reinforcement learning for urban traffic control us\u0002ing coordination graphs. In Joint European Conference on Machine\r
Learning and Knowledge Discovery in Databases, 2008.\r
[106] Charline Le Lan, Marc G Bellemare, and Pablo Samuel Castro.\r
Metrics and continuity in reinforcement learning. In AAAI Con\u0002ference on Artificial Intelligence, 2021.\r
[107] Michael Laskin, Kimin Lee, Adam Stooke, Lerrel Pinto, Pieter\r
Abbeel, and Aravind Srinivas. Reinforcement learning with aug\u0002mented data. In Advances in Neural Information Processing Sys\u0002tems, 2020.

121\r
[108] Adrien Laversanne-Finot, Alexandre Péré, and Pierre-Yves\r
Oudeyer. Curiosity driven exploration of learned disentangled\r
goal spaces. In Conference on Robot Learning, 2018.\r
[109] Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner.\r
Gradient-based learning applied to document recognition. IEEE,\r
1998.\r
[110] Yann LeCun, Sumit Chopra, Raia Hadsell, M Ranzato, and\r
F Huang. A tutorial on energy-based learning. Predicting struc\u0002tured data, 2006.\r
[111] Kimin Lee, Kibok Lee, Jinwoo Shin, and Honglak Lee. Network\r
randomization: A simple technique for generalization in deep\r
reinforcement learning. In International Conference on Learning\r
Representations, 2020.\r
[112] Lisa Lee, Emilio Parisotto, Devendra Singh Chaplot, Eric P. Xing,\r
and Ruslan Salakhutdinov. Gated path planning networks. In\r
International Conference on Machine Learning, 2018.\r
[113] Lihong Li, Thomas J. Walsh, and Michael L. Littman. Towards\r
a unified theory of state abstraction for MDPs. In International\r
Symposium on Artificial Intelligence and Mathematics, 2006.\r
[114] Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard\r
Zemel. Gated graph sequence neural networks. arXiv preprint\r
arXiv:1511.05493, 2015.\r
[115] Yijiong Lin, Jiancong Huang, Matthieu Zimmer, Yisheng Guan,\r
Juan Rojas, and Paul Weng. Invariant transform experience re\u0002play: Data augmentation for deep reinforcement learning. IEEE\r
Robotics and Automation Letters, 2020.\r
[116] Guoqing Liu, Chuheng Zhang, Li Zhao, Tao Qin, Jinhua Zhu,\r
Jian Li, Nenghai Yu, and Tie-Yan Liu. Return-based contrastive\r
representation learning for reinforcement learning. In Interna\u0002tional Conference on Learning Representations, 2021.\r
[117] Iou-Jen Liu, Raymond A. Yeh, and Alexander G. Schwing. PIC:\r
Permutation invariant critic for multi-agent deep reinforcement\r
learning. In Conference on Robot Learning, 2019.\r
[118] Anuj Mahajan and Theja Tulabandhula. Symmetry learn\u0002ing for function approximation in reinforcement learning.\r
arXiv:1706.02999, 2017.\r
[119] Aditi Mavalankar. Goal-conditioned batch reinforcement learn\u0002ing for rotation invariant locomotion. In ICLR Workshop Beyond\r
Tabula Rasa in RL, 2020.

122\r
[120] Pascal Mettes, Elise van der Pol, and Cees Snoek. Hyperspheri\u0002cal prototype networks. Advances in Neural Information Processing\r
Systems, 2019.\r
[121] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Ef\u0002ficient estimation of word representations in vector space. arXiv\r
preprint arXiv:1301.3781, 2013.\r
[122] Shruti Mishra, Abbas Abdolmaleki, Arthur Guez, Piotr Trochim,\r
and Doina Precup. Augmenting learning using symmetry in\r
a biologically-inspired domain. arXiv preprint arXiv:1910.00528,\r
2019.\r
[123] Andriy Mnih and Yee Whye Teh. A fast and simple algorithm for\r
training neural probabilistic language models. In International\r
Conference on Machine Learning, 2012.\r
[124] Volodymyr Mnih, Adrià Puigdomènech Badia, Mehdi Mirza,\r
Alex Graves, Tim Harley, Timothy P. Lillicrap, David Silver, and\r
Koray Kavukcuoglu. Asynchronous methods for deep reinforce\u0002ment learning. In International Conference on Machine Learning,\r
2016.\r
[125] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A.\r
Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin Ried\u0002miller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen,\r
Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King,\r
Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis\r
Hassabis. Human-level control through deep reinforcement\r
learning. In Nature, 2015.\r
[126] Arnab Kumar Mondal, Vineet Jain, Kaleem Siddiqi, and Sia\u0002mak Ravanbakhsh. EqR: Equivariant representations for data\u0002efficient reinforcement learning. In International Conference on\r
Machine Learning, 2021.\r
[127] Shravan Matthur Narayanamurthy and Balaraman Ravindran.\r
On the hardness of finding symmetries in Markov decision pro\u0002cesses. In International Conference on Machine learning, 2008.\r
[128] Charlie Nash, Ali Eslami, Chris Burgess, Irina Higgins, Daniel\r
Zoran, Theophane Weber, and Peter Battaglia. The multi-entity\r
variational autoencoder. In Advances in Neural Information Pro\u0002cessing Workshops, 2017.\r
[129] Maximilian Nickel, Kevin Murphy, Volker Tresp, and Evgeniy\r
Gabrilovich. A review of relational machine learning for knowl\u0002edge graphs. IEEE, 2016.

123\r
[130] Sufeng Niu, Siheng Chen, Colin Targonski, Melissa Smith, Je\u0002lena Kova evi, and Hanyu Guo. Generalized value iteration\r
networks: Life beyond lattices. In AAAI Conference on Artificial\r
Intelligence, 2018.\r
[131] Future of Life Institute. Autonomous weapons: An open letter\r
from AI & robotics researchers, 2015.\r
[132] Junhyuk Oh, Satinder Singh, and Honglak Lee. Value prediction\r
network. In Advances in Neural Information Processing Systems,\r
2017.\r
[133] Frans A. Oliehoek, Matthijs T.J. Spaan, and Nikos Vlassis.\r
Optimal and approximate Q-value functions for decentralized\r
POMDPs. Journal of Artificial Intelligence Research, 2008.\r
[134] Rahul Oliehoek, Frans A.and Savani, Jose Gallego, Elise van der\r
Pol, and Roderich Groß. Beyond local Nash equilibria for ad\u0002versarial networks. In Benelux Conference on Artificial Intelligence,\r
2018.\r
[135] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representa\u0002tion learning with contrastive predictive coding. arXiv preprint\r
arXiv:1807.03748, 2018.\r
[136] Jung Yeon Park, Ondrej Biza, Linfeng Zhao, Jan-Willem van de\r
Meent, and Robin Walters. Learning symmetric embeddings for\r
equivariant world models. In International Conference on Machine\r
Learning, 2021.\r
[137] Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan,\r
Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison,\r
Luca Antiga, and Adam Lerer. Automatic differentiation in py\u0002torch. In Advances in Neural Information Processing Autodiff Work\u0002shop, 2017.\r
[138] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James\r
Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Na\u0002talia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf,\r
Edward Yang, Zachary DeVito, Martin Raison, Alykhan Te\u0002jani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai,\r
and Soumith Chintala. Pytorch: An imperative style, high\u0002performance deep learning library. In Advances in Neural Infor\u0002mation Processing Systems, 2019.\r
[139] Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. Deepwalk:\r
Online learning of social representations. In International Confer\u0002ence on Knowledge Discovery and Data Mining, 2014.

124\r
[140] Martin L Puterman. Markov Decision Processes: Discrete Stochastic\r
Dynamic Programming. John Wiley & Sons, Inc., 1994.\r
[141] Balaraman Ravindran and Andrew G. Barto. Symmetries and\r
model minimization in Markov decision processes. Technical\r
report, University of Massachusetts, 2001.\r
[142] Balaraman Ravindran and Andrew G. Barto. SMDP homomor\u0002phisms: An algebraic approach to abstraction in semi-Markov\r
decision processes. In International Joint Conference on Artificial\r
Intelligence, 2003.\r
[143] Balaraman Ravindran and Andrew G. Barto. Approximate ho\u0002momorphisms: A framework for non-exact minimization in\r
Markov decision processes. In International Conference on Knowl\u0002edge Based Computer Systems, 2004.\r
[144] Philipp Robbel, Frans A. Oliehoek, and Mykel J. Kochender\u0002fer. Exploiting anonymity in approximate linear programming:\r
Scaling to large multiagent MDPs. In AAAI Conference on Artifi\u0002cial Intelligence, 2016.\r
[145] Sara Sabour, Nicholas Frosst, and Geoffrey E Hinton. Dynamic\r
routing between capsules. In Advances in Neural Information Pro\u0002cessing, 2017.\r
[146] Alvaro Sanchez-Gonzalez, Nicolas Heess, Jost Tobias Springen\u0002berg, Josh Merel, Martin Riedmiller, Raia Hadsell, and Peter\r
Battaglia. Graph networks as learnable physics engines for infer\u0002ence and control. In International Conference on Machine Learning,\r
2018.\r
[147] Victor Garcia Satorras, Emiel Hoogeboom, and Max Welling.\r
E(n) equivariant graph neural networks. In International Con\u0002ference on Machine Learning, 2021.\r
[148] Victor Garcia Satorras and Max Welling. Neural enhanced be\u0002lief propagation on factor graphs. In International Conference on\r
Artificial Intelligence and Statistics, 2021.\r
[149] Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagen\u0002buchner, and Gabriele Monfardini. The graph neural network\r
model. IEEE Transactions on Neural Networks, 2009.\r
[150] Michael Schlichtkrull, Thomas N Kipf, Peter Bloem, Rianne Van\r
Den Berg, Ivan Titov, and Max Welling. Modeling relational data\r
with graph convolutional networks. 2017.

125\r
[151] Nicol N. Schraudolph, Peter Dayan, and Terrence J. Sejnowski.\r
Temporal difference learning of position evaluation in the game\r
of Go. In Advances in Neural Information Processing Systems, 1994.\r
[152] Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert,\r
Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez,\r
Edward Lockhart, Demis Hassabis, Thore Graepel, Timothy P.\r
Lillicrap, and David Silver. Mastering Atari, Go, chess and Shogi\r
by planning with a learned model. 2019.\r
[153] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford,\r
and Oleg Klimov. Proximal policy optimization algorithms. In\r
arXiv:1707.06347, 2017.\r
[154] Dale Schuurmans and Relu Patrascu. Direct value\u0002approximation for factored MDPs. Advances in Neural Informa\u0002tion Processing Systems, 2001.\r
[155] Dhruv Sharma, Alihusein Kuwajerwala, and Florian Shkurti.\r
Augmenting imitation experience via equivariant representa\u0002tions. In International Conference on Robotics and Automation, 2022.\r
[156] David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Lau\u0002rent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioan\u0002nis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mas\u0002tering the game of Go with deep neural networks and tree\r
search. In Nature, 2016.\r
[157] Gregor N.C. Simm, Robert Pinsler, Gábor Csányi, and\r
José Miguel Hernández-Lobato. Symmetry-aware actor-critic for\r
3D molecular design. In International Conference on Learning Rep\u0002resentations, 2021.\r
[158] Aravind Srinivas, Michael Laskin, and Pieter Abbeel. CURL:\r
Contrastive unsupervised representations for reinforcement\r
learning. In International Conference on Machine Learning, 2020.\r
[159] Adam Stooke and Pieter Abbeel. rlpyt: A research code\r
base for deep reinforcement learning in Pytorch. arxiv preprint\r
arXiv:1909.01500, 2019.\r
[160] Sainbayar Sukhbaatar, Arthur Szlam, and Rob Fergus. Learning\r
multiagent communication with backpropagation. In Advances\r
in Neural Information Processing Systems, 2016.\r
[161] Chen Sun, Abhinav Shrivastava, Carl Vondrick, Kevin Murphy,\r
Rahul Sukthankar, and Cordelia Schmid. Actor-centric relation\r
network. In European Conference on Computer Vision, 2018.

126\r
[162] Chen Sun, Abhinav Shrivastava, Carl Vondrick, Rahul Suk\u0002thankar, Kevin Murphy, and Cordelia Schmid. Relational action\r
forecasting. In IEEE Conference on Computer Vision and Pattern\r
Recognition, 2019.\r
[163] Peter Sunehag, Guy Lever, Audrunas Gruslys, Wojciech Mar\u0002ian Czarnecki, Vinicius Zambaldi, Max Jaderberg, Marc Lanc\u0002tot, Nicolas Sonnerat, Joel Z Leibo, Karl Tuyls, et al. Value\u0002decomposition networks for cooperative multi-agent learning.\r
In International Conference on Autonomous Agents and Multi-Agent\r
Systems, 2018.\r
[164] Aviv Tamar, Yi Wu, Garrett Thomas, Sergey Levine, and Pieter\r
Abbeel. Value iteration networks. In Advances in Neural Informa\u0002tion Processing Systems, 2016.\r
[165] Jonathan Taylor, Doina Precup, and Prakash Panagaden. Bound\u0002ing performance loss in approximate MDP homomorphisms. In\r
Advances in Neural Information Processing Systems, 2008.\r
[166] Nathaniel Thomas, Tess Smidt, Steven Kearnes, Lusann Yang,\r
Li Li, Kai Kohlhoff, and Patrick Riley. Tensor field networks:\r
Rotation-and translation-equivariant neural networks for 3D\r
point clouds. arXiv preprint arXiv:1802.08219, 2018.\r
[167] Valentin Thomas, Emmanuel Bengio, William Fedus, Jules Pon\u0002dard, Philippe Beaudoin, Hugo Larochelle, Joelle Pineau, Doina\r
Precup, and Yoshua Bengio. Disentangling the independently\r
controllable factors of variation by interacting with the world.\r
arXiv preprint arXiv:1802.09484, 2018.\r
[168] Valentin Thomas, Jules Pondard, Emmanuel Bengio, Marc Sar\u0002fati, Philippe Beaudoin, Marie-Jean Meurs, Joelle Pineau, Doina\r
Precup, and Yoshua Bengio. Independently controllable factors.\r
arxiv preprint arXiv:1708.01289, 2017.\r
[169] Elise van der Pol, Ian Gemp, Yoram Bachrach, and Richard Ev\u0002erett. Stochastic parallelizable eigengap dilation for large graph\r
clustering. arXiv preprint arXiv:2207.14589, 2022.\r
[170] Elise van der Pol, Thomas Kipf, Frans A. Oliehoek, and Max\r
Welling. Plannable approximations to MDP homomorphisms:\r
Equivariance under actions. In International Conference on Au\u0002tonomous Agents and Multi-Agent Systems, 2020.\r
[171] Elise van der Pol and Frans A. Oliehoek. Coordinated deep rein\u0002forcement learners for traffic light control. In NeurIPS Workshop\r
on Learning, Inference and Control of Multi-Agent Systems, 2016.

127\r
[172] Elise van der Pol, Herke van Hoof, Frans A. Oliehoek, and Max\r
Welling. Multi-agent MDP homomorphic networks. In Interna\u0002tional Conference on Learning Representations, 2022.\r
[173] Elise van der Pol, Daniel E. Worrall, Herke van Hoof, Frans A.\r
Oliehoek, and Max Welling. MDP homomorphic networks:\r
Group symmetries in reinforcement learning. In Advances in\r
Neural Information Processing Systems, 2020.\r
[174] Sjoerd van Steenkiste, Michael Chang, Klaus Greff, and Jürgen\r
Schmidhuber. Relational neural expectation maximization: Un\u0002supervised discovery of objects and their interactions. In Inter\u0002national Conference on Learning Representations, 2018.\r
[175] Pradeep Varakantham, Yossiri Adulyasak, and Patrick Jaillet.\r
Decentralized stochastic planning with anonymity in interac\u0002tions. In AAAI Conference on Artificial Intelligence, 2014.\r
[176] Petar Veliˇckovi´c, William Fedus, William L. Hamilton, Pietro\r
Liò, Yoshua Bengio, and R. Devon Hjelm. Deep graph infomax.\r
In International Conference on Learning Representations, 2019.\r
[177] K.P. Wabersich and M.N. Zeilinger. Linear model predictive\r
safety certification for learning-based control. In IEEE Confer\u0002ence on Decision and Control, 2018.\r
[178] Angelina Wang, Thanard Kurutach, Kara Liu, Pieter Abbeel, and\r
Aviv Tamar. Learning robotic manipulation through visual plan\u0002ning and acting. In Robotics: Science and Systems, 2019.\r
[179] Dian Wang, Robin Walters, and Robert Platt. SO(2)-equivariant\r
reinforcement learning. In International Conference on Learning\r
Representations, 2022.\r
[180] Tingwu Wang, Renjie Liao, Jimmy Ba, and Sanja Fidler. Ner\u0002venet: Learning structured policy with graph neural networks.\r
In International Conference on Learning Representations, 2018.\r
[181] Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng Chen.\r
Knowledge graph embedding by translating on hyperplanes. In\r
AAAI Conference on Artificial Intelligence, 2014.\r
[182] Manuel Watter, Jost Tobias Springenberg, Joschka Boedecker,\r
and Martin Riedmiller. Embed to control: a locally linear la\u0002tent dynamics model for control from raw images. In Advances\r
in Neural Information Processing Systems, 2015.

128\r
[183] Nicholas Watters, Loic Matthey, Matko Bosnjak, Christopher P.\r
Burgess, and Alexander Lerchner. COBRA: data-efficient model\u0002based RL through unsupervised object discovery and curiosity\u0002driven exploration. arxiv preprint arXiv:1905.09275, 2019.\r
[184] Nicholas Watters, Daniel Zoran, Theophane Weber, Peter\r
Battaglia, Razvan Pascanu, and Andrea Tacchetti. Visual inter\u0002action networks: Learning a physics simulator from video. In\r
Advances in Neural Information Processing, 2017.\r
[185] Hua Wei, Guanjie Zheng, Vikash Gayah, and Zhenhui Li.\r
A survey on traffic signal control methods. arXiv preprint\r
arXiv:1904.08117, 2019.\r
[186] Maurice Weiler and Gabriele Cesa. General E(2)-equivariant\r
steerable CNNs. In Advances in Neural Information Processing Sys\u0002tems, 2019.\r
[187] Maurice Weiler, Mario Geiger, Max Welling, Wouter Boomsma,\r
and Taco S Cohen. 3D steerable CNNs: Learning rotationally\r
equivariant features in volumetric data. In Advances in Neural\r
Information Processing Systems, 2018.\r
[188] Maurice Weiler, Fred A. Hamprecht, and Martin Storath. Learn\u0002ing steerable filters for rotation equivariant CNNs. In IEEE Con\u0002ference on Computer Vision and Pattern Recognition, 2018.\r
[189] Matthias Weissenbacher, Samarth Sinha, Animesh Garg, and\r
Yoshinobu Kawahara. Koopman Q-learning: Offline reinforce\u0002ment learning via symmetries of dynamics. In International Con\u0002ference on Learning Representations, 2022.\r
[190] Laurens Weitkamp, Elise van der Pol, and Zeynep Akata. Visual\r
rationalizations in deep reinforcement learning for Atari games.\r
In Benelux Conference on Artificial Intelligence, 2018.\r
[191] Marysia Winkels and Taco S. Cohen. 3D G-CNNs for pulmonary\r
nodule detection. In Medical Imaging with Deep Learning Confer\u0002ence, 2018.\r
[192] Daniel E. Worrall and Gabriel J. Brostow. CubeNet: Equivari\u0002ance to 3D rotation and translation. In European Conference on\r
Computer Vision, 2018.\r
[193] Daniel E. Worrall, Stephan J. Garbin, Daniyar Turmukhambetov,\r
and Gabriel J. Brostow. Harmonic networks: Deep translation\r
and rotation equivariance. In IEEE Conference on Computer Vision\r
and Pattern Recognition, 2017.

129\r
[194] Daniel E. Worrall and Max Welling. Deep scale-spaces: Equiv\u0002ariance over scale. In Advances in Neural Information Processing\r
Systems, 2019.\r
[195] Wenxuan Wu, Zhongang Qi, and Li Fuxin. Pointconv: Deep\r
convolutional networks on 3D point clouds. In IEEE Conference\r
on Computer Vision and Pattern Recognition, 2019.\r
[196] Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-MNIST:\r
A novel image dataset for benchmarking machine learning algo\u0002rithms. arxiv preprint arXiv:1708.07747, 2017.\r
[197] Zhenjia Xu, Zhijian Liu, Chen Sun, Kevin Murphy, William T\r
Freeman, Joshua B Tenenbaum, and Jiajun Wu. Unsupervised\r
discovery of parts, structure, and dynamics. In International Con\u0002ference on Learning Representations, 2019.\r
[198] Dmitry Yarotsky. Universal approximations of invariant maps\r
by neural networks. arxiv preprint arXiv:1804.10306, 2018.\r
[199] KiJung Yoon, Renjie Liao, Yuwen Xiong, Lisa Zhang, Ethan Fe\u0002taya, Raquel Urtasun, Richard Zemel, and Xaq Pitkow. Inference\r
in probabilistic graphical models by graph neural networks. In\r
ICLR Workshop Track, 2018.\r
[200] Amy Zhang, Adam Lerer, Sainbayar Sukhbaatar, Rob Fergus,\r
and Arthur Szlam. Composable planning with attributes. In\r
International Conference on Machine Learning, 2018.\r
[201] Amy Zhang, Rowan McAllister, Roberto Calandra, Yarin Gal,\r
and Sergey Levine. Learning invariant representations for rein\u0002forcement learning without reconstruction. In International Con\u0002ference on Learning Representations, 2020.\r
[202] Marvin Zhang, Sharad Vikram, Laura Smith, Pieter Abbeel,\r
Matthew Johnson, and Sergey Levine. SOLAR: Deep structured\r
representations for model-based reinforcement learning. In In\u0002ternational Conference on Machine Learning, 2019.\r
[203] Xupeng Zhu, Dian Wang, Ondrej Biza, Guanang Su, Robin Wal\u0002ters, and Robert Platt. Sample efficient grasp learning using\r
equivariant models. In Robotics: Science and Systems (RSS), 2022.\r
[204] Martin Zinkevich and Tucker Balch. Symmetry in Markov de\u0002cision processes and its implications for single agent and multi\r
agent learning. In International Conference on Machine Learning,\r
2001.

131\r
8\r
Summary\r
In this thesis, we study symmetry and structure in deep reinforcement\r
learning. We divide the thesis into two different parts. In the first, we\r
explore how to leverage knowledge of symmetries in reinforcement\r
learning problems. In the second, we propose methods to learning\r
about the structure of an agent’s environment and individual states.\r
Our contributions are as follows: In Part 1 we use existing knowl\u0002edge of symmetries to gain improvements in data efficiency in MDPs\r
(Chapter 2) and knowledge of symmetries and structure to improve\r
data efficiency in multi-agent MDPs (Chapter 3).\r
• We propose MDP Homomorphic Networks (Chapter 2) [173]. MDP\r
homomorphic networks are neural networks that are equivariant\r
under symmetries in the joint state-action space of an MDP. Due to\r
their equivariance, we find improved data efficiency compared to\r
non-equivariant baselines.\r
• We propose Multi-Agent MDP Homomorphic Networks (Chapter 3) [172].\r
Multi-Agent MDP Homomorphic Networks form a class of net\u0002works that allows distributed execution using only local informa\u0002tion, yet is able to share experience between global symmetries in\r
the joint state-action space of cooperative multi-agent systems. We\r
show that global equivariance improves data efficiency compared\r
to non-equivariant distributed networks on symmetric coordination\r
problems.

132\r
In Part 2 we consider learning the underlying graphs of MDPs (Chap\u0002ter 4) and structure in individual states (Chapter 5).\r
• We propose PRAE (Chapter 4) [170]. PRAE exploits action equivari\u0002ance for representation learning in reinforcement learning. Equiv\u0002ariance under actions states that transitions in the input space are\r
mirrored by equivalent transitions in latent space, while the map\r
and transition functions should also commute. We prove that under\r
certain assumptions, the map we learn is an MDP homomorphism\r
and show empirically that the approach is data-efficient and fast to\r
train, generalizing well to new goal states and instances with the\r
same environmental dynamics.\r
• We propose C-SWMs (Chapter 5) [94]. C-SWMs find object-oriented\r
representations of states from pixels, using contrastive coding and\r
graph neural network transition functions. We show improvement\r
in multi-step prediction and generalization to unseen environment\r
configurations compared to models that use decoders, unstructured\r
transitions, or unstructured representations.

133\r
9\r
Samenvatting - Dutch\r
Summary\r
In deze dissertatie bestuderen we symmetrieën en structuur in deep\r
reinforcement learning. De dissertatie bestaat uit twee delen. In het\r
eerste deel onderzoeken we hoe we kennis over symmetrieën kun\u0002nen gebruiken bij het oplossen van taken in reinforcement learning. In\r
het tweede deel stellen we methoden voor om over de structuur van\r
de omgeving van een agent en de structuur van individuele omgev\u0002ingstoestanden te leren.\r
Onze bijdragen zijn als volgt: In Deel 1 gebruiken we voorafgaande\r
kennis over symmetrieen om verbeteringen wat betreft data efficiëntie\r
in MDPs (Hoofdstuk 2) en multi-agent MDPs (Hoofdstuk 3) te verkrij\u0002gen.\r
• We stellen MDP Homomorphic Networks (Hoofdstuk 2) [173] voor.\r
MDP homomorphic networks zijn neurale netwerken die equivariant\r
zijn onder symmetrieen in de gezamenlijke ruimte van omgevingstoe\u0002standen en acties in een MDP. Door deze equivariantie ontdekken\r
we een verbetering in data efficiëntie vergeleken met netwerken die\r
niet equivariant zijn.\r
• We stellen Multi-Agent MDP Homomorphic Networks (Hoofdstuk 3) [172]\r
voor. Multi-Agent MDP Homomorphic Networks zijn een klasse neu\u0002rale netwerken die gedistribueerde uitvoering toestaan en daarvoor\r
alleen lokale informatie nodig hebben, doch alsnog ervaringen kan

134\r
delen tussen globale symmetrieen in de gezamenlijke ruimte van\r
omgevingstoestanden en acties in cooperatieve multi-agent syste\u0002men. We laten zien dat in symmetrische coordinatie problemen het\r
gebruik van globale equivariantie de data efficientie verbetert ten\r
opzichte van niet-equivariante gedistribueerde netwerken.\r
In Deel 2 beschouwen we het leren van de onderliggende grafen van\r
MDPs (Hoofdstuk 4) en het leren van de structuur in individuele\r
omgevingstoestanden (Hoofdstuk 5).\r
• We stellen PRAE (Hoofdstuk 4) [170] voor. PRAE maakt gebruik\r
van actie-equivariantie voor het leren van representaties in reinforce\u0002ment learning. Equivariantie onder acties betekent dat transities in\r
de invoerruimte gespiegeld worden door equivalente transities in\r
de latente ruimte, terwijl de functie naar de latente ruimte en de\r
transitiefunctie commutatief moeten zijn. We bewijzen dat de func\u0002tie naar de latente ruimte die we leren onder bepaalde aannames\r
een MDP homomorphisme is. Verder laten we zien dat de aan\u0002pak data-efficient is en snel leert, en goed generaliseert naar nieuwe\r
doelen en instanties met dezelfde omgevingsdynamiek.\r
• We stellen C-SWMs (Hoofdstuk 5) [94] voor. C-SWMs ontdekken\r
object-georienteerde representaties van omgevingstoestanden vanuit\r
pixels, daarbij gebruik makende van contrastive coding en graph neu\u0002ral network transitiefuncties. We laten vergeleken met modellen met\r
decoders, ongestructureerde transities of ongestructureerde repre\u0002sentaties verbetering zien in voorspellingen over meerdere stappen\r
en in generalisatie naar nieuwe configuraties van de omgeving."""

[metadata]
title = "vanderpol 2023 symmetry structure deep rl thesis"
authors = ["Unknown"]
year = 2023

[[sections]]
number = "0"
title = "Preamble"
text = """
Elise van der Pol\r
Symmetry and Structure\r
in\r
Deep Reinforcement Learning\r
Universiteit van Amsterdam

This book was typeset by the author using LATEX and the Tufte ebook\r
template. The cover was designed by the author in Inkscape, adapted\r
from a generated image.\r
Printing: www.proefschriftmaken.nl.\r
Copyright © 2023 Elise van der Pol\r
ISBN: 978-94-6469-413-0

Symmetry and Structure in Deep Reinforcement Learning\r
ACADEMISCH PROEFSCHRIFT\r
ter verkrijging van de graad van doctor\r
aan de Universiteit van Amsterdam\r
op gezag van de Rector Magnificus\r
prof. dr. ir. P.P.C.C. Verbeek\r
ten overstaan van een door het College voor Promoties ingestelde commissie,\r
in het openbaar te verdedigen in de Agnietenkapel\r
op woensdag 12 juli 2023, te 16.00 uur\r
door Elise Esmeralda van der Pol\r
geboren te Haarlem

Promotiecommissie\r
Promotor: prof. dr. M. Welling Universiteit van Amsterdam\r
Copromotores: dr. H.C. van Hoof Universiteit van Amsterdam\r
dr. F.A. Oliehoek Technische Universiteit Delft\r
Overige leden: prof. dr. M. de Rijke Universiteit van Amsterdam\r
prof. dr. E. Kanoulas Universiteit van Amsterdam\r
dr. ir. E.J. Bekkers Universiteit van Amsterdam\r
dr. D. Precup McGill University\r
prof. dr. J. Peters Technische Universität Darmstadt\r
Faculteit der Natuurwetenschappen, Wiskunde en Informatica

Voor Bibi\r
and we pour bag after bag\r
of leaves on the lawn,\r
waiting for them to leap\r
onto the bare branches.\r
— Matt Rasmussen

vii\r
Contents"""

[[sections]]
number = "1"
title = "Introduction 1"
text = ""

[[sections]]
number = "1.1"
title = "List of Publications 6"
text = "Part I Symmetry 9"

[[sections]]
number = "2"
title = "MDP Homomorphic Networks:"
text = "Group Symmetries in Reinforcement Learning 11"

[[sections]]
number = "2.1"
title = "Introduction 11"
text = ""

[[sections]]
number = "2.2"
title = "Background 14"
text = ""

[[sections]]
number = "2.3"
title = "Method 18"
text = ""

[[sections]]
number = "2.4"
title = "Experiments 21"
text = ""

[[sections]]
number = "2.5"
title = "Related Work 25"
text = ""

[[sections]]
number = "2.6"
title = "Conclusion 26"
text = ""

[[sections]]
number = "2.7"
title = "Broader Impact Statement 26"
text = "Appendices 29"

[[sections]]
number = "2"
title = "A The Symmetrizer 29"
text = ""

[[sections]]
number = "2"
title = "B Experimental Settings 32"
text = ""

[[sections]]
number = "2"
title = "C Breakout Experiments 39"
text = ""

[[sections]]
number = "2"
title = "D Cartpole-v1 Deeper Network Results 40"
text = ""

[[sections]]
number = "2"
title = "E Bellman Equations 40"
text = ""

[[sections]]
number = "3"
title = "Multi-Agent MDP Homomorphic Networks 41"
text = ""

[[sections]]
number = "3.1"
title = "Introduction 41"
text = "viii"

[[sections]]
number = "3.2"
title = "Related Work 43"
text = ""

[[sections]]
number = "3.3"
title = "Background 44"
text = ""

[[sections]]
number = "3.4"
title = "Distributing Symmetries over Multiple Agents 45"
text = ""

[[sections]]
number = "3.5"
title = "Experiments 52"
text = ""

[[sections]]
number = "3.6"
title = "E(3) Equivariance 55"
text = ""

[[sections]]
number = "3.7"
title = "Conclusion 56"
text = ""

[[sections]]
number = "3.8"
title = "Ethics Statement 57"
text = ""

[[sections]]
number = "3.9"
title = "Reproducibility Statement 57"
text = "Appendices 59"

[[sections]]
number = "3"
title = "A Message Passing Networks, Communication, and Distribution 59"
text = ""

[[sections]]
number = "3"
title = "B Equivariance of Proposed Message Passing Layers 60"
text = ""

[[sections]]
number = "3"
title = "C Discrete Rotations of Continuous Vectors 61"
text = ""

[[sections]]
number = "3"
title = "D Experimental Details 61"
text = ""

[[sections]]
number = "3"
title = "E Architectural details 62"
text = "Part II Structure 69"

[[sections]]
number = "4"
title = "Plannable Approximations to MDP Homomorphisms 71"
text = ""

[[sections]]
number = "4.1"
title = "Introduction 71"
text = ""

[[sections]]
number = "4.2"
title = "Background 73"
text = ""

[[sections]]
number = "4.3"
title = "Learning MDP Homomorphisms 75"
text = ""

[[sections]]
number = "4.4"
title = "Experiments 80"
text = ""

[[sections]]
number = "4.5"
title = "Related Work 88"
text = ""

[[sections]]
number = "4.6"
title = "Relation to Group Equivariance 89"
text = ""

[[sections]]
number = "4.7"
title = "Conclusion 89"
text = ""

[[sections]]
number = "5"
title = "Learning Factored Representations of Markov Decision Processes 91"
text = ""

[[sections]]
number = "5.1"
title = "Introduction 91"
text = ""

[[sections]]
number = "5.2"
title = "Background 93"
text = ""

[[sections]]
number = "5.3"
title = "Structured World Models 93"
text = ""

[[sections]]
number = "5.4"
title = "Transition Model 95"
text = ""

[[sections]]
number = "5.5"
title = "Related Work 96"
text = ""

[[sections]]
number = "5.6"
title = "Experiments 97"
text = ""

[[sections]]
number = "5.7"
title = "Conclusions 101"
text = "ix"

[[sections]]
number = "6"
title = "Conclusion 103"
text = ""

[[sections]]
number = "7"
title = "Acknowledgments 109"
text = ""

[[sections]]
number = "8"
title = "Summary 131"
text = ""

[[sections]]
number = "9"
title = "Samenvatting - Dutch Summary 133"
text = "1"

[[sections]]
number = "1"
title = "Introduction"
text = ""

[[sections]]
number = "2"
title = "Symmetry and structure are everywhere in the world. When we walk,"
text = """
the movement of our right leg mirrors that of our left leg. When\r
molecules are rotated, their molecular properties are unchanged. When\r
we navigate to a destination, we take the connectivity of different road\r
segments into account. When we talk, we can string words together to\r
form completely novel sentences. In every day life, we use information\r
about the symmetry and structure of our tasks to guide our decision\r
making.\r
In Artificial Intelligence, symmetries and structure are also ubiquitous.\r
Consider a robot that mirrors its left and right leg movements during\r
locomotion, automated chip design, a drone swarm tracking wildlife\r
movement, a bot playing Atari Pong where the top and bottom part\r
of the screen are reflections of each other, molecular design, a com\u0002puter player considering rotated board states in the game of Go, and\r
autonomous vehicles switching from the right side of the road in the\r
Netherlands to the left side of the road in the UK. These are all exam\u0002ples of tasks within AI that exhibit some form of symmetry or struc\u0002ture. Leveraging knowledge of inherent symmetry and structure is an\r
important step towards building systems that scale.\r
Reinforcement learning is a fundamental field of study in Artificial In\u0002telligence that encourages artificial agents to learn from positive and\r
negative feedback signals, which we call rewards. By trial-and-error,\r
the agent can learn to associate situations, actions, and feedback in\r
order to improve its decisions. For example, we can give a robot a\r
positive reward for walking fast and a negative reward for falling over.\r
Similarly, we can give a computer player a positive reward for winning\r
a game, and a negative reward for losing a game, or give a positive re\u0002ward to an agent that proposes a particularly efficient chip design.\r
Using concepts from the field of reinforcement learning, we can for\u0002malize the examples above in order to propose approaches that lead to\r
good decision making from an agent. In deep reinforcement learning, an\r
agent uses neural networks to decide on which action to take, where\r
the neural networks are adapted to the task using the received reward\r
signals. However, even tasks that require intelligence far below human\r
capabilities can present issues to artificial decision makers. Consider\r
any vision-based control system acting in the real world. The agent re\u0002ceives observations as camera input, and has to learn the best action to\r
take. The number of possible observations is prohibitively large, and it\r
is unlikely that the agent will encounter two states that are exactly the\r
same. As such, we would like to the agent to be able to re-use expe\u0002rience from earlier states to take good decisions in unseen states with\r
similar characteristics. For example, when deciding how to move its

3\r
left leg, the agent should mirror the movements it learned for moving\r
its right leg.\r
The examples above are a few of the cases where symmetry and struc\u0002ture appear in reinforcement learning problems. These can be formal\u0002ized by considering when taking an action in a state is equivalent to\r
taking another action in another state. In this thesis, we will study how\r
we can use symmetry and structure in reinforcement learning when it\r
is known, and how we can extract it if it is not.\r
An agent should not learn what is already known. Whether knowl\u0002edge is provided by a system designer as prior knowledge or obtained\r
by the agent itself through generalization should depend on the con\u0002text of the problem. By properly re-using knowledge, we can reduce\r
the number of times the agent needs to interact with the world, an\r
essential part of scaling to real world settings. In this thesis, we will\r
particularly look at symmetry and structure in reinforcement learning.\r
As such, our main research question is:\r
Main Research Question: How can we incorporate and extract\r
symmetry and structure in reinforcement learning?\r
We first consider the case where there is an obvious symmetry in the\r
problem we are trying to solve. Many problems exhibit symmetry,\r
since symmetry is a natural part of the physical world. To illustrate,\r
consider the classic pole balancing task. In pole balancing, the agent\r
controls a cart that can move left or right. Her goal is to move the cart\r
left and right to ensure that a pole standing upright on the cart does\r
not fall down. It does not matter if the pole is currently leaning to the\r
left, and the car is moving right, or the pole is leaning right and the\r
cart is moving left. Both cases are mirrored versions of the same un\u0002derlying situation. An agent moving east to reach a goal on the east,\r
or north to reach a goal in the north, are also two very similar situa\u0002tions. These problems and those mentioned above have something in\r
common: they exhibit equivalences between different pairs of states\r
and actions. The beauty of state-action equivalence is that it allows us\r
to consider similarity between taking one action in one state and tak\u0002ing another action in another state. This is what allows us to express\r
symmetry in reinforcement learning problems. In Chapter 2 we con\u0002sider the problem of constraining the class of neural network policies\r
to only those that are symmetric under certain transformations, and\r
answer the first research question:"""

[[sections]]
number = "4"
title = "Research Question 1: How can we leverage knowledge of symmetries"
text = """
in deep reinforcement learning?\r
Our main contribution in Chapter 2 is proposing MDP Homomorphic\r
Networks, a class of neural networks that incorporate reinforcement\r
learning symmetries into neural networks. This approach bridges\r
deep reinforcement learning and equivariant networks, and shows a\r
substantial improvement in data efficiency compared to unstructured\r
baselines. Additionally, we propose a new method for constructing\r
equivariant neural network weights.\r
After considering the case of symmetric reinforcement learning tasks,\r
we investigate the use of similar methods in a more complex setting:\r
that of distributed cooperative multi-agent systems. In such settings,\r
the task at hand must be solved by agents taking local actions and\r
communicating locally with each other. In Chapter 3 we answer the\r
second research question:\r
Research Question 2. How can we leverage knowledge of global\r
symmetries in distributed cooperative multi-agent systems?\r
A straightforward approach to this problem would be a naive applica\u0002tion of single agent approaches to symmetry in reinforcement learning.\r
However, such a method would prohibit us from using distributed exe\u0002cution methods. Instead, we propose an equivariant distributed policy\r
network that allows the policy to be distributed at execution time, re\u0002quiring only local communication and computation to ensure global\r
symmetries. This approach results in improved data efficiency in sym\u0002metric cooperative multi-agent problems.\r
In the second part of this thesis, we consider that there is usually some\r
underlying structure in the unstructured information we receive. For\r
example, objects of the same type tend to behave similarly when force\r
is applied to them. Similarly, there are dynamics underlying a sim\u0002ple pole balancing system. An agent may observe only feature vec\u0002tors, and not the equations governing the system. Finally, the space\r
of possible images of 3 × 48 × 48 pixels is significantly larger than the\r
number of states in a simple grid world. However, an agent does not\r
a priori know the number of possible actual states. It only knows that\r
it receives images of a certain size. In the second part of this thesis,\r
we consider how to extract structure from interaction data. First, in\r
Chapter 4, we consider the problem of learning representations of de\u0002cision making problems that are plannable. By plannable, we mean

5\r
that if we run standard planning algorithms on the graph of learned\r
representations, we get good decision making strategies for the orig\u0002inal problem. In practice, this means that we want the dynamics of\r
the transition function in the original problem to be mirrored by the\r
dynamics of the latent transition function. Our 3rd research question\r
is therefore:\r
Research Question 3. How can we learn representations of the world\r
that capture the structure of the environment?\r
In Chapter 4, we take cues from the equivariance literature [34, 193,\r
187] and contrastive learning [135, 94] to learn the abstract graph un\u0002derlying a decision making problem. Noting that the effect of an action\r
in the decision making problem should be matched by the effect on the\r
abstract graph, we learn representations of the original problem and\r
show that they are indeed plannable for a variety of problems, includ\u0002ing continuous state spaces, and generalizing to unseen objects and\r
goals. This action-equivariant planning approach results in improved\r
data efficiency compared to model-free baselines and reconstruction\r
baselines.\r
In Chapter 5, we consider a further structure in the problem: if we have\r
a set of objects that can be acted upon individually, each state is itself\r
structured. If we can recover the individual objects and a factored\r
transition function from pixel observations, this can improve predic\u0002tion and generalization. As such, our 4th research question is:\r
Research Question 4. How can we learn representations of the world\r
that capture the structure in individual states?\r
In Chapter 5 we show that it is possible to find object-oriented repre\u0002sentations from pixels based on factored actions and a fixed dataset of\r
interactions. We propose an approach that uses contrastive learning\r
on environment interaction samples, resulting in a structured repre\u0002sentation that leverages a graph neural network transition function.\r
The approach we propose improves prediction performance in latent\r
space compared to unstructured and reconstruction baselines.\r
In this thesis, we explore symmetry and structure in deep reinforce\u0002ment learning. We leverage prior knowledge of symmetries in single\r
agent and multi-agent systems in Chapters 2- 3, and extract structure\r
from interactions with the world in Chapters 4-5. We will conclude in"""

[[sections]]
number = "6"
title = "Chapter 6 and provide suggestions for future work."
text = ""

[[sections]]
number = "1.1"
title = "List of Publications"
text = """
The following publications form the basis of this thesis:\r
• Elise van der Pol, Thomas Kipf, Frans A. Oliehoek, Max Welling\r
(2020). "Plannable Approximations to MDP Homomorphisms: Equiv\u0002ariance under Actions." In: International Conference on Autonomous\r
Agents and Multi-Agent Systems (AAMAS) [170]. Chapter 4.\r
• Elise van der Pol, Daniel E. Worrall, Herke van Hoof, Frans A.\r
Oliehoek, Max Welling (2020). "MDP Homomorphic Networks:\r
Group Symmetries in Reinforcement Learning." In: Advances in\r
Neural Information Processing Systems (NeurIPS) [173]. Chapter 2.\r
• Elise van der Pol, Herke van Hoof, Frans A. Oliehoek, Max Welling\r
(2021). "Multi-Agent MDP Homomorphic Networks." In: Interna\u0002tional Conference on Learning Representations (ICLR) [172]. Chap\u0002ter 3.\r
• Thomas Kipf, Elise van der Pol, Max Welling (2019). "Contrastive\r
Learning of Structured World Models." In: International Conference\r
on Learning Representations (ICLR) [94]. Chapter 5.\r
• Johannes Brandstetter, Rob Hesselink, Elise van der Pol, Erik J.\r
Bekkers, Max Welling (2021). "Geometric and Physical Quantities\r
improve E(3) Equivariant Message Passing." In: International Con\u0002ference on Learning Representations (ICLR) [23]. Chapter 3.\r
I have contributed in all aspects to all first author publications listed.\r
Max Welling, Frans Oliehoek, and Herke van Hoof provided supervi\u0002sion, guidance, insight, and technical advice. In "MDP Homomorphic\r
Networks: Group Symmetries in Reinforcement Learning." Daniel Wor\u0002rall proposed the Symmetrizer, which was implemented by us jointly.\r
In "Contrastive Learning of Structured World Models." Thomas Kipf\r
contributed in all aspects. I provided reinforcement learning insights,\r
proposed the use of action factorization and the evaluation method,\r
and ran baseline experiments. Figures and tables reproduced with per\u0002mission. In "Geometric and Physical Quantities improve E(3) Equivari\u0002ant Message Passing.", Johannes Brandstetter, Rob Hesselink, and Erik\r
Bekkers contributed in all aspects. I provided the original architectural

7\r
idea and took an advisory role. Figures reproduced with permission.\r
I have further contributed to:\r
• Tejaswi Kasarla, Gertjan J. Burghouts, Max van Spengler, Elise van\r
der Pol, Rita Cucchiara, Pascal Mettes (2022). "Maximum Class Sep\u0002aration as Inductive Bias in One Matrix." Accepted to: Neural Infor\u0002mation Processing Systems (NeurIPS) [87].\r
• Darius Muglich, Christian Schroeder de Witt, Elise van der Pol,\r
Shimon Whiteson, Jakob Foerster (2022). "Equivariant Networks for\r
Zero-Shot Coordination." Accepted to: Neural Information Process\u0002ing Systems (NeurIPS).\r
• Elise van der Pol, Ian Gemp, Yoram Bachrach, Richard Everett\r
(2022). "Stochastic Parallelizable Eigengap Dilation for Large Graph\r
Clustering". In: ICML 2022 Workshop on Topology, Algebra, and\r
Geometry in Machine Learning [169].\r
• Pascal Mettes, Elise van der Pol, Cees G.M. Snoek (2019). "Hyper\u0002spherical Prototype Networks." In: Advances in Neural Information\r
Processing Systems (NeurIPS) [120].\r
• Ondrej Biza, Elise van der Pol, Thomas Kipf (2021). "The Impact\r
of Negative Sampling on Contrastive Structured World Models." In:\r
ICML Workshop on Self-Supervised Learning for Reasoning and\r
Perception [17].\r
• Laurens Weitkamp, Elise van der Pol, Zeynep Akata (2018). "Visual\r
Rationalizations in Deep Reinforcement Learning for Atari Games."\r
In: Benelux Conference on Artificial intelligence (BNAIC) [190].\r
• Frans A. Oliehoek, Rahul Savani, Jose Gallego-Posada, Elise van\r
der Pol, Roderich Groß (2018). "Beyond Local Nash Equilibria for\r
Adversarial Networks." In: Annual Machine Learning Conference\r
of Belgium and the Netherlands (Benelearn) [134]."""

[[sections]]
number = "9"
title = "Part I"
text = """
Symmetry

11"""

[[sections]]
number = "2"
title = "MDP Homomorphic Networks:"
text = """
Group Symmetries in\r
Reinforcement Learning"""

[[sections]]
number = "2.1"
title = "Introduction"
text = """
This part of the dissertation leverages knowledge of symmetries in sin\u0002gle and multi-agent reinforcement learning problems. In this Chapter,\r
we propose MDP homomorphic networks, which enforce symmetries\r
in reinforcement learning problems. In the following chapter we take\r
this approach beyond single-agent symmetries.\r
This chapter introduces MDP homomorphic networks for deep rein\u0002forcement learning. MDP homomorphic networks are neural networks\r
that are equivariant under symmetries in the joint state-action space of\r
an MDP. Current approaches to deep reinforcement learning do not\r
usually exploit knowledge about such structure. By building this prior\r
knowledge into policy and value networks using an equivariance con\u0002straint, we can reduce the size of the solution space. We specifically fo\u0002cus on group-structured symmetries (invertible transformations). Ad\u0002ditionally, we introduce an easy method for constructing equivariant\r
network layers numerically, so the system designer need not solve the\r
equivariance constraints by hand, as is typically done. We construct\r
MDP homomorphic MLPs and CNNs that are equivariant under ei-

12\r
ther a group of reflections or rotations. We show that such networks\r
converge faster than unstructured baselines on CartPole, a grid world\r
and Pong.\r
This chapter considers learning decision-making systems that exploit\r
symmetries in the structure of the world. Deep reinforcement learning\r
(DRL) is concerned with learning neural function approximators for\r
decision making strategies. While DRL algorithms have been shown\r
to solve complex, high-dimensional problems [156, 153, 125, 124], they\r
are often used in problems with large state-action spaces, and thus\r
require many samples before convergence. Many tasks exhibit sym\u0002metries, easily recognized by a designer of a reinforcement learning\r
system. Consider the classic control task of balancing a pole on a cart.\r
Balancing a pole that falls to the right requires an equivalent, but mir\u0002rored, strategy to one that falls to the left. See Figure 2.1. In this\r
chapter, we exploit knowledge of such symmetries in the state-action\r
space of Markov decision processes (MDPs) to reduce the size of the\r
solution space.\r
We use the notion of MDP homomorphisms [143, 141] to formalize these\r
symmetries. Intuitively, an MDP homomorphism is a map between\r
MDPs, preserving the essential structure of the original MDP, while\r
removing redundancies in the problem description, i.e., equivalent\r
state-action pairs. The removal of these redundancies results in a\r
smaller state-action space, upon which we may more easily build a\r
policy. While earlier work has been concerned with discovering an\r
MDP homomorphism for a given MDP [143, 141, 127, 142, 16, 170],\r
we are instead concerned with how to construct deep policies, satisfy\u0002ing the MDP homomorphism. We call these models MDP homomorphic\r
networks.\r
MDP homomorphic networks use experience from one state-action\r
pair to improve the policy for all ‘equivalent’ pairs. See Section 2.2\r
for a definition. They do this by tying the weights for two states if\r
they are equivalent under a transformation chosen by the designer,\r
such as s and L[s] in Figure 2.1.\r
Such weight-tying follows a similar principle to the use of convo\u0002lutional networks [109], which are equivariant to translations of the\r
input [34]. In particular, when equivalent state-action pairs can be\r
related by an invertible transformation, which we refer to as group\u0002structured, we show that the policy network belongs to the class of\r
group-equivariant neural networks [34, 193]. Equivariant neural networks\r
are a class of neural network, which have built-in symmetries [34,"""

[[sections]]
number = "13"
title = "Figure 2.1: Example state-action"
text = """
space symmetry, where L is a\r
horizontal reflection. The pairs\r
(s, ←) and (L[s], →) (and by ex\u0002tension (s, →) and (L[s], ←)) are\r
symmetric under a horizontal\r
flip. Constraining the set of poli\u0002cies to those where π(s, ←) =\r
π(L[s], →) reduces the size of\r
the solution space.\r
35, 193, 188, 186]. They are a generalization of convolutional neu\u0002ral networks—which exhibit translation symmetry—to transformation\r
groups (group-structured equivariance) and transformation semigroups\r
[194] (semigroup-structured equivariance). They have been shown to\r
reduce sample complexity for classification tasks [193, 191] and also to\r
be universal approximators of symmetric functions1[198]. We borrow"""

[[sections]]
number = "1"
title = "Specifically group equivariant net\u0002works are universal approximators to"
text = """
functions symmetric under linear repre\u0002sentations of compact groups.\r
from the literature on group equivariant networks to design policies\r
that tie weights for state-action pairs given their equivalence classes,\r
with the goal of reducing the number of samples needed to find good\r
policies. Furthermore, we can use the MDP homomorphism property\r
to design not just policy networks, but also value networks and even\r
environment models. MDP homomorphic networks are agnostic to the\r
type of model-free DRL algorithm, as long as an appropriate transfor\u0002mation on the output is given. In this chapter we focus on equivariant\r
policy and invariant value networks. See Figure 2.1 for an example\r
policy.\r
An additional contribution of this chapter is a novel numerical way\r
of finding equivariant layers for arbitrary transformation groups. The\r
design of equivariant networks imposes a system of linear constraint\r
equations on the linear/convolutional layers [35, 34, 193, 188]. Solving\r
these equations has typically been done analytically by hand, which\r
is a time-consuming and intricate process, barring rapid prototyp\u0002ing. Rather than requiring analytical derivation, our method only re\u0002quires that the system designer specify input and output transforma\u0002tion groups of the form {state transformation, policy transformation}."""

[[sections]]
number = "14"
title = "We provide Pytorch [138] implementations of our equivariant network"
text = """
layers, and implementations of the transformations used in this chap\u0002ter. We also experimentally demonstrate that exploiting equivalences\r
in MDPs leads to faster learning of policies for DRL.\r
Our contributions are two-fold:\r
• We draw a connection between MDP homomorphisms and group\r
equivariant networks, proposing MDP homomorphic networks to\r
exploit symmetries in decision-making problems;\r
• We introduce a numerical algorithm for the automated construction\r
of equivariant layers."""

[[sections]]
number = "2.2"
title = "Background"
text = """
Here we outline the basics of the theory behind MDP homomorphisms\r
and equivariance. We begin with a brief outline of the concepts of\r
equivalence, invariance, and equivariance, followed by a review of the\r
Markov decision process (MDP). We then review the MDP homomor\u0002phism, which builds a map between ‘equivalent’ MDPs.\r
Equivalence, Invariance, and Equivariance\r
Equivalence If a function f : X → Y maps two inputs x, x\r
0 ∈ X\r
to the same value, that is f(x) = f(x\r
0\r
), then we say that x and x\r
0\r
are f-equivalent. For instance, two states s,s\r
0\r
leading to the same\r
optimal value V\r
∗\r
(s) = V\r
∗\r
(s\r
0\r
) would be V\r
∗\r
-equivalent or optimal value\r
equivalent [141]. An example of two optimal value equivalent states\r
would be states s and L[s] in the CartPole example of Figure 2.1. The\r
set of all points f-equivalent to x is called the equivalence class of x.\r
Groups A group (G, ·) is a set G, together with a binary operator ·,\r
which is closed, i.e. ∀g1, g2 ∈ G, g1 · g2 ∈ G. The group (G, ·) obeys\r
the group axioms:\r
• Associativity: For all g1, g2, g3 ∈ G, (g1 · g2) · g3 = g1 · (g2 · g3);\r
• Identity: There is an element e ∈ G such that for all g1 ∈ G, e · g1 =\r
g1 · e = g1;\r
• Invertibility: For all g1 ∈ G, there exists an element g\r
−1\r
1 ∈ G such

15\r
that g1 · g\r
−1\r
1 = g\r
−1\r
1\r
· g1 = e.\r
Invariance and Symmetries Typically there exist very intuitive rela\u0002tionships between the points in an equivalence class. In the Cart\u0002Pole example of Figure 2.1 this relationship is a horizontal flip about\r
the vertical axis. This is formalized with the transformation operator\r
Lg : X → X , where g ∈ G and G is a mathematical group. If Lg\r
satisfies\r
f(x) = f(Lg[x]), for all g ∈ G, x ∈ X , (2.1)\r
then we say that f is invariant or symmetric to Lg and that {Lg}g∈G is a\r
set of symmetries of f . We can see that for the invariance equation to be\r
satisfied, it must be that Lg can only map x to points in its equivalence\r
class. Note that in abstract algebra for Lg to be a true transformation\r
operator, G must contain an identity operation; that is Lg[x] = x for\r
some g and all x. An interesting property of transformation operators\r
which leave f invariant, is that they can be composed and still leave\r
f invariant, so Lg ◦ Lhis also a symmetry of f for all g, h ∈ G. In\r
abstract algebra, this property is known as a semigroup property. If\r
Lg is always invertible, this is called a group property. In this work, we\r
experiment with group-structured transformation operators. For more\r
information, see [42]. Another helpful concept is that of orbits. If f is\r
invariant to Lg, then it is invariant along the orbits of G. The orbit\r
Ox of point x is the set of points reachable from x via transformation\r
operator Lg:\r
Ox , {Lg[x] ∈ X |g ∈ G}. (2.2)\r
Equivariance A related notion to invariance is equivariance. Given a\r
transformation operator Lg : X → X and a mapping f : X → Y, we\r
say that f is equivariant [34, 193] to the transformation if there exists\r
a second transformation operator Kg : Y → Y in the output space of f\r
such that\r
Kg[ f(x)] = f(Lg[x]), for all g ∈ G, x ∈ X . (2.3)\r
The operators Lg and Kg can be seen to describe the same transforma\u0002tion, but in different spaces. In fact, an equivariant map can be seen\r
to map orbits to orbits. We also see that invariance is a special case\r
of equivariance, if we set Kg to the identity operator for all g. Given\r
Lg and Kg, we can solve for the collection of equivariant functions f\r
satisfying the equivariance constraint. Moreover, for linear transfor\u0002mation operators and linear f a rich theory already exists in which\r
f is referred to as an intertwiner [35]. In the equivariant deep learn\u0002ing literature, neural networks are built from interleaving intertwiners

16\r
and equivariant nonlinearities. As far as we are aware, most of these\r
methods are hand-designed per pair of transformation operators, with\r
the exception of [40]. In this chapter, we introduce a computational\r
method to solve for intertwiners given a pair of transformation opera\u0002tors.\r
Markov Decision Processes\r
A Markov decision process (MDP) is a tuple (S, A, R, T, γ), with state\r
space S, action space A, immediate reward function R : S × A → R, tran\u0002sition function T : S × A × S → R≥0, and discount factor γ ∈ [0, 1]. The\r
goal of solving an MDP is to find a policy π ∈ Π, π : S × A → R≥0\r
(written π(a|s)), where π normalizes to unity over the action space,\r
that maximizes the expected return Rt = Eπ[∑\r
T\r
k=0\r
γ\r
k\r
rt+k+1]. The ex\u0002pected return from a state s under a policy π is given by the value\r
function Vπ. A related object is the Q-value Qπ, the expected return\r
from a state s after taking action a under π. V\r
π and Qπ are governed\r
by the well-known Bellman equations [15] (see Supplementary). In an\r
MDP, optimal policies π\r
∗ attain an optimal value V∗ and correspond\u0002ing Q-value given by V\r
∗\r
(s) = max\r
π∈Π\r
V\r
π(s) and Q∗\r
(s) = max\r
π∈Π\r
Qπ(s).\r
Figure 2.2: Example of a re\u0002duction in an MDP’s state-action\r
space under an MDP homomor\u0002phism h. Here ‘equivalence’\r
is represented by a reflection\r
of the dynamics in the vertical\r
axis. This equivalence class is\r
encoded by h by mapping all\r
equivalent state-action pairs to\r
the same abstract state-actions.\r
MDP with Symmetries Symmetries can appear in MDPs. For instance,\r
in Figure 2.2 CartPole has a reflection symmetry about the vertical axis.\r
Here we define an MDP with symmetries. In an MDP with symmetries\r
there is a set of transformations on the state-action space, which leaves\r
the reward function and transition operator invariant. We define a\r
state transformation and a state-dependent action transformation as\r
Lg : S → S and K\r
s\r
g\r
: A → A respectively. Invariance of the reward\r
function and transition function is then characterized as\r
R(s, a) = R(Lg[s], K\r
s\r
g\r
[a]) for all g ∈ G,s ∈ S, a ∈ A (2.4)\r
T(s\r
0\r
|s, a) = T(Lg[s\r
0\r
]|Lg[s], K\r
s\r
g\r
[a]) for all g ∈ G,s ∈ S, a ∈ A. (2.5)\r
Written like this, we see that in an MDP with symmetries the reward\r
function and transition operator are invariant along orbits defined by\r
the transformations (Lg, K\r
s\r
g\r
).\r
MDP Homomorphisms MDPs with symmetries are closely related to\r
MDP homomorphisms, as we explain below. First we define the lat-

17\r
ter. An MDP homomorphism h [143, 141] is a mapping from one MDP\r
M = (S, A, R, T, γ) to another M¯ = (S¯, A¯, R¯, T¯, γ) defined by a sur\u0002jective map from the state-action space S × A to an abstract state-action\r
space S ×¯ A¯. In particular, h consists of a tuple of surjective maps\r
(σ, {αs|s ∈ S}), where we have the state map σ : S → S¯ and the state\u0002dependent action map αs\r
: A → A¯. These maps are built to satisfy the\r
following conditions\r
R¯(σ(s), αs(a)) , R(s, a) for all s ∈ S, a ∈ A,\r
(2.6)\r
T¯(σ(s\r
0\r
)|σ(s), αs(a)) , ∑\r
s\r
00∈σ−1(s0)\r
T(s\r
00|s, a) for all s,s0 ∈ S, a ∈ A.\r
(2.7)\r
An exact MDP homomorphism provides a model equivalence abstrac\u0002tion [113]. Given an MDP homomorphism h, two state-action pairs\r
(s, a) and (s\r
0\r
, a\r
0\r
) are called h-equivalent if σ(s) = σ(s\r
0\r
) and αs(a) =\r
αs\r
0(a\r
0\r
). Symmetries and MDP homomorphisms are connected in a\r
natural way: If an MDP has symmetries Lg and Kg, the above equa\u0002tions 2.4 and 2.5 hold. This means that we can define a corresponding\r
MDP homomorphism, which we define next.\r
Group-structured MDP Homomorphisms Specifically, for an MDP with\r
symmetries, we can define an abstract state-action space, by map\u0002ping (s, a) pairs to (a representative point of) their equivalence class\r
(σ(s), αs(a)). That is, state-action pairs and their transformed version\r
are mapped to the same abstract state in the reduced MDP:\r
(σ(s), αs(a)) =\r
\u0010\r
σ(Lg[s]), αLg[s](K\r
s\r
g\r
[a])\u0011∀g ∈ G,s ∈ S, a ∈ A (2.8)\r
In this case, we call the resulting MDP homomorphism group struc\u0002tured. In other words, all the state-action pairs in an orbit defined by\r
a group transformation are mapped to the same abstract state by a\r
group-structured MDP homomorphism.\r
Optimal Value Equivalence and Lifted Policies h-equivalent state\u0002action pairs share the same optimal Q-value and optimal value func\u0002tion [141]. There exists an abstract optimal Q-value Q¯ ∗ and abstract\r
optimal value function V¯ ∗, such that Q∗(s, a) = Q¯ ∗(σ(s), αs(a)) and\r
V\r
∗\r
(s) = V¯ ∗(σ(s)). This is known as optimal value equivalence [141].\r
Policies can thus be optimized in the simpler abstract MDP. The opti\u0002mal abstract policy π¯(a¯|σ(s)) can then be pulled back to the original\r
MDP using a procedure called lifting 2. The lifted policy is given in"""

[[sections]]
number = "2"
title = "Note that we use the terminology lifting"
text = """
to stay consistent with [141].\r
Equation 2.9. A lifted optimal abstract policy is also an optimal policy

18\r
in the original MDP [141]. Note that while other lifted policies ex\u0002ist, we follow [141, 143] and choose the lifting that divides probability\r
mass uniformly over the preimage:\r
π\r
↑\r
(a|s) ,\r
π¯(a¯|σ(s))\r
|{a ∈ α\r
−1\r
s (a¯)}|\r
, for any s ∈ S and a ∈ α\r
−1\r
s\r
(a¯). (2.9)"""

[[sections]]
number = "2.3"
title = "Method"
text = """
The focus of the next section is on the design of MDP homomorphic\r
networks—policy networks and value networks obeying the MDP ho\u0002momorphism. In the first section of the method, we show that any pol\u0002icy network satisfying the MDP homomorphism property must be an\r
equivariant neural network. In the second part of the method, we in\u0002troduce a novel numerical technique for constructing group-equivariant\r
networks, based on the transformation operators defining the equiva\u0002lence state-action pairs under the MDP homomorphism.\r
Lifted Policies Are Invariant\r
Lifted policies in symmetric MDPs with group-structured symmetries\r
are invariant under the group of symmetries. Consider the following:\r
Take an MDP with symmetries defined by transformation operators\r
(Lg, K\r
s\r
g\r
) for g ∈ G. Now, if we take s\r
0 = Lg[s] and a0 = Ks\r
g\r
[a] for\r
any g ∈ G, (s\r
0\r
, a\r
0\r
) and (s, a) are h-equivalent under the corresponding\r
MDP homomorphism h = (σ, {αs|s ∈ S}). So\r
π\r
↑\r
(a|s) = π¯(αs(a)|σ(s))\r
|{a ∈ α\r
−1\r
s (a¯)}|\r
=\r
π¯(αs\r
0(a\r
0\r
)|σ(s\r
0\r
))\r
|{a\r
0 ∈ α\r
−1\r
s\r
0 (a¯)}|\r
= π\r
↑\r
(a\r
0\r
|s\r
0\r
), (2.10)\r
for all s ∈ S, a ∈ A and g ∈ G. In the first equality we have used\r
the definition of the lifted policy. In the second equality, we have used\r
the definition of h-equivalent state-action pairs, where σ(s) = σ(Lg(s))\r
and αs(a) = αs\r
0(a\r
0\r
). In the third equality, we have reused the definition\r
of the lifted policy. Thus we see that, written in this way, the lifted pol\u0002icy is invariant under state-action transformations (Lg, K\r
s\r
g\r
). This equa\u0002tion is very general and applies for all group-structured state-action\r
transformations. For a finite action space, this statement of invariance\r
can be re-expressed as a statement of equivariance, by considering the\r
vectorized policy.\r
Invariant Policies On Finite Action Spaces Are Equivariant Vector\u0002ized Policies For convenience we introduce a vector of probabilities

19\r
for each of the discrete actions under the policy\r
π(s) ,\r
h\r
π(a1|s), π(a2|s), ..., π(aN|s)\r
i>\r
, (2.11)\r
where a1, ..., aN are the N possible discrete actions in action space A.\r
The action transformation K\r
s\r
g maps actions to actions invertibly. Thus\r
applying an action transformation to the vectorized policy permutes\r
the elements. We write the corresponding permutation matrix as Kg.\r
Note that\r
K\r
−1\r
g π(s) ,\r
h\r
π(K\r
s\r
g\r
[a1]|s), π(K\r
s\r
g\r
[a2]|s), ..., π(K\r
s\r
g\r
[aN]|s)\r
i>\r
, (2.12)\r
where writing the inverse K−1\r
g\r
instead of Kg is required to maintain\r
the property KgKh = Kgh. The invariance of the lifted policy can then\r
be written as π↑(s) = K−1\r
g π↑\r
(Lg[s]), which can be rearranged to the\r
equivariance equation\r
Kgπ\r
↑\r
(s) = π\r
↑\r
(Lg[s]) for all g ∈ G,s ∈ S, a ∈ A. (2.13)\r
This equation shows that the lifted policy must satisfy an equivariance\r
constraint. In deep learning, this has already been well-explored in the\r
context of supervised learning [34, 35, 193, 194, 188]. Next, we present\r
a novel way to construct such networks.\r
Building MDP Homomorphic Networks\r
Our goal is to build neural networks that follow Eq. 2.13; that is, we\r
wish to find neural networks that are equivariant under a set of state\r
and policy transformations. Equivariant networks are common in su\u0002pervised learning [34, 35, 193, 194, 188, 186]. For instance, in semantic\r
segmentation shifts and rotations of the input image result in shifts\r
and rotations in the segmentation. A neural network consisting of\r
only equivariant layers and non-linearities is equivariant as a whole,\r
too3[34]. Thus, once we know how to build a single equivariant layer,\r
3\r
we can simply stack such layers together. Note that this is true re- See Appendix 2.B for more details.\r
gardless of the representation of the group, i.e. this works for spatial\r
transformations of the input, feature map permutations in interme\u0002diate layers, and policy transformations in the output layer. For the\r
experiments presented in this chapter, we use the same group repre\u0002sentations for the intermediate layers as for the output, i.e. permuta\u0002tions. For finite groups, such as cyclic groups or permutations, point\u0002wise nonlinearities preserve equivariance [34], allowing the use of e.g.\r
rectified linear units without losing equivariance.\r
In the past, learnable equivariant layers were designed by hand for\r
each transformation group individually [34, 35, 193, 194, 191, 188, 186]."""

[[sections]]
number = "20"
title = "This is time-consuming and laborious. Here we present a novel way"
text = """
to build learnable linear layers that satisfy equivariance automatically.\r
Equivariant Layers We begin with a single linear layer z\r
0 = Wz +\r
b, where W ∈ RDout×Din and b ∈ RDout is a bias. To simplify the\r
math, we merge the bias into the weights so W 7→ [W, b] and z 7→\r
[z, 1]\r
>. We denote the space of the augmented weights as Wtotal. For\r
a given pair of linear group transformation operators in matrix form\r
(Lg, Kg), where Lg is the input transformation and Kg is the output\r
transformation, we then have to solve the equation\r
KgWz = WLgz, for all g ∈ G, z ∈ R\r
Din+1\r
. (2.14)\r
Since this equation is true for all z we can in fact drop z entirely. Our\r
task now is to find all weights W which satisfy Equation 2.14. We label\r
this space of equivariant weights as W, defined as\r
W , {W ∈ Wtotal | KgW = WLg, for all g ∈ G}, (2.15)\r
again noting that we have dropped z. To find the space W notice that\r
for each g ∈ G the constraint KgW = WLg is in fact linear in W. Thus,\r
to find W we need to solve a set of linear equations in W. For this\r
we introduce a construction, which we call a symmetrizer S(W). The\r
symmetrizer is\r
S(W) ,\r
1\r
|G| ∑\r
g∈G\r
K\r
−1\r
g WLg. (2.16)\r
S has three important properties, of which proofs are provided in Ap\u0002pendix A. First, S(W) is symmetric (S(W) ∈ W). Second, S fixes any\r
symmetric W: (W ∈ W =⇒ S(W) = W). Third, S is idempotent,\r
S(S(W)) = S(W). These properties show that S projects arbitrary\r
W ∈ Wtotal to the equivariant subspace W.\r
Since W is the solution set for a set of simultaneous linear equa\u0002tions, W is a linear subspace of the space of all possible weights\r
Algorithm 1: Equivariant layer construction\r
1: Sample N weight matrices W1,W2, ...,WN ∼ N (W; 0,I) for N ≥\r
dim(Wtotal)\r
2: Symmetrize samples: W¯\r
i = S(Wi) for i = 1, ..., N\r
3: Vectorize samples and stack as W¯ = [vec(W¯\r
1), vec(W¯\r
2), ...]\r
4: Apply SVD: W¯ = UΣV>\r
5: Keep first r = rank(W¯ ) right-singular vectors (columns of V) and\r
unvectorize to shape of Wi

21\r
Wtotal. Thus each W ∈ W can be parametrized as a linear combina\u0002tion of basis weights {Vi}\r
r\r
i=1\r
, where r is the rank of the subspace and\r
span({Vi}\r
r\r
i=1\r
) = W. To find as basis for W, we take a Gram-Schmidt\r
orthogonalization approach. We first sample weights in the total space\r
Wtotal and then project them into the equivariant subspace with the\r
symmetrizer. We do this for multiple weight matrices, which we then\r
stack and feed through a singular value decomposition to find a basis\r
for the equivariant space. This procedure is outlined in Algorithm 1.\r
Any equivariant layer can then be written as a linear combination of\r
bases\r
W =\r
r\r
∑\r
i=1\r
ciVi, (2.17)\r
where the ci’s are learnable scalar coefficients, r is the rank of the\r
equivariant space, and the matrices Vi are the basis vectors, formed\r
from the reshaped right-singular vectors in the SVD. An example is\r
shown in Figure 2.3. To run this procedure, all that is needed are the\r
Figure 2.3: Example of 4-way ro\u0002tationally symmetric filters.\r
transformation operators Lg and Kg. Note we do not need to know\r
the explicit transformation matrices, but just to be able to perform the\r
mappings W 7→ WLg and W 7→ K−1\r
g W. For instance, some matrix Lg\r
rotates an image patch, but we could equally implement WLg using a\r
built-in rotation function. Code is available 4.\r
4 https://github.com/ElisevanderPol/\r
symmetrizer/"""

[[sections]]
number = "2.4"
title = "Experiments"
text = """
We evaluated three flavors of MDP homomorphic network—an MLP,\r
a CNN, and an equivariant feature extractor—on three RL tasks that\r
exhibit group symmetry: CartPole, a grid world, and Pong. We use\r
RLPYT [159] for the algorithms. Hyperparameters (and the range con\u0002sidered), architectures, and group implementation details are in the\r
Supplementary Material. Code is available 5.\r
5 https://github.com/ElisevanderPol/\r
mdp-homomorphic-networks"""

[[sections]]
number = "22"
title = "Environment Space Transformations"
text = """
CartPole S (x, θ, x˙,\r
˙θ) (x, θ, x˙,˙θ),(−x, −θ, −x˙, − ˙θ)\r
A (←, →) (←, →), (→, ←)\r
Grid World S {0, 1}\r
21×21 Identity, y 90◦\r
, y 180◦, y 270◦\r
A (∅, ↑, →, ↓, ←) (∅, ↑, →, ↓, ←),(∅, →, ↓, ←, ↑),(∅, ↓, ←, ↑, →),(∅, ←, ↑, →, ↓)\r
Pong S {0, ..., 255}\r
4×80×80 Identity, reflect\r
A (∅, ∅, ↑, ↓, ↑, ↓) (∅, ∅, ↑, ↓, ↑, ↓), (∅, ∅, ↓, ↑, ↓, ↑)\r
Table 2.1: Environments and\r
Symmetries: We showcase a\r
visual guide of the state and\r
action spaces for each environ\u0002ment along with the effect of the\r
transformations. Note, the sym\u0002bols should not be taken to be\r
hard mathematical statements,\r
they are merely a visual guide\r
for communication.\r
Environments\r
For each environment we show S and A with respective representa\u0002tions of the group transformations.\r
CartPole In the classic pole balancing task [10], we used a two-element\r
group of reflections about the y-axis. We used OpenAI’s Cartpole\u0002v1 [24] implementation, which has a 4-dimensional observation vector:\r
(cart position x, pole angle θ, cart velocity x˙, pole velocity ˙θ). The\r
(discrete) action space consists of applying a force left and right (←\r
, →). We chose this example for its simple symmetries.\r
Grid world We evaluated on a toroidal 7-by-7 predator-prey grid world\r
with agent-centered coordinates. The prey and predator are randomly\r
placed at the start of each episode, lasting a maximum of 100 time\r
steps. The agent’s goal is to catch the prey, which takes a step in a\r
random compass direction with probability 0.15 and stands still oth\u0002erwise. Upon catching the prey, the agent receives a reward of +1,\r
and -0.1 otherwise. The observation is a 21 × 21 binary image iden\u0002tifying the position of the agent in the center and the prey in relative\r
coordinates. See Figure 2.6a. This environment was chosen due to its\r
four-fold rotational symmetry.\r
Pong We evaluated on the RLPYT [159] implementation of Pong. In\r
our experiments, the observation consisted of the 4 last observed frames,\r
with upper and lower margins cut off and downscaled to an 80 × 80\r
grayscale image. In this setting, there is a flip symmetry over the hor\u0002izontal axis: if we flip the observations, the up and down actions also\r
flip. A curious artifact of Pong is that it has duplicate (up, down)\r
actions, which means that to simplify matters, we mask out the pol\u0002icy values for the second pair of (up, down) actions. We chose Pong\r
because of its higher dimensional state space. Finally, for Pong we\r
additionally compare to two data augmentation baselines: stochas\u0002tic data augmentation, where for each state, action pair we randomly\r
transform them or not before feeding them to the network, and the\r
second an equivariant version of [99] and similar to [156], where both\r
state and transformed state are input to the network. The output of\r
the transformed state is appropriately transformed, and both policies\r
are averaged.

23\r
0 20 40 60 80 100 120 140\r
Time steps (x 500) \r
0\r
100\r
200\r
300\r
400"""

[[sections]]
number = "500"
title = "Average Return"
text = """
Nullspace\r
Random\r
Equivariant\r
(a) Cartpole-v1: Bases\r
0 20 40 60 80 100 120 140\r
Time steps (x 500) \r
0\r
100\r
200\r
300\r
400"""

[[sections]]
number = "500"
title = "Average Return"
text = """
MLP, 4 64 128 2\r
MLP, 4 128 128 2\r
Equivariant, 4 64 64 2\r
(b) Cartpole-v1: MLPs\r
0 100 200 300 400 500 600\r
Time steps (x 25000) \r
20\r
15\r
10\r
5\r
0\r
5\r
10\r
15"""

[[sections]]
number = "20"
title = "Average Return"
text = """
Nullspace\r
Random\r
Convolutional\r
Equivariant\r
(c) Pong\r
Figure 2.4: Cartpole: Trained\r
with PPO, all networks fine\u0002tuned over 7 learning rates. 25%,\r
50% and 75% quantiles over 25\r
random seeds shown. a) Equiv\u0002ariant, random, and nullspace\r
bases. b) Equivariant basis, and\r
two MLPs with different degrees\r
of freedom. Pong: Trained\r
with A2C, all networks tuned\r
over 3 learning rates. 25%,\r
50% and 75% quantiles over 15\r
random seeds shown c) Equiv\u0002ariant, nullspace, and random\r
bases, and regular CNN for\r
Pong.\r
Models\r
We implemented MDP homomorphic networks on top of two base\r
architectures: MLP and CNN (exact architectures in Supplementary).\r
We further experimented with an equivariant feature extractor, ap\u0002pended by a non-equivariant network, to isolate where equivariance\r
made the greatest impact.\r
Basis Networks We call networks whose weights are linear combi\u0002nations of basis weights basis networks. As an ablation study on all\r
equivariant networks, we sought to measure the effects of the basis\r
training dynamics. We compared an equivariant basis against a pure\r
nullspace basis, i.e. an explicitly non-symmetric basis using the right\u0002null vectors from the equivariant layer construction, and a random ba\u0002sis, where we skip the symmetrization step in the layer construction\r
and use the full rank basis. Unless stated otherwise, we reduce the\r
number of ‘channels’ in the basis networks compared to the regular\r
networks by dividing by the square root of the group size, ending up\r
with a comparable number of trainable parameters.\r
Results and Discussion\r
We show training curves for CartPole in Figures 2.4a-b, Pong in Fig\u0002ure 2.4c and for the grid world in Figure 2.6. Across all experiments\r
we observed that the MDP homomorphic network outperforms both\r
the non-equivariant basis networks and the standard architectures, in\r
terms of convergence speed.\r
This confirms our motivations that building symmetry-preserving pol\u0002icy networks leads to faster convergence. Additionally, when com\u0002pared to the data augmentation baselines in Figure 2.5, using equiv\u0002ariant networks is more beneficial. This is consistent with other results\r
in the equivariance literature [14, 187, 191, 193]. While data augmenta\u0002tion can be used to create a larger dataset by exploiting symmetries, it\r
does not directly lead to effective parameter sharing (as our approach\r
does). Note, in Pong we only train the first 15 million frames to high-

24\r
0 100 200 300 400 500 600\r
Time steps (x 25000) \r
20\r
15\r
10\r
5\r
0\r
5\r
10\r
15"""

[[sections]]
number = "20"
title = "Average Return"
text = """
Stoch. Data Aug.\r
Full Data Aug.\r
Convolutional\r
Equivariant\r
Figure 2.5: Data augmentation\r
comparison on Pong.\r
light the difference in the beginning; in constrast, a typical training\r
duration is 50-200 million frames [124, 159].\r
For our ablation experiment, we wanted to control for the introduc\u0002tion of bases. It is not clear a priori that a network with a basis has\r
the same gradient descent dynamics as an equivalent ‘basisless’ net\u0002work. We compared equivariant, non-equivariant, and random bases,\r
as mentioned above. We found the equivariant basis led to the fastest\r
convergence. Figures 2.4a and 2.4c show that for CartPole and Pong\r
the nullspace basis converged faster than the random basis. In the grid\r
world there was no clear winner between the two. This is a curious\r
result, requiring deeper investigation in a follow-up.\r
For a third experiment, we investigated what happens if we sacrifice\r
strict equivariance of the policy. This is attractive because it removes\r
the need to find a transformation operator for a flattened output fea\u0002ture map. Instead, we only maintained an equivariant feature extrac\u0002tor, compared against a basic CNN feature extractor. The networks\r
built on top of these extractors were MLPs. The results, in Figure 2.4c,\r
are two-fold: 1) Basis feature extractors converge faster than standard\r
CNNs, and 2) the equivariant feature extractor has fastest convergence.\r
We hypothesize the equivariant feature extractor is fastest as it is easi\u0002est to learn an equivariant policy from equivariant features.\r
We have additionally compared an equivariant feature extractor to a\r
regular convolutional network on the Atari game Breakout, where the\r
difference between the equivariant network and the regular network\r
is much less pronounced. For details, see Appendix 2.C.

25\r
(a) Symmetries\r
0 25 50 75 100 125 150 175 200\r
Time steps (x 10000) \r
10\r
1\r
10\r
0"""

[[sections]]
number = "0"
title = "Average Return"
text = """
Nullspace\r
Random\r
Equivariant\r
(b) Grid World: Bases\r
0 25 50 75 100 125 150 175 200\r
Time steps (x 10000) \r
101\r
100"""

[[sections]]
number = "0"
title = "Average Return"
text = """
Convolutional\r
Equivariant\r
(c) Grid World: CNNs\r
Figure 2.6: Grid World:\r
Trained with A2C, all networks\r
fine-tuned over 6 learning rates.\r
25%, 50% and 75% quantiles\r
over 20 random seeds shown.\r
a) showcase of symmetries, b)\r
Equivariant, nullspace, and ran\u0002dom bases c) plain CNN and\r
equivariant CNN."""

[[sections]]
number = "2.5"
title = "Related Work"
text = """
Past work on MDP homomorphisms has often aimed at discovering\r
the map itself based on knowledge of the transition and reward func\u0002tion, and under the assumption of enumerable state spaces [141, 142,\r
143, 165]. Other work relies on learning the map from sampled expe\u0002rience from the MDP [170, 16, 118]. Exactly computing symmetries in\r
MDPs is graph isomorphism complete [127] even with full knowledge\r
of the MDP dynamics. Rather than assuming knowledge of the tran\u0002sition and reward function, and small and enumerable state spaces, in\r
this work we take the inverse view: we assume that we have an eas\u0002ily identifiable transformation of the joint state–action space and ex\u0002ploit this knowledge to learn more efficiently. Exploiting symmetries\r
in deep RL has been previously explored in the game of Go, in the\r
form of symmetric filter weights [151, 30] or data augmentation [156].\r
Other work on data augmentation increases sample efficiency and gen\u0002eralization on well-known benchmarks by augmenting existing data\r
points state transformations such as random translations, cutout, color\r
jitter and random convolutions [99, 31, 107, 111]. In contrast, we en\u0002code symmetries into the neural network weights, leading to more\r
parameter sharing. Additionally, such data augmentation approaches\r
tend to take the invariance view, augmenting existing data with state\r
transformations that leave the state’s Q-values intact [99, 31, 107, 111]\r
(the exception being [115] and [119], who augment trajectories rather\r
than just states). Similarly, permutation invariant networks are com\u0002monly used in approaches to multi-agent RL [160, 117, 81]. We instead\r
take the equivariance view, which accommodates a much larger class of\r
symmetries that includes transformations on the action space. Abdol\u0002hosseini et al. [1] have previously manually constructed an equivari\u0002ant network for a single group of symmetries in a single RL problem,\r
namely reflections in a bipedal locomotion task. Our MDP homomor\u0002phic networks allow for automated construction of networks that are\r
equivariant under arbitrary discrete groups and are therefore applica\u0002ble to a wide variety of problems.

26\r
From an equivariance point-of-view, the automatic construction of equiv\u0002ariant layers is new. [35] comes close to specifying a procedure, outlin\u0002ing the system of equations to solve, but does not specify an algorithm.\r
The basic theory of group equivariant networks was outlined in [34,\r
35] and [33], with notable implementations to 2D roto-translations on\r
grids [193, 188, 186] and 3D roto-translations on grids [192, 191, 187].\r
All of these works have relied on hand-constructed equivariant layers."""

[[sections]]
number = "2.6"
title = "Conclusion"
text = """
This chapter introduced MDP homomorphic networks, a family of\r
deep architectures for reinforcement learning problems where symme\u0002tries have been identified. MDP homomorphic networks tie weights\r
over symmetric state-action pairs. This weight-tying leads to fewer\r
degrees-of-freedom and in our experiments we found that this trans\u0002lates into faster convergence. We used the established theory of MDP\r
homomorphisms to motivate the use of equivariant networks in sym\u0002metric MDPs, thus formalizing the connection between equivariant\r
networks and symmetries in reinforcement learning. As an innova\u0002tion, we also introduced the first method to automatically construct\r
equivariant network layers, given a specification of the symmetries in\r
question, thus removing a significant implementational obstacle. For\r
future work, we want to further understand the symmetrizer and its\r
effect on learning dynamics, as well as generalizing to problems that\r
are not fully symmetric."""

[[sections]]
number = "2.7"
title = "Broader Impact Statement"
text = """
The goal of this chapter is to make (deep) reinforcement learning tech\u0002niques more efficient at solving Markov decision processes (MDPs)\r
by making use of prior knowledge about symmetries. We do not ex\u0002pect the particular algorithm we develop to lead to immediate societal\r
risks. However, Markov decision processes are very general, and can\r
e.g. be used to model problems in autonomous driving, smart grids,\r
and scheduling. Thus, solving such problems more efficiently can in\r
the long run cause positive or negative societal impact.\r
For example, making transportation or power grids more efficient,\r
thereby making better use of scarce resources, would be a significantly\r
positive impact. Other potential applications, such as in autonomous

27\r
weapons, pose a societal risk [131]. Like many AI technologies, when\r
used in automation, our technology can have a positive impact (in\u0002creased productivity) and a negative impact (decreased demand) on\r
labor markets.\r
More immediately, control strategies learned using RL techniques are\r
hard to verify and validate. Without proper precaution (e.g. [177]),\r
employing such control strategies on physical systems thus run the\r
risk of causing accidents involving people, e.g. due to reward mis\u0002specification, unsafe exploration, or distributional shift [5]."""

[[sections]]
number = "29"
title = "Chapter Appendix"
text = ""

[[sections]]
number = "2"
title = "A The Symmetrizer"
text = """
In this section we prove three properties of the symmetrizer: the sym\u0002metric property (S(W) ∈ W for all W ∈ Wtotal ), the fixing prop\u0002erty (W ∈ W =⇒ S(W) = W) , and the idempotence property\r
(S(S(W)) = S(W) for all W ∈ Wtotal).\r
The Symmetric Property Here we show that the symmetrizer S maps\r
matrices W ∈ Wtotal to equivariant matrices S(W) ∈ W. For this,\r
we show that a symmetrized weight matrix S(W) from Equation 16\r
satisfies the equivariance constraint of Equation 14.\r
Proof 1 (The symmetric property) We begin by recalling the equivari\u0002ance constraint\r
KgWz = WLgz, for all g ∈ G, z ∈ R\r
Din+1\r
. (2.18)\r
Now note that we can drop the dependence on z, since this equation is true for\r
all z. At the same time, we left-multiply both sides of this equation by Kg\r
−1\r
,\r
which is possible because group representations are invertible. This results in\r
the following set of equations\r
W = K\r
−1\r
g WLg, for all g ∈ G. (2.19)\r
Any W satisfying this equation satisfies Equation 2.18 and is thus a member\r
of W. To show that S(W) is a member of W, we thus would need show that\r
S(W) = K\r
−1\r
g S(W)Lg for all W ∈ Wtotal and g ∈ G. This can be shown as

30\r
follows:\r
K\r
−1\r
g S(W)Lg = K\r
−1\r
g\r
 \r
1\r
|G| ∑\r
h∈G\r
K\r
−1\r
h WLh\r
!\r
Lg substitute S(W) = K\r
−1\r
g S(W)Lg\r
(2.20)\r
=\r
1\r
|G| ∑\r
h∈G\r
K\r
−1\r
g K\r
−1\r
h WLhLg (2.21)\r
=\r
1\r
|G| ∑\r
h∈G\r
K\r
−1\r
hg WLhg representation definition: LhLg = Lhg\r
(2.22)\r
=\r
1\r
|G| ∑\r
g\r
0g−1∈G\r
K\r
−1\r
g"""

[[sections]]
number = "0"
title = "WLg"
text = """
0 change of variables g0 = hg, h = g\r
0\r
g\r
−1\r
(2.23)\r
=\r
1\r
|G| ∑\r
g\r
0∈Gg\r
K\r
−1\r
g"""

[[sections]]
number = "0"
title = "WLg"
text = """
0 g\r
0\r
g\r
−1 ∈ G ⇐⇒ g0 ∈ Gg\r
(2.24)\r
=\r
1\r
|G| ∑\r
g\r
0∈G\r
K\r
−1\r
g"""

[[sections]]
number = "0"
title = "WLg"
text = ""

[[sections]]
number = "0"
title = "G = Gg (2.25)"
text = """
= S(W) definition of symmetrizer.\r
(2.26)\r
Thus we see that S(W) satisfies the equivariance constraint, which implies\r
that S(W) ∈ W.\r
The Fixing Property For the symmetrizer to be useful, we need to\r
make sure that its range covers the equivariant subspace W, and not\r
just a subset of it; that is, we need to show that\r
W = {S(W) ∈ W|W ∈ Wtotal}. (2.27)\r
We show this by picking a matrix W ∈ W and showing that W ∈\r
W =⇒ S(W) = W.\r
Proof 2 (The fixing property) We begin by assuming that W ∈ W, then\r
S(W) = 1\r
|G| ∑\r
g∈G\r
K\r
−1\r
g WLg definition (2.28)\r
=\r
1\r
|G| ∑\r
g∈G\r
K\r
−1\r
g KgW W ∈ W ⇐⇒ KgW = WLg, ∀g ∈ G\r
(2.29)\r
=\r
1\r
|G| ∑\r
g∈G\r
W (2.30)\r
= W (2.31)"""

[[sections]]
number = "31"
title = "This means that the symmetrizer leaves the equivariant subspace invariant."
text = """
In fact, the statement we just showed is stronger in saying that each point in\r
the equivariant subspace is unaltered by the symmetrizer. In the language of\r
group theory we say that subspace W is fixed under G. Since S : Wtotal →\r
W and there exist matrices W such that for every W ∈ W, S(W) = W, we\r
have shown that\r
W = {S(W) ∈ W|W ∈ Wtotal}. (2.32)\r
The Idempotence Property Here we show that the symmetrizer S(W)\r
from Equation 16 is idempotent, S(S(W)) = S(W).\r
Proof 3 (The idempotence property) Recall the definition of the symmetrizer\r
S(W) = 1\r
|G| ∑\r
g∈G\r
K\r
−1\r
g WLg. (2.33)\r
Now let’s expand S(S(W)):\r
S(S(W)) = S\r
 \r
1\r
|G| ∑\r
h∈G\r
K\r
−1\r
h WLh\r
!\r
(2.34)\r
=\r
1\r
|G| ∑\r
g∈G\r
K\r
−1\r
g\r
 \r
1\r
|G| ∑\r
h∈G\r
K\r
−1\r
h WLh\r
!\r
Lg (2.35)\r
=\r
1\r
|G| ∑\r
g∈G\r
 \r
1\r
|G| ∑\r
h∈G\r
K\r
−1\r
g K\r
−1\r
h WLhLg\r
!\r
linearity of sum\r
(2.36)\r
=\r
1\r
|G| ∑\r
g∈G\r
 \r
1\r
|G| ∑\r
h∈G\r
K\r
−1\r
hg WLhg!\r
definition of group representations\r
(2.37)\r
=\r
1\r
|G| ∑\r
g∈G\r
\r
\r
1\r
|G| ∑\r
g\r
0g−1∈G\r
K\r
−1\r
g"""

[[sections]]
number = "0"
title = "WLg"
text = """
0\r
\r
 change of variables g0 = hg\r
(2.38)\r
=\r
1\r
|G| ∑\r
g∈G\r
\r
\r
1\r
|G| ∑\r
g\r
0∈Gg\r
K\r
−1\r
g"""

[[sections]]
number = "0"
title = "WLg"
text = """
0\r
\r
 g\r
0\r
g\r
−1 ∈ G ⇐⇒ g0 ∈ Gg\r
(2.39)\r
=\r
1\r
|G| ∑\r
g∈G\r
\r
\r
1\r
|G| ∑\r
g\r
0∈G\r
K\r
−1\r
g"""

[[sections]]
number = "0"
title = "WLg"
text = """
0\r
\r
 Gg = G (2.40)\r
=\r
1\r
|G| ∑\r
g\r
0∈G\r
K\r
−1\r
g"""

[[sections]]
number = "0"
title = "WLg"
text = """
0 sum over constant\r
(2.41)\r
= S(W) (2.42)"""

[[sections]]
number = "32"
title = "Thus we see that S(W) satisfies the equivariance constraint, which implies"
text = "that S(W) ∈ W."

[[sections]]
number = "2"
title = "B Experimental Settings"
text = """
Designing representations\r
In the main text we presented a method to construct a space of in\u0002tertwiners W using the symmetrizer. This relies on us already hav\u0002ing chosen specific representations/transformation operators for the\r
input, the output, and for every intermediate layer of the MDP homo\u0002morphic networks. While for the input space (state space) and output\r
space (policy space), these transformation operators are easy to define,\r
it is an open question how to design a transformation operator for the inter\u0002mediate layers of our networks. Here we give some rules of thumb that\r
we used, followed by the specific transformation operators we used in\r
our experiments.\r
For each experiment we first identified the group G of transforma\u0002tions. In every case, this was a finite group of size |G|, where the size\r
is the number of elements in the group (number of distinct transfor\u0002mation operators). For example, a simple flip group as in Pong has\r
two elements, so |G| = 2. Note that the group size |G| does not nec\u0002essarily equal the size of the transformation operators, whose size is\r
determined by the dimensionality of the input/activation layer/policy.\r
Stacking Equivariant Layers If we stack equivariant layers, the result\u0002ing network is equivariant as a whole too [34]. To see that this is\r
the case, consider the following example. Assume we have network f ,\r
consisting of layers f1 and f2, which satisfy the layer-wise equivariance\r
constraints:\r
Pg[ f1(x)] = f1(Lg[x]) (2.43)\r
Kg[ f2(x)] = f2(Pg[x]) (2.44)\r
With Kg the output transformation of the network, Lg the input trans\u0002formation, and Pg the intermediate transformation. Now,\r
Kg[ f(x)] = Kg[ f2(f1(x))] (2.45)\r
= f2(Pg[ f1(x)] (f2 equivariance constraint) (2.46)\r
= f2(f1(Lg[x])) (f1 equivariance constraint) (2.47)\r
= f(Lg[x]) (2.48)

33\r
and so the whole network f is equivariant with regards to the input\r
transformation Lg and the output transformation Kg. Note that this\r
depends on the intermediate representation Pg being shared between\r
layers, i.e. f1’s output transformation is the same as f2’s input trans\u0002formation.\r
MLP-structured networks For MLP-structured networks (CartPole), typ\u0002ically the activations have shape [batch_size, nc], with nc the num\u0002ber of channels. Instead we used a shape of [batch_size, nc,\r
representation_size], where for the intermediate layers\r
representation_size=|G|+1 (we have a +1 because of the bias). The\r
transformation operators we then apply to the activations is the set of\r
permutations for group size |G| appended with a 1 on the diagonal for\r
the bias, acting on this last ‘representation dimension’. Thus a forward\r
pass of a layer is computed as\r
yb,cout,rout=\r
nc\r
∑\r
cin=1\r
|G|+1\r
∑\r
rin=1\r
zb,cin,rinWcout,rout,cin,rin (2.49)\r
where\r
Wcout,rout,cin,rin =\r
rank(W)\r
∑\r
i=1\r
ci,cout,cinVi,rout,rin . (2.50)\r
CNN-structured networks For CNN-structured networks (Pong and Grid\r
World), typically the activations have shape [batch_size,\r
nc, height, width]. Instead we used a shape of\r
[batch_size, nc, representation_size, height, width], where for\r
the intermediate layers representation_size=|G|+1. The transforma\u0002tion operators we apply to the input of the layer is a spatial transfor\u0002mation on the height, width dimensions and a permutation on the\r
representation dimension. This is because in the intermediate layers\r
of the network the activations do not only transform in space, but also\r
along the representation dimensions of the tensor. The transformation\r
operators we apply to the output of the layer is just a permutation\r
on the representation dimension. Thus a forward pass of a layer is\r
computed as\r
yb,cout,rout,hout,wout=\r
nc\r
∑\r
cin=1\r
|G|+1\r
∑\r
rin=1\r
∑\r
hin,win\r
zb,cin,rin,hout+hin,wout+winWcout,rout,cin,rin,hin,win\r
(2.51)\r
where\r
Wcout,rout,cin,rin,hin,win =\r
rank(W)\r
∑\r
i=1\r
ci,cout,cinVi,rout,rin,hin,win . (2.52)"""

[[sections]]
number = "34"
title = "Equivariant Nullspace Random MLP"
text = """
0.01 0.005 0.001 0.001\r
Table 2.B.1: Final learning\r
rates used in CartPole-v1 exper\u0002iments.\r
Cartpole-v1\r
Group Representations For states:\r
Lge =\r
\r
\r
1 0 0 0\r
0 1 0 0\r
0 0 1 0\r
0 0 0 1\r
\r
\r
, Lg1 =\r
\r
\r
−1 0 0 0\r
0 −1 0 0\r
0 0 −1 0\r
0 0 0 −1\r
\r
\r
For intermediate layers and policies:\r
K\r
π\r
ge =\r
 \r
1 0\r
0 1!\r
, K\r
π\r
g1 =\r
 \r
0 1\r
1 0!\r
For values we require an invariant rather than equivariant output. This\r
invariance is implemented by defining the output representations to be\r
|G| identity matrices of the desired output dimensionality. For predict\u0002ing state values we required a 1-dimensional output, and we thus used\r
|G| 1-dimensional identity matrices, i.e. for value output V:\r
K\r
V\r
ge =\r
\u0010\r
1\r
\u0011\r
, K\r
V\r
g1 =\r
\u0010\r
1\r
\u0011\r
Hyperparameters For both the basis networks and the MLP, we used\r
Xavier initialization. We trained PPO using ADAM on 16 parallel envi\u0002ronments and fine-tuned over the learning rates {0.01, 0.05, 0.001, 0.005,\r
0.0001, 0.0003, 0.0005} by running 25 random seeds for each setting,\r
and report the best curve. The final learning rates used are shown\r
in Table 2.B.1. Other hyperparameters were defaults in RLPYT [159],\r
except that we turn off learning rate decay.\r
Architecture\r
Basis networks:\r
Listing 2.1: Basis Networks Architecture for CartPole-v1"""

[[sections]]
number = "1"
title = "BasisLinear(repr_in=4, channels_in=1, repr_out=2, channels_out=64)"
text = ""

[[sections]]
number = "2"
title = "ReLU()"
text = ""

[[sections]]
number = "3"
title = "BasisLinear(repr_in=2, channels_in=64, repr_out=2, channels_out=64)"
text = ""

[[sections]]
number = "4"
title = "ReLU()"
text = ""

[[sections]]
number = "5"
title = "BasisLinear(repr_in=2, channels_in=64, repr_out=2, channels_out=1)"
text = ""

[[sections]]
number = "6"
title = "BasisLinear(repr_in=2, channels_in=64, repr_out=1, channels_out=1)"
text = "First MLP variant:"

[[sections]]
number = "35"
title = "Listing 2.2: First MLP Architecture for CartPole-v1"
text = ""

[[sections]]
number = "1"
title = "Linear(channels_in=1, channels_out=64)"
text = ""

[[sections]]
number = "2"
title = "ReLU()"
text = ""

[[sections]]
number = "3"
title = "Linear(channels_in=64, channels_out=128)"
text = ""

[[sections]]
number = "4"
title = "ReLU()"
text = ""

[[sections]]
number = "5"
title = "Linear(channels_in=128, channels_out=1)"
text = ""

[[sections]]
number = "6"
title = "Linear(channels_in=128, channels_out=1)"
text = """
Second MLP variant:\r
Listing 2.3: Second MLP Architecture for CartPole-v1"""

[[sections]]
number = "1"
title = "Linear(channels_in=1, channels_out=128)"
text = ""

[[sections]]
number = "2"
title = "ReLU()"
text = ""

[[sections]]
number = "3"
title = "Linear(channels_in=128, channels_out=128)"
text = ""

[[sections]]
number = "4"
title = "ReLU()"
text = ""

[[sections]]
number = "5"
title = "Linear(channels_in=128, channels_out=1)"
text = ""

[[sections]]
number = "6"
title = "Linear(channels_in=128, channels_out=1)"
text = """
GridWorld\r
Group Representations For states we use numpy.rot90. The stack of\r
weights is rolled.\r
For the intermediate representations:\r
Lge =\r
\r
\r
1 0 0 0\r
0 1 0 0\r
0 0 1 0\r
0 0 0 1\r
\r
\r
, Lg1 =\r
\r
\r
0 0 0 1\r
1 0 0 0\r
0 1 0 0\r
0 0 1 0\r
\r
\r
,\r
Lg2 =\r
\r
\r
0 0 1 0\r
0 0 0 1\r
1 0 0 0\r
0 1 0 0\r
\r
\r
, Lg3 =\r
\r
\r
0 1 0 0\r
0 0 1 0\r
0 0 0 1\r
1 0 0 0\r
\r
\r
For the policies:\r
K\r
π\r
ge =\r
\r
\r
1 0 0 0 0\r
0 1 0 0 0\r
0 0 1 0 0\r
0 0 0 1 0\r
0 0 0 0 1\r
\r
\r
, K\r
π\r
g1 =\r
\r
\r
1 0 0 0 0\r
0 0 0 0 1\r
0 1 0 0 0\r
0 0 1 0 0\r
0 0 0 1 0\r
\r
\r
,\r
K\r
π\r
g2 =\r
\r
\r
1 0 0 0 0\r
0 0 0 1 0\r
0 0 0 0 1\r
0 1 0 0 0\r
0 0 1 0 0\r
\r
\r
, K\r
π\r
g3 =\r
\r
\r
1 0 0 0 0\r
0 0 1 0 0\r
0 0 0 1 0\r
0 0 0 0 1\r
0 1 0 0 0\r
\r
"""

[[sections]]
number = "36"
title = "For the values:"
text = """
K\r
V\r
ge =\r
\u0010\r
1\r
\u0011\r
, K\r
V\r
g1 =\r
\u0010\r
1\r
\u0011\r
, K\r
V\r
g2 =\r
\u0010\r
1\r
\u0011\r
, K\r
V\r
g3 =\r
\u0010\r
1\r
\u0011\r
Hyperparameters For both the basis networks and the CNN, we used\r
He initialization. We trained A2C using ADAM on 16 parallel environ\u0002ments and fine-tuned over the learning rates {0.00001, 0.00003, 0.0001,\r
0.0003, 0.001, 0.003} on 20 random seeds for each setting, and reporting\r
the best curve. The final learning rates used are shown in Table 2.B.2.\r
Other hyperparameters were defaults in RLPYT [159].\r
Equivariant Nullspace Random CNN\r
0.001 0.003 0.001 0.003\r
Table 2.B.2: Final learning rates\r
used in grid world experiments.\r
Architecture\r
Basis networks:\r
Listing 2.4: Basis Networks Architecture for GridWorld"""

[[sections]]
number = "1"
title = "BasisConv2d(repr_in=1, channels_in=1, repr_out=4, channels_out=b √"
text = """
16\r
4\r
c,\r
2 filter_size=(7, 7), stride=2, padding=0)"""

[[sections]]
number = "3"
title = "ReLU()"
text = ""

[[sections]]
number = "4"
title = "BasisConv2d(repr_in=4, channels_in=b √"
text = """
16\r
4\r
c, repr_out=4, channels_out=b √\r
32\r
4\r
c,\r
5 filter_size=(5, 5), stride=1, padding=0)"""

[[sections]]
number = "6"
title = "ReLU()"
text = ""

[[sections]]
number = "7"
title = "GlobalMaxPool()"
text = ""

[[sections]]
number = "8"
title = "BasisLinear(repr_in=4, channels_in=b √"
text = """
32\r
4\r
c, repr_out=4, channels_out=b\r
512\r
√\r
4\r
c)"""

[[sections]]
number = "9"
title = "ReLU()"
text = ""

[[sections]]
number = "10"
title = "BasisLinear(repr_in=4, channels_in=b"
text = """
512\r
√\r
4\r
c, repr_out=5, channels_out=1)"""

[[sections]]
number = "11"
title = "BasisLinear(repr_in=4, channels_in=b"
text = """
512\r
√\r
4\r
c, repr_out=1, channels_out=1)\r
CNN:\r
Listing 2.5: CNN Architecture for GridWorld"""

[[sections]]
number = "1"
title = "Conv2d(channels_in=1, channels_out=16,"
text = "2 filter_size=(7, 7), stride=2, padding=0)"

[[sections]]
number = "3"
title = "ReLU()"
text = ""

[[sections]]
number = "4"
title = "Conv2d(channels_in=16,channels_out=32,"
text = "5 filter_size=(5, 5), stride=1, padding=0)"

[[sections]]
number = "6"
title = "ReLU()"
text = ""

[[sections]]
number = "7"
title = "GlobalMaxPool()"
text = ""

[[sections]]
number = "8"
title = "Linear(channels_in=32, channels_out=512)"
text = ""

[[sections]]
number = "9"
title = "ReLU()"
text = ""

[[sections]]
number = "10"
title = "Linear(channels_in=512, channels_out=5)"
text = ""

[[sections]]
number = "11"
title = "Linear(channels_in=512, channels_out=1)"
text = ""

[[sections]]
number = "37"
title = "Pong"
text = """
Group Representations For the states we use numpy’s indexing to flip\r
the input, i.e.\r
w = w[..., ::-1, :], then the permutation on the representation\r
dimension of the weights is a numpy.roll, since the group is cyclic.\r
For the intermediate layers:\r
Lge =\r
 \r
1 0\r
0 1!\r
, Lg1 =\r
 \r
0 1\r
1 0!\r
Hyperparameters For both the basis networks and the CNN, we used\r
He initialization. We trained A2C using ADAM on 4 parallel environ\u0002ments and fine-tuned over the learning rates {0.0001, 0.0002, 0.0003}\r
on 15 random seeds for each setting, and reporting the best curve. The\r
learning rates to fine-tune over were selected to be close to where the\r
baseline performed well in preliminary experiments. The final learn\u0002ing rates used are shown in Table 2.B.3. Other hyperparameters were\r
defaults in RLPYT [159].\r
Equivariant Nullspace Random CNN\r
0.0002 0.0002 0.0002 0.0001\r
Table 2.B.3: Learning rates used\r
in Pong experiments.\r
Architecture\r
Basis Networks:\r
Listing 2.6: Basis Networks Architecture for Pong"""

[[sections]]
number = "1"
title = "BasisConv2d(repr_in=1, channels_in=4, repr_out=2, channels_out=b √"
text = """
16\r
2\r
c,\r
2 filter_size=(8, 8), stride=4, padding=0)"""

[[sections]]
number = "3"
title = "ReLU()"
text = ""

[[sections]]
number = "4"
title = "BasisConv2d(repr_in=2, channels_in=b √"
text = """
16\r
2\r
c, repr_out=2, channels_out=b √\r
32\r
2\r
c,\r
5 filter_size=(5, 5), stride=2, padding=0)"""

[[sections]]
number = "6"
title = "ReLU()"
text = ""

[[sections]]
number = "7"
title = "Linear(channels_in=2816, channels_out=b"
text = """
512\r
√\r
2\r
c)"""

[[sections]]
number = "8"
title = "ReLU()"
text = ""

[[sections]]
number = "9"
title = "Linear(channels_in=b"
text = """
512\r
√\r
2\r
c, channels_out=6)"""

[[sections]]
number = "10"
title = "Linear(channels_in=b"
text = """
512\r
√\r
2\r
c, channels_out=1)\r
CNN:\r
Listing 2.7: CNN Architecture for Pong"""

[[sections]]
number = "1"
title = "Conv2d(channels_in=4, channels_out=16, filter_size=(8, 8), stride=4, padding=0)"
text = ""

[[sections]]
number = "2"
title = "ReLU()"
text = "38"

[[sections]]
number = "3"
title = "Conv2d(channels_in=16,channels_out=32, filter_size=(5, 5), stride=2, padding=0)"
text = ""

[[sections]]
number = "4"
title = "ReLU()"
text = ""

[[sections]]
number = "5"
title = "Linear(channels_in=2048, channels_out=512)"
text = ""

[[sections]]
number = "6"
title = "ReLU()"
text = ""

[[sections]]
number = "7"
title = "Linear(channels_in=512, channels_out=6)"
text = ""

[[sections]]
number = "8"
title = "Linear(channels_in=512, channels_out=1)"
text = "39"

[[sections]]
number = "2"
title = "C Breakout Experiments"
text = """
We evaluated the effect of an equivariant basis extractor on Breakout,\r
compared to a baseline convolutional network. The hyperparameter\r
settings and architecture were largely the same as those of Pong, ex\u0002cept for the input group representation, a longer training time, and\r
that we considered a larger range of learning rates. To ensure sym\u0002metric states, we remove the two small decorative blocks in the bottom\r
corners.\r
Group Representations For the states we use numpy’s indexing to flip\r
the input, i.e.\r
w = w[..., :, ::-1] (note the different axis than in Pong), then the\r
permutation on the representation dimension of the weights is a\r
numpy.roll, since the group is cyclic.\r
For the intermediate layers:\r
Lge =\r
 \r
1 0\r
0 1!\r
, Lg1 =\r
 \r
0 1\r
1 0!\r
Hyperparameters We used He initialization. We trained A2C using\r
ADAM on 4 parallel environments and fine-tuned over the learning\r
rates {0.001, 0.005, 0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.00001, 0.00005}\r
on 15 random seeds for each setting, and reporting the best curve. The\r
final learning rates used are shown in Table 2.C.1. Other hyperparam\u0002eters were defaults in RLPYT [159].\r
Equivariant CNN\r
0.0002 0.0002\r
Table 2.C.1: Learning rates used\r
in Breakout experiments.\r
0 250 500 750 1000 1250 1500 1750 2000\r
Time steps (x 25000) \r
0\r
20\r
40\r
60\r
80"""

[[sections]]
number = "100"
title = "Average Return"
text = """
Equivariant\r
Convolutional\r
Figure 2.C.1: Breakout:\r
Trained with A2C, all networks\r
fine-tuned over 9 learning rates.\r
25%, 50% and 75% quantiles\r
over 14 random seeds shown.

40\r
Results Figure 2.C.1 shows the result of the equivariant feature ex\u0002tractor versus the convolutional baseline. While we again see an im\u0002provement over the standard convolutional approach, the difference is\r
much less pronounced than in CartPole, Pong or the grid world. It\r
is not straightforward why. One factor could be that the equivariant\r
feature extractor is not end-to-end MDP homomorphic. It instead out\u0002puts a type of MDP homomorphic state representations and learns a\r
regular policy on top. As a result, the unconstrained final layers may\r
negate some of the advantages of the equivariant feature extractor.\r
This may be more of an issue for Breakout than Pong, since Breakout\r
is a more complex game."""

[[sections]]
number = "2"
title = "D Cartpole-v1 Deeper Network Results"
text = """
We show the effect of training a deeper network – 4 layers instead of\r
2 – for CartPole-v1 in Figure 2.D.1. The performance of the regular\r
depth networks in Figure 4b and the deeper networks in Figure 2.D.1\r
is comparable, except that for the regular MLP, the variance is much\r
higher when using deeper networks.\r
0 20 40 60 80 100 120 140\r
Time steps (x 500) \r
0\r
100\r
200\r
300\r
400"""

[[sections]]
number = "500"
title = "Average Return"
text = """
Nullspace\r
Random\r
Equivariant\r
(a) Cartpole-v1: Bases\r
0 20 40 60 80 100 120 140\r
Time steps (x 500) \r
0\r
100\r
200\r
300\r
400"""

[[sections]]
number = "500"
title = "Average Return"
text = """
MLP, 4 64 64 128 128 2\r
MLP, 4 128 128 128 128 2\r
Equivariant, 4 64 64 64 64 2\r
(b) Cartpole-v1: MLPs\r
Figure 2.D.1: Cartpole:\r
Trained with PPO, all networks\r
fine-tuned over 7 learning rates.\r
25%, 50% and 75% quantiles\r
over 25 random seeds shown.\r
a) Equivariant, random, and\r
nullspace bases. b) Equivariant\r
basis, and two MLPs with\r
different degrees of freedom."""

[[sections]]
number = "2"
title = "E Bellman Equations"
text = """
V\r
π\r
(s) = ∑\r
a∈A\r
π(s, a)\r
"\r
R(s, a) + γ ∑\r
s\r
0∈S\r
T(s, a,s\r
0\r
)V\r
π\r
(s\r
0\r
)\r
#\r
(2.53)\r
Q\r
π\r
(s, a) = R(s, a) + γ ∑\r
s\r
0∈S\r
T(s, a,s\r
0\r
)V\r
π\r
(s\r
0\r
). (2.54)

41"""

[[sections]]
number = "3"
title = "Multi-Agent MDP"
text = "Homomorphic Networks"

[[sections]]
number = "3.1"
title = "Introduction"
text = """
In the previous Chapter, we have shown that enforcing group symme\u0002tries in single agent reinforcement learning improves data efficiency.\r
In this Chapter, we will go beyond the single agent case and consider\r
global symmetries in distributed cooperative multi-agent learning.\r
Equivariant and geometric deep learning have gained traction in re\u0002cent years, showing promising results in supervised learning [34, 191,\r
187, 186, 193, 52, 166], unsupervised learning [39] and reinforcement\r
learning [173, 157]. In single agent reinforcement learning, incorpo\u0002rating symmetry information has been a successful approach, for ex\u0002ample using MDP homomorphic networks [173], trajectory augmen\u0002tation [115, 119], or symmetric locomotion policies [1]. Equivariance\r
enables an agent to learn policies more efficiently within its environ\u0002ment by sharing weights between state-action pairs that are equivalent\r
under a transformation. As a result of this weight sharing, the agent\r
implicitly learns a policy in a reduced version of the MDP. We are inter\u0002ested in cooperative multi-agent reinforcement learning, where sym\u0002metries exist both in the global environment, and between individual\r
agents in the larger multi-agent system. Existing work on symmetries\r
in single agent reinforcement learning can only be generalized to the\r
fully centralized multi-agent setting, because such approaches rely on

42\r
the global symmetry in the full state-action spaces and these can result\r
in correspondences across agents, as shown in Figure 3.1.1.\r
Figure 3.1.1: Example of a global\r
symmetry in a traffic light con\u0002trol problem, where each agent,\r
locally controlling the traffic\r
lights of a single intersection,\r
has to decide to give a green\r
light to either the horizontal\r
or vertical lanes. When a ro\u0002tated state occurs in the envi\u0002ronment the optimal policy is\r
permuted, both between and\r
within agents. Local policies\r
are color coded. Multi-Agent\r
MDP Homomorphic Networks\r
are equivariant to global sym\u0002metries while still allowing dis\u0002tributed execution based on lo\u0002cal observations and local com\u0002munications only. This means such approaches cannot be used in distributed multi-agent\r
systems with communication constraints. Here, we seek to be equiv\u0002ariant to global symmetries of cooperative multi-agent systems while\r
still being able to execute policies in a distributed manner.\r
Existing work in deep multi-agent reinforcement learning has shown\r
the potential of using permutation symmetries and invariance between\r
agents [117, 81, 163, 144, 18, 160, 171]. Such work takes an anonymity\r
view of homogeneous agents, where the agent’s observations matter\r
for the policy but not its identity. Using permutation symmetries en\u0002sures extensive weight sharing between agents, resulting in improved\r
data efficiency. Here, we go beyond such permutation symmetries,\r
and consider more general symmetries of global multi-agent systems,\r
such as rotational symmetries.\r
In this chapter, we propose Multi-Agent MDP Homomorphic Net\u0002works, a class of distributed policy networks which are equivariant\r
to global symmetries of the multi-agent system, as well as to standard\r
permutation symmetries. Our contributions are as follows. (i) We\r
propose a factorization of global symmetries in the joint state-action\r
space of cooperative multi-agent systems. (ii) We introduce a multi\u0002agent equivariant policy network based on this factorization. (iii) Our\r
main contribution is an approach to cooperative multi-agent reinforce-

43\r
ment learning that is globally equivariant while requiring only local\r
agent computation and local communication between agents at execu\u0002tion time. We evaluate Multi-Agent MDP Homomorphic Networks on\r
symmetric multi-agent problems and show improved data efficiency\r
compared to non-equivariant baselines."""

[[sections]]
number = "3.2"
title = "Related Work"
text = """
Symmetries in single agent reinforcement learning Symmetries in Markov\r
Decision Processes have been formalized by [204, 141]. Recent work\r
on symmetries in single agent deep reinforcement learning has shown\r
improvements in terms of data efficiency. Such work revolves around\r
symmetries in policy networks [173, 157], symmetric filters [30], in\u0002variant data augmentation [107, 99] or equivariant trajectory augmen\u0002tation [115, 119, 122] These approaches are only suitable for single\r
agent problems or centralized multi-agent controllers. Here, we solve\r
the problem of enforcing global equivariance while still allowing dis\u0002tributed execution.\r
Graphs and permutation symmetries in multi agent reinforcement learning\r
Graph-based methods in cooperative multiagent reinforcement learn\u0002ing are well-explored. Much work is based around coordination graphs\r
[64, 62, 97], including approaches that approximate local Q-functions\r
with neural networks and use max-plus to find a joint policy [171, 18],\r
and approaches that use graph-structured networks to find joint poli\u0002cies or value functions [81, 160]. In deep learning for multi-agent sys\u0002tems, the use of permutation symmetries is common, either through\r
explicit formulations [163, 18] or through the use of graph or mes\u0002sage passing networks [117, 81, 160]. Policies in multi-agent systems\r
with permutation symmetries between agents are also known as func\u0002tionally homogeneous policies [204] or policies with agent anonymity\r
[144, 175]. Here, we move beyond permutation symmetries to a broader\r
group of symmetries in multiagent reinforcement learning.\r
Symmetries in multi agent reinforcement learning Recently, [74] used\r
knowledge of symmetries to improve zero-shot coordination in games\r
which require symmetry-breaking. Here, we instead use symmetries\r
in cooperative multi-agent systems to improve data efficiency by pa\u0002rameter sharing between different transformations of the global sys\u0002tem.

44"""

[[sections]]
number = "3.3"
title = "Background"
text = """
In this section we introduce the necessary definitions and notation\r
used in the rest of the chapter.\r
Multi-Agent MDPs\r
We will start from the definition of Multi-Agent MDPs, a class of fully\r
observable cooperative multi-agent systems. Full observability implies\r
that each agent can execute the same centralized policy. Later on we\r
will define a distributed variant of this type of decision making prob\u0002lem.\r
Definition 1 A Multi-Agent Markov Decision Process (MMDP) [21] is\r
a tuple (N , S, A, T, R) where N is a set of m agents, S is the state space, A =\r
A1 × · · · × Am is the joint action space of the MMDP, T : S × A1 × · · · ×\r
Am × S → [0, 1] is the transition function, and R : S × A1 × · · · × Am → R\r
is the immediate reward function.\r
The goal of an MMDP, as in the single agent case, is to find a joint\r
policy that maps states to probability distributions over joint actions,\r
π : S → ∆(A), (with ∆(A) the space of such distributions) to maximize\r
the expected discounted return of the system, Rt = E[∑\r
T\r
k=0\r
γ\r
k\r
rt+k+1]\r
with γ ∈ [0, 1] a discount factor. An MMDP can be viewed as a single\u0002agent MDP where the agent takes joint actions.\r
Groups and Transformations\r
In this chapter we will refer extensively to group symmetries. Here,\r
we will briefly introduce these concepts and explain their significance\r
to discussing equivalences of decision making problems.\r
A group G is a set with a binary operator · that obeys the group ax\u0002ioms: identity, inverse, closure, and associativity. Consider as a run\u0002ning example the set of 90 degree rotations {0\r
◦\r
, 90◦, 180◦, 270◦}, which\r
we can write as rotation matrices:\r
R(θ) = "\r
cos θ − sin θ\r
sin θ cos θ\r
#\r
(3.1)\r
with θ ∈ {0, π\r
2\r
, π,\r
3π\r
2\r
}. Composing any two matrices in this set results\r
in another matrix in the set, meaning the set is closed under com\u0002position. For example, composing R(\r
π\r
2\r
) and R(π) results in another\r
member of the set, in this case R(\r
3π\r
2\r
). Similarly, each member of the\r
set has an inverse that is also in the set, and R(0) is an identity ele-

45\r
ment. Since matrix multiplication is associative, the group axioms are\r
satisfied and the set is a group under composition.\r
A group action is a function G × X → X that satisfies ex = x (where e\r
is the identity element) and (g · h)x = g · (hx). For example, the group\r
of 90 degree rotation matrices acts on vectors to rotate them. Similar to\r
the action of this group on vectors, we can define an action of the same\r
group on image space: e.g., the NumPy [68] function np.rot90 acts on\r
the set of images. We will consider group actions on the set of states\r
represented as image observations. We match these with group actions\r
on policies. Since we consider discrete action spaces, a group element\r
g acting on a policy π will be represented as a matrix multiplication\r
of the policy with a permutation matrix.\r
Figure 3.3.1: The orbit of a traffic\r
light state under the group of 90 de\u0002gree rotations.\r
When discussing symmetries in decision making problems, we iden\u0002tify sets of state-action pairs that are equivalent: if the state is trans\u0002formed, the policy should be transformed as well, but potentially\r
with a different representation of the transformation. See Figure 3.1.1.\r
We are interested in the case where the reward and transition func\u0002tions are invariant in the orbit of state-action pairs under a symmetry\r
group. The orbit of a point v ∈ V, with V a vector space, is the set\r
of all its transformations (e.g. all rotations of the point), defined as\r
O(v) = {gv|∀g ∈ G}. The orbit of a point under a group forms an\r
equivalence class. See Figure 3.3.1 for an example of an orbit of a\r
traffic light state."""

[[sections]]
number = "3.4"
title = "Distributing Symmetries over Multiple Agents"
text = """
Consider the cooperative traffic light control system in Figure 3.1.1\r
that contains transformation-equivalent global state-action pairs. We\r
first formalize global symmetries of the system similarly to symmetries\r
in a single agent MDP. Then we will discuss how we can formulate\r
distributed symmetries in a distributed MMDP. Finally, we introduce\r
Multi-Agent MDP Homomorphic Networks."""

[[sections]]
number = "46"
title = "Symmetries in MMDPs"
text = """
We define symmetries in an MMDP similar to an MDP with symme\u0002tries [173].\r
Definition 2 An MMDP is an MMDP with symmetries if reward and tran\u0002sition functions are invariant under a transformation group G. That is, the\r
MMDP has symmetries if there is at least one non-trivial group G of trans\u0002formations Lg : S → S and for every s, Ks\r
g\r
: A → A such that\r
R(s, a) = R(Lg[s], K\r
s\r
g\r
[a]) ∀g ∈ G,s ∈ S, a ∈ A, (3.2)\r
T(s, a,s\r
0\r
) = T(Lg[s], K\r
s\r
g\r
[a], Lg[s\r
0\r
]) ∀g ∈ G,s,s\r
0 ∈ S, a ∈ A. (3.3)\r
If two state-action pairs s, a and Lg[s], K\r
s\r
g\r
[a] obey Eq. 3.2 and 3.3, then\r
they are equivalent [173]. Consider as an example the symmetries in\r
Figure 3.1.1. These symmetries can result in correspondences across\r
agents, for example when the observation of agent i is mapped by\r
the symmetry to another agent j that is arbitrarily far away and with\r
which there is no communication channel. In the next section, we will\r
resolve this problem by defining distributed symmetries in terms of\r
local observations and the communication graph defined by the state.\r
If we have an MMDP with symmetries, that means that there are sym\u0002metric optimal policies, i.e. if the state of the MMDP transforms, the\r
policy transforms accordingly. The above definition of an MMDP with\r
symmetries is only applicable to the centralized setting. If we want\r
to be able to execute policies in a distributed manner, we will need to\r
enforce equivariance in a distributed manner.\r
Distributed Multi-Agent Symmetries\r
In a distributed MMDP, agents make decisions based on local infor\u0002mation only, i.e. the local states they observe, and the communications\r
they receive from neighbors, defined as follows:\r
Definition 3 A Distributed Multiagent Markov Decision Process (Dis\u0002tributed MMDP) (N , S, A, T, R) is an MMDP where agents can communi\u0002cate as specified by a graph G = (V, E) with one node vi ∈ V per agent\r
and an edge (i, j) ∈ E if agents i and j can communicate. Thus, S =\r
({Si}i∈N , {Eij}(i,j)∈E ), with Sithe set of state features observable by agent\r
i, which may include shared global features, and Eij the set of edge features\r
between i and j. In a distributed MMDP, each agent’s action can only depend\r
on the local state and the communications it receives1."""

[[sections]]
number = "1"
title = "Communication is constrained, i.e."
text = """
agents cannot simply share their full ob\u0002servations with each other.

47\r
Here, we focus on Distributed MMDPs which have a spatial compo\u0002nent, i.e. each agent has a coordinate in some space, and the attributes\r
of the edges between the agents in the communication graph contain\r
spatial information as well. For instance, the attributes eij ∈ E for edge\r
(i, j) might be the difference vector between agent i and agent j’s co\u0002ordinates. Since both agent observations and interaction edges have\r
spatial features, a global symmetry will affect both the agent observa\u0002tions, the agent locations, and the features on the interaction edges.\r
See Figure 3.4.1.\r
permute observations transform edges rotate observations\r
original state symmetric state\r
Figure 3.4.1: Example of how\r
a global transformation of a dis\u0002tributed traffic light control state\r
can be viewed as 1) a permutation\r
of the observations over the agents,\r
2) a permutation of the interaction\r
edges, 3) a transformation of the lo\u0002cal observations.\r
To allow a globally equivariant policy network with distributed exe\u0002cution, we might naively decide to restrict each agent’s local policy\r
network to be equivariant to local transformations. However, this does\r
not give us the correct global transformation, as joining the local trans\u0002formations does not give us the same as the global transformations, as\r
illustrated in Figure 3.4.2.\r
Instead, to get the correct transformation as shown in the left side\r
of Figure 3.4.2, the local state is transformed, but also its position is\r
changed, which can be seen as a permutation of the agents and their\r
neighbors. To give an example of the equivariance constraint that we\r
want to impose: the lower left agent (before transformation) should\r
select an action based on its local state and communication received\r
from its northern and eastern neighbor, while the top left agent (after\r
transformation) should select the transformed version of the action\r
based on its rotated local state and communication from its eastern\r
and southern neighbor.\r
Since the agent has no other information about the system, if the lo\u0002cal observations are transformed (e.g. rotated), and the messages it\r
receives are transformed similarly, then from a local perspective the\r
agent is in an equivalent state and should execute the same policy, but\r
with an equivalently transformed action.\r
From the perspective of our agent and all its neighbors, the equiva\u0002lence holds for this local subgraph as well: if the observations and

48\r
Figure 3.4.2: Example of the dif\u0002ference between a global transfor\u0002mation on the global state, and a\r
set of local transformations on lo\u0002cal states. On the left we rotate\r
the entire world by 90 degrees clock\u0002wise, which involves rotating cross\u0002ings and streets. On the right we\r
perform local uncoordinated trans\u0002formations only at the street level.\r
The latter is not a symmetry of the\r
problem. local interactions rotate relative to each other, then the whole subgraph\r
rotates. See Figure 3.4.1. Thus, as long as the transformations are\r
applied to the full set of observations and the full set of communica\u0002tions, we have a global symmetry. We therefore propose the following\r
definition of a Distributed MMDP with Symmetries.\r
Definition 4 Let s = ({si}i∈N , {eij}(i,j)∈E ). Then a Distributed MMDP\r
with symmetries is a Distributed MMDP for which the following equations\r
hold for at least one non-trivial set of group transformations Lg : S → S and\r
for every s, Ks\r
g\r
: A → A such that\r
R(s, a) = R(Lg[s], K\r
s\r
g\r
[a]) ∀g ∈ G, s ∈ S, a ∈ A (3.4)\r
T(s, a, s\r
0\r
) = T(Lg[s], K\r
s\r
g\r
[a], Lg[s\r
0\r
]) ∀g ∈ G, s, s\r
0 ∈ S, a ∈ A (3.5)\r
where equivalently to acting on s with Lg, we can act on the interaction and\r
agent features separately with L˜\r
g and Ug, to end up in the same global state:\r
Lg[s] = (P\r
s\r
g\r
[{L˜\r
g[si\r
]}i∈N ], P\r
e\r
g\r
[{Ug[eij]}(i,j)∈E ]) (3.6)\r
for L˜\r
g : Si → Si\r
, Ug : E → E, and Ps\r
g\r
and Pe\r
g\r
the state and edge permuta\u0002tions.\r
Here, E is the set of edge features. The symmetries acting on the agents\r
and agent interactions in the Distributed MMDP are a class of symme\u0002tries we call distributable symmetries. We have now defined a class of\r
Distributed MMDPs with symmetries for which we can distribute a\r
global symmetry into a set of symmetries on agents and agent inter\u0002actions. This distribution allows us to define distributed policies that\r
respect the global symmetry.\r
Multi-Agent MDP Homomorphic Networks\r
We have shown above how distributable global symmetries can be de\u0002composed into local symmetries on agent observations and agent inter-

49\r
actions. Here, we discuss how to implement distributable symmetries\r
in multi-agent systems in practice.\r
General Formulation\r
We want to build a neural network that1) allows distributed execution,\r
so that we can compute policies without a centralized controller2) al\u0002lows us to pass communications between agents (agent interactions),\r
to enable coordination and3) exhibits the following global equivari\u0002ance constraint: π~ θ (Lg[s]) = Hsg[π~ θ (s)] (3.7)\r
withH\r
sg\r
:Π→Π. Thus, the policy networkπ~ that outputs the joint\r
policy must be equivariant under group transformations of the global\r
state. To satisfy1) and2), i.e. allowing distributed execution and\r
agent-to-agent communication, as well as permutation equivariance,\r
we formulate the network as a message passing network (MPN), but\r
with global equivariance constraints. Hsg [π~ ] = MPN\r
θ\r
(L\r
g\r
[s])(3.8)\r
Since a network is end-to-end equivariant if all its layers are equivari\u0002ant with matching representations [34], we require layer-wise equiv\u0002ariance constraints on the layers. A single layer in an MPN is given by\r
a set of node updates, i.e.f\r
(l+1)\r
i\r
=φ\r
u\r
\u0010\r
f\r
(l)\r
i\r
,\r
∑\r
|Ni| j=1\r
φ\r
m\r
\u0010\r
eij,f\r
(l)\r
j\r
\u0011\u0011, with\r
f\r
(l)\r
j\r
the current encoding of agentj in layerl,φ\r
m the message function\r
that computesm\r
j→i based on the edge\r
eij and the current encoding\r
of agentj, andφ\r
u the node update function that updates agent\r
i’s\r
current encoding based onf\r
(l)\r
i\r
and the aggregated received message\r
m\r
(l)\r
i\r
. Since the layer is given by a set of node updates, the equivari\u0002ance constraint is on the node updates. In other words, φu must be\r
an equivariant function of local encodingf\r
(l)\r
i\r
and aggregated message\r
m\r
(l)\r
i\r
:\r
Pg\r
h\r
φ\r
u\r
(f\r
(l)\r
i\r
,m\r
(l)\r
i\r
)\r
i\r
=φ\r
u\r
\u0010\r
L\r
g\r
[f\r
(l)\r
i\r
],L\r
g\r
[m\r
(l)\r
i\r
]\r
\u0011\r
(3.9)\r
Thus, the node update functionφ\r
u is constrained to be equivariant to\r
transformations of inputsf\r
(l)\r
i\r
andm\r
(l)\r
i\r
. Therefore, to conclude that\r
the outputs ofφ\r
u transform according to\r
Pg, we only need to enforce\r
that its inputsfi andm\r
i\r
transform according toL\r
g. Thus, the sub\u0002function φm that computes the messages m(l) i must be constrained to\r
be equivariant as well. Sinceφ\r
m takes as input the previous layer’s\r
encodings as well as the edgeseij, this means that1) the encodings\r
must contain geometric information about the state, e.g. which rota\u0002tion the local state is in and 2) the edge attributes contain geometric

50\r
information as well, i.e. they transform when the global state trans\u0002forms (Appendix 3.B).\r
Lg\r
h\r
m\r
(l)\r
i\r
i\r
=\r
|Ni|\r
∑\r
j=1\r
φm\r
\u0010\r
Ug\r
\u0002\r
eij\u0003\r
, Lg[ f\r
(l)\r
j\r
]\r
\u0011\r
(3.10)\r
Note that this constraint is satisfied when φm is equivariant, since lin\u0002ear combinations of equivariant functions are also equivariant [35].\r
Putting this all together, the local encoding f\r
(l)\r
i\r
for each agent is equiv\u0002ariant to the set of edge rotations and the set of rotations of encodings\r
in the previous layer. For more details, see Appendix 3.B. Thus, we\r
now have the general formulation of Multi-Agent MDP Homomor\u0002phic Networks. At execution time, the distributed nature of Multi\u0002Agent MDP Homomorphic Networks allows them to be copied onto\r
different devices and messages exchanged between agents only locally,\r
while still enforcing the global symmetries.\r
Multi-Agent MDP Homomorphic Network Architecture\r
Multi-Agent MDP Homomorphic Networks consist of equivariant lo\u0002cal observation encoders φe\r
: Si → R|G|×D, where G is the group,\r
|G| is its size, and D the dimension of the encoding, equivariant lo\u0002cal message functions φm : E × R|G|×D → R|G|×F where F is di\u0002mension of the message encoding, equivariant local update functions\r
φu : R|G|×D × R|G|×F → R|G|×D, and equivariant local policy predic\u0002tors φπ : R|G|×D → Π(Ai). Take the example of multi-agent traffic\r
light control with 90 degree rotation symmetries, which we evaluate\r
on in Section 3.5. In this setting, we wish to constrain φe to be equiv\u0002ariant to rotations Rg of the local observations. We will require the\r
outputs of φe to permute according to L\r
−1\r
g whenever the input rotates\r
by Rg.\r
Lg [φe(si)] = φe(Rg [si\r
]) ∀g ∈ G (3.11)\r
This has the form of a standard equivariance constraint, which allows\r
conventional approaches to enforcing group equivariance, e.g. [34].\r
In this chapter, we will enforce group equivariance using the Sym\u0002metrizer [173]. Before training, the Symmetrizer enforces group (e.g.\r
rotational) symmetries by projecting neural network weights onto an\r
equivariant subspace, and then uses SVD to find a basis for the equiv\u0002ariant subspace. Then, during and after training, the weight matrix of\r
the neural network is realised as a linear combination of equivariant\r
basis weights, and the coefficients of the linear combination are up\u0002dated during training with PPO [153]. We use ReLU non-linearities\r
and regular representations. For more details on the Symmetrizer, we\r
refer the reader to [173]."""

[[sections]]
number = "51"
title = "After encoding the input observations with the equivariant encoding"
text = """
functionφ\r
e\r
, we have an equivariant encoding of the local states that\r
has a compositional form: the rotation of the state is represented by the\r
ordering of group channels (see [173]) and the other state features are\r
represented by the information in those channels. Similarly, we con\u0002strain the message update functions to be equivariant to the permuta\u0002tion Lg in the group channels of the state encodings and the rotation Ug of a difference vector eij, representing the edge (i, j): Lg hφm \u0010eij, f (l) j\r
\u0011i\r
=φ\r
m\r
\u0010\r
Ug\r
\u0002\r
eij\r
\u0003\r
,L\r
g\r
h\r
f\r
(l)\r
j\r
i\u0011\r
(3.12)\r
Since linear combinations of equivariant functions are equivariant as\r
well [35], the aggregated messagem\r
(l)\r
i\r
=\r
∑\r
|Ni| j\r
m\r
j→i\r
is equivariant too.\r
Whileeij andf\r
(l)\r
j\r
transform under the same groupG, they do not\r
transform under the same group action:eij is a vector that transforms\r
with a rotation matrix, whereasf\r
(l)\r
j\r
transforms with a permutation of\r
group channels. The question arises how to build group equivariant\r
layers that transform both the edge and the agent features appropri\u0002ately. The method we use is to build equivariant layers using direct\r
sum representations, where the representationsUg andL\r
g are com\u0002bined as follows: Tg = Ug ⊕ Lg = "Ug 0 0 Lg# (3.13)\r
where 0 represents a zero-matrix of the appropriate size. Consider a\r
weight matrixW\r
l acting on\r
"\r
eij f (l\r
)\r
j\r
#\r
. The equivariance constraint then\r
becomesW\r
(l)\r
Tg=L\r
g\r
W\r
(l)\r
.\r
To preserve the geometric information coming from the messages, the\r
node update function is similarly constrained to be equivariant. Im\u0002portantly, the permutation on the outputs of φu must match the per\u0002mutation on the inputs of the next layer’s φm (i.e. the output of one\r
layer must use the same group representation as the input of the next\r
layer). This ensures that we can add multiple layers together while\r
preserving the geometric information. In practice, it is convenient to\r
use a single representationL\r
g for all permutation representations.\r
L\r
g\r
h\r
φ\r
u\r
(f\r
(l)\r
i\r
,m\r
(l)\r
i\r
)\r
i\r
=φ\r
u\r
\u0010\r
L\r
g\r
[f\r
(l)\r
i\r
],L\r
g\r
[m\r
(l)\r
i\r
]\r
\u0011\r
(3.14)\r
Finally, afterM layers (M message passing rounds), we output local\r
equivariant policies based on the state encodings at layerM using local\r
policy networkπ:\r
π\r
i\r
\u0010\r
L\r
g\r
h\r
f\r
(M)\r
i\r
i\u0011\r
=Pg\r
h\r
π\r
i\r
\u0010\r
f\r
(M)\r
i\r
\u0011i\r
(3.15)

52\r
Here, Pg is the permutation representation on the actions of the indi\u0002vidual agent, e.g. if in a grid world the state is flipped, Pg is the matrix\r
that permutes the left and right actions accordingly."""

[[sections]]
number = "3.5"
title = "Experiments"
text = """
The evaluation of Multi Agent MDP Homomorphic networks has a\r
singular goal: to investigate and quantify the effect of distributed ver\u0002sions of global equivariance in symmetric cooperative multi-agent re\u0002inforcement learning. We compare to three baselines. The first is a\r
non-homomorphic variant of our network. This is a standard MPN,\r
which is a permutation equivariant multi-agent graph network but\r
not equivariant to global rotations. The other two are variants with\r
symmetric data augmentation, in the spirit of [107, 99]. For a stochas\u0002tic data augmentation baseline, on each forward pass one of the group\r
elements is sampled and used to transform the input, and appropri\u0002ately transform the output as well. For a full augmentation baseline,\r
every state and policy is augmented with all its rotations in the group.\r
For evaluation, we use the common centralized training, decentralized\r
execution paradigm [100, 133] (see Appendix 3.A for more details).\r
We train in a centralized fashion, with PPO [153], which will adjust\r
the coefficients of the weight matrices in the network. The informa\u0002tion available to the actors and critics is their local information and the\r
information received from neighbors. We first evaluate on a wildlife\r
monitoring task, a variant of predator-prey type problems with pixel\u0002based inputs where agents can have information none of the other\r
agents have. Additionally, we evaluate the networks on the more com\u0002plex coordination problem of traffic light control, with pixel-based in\u0002puts. We focus on C4 as the discrete group to investigate whether\r
equivariance improves multi-agent systems, as C4 has been shown to\r
be effective in supervised learning and single-agent settings.\r
Wildlife Monitoring\r
0 200 400 600 800 1000\r
Time steps (x 500) \r
3\r
2\r
1\r
0\r
1\r
2"""

[[sections]]
number = "3"
title = "Average Return"
text = """
Standard MPN\r
Augmented MPN\r
Equivariant MPN\r
(a) 3 agents.\r
0 200 400 600 800 1000\r
Time steps (x 500) \r
3\r
2\r
1\r
0\r
1\r
2"""

[[sections]]
number = "3"
title = "Average Return"
text = """
Standard MPN\r
Augmented MPN\r
Equivariant MPN\r
(b) 4 agents.\r
Figure 3.5.1: Results for the dis\u0002tributed drone wildlife monitoring\r
task. 25%, 50% and 75% quan\u0002tiles shown over 15 random seeds.\r
All approaches tuned over 6 learn\u0002ing rates."""

[[sections]]
number = "53"
title = "Setup We evaluate on a distributed wildlife monitoring setup, where"
text = """
a set of drones has to coordinate to trap poachers. To trap a poacher,\r
one drone has to hover above them while the other assists from the\r
side, and for each drone that assists the team receives +1 reward. Two\r
drones cannot be in the same location at the same time. Since the\r
drones have only cameras mounted at the bottom, they cannot see\r
each other. The episode ends when the poacher is trapped by at least 2\r
drones, or 100 time steps have passed. On each time step the team gets\r
-0.05 reward. All agents (including the poacher) can stand still or move\r
in the compass directions. The poacher samples actions uniformly. We\r
train for 500k time steps. The drones can send communications to\r
drones within a 3 by 3 radius around their current location, meaning\r
that the problem is a distributed MMDP. Due to changing agent loca\u0002tions and the limited communication radius, the communication graph\r
is dynamic and can change between time steps. The observations are\r
21 by 21 images representing a agent-centric view of a 7 by 7 toroidal\r
grid environment that shows where the target is relative to the drone.\r
While the grid is toroidal, the communication distance is not: at the\r
edges of the grid, communication is blocked. This problem exhibits 90\r
degree rotations: when the global state rotates, the agents’ local poli\u0002cies should permute, and so should the probabilities assigned to the\r
actions in the local policies.\r
Results Results for this task are shown in Figure 3.5.1, with on the y\u0002axis the average return and on the x-axis the number of time steps. In\r
both the 3-agent and 4-agent case, using a Multi-Agent MDP Homo\u0002morphic Network improves compared to using MPNs without sym\u0002metry information, and compared to using symmetric data augmen\u0002tation. We conclude that in the proposed task, our approach learns\r
effective joint policies in fewer environment interactions compared to\r
the baselines.\r
Traffic Light Control\r
For a second experiment, we focus on a more complex coordination\r
problem: reducing vehicle wait times in traffic light control. Traffic\r
light control constitutes a longstanding and open problem (see [185]\r
for an overview): not only is the optimal coordination strategy non\u0002obvious, traffic light control is a problem where wrong decisions can\r
quickly lead to highly suboptimal states due to congestion. We use this\r
setting to answer the following question: does enforcing symmetries\r
help in complex coordination problems?

54\r
0 200 400 600 800 1000\r
Time steps (x 500) \r
8\r
9\r
10\r
11\r
12\r
13\r
14\r
15\r
16"""

[[sections]]
number = "17"
title = "Average Wait Time"
text = """
Standard MPN\r
Stoch. Augmented MPN\r
Full Augmented MPN\r
Equivariant MPN\r
Figure 3.5.2: Average vehicle wait\r
times for distributed settings of the\r
traffic light control task. Graphs\r
show 25%, 50% and 75% quantiles\r
over 20 independent training runs.\r
All approaches tuned over 6 learn\u0002ing rates.\r
Setup We use a traffic simulator with four traffic lights. On each of\r
eight entry roads, for 100 time steps, a vehicle enters the simulation on\r
each step with probability 0.1. Each agent controls the lights of a single\r
intersection and has a local action space of (grgr, rgrg), indicating\r
which two of its four lanes get a red or green light. Vehicles move\r
at a rate of one unit per step, unless they are blocked by a red light\r
or a vehicle. If blocked, the vehicle needs one step to restart. The\r
goal is reducing the average vehicle waiting time. The simulation ends\r
after all vehicles have exited the system, or after 500 steps. The team\r
reward is − 1\r
1000"""

[[sections]]
number = "1"
title = "C ∑c∈C w(c), with C the vehicles in the system and"
text = """
w(c) vehicle c’s cumulative waiting time.\r
Results We show results in Figure 3.5.2. While the standard MPN\r
architecture had reasonable performance on the toy problem, it takes\r
many environment interactions to improve the policy in the more com\u0002plex coordination problem presented by traffic light control. Adding\r
data augmentation helps slightly. However, we see that enforcing\r
the global symmetry helps the network find an effective policy much\r
faster. In this setting, the coordination problem is hard to solve: in ex\u0002periments with centralized controllers, the standard baseline performs\r
better, though it is still slower to converge than the equivariant cen\u0002tralized controller. Overall, enforcing global symmetries in distributed\r
traffic light control leads to effective policies in fewer environment in\u0002teractions.

55"""

[[sections]]
number = "3.6"
title = "E(3) Equivariance"
text = """
Many tasks can benefit from making use of their inherent symmetries\r
and structure. We saw in this Chapter that we can construct graph\r
networks that are equivariant to global transformations while still al\u0002lowing for distributed execution. Such networks make use of both the\r
structure and the symmetry of cooperative multi-agent problems.\r
In this Chapter and Chapter 2, we consider symmetries in decision\r
making problems, looking in particular at discrete groups of 2 or 4\r
elements using regular representations. However, many real world\r
problems exhibit symmetries that are continuous, for example as is\r
the case in robotics [203, 179, 136] or molecular design [157]. For that\r
reason, later work has looked at a broader class of symmetries in single\r
agent learning, such as the SE(2) [203], SO(2) [179], SO(3) [136, 157]\r
groups. In this Section, we discuss Steerable E(3) Equivariant graph\r
networks (SEGNNs), which take the ideas presented in this Chapter a\r
step further: we consider the E(3) group, the group of all isometries of\r
3d Euclidean space (rotations, reflections, and translations). SEGNNs\r
are a class of E(3) equivariant graph neural networks that use non\u0002linear convolutions. SEGNNs show promising results on physics and\r
chemistry tasks, where for example molecular data tends to exhibit\r
both graph structure and E(3) symmetry. See Figure 3.6.1.\r
Figure 3.6.1: Equivariance di\u0002agram for equivariant operator\r
φ for 3D molecular graph with\r
steerable node features. If the\r
molecule rotates, the node fea\u0002tures do as well.\r
Earlier work considered equivariant convolutional neural networks [34,\r
35, 193, 32, 98, 187, 14, 13, 186]. Similarly, there is earlier work on\r
equivariant GNN architectures [195, 101, 166, 7, 52, 50]. It turns out\r
that where early work tends to linearly transform the node and edge\r
features2followed by a non-linearity, Multi-Agent MDP Homomor\u00022 Pseudo-linearly for methods using at\u0002tention [52, 7]\r
phic Networks and SEGNNs use non-linear message aggregation. SEG\u0002NNs in particular also use steerable messages (rather than invariant or\r
discrete equivariant messages). SEGNNs use non-linear group convo\u0002lutions, and can be seen as a generalization of EGNNs [147], which\r
sends invariant messages, where SEGNNs send equivariant messages.\r
SEGNNs are able to perform non-linear convolutions with rotation-

56\r
ally invariant scalars or covariant vectors or tensors. Since molecu\u0002lar data sets can include node information such as velocity, force or\r
atomic spin, SEGNNs are evaluated on n-Body toy datasets, QM9, and\r
OC20, where they are able to use the geometric and physical attributes\r
present in the data. SEGNNs set a new state of the art on n-Body toy\r
datasets. Additionally, SEGNNs are a new state of the art in ISRE of\r
OC20 and competitive on QM9. For more details, see [23]. Seeing the\r
potential of SEGNNs on molecular prediction tasks, it is likely they can\r
provide possible benefits when adapted into Multi-agent MDP Homo\u0002morphic Networks for E(3) Equivariant multi-agent policy networks\r
following the ideas in this Chapter, for example for applications in\r
robotics or molecular design."""

[[sections]]
number = "3.7"
title = "Conclusion"
text = """
We consider distributed cooperative multi-agent systems that exhibit\r
global symmetries. In particular, we propose a factorization of global\r
symmetries into symmetries on local observations and local interac\u0002tions. On this basis, we propose Multi-Agent MDP Homomorphic\r
Networks, a class of policy networks that allows distributed execution\r
while being equivariant to global symmetries. We compare to non\u0002equivariant distributed networks, and show that global equivariance\r
improves data efficiency on both a predator-prey variant, and on the\r
complex coordination problem of traffic light control.\r
Scope We focus on discrete groups. For future work, this could be\r
generalized by using steerable representations, at the cost of not being\r
able to use pointwise nonlinearities. We also focus on discrete actions.\r
This might be generalized by going beyond action permutations, e.g.,\r
for 2D continuous worlds a continuous rotation of the actions. Fur\u0002thermore, our approach uses group channels for each layer (regular\r
representations). For small groups this is not an issue, but for much\r
larger groups this would require infeasible computational resources.\r
Finally, this work has focused on exact symmetries and considers im\u0002perfect symmetries and violations of symmetry constraints a promis\u0002ing future topic.

57"""

[[sections]]
number = "3.8"
title = "Ethics Statement"
text = """
Our work has several potential future applications, e.g. in autonomous\r
driving, decentralized smart grids or robotics. Such applications hope\u0002fully have a positive societal impact, but there are also risks of nega\u0002tive societal impact: through the application itself (e.g. military), labor\r
market impact, or by use in safety-critical applications without proper\r
verification and validation. These factors should be taken into account\r
when developing such applications."""

[[sections]]
number = "3.9"
title = "Reproducibility Statement"
text = """
To ensure reproducibility, we describe our setup in the Experiments\r
section and we include hyperparameters, group actions, and architec\u0002ture details in the Appendix. We will also make our code publicly\r
available."""

[[sections]]
number = "59"
title = "Chapter Appendix"
text = ""

[[sections]]
number = "3"
title = "A Message Passing Networks, Communication, and Dis\u0002tribution"
text = """
Message passing algorithms are commonly used to coordinate be\u0002tween agents while allowing for factorization of the global decision\r
making function [64, 105, 171, 18]. Message passing networks approx\u0002imate such message passing algorithms [199, 148]. Thus, we can view\r
message passing networks as a type of learned communication be\u0002tween coordinating agents. Message passing networks can be executed\r
using only local communication and computation. To see that this is\r
the case, consider the equations that determine the message passing\r
networks in this paper:\r
f\r
(0)\r
i = φe(si) (3.16)\r
m\r
(l)\r
j→i = φm(eij, f\r
(l)\r
j\r
) (3.17)\r
m\r
(l)\r
i =\r
|Ni|\r
∑\r
j\r
mj→i(3.18)\r
f\r
(l+1)\r
i = φu(f\r
(l)\r
i\r
, m\r
(l)\r
i\r
) (3.19)\r
for all agents i and layers (message passing rounds) l. In Eq. 3.16,\r
each agent encodes its local observation siinto a local feature vector\r
f\r
(0)\r
i\r
. In Eq. 3.17, each agent j computes its message to agent i using its\r
own local feature vector f\r
(l)\r
j\r
and its shared edge features eij = xi − xj.\r
After agent i receives its neighbors’ messages, it aggregates them in\r
Eq. 3.18. Finally, in Eq. 3.19 agent i updates its local feature vector\r
using the aggregated message and its local feature vector. Clearly,\r
every step in this network requires only local computation and local\r
communication, therefore allowing the network to be distributed over\r
agents at execution time.

60"""

[[sections]]
number = "3"
title = "B Equivariance of Proposed Message Passing Layers"
text = """
Recall that eij = xi − xj(the difference between the locations of agent i\r
and agent j). Therefore,\r
Ug[eij] = Ug[xi − xj] = Ug[xi] − Ug[xj] (3.20)\r
So, when both agent i and agent j are moved to a location transformed\r
by Ug, the edge features eij are transformed by Ug as well. If all agent\r
positions and observations rotate by the same underlying group, this\r
means the full system has rotated. In this paper, we place equivariance\r
constraints on the message function:\r
Kg[∑\r
j\r
φm(eij, fj)] = ∑\r
j\r
φm(Ug[eij], Lg[ fj]) (3.21)\r
This means that the messages are only permuted by Kg if both the\r
local features fj and the edge features eij are transformed (by Lg and\r
Ug respectively). To see that the proposed message passing layers are\r
equivariant to transformations on agent features and edge features,\r
consider the following example. Assume we have a message passing\r
layer φ consisting of node update function φu and message update\r
function φm, such that with agent features { fi}, and edge features {eij},\r
the output for agent i is given by\r
φ\r
(i)\r
\r
{eij}, { fj}\r
\u0001\r
= φu (fi\r
, mi) (3.22)\r
= φu\r
 \r
fi,∑\r
j\r
φm\r
\r
eij, fj\r
\u0001\r
!\r
(3.23)\r
Assume φu and φm are constrained to be equivariant in the following\r
way:\r
Pg[φu(fi, mi)] = φu(Lg[ fi], Kg[mi]) (3.24)\r
Kg[∑\r
j\r
φm(eij, fj)] = ∑\r
j\r
φm(Ug[eij], Lg[ fj]) (3.25)

61\r
for group elements g ∈ G. Then, for layer φ:\r
Pg\r
h\r
φ\r
(i)\r
\r
{eij}, { fj}\r
\u0001\r
i\r
= Pg [φu (fi\r
, mi)] (3.26)\r
= Pg\r
"\r
φu\r
 \r
fi,∑\r
j\r
φm\r
\r
eij, fj\r
\u0001\r
!# (3.27)\r
= φu\r
 \r
Lg [ fi\r
] , Kg\r
"\r
∑\r
j\r
φm\r
\r
eij, fj\r
\u0001\r
#! (3.28)\r
(using Eq. 3.24) (3.29)\r
= φu\r
 \r
Lg [ fi\r
] ,∑\r
j\r
φm\r
\r
Ug\r
\u0002\r
eij\u0003\r
, Lg\r
\u0002\r
fj\r
\u0003\u0001!\r
(3.30)\r
(using Eq. 3.25) (3.31)\r
= φ\r
(i)\r
\r
{Ug\r
\u0002\r
eij\u0003\r
}, {Lg\r
\u0002\r
fj\r
\u0003\r
}\r
\u0001\r
(3.32)"""

[[sections]]
number = "3"
title = "C Discrete Rotations of Continuous Vectors"
text = """
Here we outline how to build weight matrices equivariant to discrete\r
rotations of continuous vectors. Let eij = xj − xi be an arbitrary, con\u0002tinuous difference vector between the coordinates of agent j and the\r
coordinates of agent i. Then this difference vector transforms under 90\r
degree rotations using the group of standard 2D rotation matrices of\r
the form\r
Rg = R(θ) = "\r
cos θ − sin θ\r
sin θ cos θ\r
#\r
(3.33)\r
for θ ∈ [0, π\r
2\r
, π,\r
3π\r
2\r
]. A weight matrix W is now equivariant to 90\r
degree rotations of eij if\r
KgWeij = WRgeij (3.34)\r
with {Kg}g∈G e.g. a permutation matrix representation of the same\r
group. So,\r
W = K\r
−1\r
g WRg (3.35)\r
which we can solve using standard approaches."""

[[sections]]
number = "3"
title = "D Experimental Details"
text = """
For all approaches, including baselines, we run at least 15 random\r
seeds for 6 different learning rates, {0.001, 0.003, 0.0001, 0.0003, 0.00001,

62\r
0.00003}, and report the best learning rate for each. Other hyperpa\u0002rameters are taken as default in the codebase [159, 173].\r
Learning Rates Reported\r
After tuning the learning rate, we report the best one for each ap\u0002proach. See Table 3.D.1.\r
Distributed Settings Standard Augmented Equivariant\r
Drones, 3 agents 0.001 0.0003 0.001\r
Drones, 4 agents 0.0003 0.001 0.001\r
Traffic, 4 agents 0.0001 0.0001 0.0001\r
Table 3.D.1: Best learning rates\r
for distributed settings for\r
MPNs.\r
Assets Used\r
• Numpy [68] 1.19.2: BSD 3-Clause "New" or "Revised" License;\r
• PyTorch [137] 1.2.0: Modified BSD license;\r
• RLPYT [159]: MIT license;\r
• MDP Homomorphic Networks & Symmetrizer [173]: MIT license."""

[[sections]]
number = "3"
title = "E Architectural details"
text = """
Architectures are given below and were chosen to be as similar as\r
possible between different approaches, keeping the number of train\u0002able parameters comparable between approaches. We chose 2 message\r
passing layers to allow for 2 message passing hops. For the message\r
passing networks, we use L1-normalization of the adjacency matrix.\r
Architectural Overview\r
The global structure of our network is given in Figure 3.E.1.\r
G-CNN\r
z3\r
Equivariant \r
Message \r
Passing\r
ⲡN\r
ⲡ3\r
ⲡ2\r
ⲡ1\r
G-CNN\r
G-CNN\r
G-CNN\r
z2\r
z1\r
zN\r
xNx2\r
x1\r
x3\r
... ... ...\r
vN\r
v3\r
v2\r
v1...\r
Figure 3.E.1: General overview\r
of Multi-Agent MDP Homo\u0002morphic Networks. G-CNN\r
refers to a group-equivariant\r
CNN encoder. Equivariant\r
message passing refers to the\r
proposed equivariant message\r
passing networks. Encoding lo\u0002cal states with group-CNNs en\u0002sure the state encodings zi are\r
group-equivariant. The loca\u0002tions xi are input to the equiv\u0002ariant message passing network."""

[[sections]]
number = "63"
title = "Architectures"
text = """
Wildlife Monitoring\r
Listing 3.1: Equivariant Network Architecture for Centralized Drones"""

[[sections]]
number = "1"
title = "EqConv2d(repr_in=1, channels_in=m, repr_out=4, channels_out=b √"
text = """
16\r
4\r
c,\r
2 filter_size=(7, 7), stride=2, padding=0)"""

[[sections]]
number = "3"
title = "ReLU()"
text = ""

[[sections]]
number = "4"
title = "EqConv2d(repr_in=4, channels_in=b √"
text = """
16\r
4\r
c, repr_out=4, channels_out=b √\r
32\r
4\r
c,\r
5 filter_size=(5, 5), stride=1, padding=0)"""

[[sections]]
number = "6"
title = "ReLU()"
text = ""

[[sections]]
number = "7"
title = "GlobalMaxPool()"
text = ""

[[sections]]
number = "8"
title = "EqLinear(repr_in=4, channels_in=b √"
text = """
32\r
4\r
c, repr_out=4, channels_out=b\r
128\r
√\r
4\r
c)"""

[[sections]]
number = "9"
title = "ReLU()"
text = ""

[[sections]]
number = "10"
title = "EqLinear(repr_in=4, channels_in=b"
text = """
128\r
√\r
4\r
c, repr_out=5, channels_out=b √\r
64\r
4\r
c)"""

[[sections]]
number = "11"
title = "ReLU()"
text = ""

[[sections]]
number = "12"
title = "ModuleList([EqLinear(repr_in=4, channels_in=b √"
text = """
64\r
4\r
c, repr_out=5,\r
13 channels_out=1) for i in range(m)])"""

[[sections]]
number = "14"
title = "EqLinear(repr_in=4, channels_in=b √"
text = """
64\r
4\r
c, repr_out=1, channels_out=1)\r
Listing 3.2: CNN Architecture for Centralized Drones"""

[[sections]]
number = "1"
title = "Conv2d(channels_in=m, channels_out=16,"
text = "2 filter_size=(7, 7), stride=2, padding=0)"

[[sections]]
number = "3"
title = "ReLU()"
text = ""

[[sections]]
number = "4"
title = "Conv2d(channels_in=16,channels_out=32,"
text = "5 filter_size=(5, 5), stride=1, padding=0)"

[[sections]]
number = "6"
title = "ReLU()"
text = ""

[[sections]]
number = "7"
title = "GlobalMaxPool()"
text = ""

[[sections]]
number = "8"
title = "Linear(channels_in=32, channels_out=256)"
text = ""

[[sections]]
number = "9"
title = "ReLU()"
text = ""

[[sections]]
number = "10"
title = "ModuleList([Linear(channels_in=256,"
text = "11 channels_out=5) for i in range(m)])"

[[sections]]
number = "12"
title = "Linear(channels_in=256, channels_out=1)"
text = "Listing 3.3: Equivariant Network Architecture for Distributed Drones"

[[sections]]
number = "1"
title = "EqConv2d(repr_in=1, channels_in=1, repr_out=4, channels_out=b √"
text = """
16\r
4\r
c,\r
2 filter_size=(7, 7), stride=2, padding=0)"""

[[sections]]
number = "3"
title = "ReLU()"
text = ""

[[sections]]
number = "4"
title = "EqConv2d(repr_in=4, channels_in=b √"
text = """
16\r
4\r
c, repr_out=4, channels_out=b √\r
32\r
4\r
c,\r
5 filter_size=(5, 5), stride=1, padding=0)"""

[[sections]]
number = "6"
title = "ReLU()"
text = ""

[[sections]]
number = "7"
title = "GlobalMaxPool()"
text = ""

[[sections]]
number = "8"
title = "EqMessagePassingLayer(repr_in=4+4+2, channels_in=b √"
text = """
32\r
4\r
c, repr_out=4,\r
9 channels_out=b √\r
64\r
4\r
c)"""

[[sections]]
number = "10"
title = "ReLU()"
text = ""

[[sections]]
number = "11"
title = "EqMessagePassingLayer(repr_in=4+4+2, channels_in=b √"
text = """
64\r
4\r
c, repr_out=4,\r
12 channels_out=b √\r
64\r
4\r
c)

64"""

[[sections]]
number = "13"
title = "ReLU()"
text = ""

[[sections]]
number = "14"
title = "EqMessagePassingLayer(repr_in=4, channels_in=b √"
text = """
64\r
4\r
c, repr_out=5,\r
15 channels_out=1)"""

[[sections]]
number = "16"
title = "EqMessagePassingLayer(repr_in=4, channels_in=b √"
text = """
64\r
4\r
c, repr_out=1,\r
17 channels_out=1)\r
Listing 3.4: MPN Architecture for Distributed Drones"""

[[sections]]
number = "1"
title = "Conv2d(channels_in=1, channels_out=16,"
text = "2 filter_size=(7, 7), stride=2, padding=0)"

[[sections]]
number = "3"
title = "ReLU()"
text = ""

[[sections]]
number = "4"
title = "Conv2d(channels_in=16, channels_out=32,"
text = "5 filter_size=(5, 5), stride=1, padding=0)"

[[sections]]
number = "6"
title = "ReLU()"
text = ""

[[sections]]
number = "7"
title = "GlobalMaxPool()"
text = ""

[[sections]]
number = "8"
title = "MessagePassingLayer(channels_in=32+32+2, channels_out=64)"
text = ""

[[sections]]
number = "9"
title = "ReLU()"
text = ""

[[sections]]
number = "10"
title = "MessagePassingLayer(channels_in=64+64+2, channels_out=64)"
text = ""

[[sections]]
number = "11"
title = "ReLU()"
text = ""

[[sections]]
number = "12"
title = "Linear(channels_in=64, channels_out=5)"
text = ""

[[sections]]
number = "13"
title = "Linear(channels_in=64, channels_out=1)"
text = """
Traffic Light Control\r
Listing 3.5: Equivariant Network Architecture for Centralized Traffic"""

[[sections]]
number = "1"
title = "EqConv2d(repr_in=1, channels_in=3, repr_out=4, channels_out=b √"
text = """
16\r
4\r
c,\r
2 filter_size=(7, 7), stride=2, padding=0)"""

[[sections]]
number = "3"
title = "ReLU()"
text = ""

[[sections]]
number = "4"
title = "EqConv2d(repr_in=4, channels_in=b √"
text = """
16\r
4\r
c, repr_out=4, channels_out=b √\r
32\r
4\r
c,\r
5 filter_size=(5, 5), stride=1, padding=0)"""

[[sections]]
number = "6"
title = "ReLU()"
text = ""

[[sections]]
number = "7"
title = "GlobalMaxPool()"
text = ""

[[sections]]
number = "8"
title = "EqLinear(repr_in=4, channels_in=b √"
text = """
32\r
4\r
c, repr_out=4, channels_out=b\r
128\r
√\r
4\r
c)"""

[[sections]]
number = "9"
title = "ReLU()"
text = ""

[[sections]]
number = "10"
title = "EqLinear(repr_in=4, channels_in=b"
text = """
128\r
√\r
4\r
c, repr_out=5, channels_out=b √\r
64\r
4\r
c)"""

[[sections]]
number = "11"
title = "ReLU()"
text = ""

[[sections]]
number = "12"
title = "EqLinear(repr_in=4, channels_in=b √"
text = """
64\r
4\r
c, repr_out=8, channels_out=1)"""

[[sections]]
number = "13"
title = "EqLinear(repr_in=4, channels_in=b √"
text = """
64\r
4\r
c, repr_out=1, channels_out=1)\r
Listing 3.6: CNN Architecture for Centralized Traffic"""

[[sections]]
number = "1"
title = "Conv2d(channels_in=3, channels_out=16,"
text = "2 filter_size=(7, 7), stride=2, padding=0)"

[[sections]]
number = "3"
title = "ReLU()"
text = ""

[[sections]]
number = "4"
title = "Conv2d(channels_in=16,channels_out=32,"
text = "5 filter_size=(5, 5), stride=1, padding=0)"

[[sections]]
number = "6"
title = "ReLU()"
text = ""

[[sections]]
number = "7"
title = "GlobalMaxPool()"
text = ""

[[sections]]
number = "8"
title = "Linear(channels_in=32, channels_out=256)"
text = "65"

[[sections]]
number = "9"
title = "ReLU()"
text = ""

[[sections]]
number = "10"
title = "Linear(channels_in=256, channels_out=8)"
text = ""

[[sections]]
number = "11"
title = "Linear(channels_in=256, channels_out=1)"
text = "Listing 3.7: Equivariant Network Architecture for Distributed Traffic"

[[sections]]
number = "1"
title = "EqConv2d(repr_in=1, channels_in=3, repr_out=4, channels_out=b √"
text = """
16\r
4\r
c,\r
2 filter_size=(7, 7), stride=2, padding=0)"""

[[sections]]
number = "3"
title = "ReLU()"
text = ""

[[sections]]
number = "4"
title = "EqConv2d(repr_in=4, channels_in=b √"
text = """
16\r
4\r
c, repr_out=4, channels_out=b √\r
32\r
4\r
c,\r
5 filter_size=(5, 5), stride=1, padding=0)"""

[[sections]]
number = "6"
title = "ReLU()"
text = ""

[[sections]]
number = "7"
title = "GlobalMaxPool()"
text = ""

[[sections]]
number = "8"
title = "EqMessagePassingLayer(repr_in=4+4+2, channels_in=b √"
text = """
32\r
4\r
c, repr_out=4,\r
9 channels_out=b √\r
64\r
4\r
c)"""

[[sections]]
number = "10"
title = "ReLU()"
text = ""

[[sections]]
number = "11"
title = "EqMessagePassingLayer(repr_in=4+4+2, channels_in=b √"
text = """
64\r
4\r
c, repr_out=4,\r
12 channels_out=b √\r
64\r
4\r
c)"""

[[sections]]
number = "13"
title = "ReLU()"
text = ""

[[sections]]
number = "14"
title = "EqMessagePassingLayer(repr_in=4, channels_in=b √"
text = """
64\r
4\r
c, repr_out=2,\r
15 channels_out=1)"""

[[sections]]
number = "16"
title = "EqMessagePassingLayer(repr_in=4, channels_in=b √"
text = """
64\r
4\r
c, repr_out=1,\r
17 channels_out=1)\r
Listing 3.8: MPN Architecture for Distributed Traffic"""

[[sections]]
number = "1"
title = "Conv2d(channels_in=3, channels_out=16,"
text = "2 filter_size=(7, 7), stride=2, padding=0)"

[[sections]]
number = "3"
title = "ReLU()"
text = ""

[[sections]]
number = "4"
title = "Conv2d(channels_in=16, channels_out=32,"
text = "5 filter_size=(5, 5), stride=1, padding=0)"

[[sections]]
number = "6"
title = "ReLU()"
text = ""

[[sections]]
number = "7"
title = "GlobalMaxPool()"
text = ""

[[sections]]
number = "8"
title = "MessagePassingLayer(channels_in=32+32+2, channels_out=64)"
text = ""

[[sections]]
number = "9"
title = "ReLU()"
text = ""

[[sections]]
number = "10"
title = "MessagePassingLayer(channels_in=64+64+2, channels_out=64)"
text = ""

[[sections]]
number = "11"
title = "ReLU()"
text = ""

[[sections]]
number = "12"
title = "Linear(channels_in=64, channels_out=2)"
text = ""

[[sections]]
number = "13"
title = "Linear(channels_in=64, channels_out=1)"
text = """
Group Actions\r
Here we list the group actions used in different equivariant layers\r
throughout our experiments. For all equivariant layers, we use the\r
Symmetrizer [173] to find equivariant weight bases.\r
Rotation-equivariant Filters For all equivariant encoder networks, we\r
create 90 degree rotation-equivariant filters using np.rot90."""

[[sections]]
number = "66"
title = "Group Actions for Wildlife Monitoring"
text = """
Linear layers Permutation matrices representing the following permu\u0002tations:\r
e = [0, 1, 2, 3]\r
g1 = [3, 0, 1, 2]\r
g2 = [2, 3, 0, 1]\r
g3 = [1, 2, 3, 0]\r
Policy layers, centralized Permutation matrices representing the fol\u0002lowing permutations:\r
e = [0, 1, 2, 3, 4]\r
g1 = [0, 2, 3, 4, 1]\r
g2 = [0, 3, 4, 1, 2]\r
g3 = [0, 4, 1, 2, 3]\r
Value layers, centralized Permutation matrices representing the follow\u0002ing permutations:\r
e = [1]\r
g1 = [1]\r
g2 = [1]\r
g3 = [1]\r
Message Passing Layers Acting on state features, permutation matrices\r
representing the following permutations:\r
e = [0, 1, 2, 3]\r
g1 = [3, 0, 1, 2]\r
g2 = [2, 3, 0, 1]\r
g3 = [1, 2, 3, 0]\r
Acting on edge features, the following rotation matrices:\r
e=np.eye(2)\r
g1=np.array([[0, -1], [1, 0]])\r
g2=np.array([[-1, 0], [0, -1]])\r
g3=np.array([[0, 1], [-1, 0]])\r
Policy layers, distributed Permutation matrices representing the fol\u0002lowing permutations:\r
e = [0, 1, 2, 3, 4]\r
g1 = [0, 2, 3, 4, 1]\r
g2 = [0, 3, 4, 1, 2]

67\r
g3 = [0, 4, 1, 2, 3]\r
Value layers, distributed Permutation matrices representing the follow\u0002ing permutations:\r
e = [1]\r
g1 = [1]\r
g2 = [1]\r
g3 = [1]\r
Group Actions for Traffic Light Control\r
Linear layers Permutation matrices representing the following permu\u0002tations:\r
e = [0, 1, 2, 3]\r
g1 = [3, 0, 1, 2]\r
g2 = [2, 3, 0, 1]\r
g3 = [1, 2, 3, 0]\r
Policy layers, centralized Permutation matrices representing the fol\u0002lowing permutations:\r
e = [0, 1, 2, 3, 4, 5, 6, 7]\r
g1 = [5, 4, 1, 0, 7, 6, 3, 2]\r
g2 = [6, 7, 4, 5, 2, 3, 0, 1]\r
g3 = [3, 2, 7, 6, 1, 0, 5, 4]\r
Value layers, centralized Permutation matrices representing the follow\u0002ing permutations:\r
e = [1]\r
g1 = [1]\r
g2 = [1]\r
g3 = [1]\r
Message Passing Layers Acting on state features, permutation matrices\r
representing the following permutations:\r
e = [0, 1, 2, 3]\r
g1 = [3, 0, 1, 2]\r
g2 = [2, 3, 0, 1]\r
g3 = [1, 2, 3, 0]\r
Acting on edge features, the following rotation matrices:

68\r
e =np.eye(2)\r
g1 =np.array([[0, -1], [1, 0]])\r
g2 =np.array([[-1, 0], [0, -1]])\r
g3 =np.array([[0, 1], [-1, 0]])\r
Policy layers, distributed Permutation matrices representing the fol\u0002lowing permutations:\r
e = [0, 1]\r
g1 = [1, 0]\r
g2 = [0, 1]\r
g3 = [1, 0]\r
Value layers, distributed Permutation matrices representing the follow\u0002ing permutations:\r
e = [1]\r
g1 = [1]\r
g2 = [1]\r
g3 = [1]"""

[[sections]]
number = "69"
title = "Part II"
text = """
Structure

71"""

[[sections]]
number = "4"
title = "Plannable Approximations to"
text = "MDP Homomorphisms"

[[sections]]
number = "4.1"
title = "Introduction"
text = """
For this part of the dissertation, we turn our attention to the problem of\r
learning representations for decision making problems. In this Chap\u0002ter, we learn the structure of the environment, and propose the concept\r
of action-equivariance as a generalization of group-equivariance. In the\r
following Chapter we will capture the structure in individual states.\r
Dealing with high dimensional state spaces and unknown environ\u0002mental dynamics presents an open problem in decision making [70].\r
Classical dynamic programming approaches require knowledge of en\u0002vironmental dynamics and low dimensional, tabular state spaces [140].\r
Recent deep reinforcement learning methods on the other hand offer\r
good performance, but often at the cost of being unstable and sample\u0002hungry [70, 124, 77, 125]. The deep model-based reinforcement learn\u0002ing literature aims to fill this gap, for example by finding policies after\r
learning models based on input reconstruction [103, 66, 202, 36], by us\u0002ing environmental models in auxiliary losses [45, 77], or by forcing net\u0002work architectures to resemble planning algorithms [164, 132]. While\r
effective in learning end-to-end policies, these types of approaches\r
are not forced to learn good representations and may thus not build\r
proper environmental models. In this work, we focus on learning rep\u0002resentations of the world that are suitable for exact planning methods."""

[[sections]]
number = "72"
title = "To combine dynamic programming with the representational power of"
text = """
deep networks, we factorize the online decision-making problem into\r
a self-supervised model learning stage and a dynamic programming\r
stage.\r
We do this under the assumption that good representations minimize\r
MDP metrics [57, 46, 113, 165]. While such metrics have desirable theo\u0002retical guarantees, they require an enumerable state space and knowl\u0002edge of the environmental dynamics, and are thus not usable in many\r
problems. To resolve this issue, we propose to learn representations\r
using the more flexible notion of action equivariant mappings, where\r
the effects of actions in input space are matched by equivalent action\r
effects in the latent space. See Figure 4.1.1.\r
Figure 4.1.1: Visualization of\r
the notion of equivariance un\u0002der actions. We say Z is an\r
action equivariant mapping if\r
Z(T(s, a)) = K\r
s\r
g\r
(Z(s), A¯\r
s(a)).\r
We make the following contributions. First, we propose learning an\r
equivariant map and corresponding action embeddings. This corre\u0002sponds to using MDP homomorphism metrics [165] of deterministic\r
MDPs, enabling planning in the homomorphic image of the original\r
MDP. Second, we prove that for deterministic MDPs, when our loss\r
is zero, we have an MDP homomorphism [143]. This means that the\r
resulting policy can be lifted to the original MDP. Third, we provide\r
experimental evaluation in a variety of settings to show 1) that we can\r
recover the graph structure of the input MDP, 2) that planning in this

73\r
abstract space results in good policies for the original space, 3) that\r
we can change to arbitrary new goal states without further gradient\r
descent updates and 4) that this works even when the input states\r
are continuous, or when generalizing to new instances with the same\r
dynamics."""

[[sections]]
number = "4.2"
title = "Background"
text = """
Markov Decision Processes An infinite horizon Markov Decision Pro\u0002cess (MDP) is a tuple M = (S, A, R, T, γ), where s ∈ S is a Markov\r
state, a ∈ A is an action that an agent can take, R : S × A → R is\r
a reward function that returns a scalar signal r defining the desirabil\u0002ity of some observed transition, 0 ≤ γ ≤ 1 is a discount factor that\r
discounts future rewards exponentially and T : S × A × S → [0, 1]\r
is a transition function, that for a pair of states and an action assigns\r
a probability of transitioning from the first to the second state. The\r
goal of an agent in an MDP is to find a policy π : S × A → [0, 1],\r
a function assigning probabilities to actions in states, that maximizes\r
the return Gt = ∑\r
∞\r
k=0\r
γ\r
k\r
rt+k+1. The expected return of a state, action\r
pair under a policy π is given by a Q-value function Qπ : S × A → R\r
where Qπ(s, a) = Eπ [Gt\r
|st = s, at = a]. The value of a state under an\r
optimal policy π\r
∗\r
is given by the value function V\r
∗\r
: S → R, defined\r
as V\r
∗ = maxa Q∗\r
(s, a) under the Bellman optimality equation.\r
Value Iteration Value Iteration (VI) is a dynamic programming algo\u0002rithm that finds Q-values in MDPs, by iteratively applying the Bellman\r
optimality operator. This can be viewed as a graph diffusion where\r
each state is a vertex and transition probabilities define weighted edges.\r
VI is guaranteed to find the optimal policy in an MDP. For more de\u0002tails, see [140].\r
Bisimulation Metrics To enable computing optimal policies in MDPs\r
with very large or continuous state spaces, one approach is aggregat\u0002ing states based on their similarity in terms of environmental dynam\u0002ics [38, 113]. A key concept is the notion of stochastic bisimulations for\r
MDPs, which was first introduced by [38]. Stochastic bisimulation de\u0002fines an equivalence relation on MDP states based on matching reward\r
and transition functions, allowing states to be compared to each other.\r
Later work [46] observes that the notion of stochastic bisimulation is\r
too stringent (the dynamics must match exactly) and proposes using a

74\r
more general bisimulation metric instead, with the general form\r
d(s,s\r
0\r
) = max\r
a\r
\u0010\r
cR|R(s, a) − R(s\r
0\r
, a)| + cTdP(T(s, a), T(s\r
0\r
, a))\u0011(4.1)\r
where cR and cT are weighting constants, T(·, a) is a distribution over\r
next states and dP is some probability metric, such as the Kantorovich\r
(Wasserstein) metric. Such probability metrics are recursively com\u0002puted. For more details, see [46]. The bisimulation metric provides\r
a distance between states that is not based on input features but on\r
environmental dynamics.\r
MDP Homomorphism A generalization of the mapping induced by\r
bisimulations is the notion of MDP homomorphisms [143]. MDP ho\u0002momorphisms were introduced by [141] as an extension of [38]. An\r
MDP homomorphism (σ, {αs|s ∈ S}) is a tuple of functions 
Z,\r
\b\r
A¯\r
s\r
\t\u000B\r
with Z : S → Z a function that maps states to abstract states, and\r
each A¯\r
s\r
: A → A¯ a state-dependent function that maps actions to ab\u0002stract actions, that preserves the structure of the input MDP. We use\r
the definition given by [143]:\r
Definition 5 (Stochastic MDP Homomorphism) A Stochastic MDP ho\u0002momorphism from a stochastic MDP M = hS, A, T, Ri to an MDP M¯ = D\r
Z, A¯, K\r
s\r
g\r
, R¯\r
E\r
is a tuple (σ, {αs|s ∈ S}) = 
Z,\r
\b\r
A¯\r
s\r
\t\u000B , with\r
• Z : S → Z the state embedding function, and\r
• A¯\r
s\r
: A → A¯ the action embedding functions,\r
such that the following identities hold:\r
∀s,s\r
0∈S,a∈A K\r
s\r
g\r
(Z(s\r
0\r
)|Z(s), A¯\r
s(a)) = ∑\r
s\r
00∈[s0\r
]Z\r
T(s\r
00|s, a) (4.2)\r
∀s∈S,a∈A R¯(Z(s), A¯\r
s(a)) = R(s, a) (4.3)\r
Here, [s\r
0\r
]Z = Z\r
−1\r
(Z(s\r
0\r
)) is the equivalence class of s0 under Z.\r
We specifically consider deterministic MDPs. In that case:\r
Definition 6 (Deterministic MDP Homomorphism) A Deterministic MDP\r
homomorphism from a deterministic MDP M = hS, A, T, Ri to an MDP\r
M¯ =\r
D\r
Z, A¯, K\r
s\r
g\r
, R¯\r
E\r
is a tuple (σ, {αs|s ∈ S}) = 
Z,\r
\b\r
A¯\r
s\r
\t\u000B , with\r
• Z : S → Z the state embedding function, and\r
• A¯\r
s\r
: A → A¯ the action embedding functions,

75\r
such that the following identities hold:\r
∀s,s\r
0∈S,a∈A T(s, a) = s\r
0 =⇒ Ks\r
g\r
(Z(s), A¯\r
s(a)) = Z(s\r
0\r
) (4.4)\r
∀s∈S,a∈A R¯(Z(s), A¯\r
s(a)) = R(s, a) (4.5)\r
The states s are organized into equivalence classes under Z if they fol\u0002low the same dynamics in z-space. The MDP M¯ is referred to as the\r
homomorphic image of M under h [143]. An important property of MDP\r
homomorphisms is that a policy optimal in homomorphic image M¯\r
can be lifted to an optimal policy in M [143, 69]. Looking at these def\u0002initions, it may be clear that MDP homomorphisms and bisimulation\r
metrics are closely related. The difference is that the latter measures\r
distances between two MDP states, while the former is a map from\r
one MDP to another. However, the idea of forming a distance metric\r
by taking a sum of the distances can be extended to homomorphisms,\r
as proposed by [165]:\r
d((s, a),(Z(s), A¯\r
s(a))) = cR|R(s, a) − R¯(Z(s), A¯s(a))|\r
+ cTdP(ZT(s, a), K\r
s\r
g\r
(Z(s), A¯\r
s(a))), (4.6)\r
with dP a suitable measure of the difference between distributions (e.g.,\r
Kantorovich metric), and ZT(s, a) shorthand for projecting the distri\u0002bution over next states into the space of Z (see [54] for details). We\r
refer to this as the MDP homomorphism metric.\r
Action-Equivariance We define a mapping Z : S → Z to be action\u0002equivariant if Z(T(s, a)) = T¯(Z(s), A¯\r
s(a)) and R(s, a) = R¯(Z(s), A¯s(a)),\r
i.e. when the constraints in Eq. 4.4 and Eq. 4.5 hold."""

[[sections]]
number = "4.3"
title = "Learning MDP Homomorphisms"
text = """
We are interested in learning compact, plannable representations of\r
MDPs. We call MDP representations plannable if the optimal policy\r
found by planning algorithms such as VI can be lifted to the original\r
MDP and still be close to optimal. This is the case when the rep\u0002resentation respects the original MDP’s dynamics, such as when the\r
equivariance constraints in Eq. 4.4 and Eq. 4.5 hold. In this chapter we\r
leverage MDP homomorphism metrics to find such representations.\r
In particular, we introduce a loss function that enforces these equivari-

76\r
ance constraints, then construct an abstract MDP in the learned repre\u0002sentation space. We compute a policy in the abstract MDP M¯ using\r
VI, and lift the abstract policy to the original space. To keep things\r
simple, we focus on deterministic MDPs, but in preliminary experi\u0002ments our method performed well out of the box on stochastic MDPs.\r
Additionally, the framework we outline here can be extended to the\r
stochastic case, as [54] does for bisimulation metrics.\r
Learning State Representations\r
Here we show how to learn state representations that respect action\u0002equivariance. We embed the states in S into Euclidean space using a\r
contrastive loss based on MDP homomorphism metrics. Similar losses\r
have often been used in related work [54, 94, 6, 135, 51], which we\r
compare in Section 4.5. We represent the mapping Z using a neural\r
network parameterized by θ, whose output will be denoted Zθ. This\r
function maps a state s ∈ S to a latent representation z ∈ Z ⊆ RD.\r
We additionally approximate the abstract transition K\r
s\r
g by a function\r
T¯\r
φ : Z × A → Z ¯ parameterized by φ, and the abstract rewards R¯\r
by a neural network R¯\r
ζ\r
: Z → R, parameterized by ζ, that predicts\r
the reward for an abstract state. From Eq. 4.5 we simplify to a state\u0002dependent reward using R(s) = R¯ (Z(s)) where R(s) is the reward\r
function that outputs a scalar value for an s ∈ S, and R¯ is its equiv\u0002alent in M¯ . During training, we first sample a set of experience tuples\r
D = {(st, at,rt,st+1)}\r
N\r
n=1\r
by rolling out an exploration policy πe for\r
K trajectories. To learn representations that respect Eq. 4.4 and 4.5,\r
we minimize the distance between the result of transitioning in ob\u0002servation space, and then mapping to Z, or first mapping to Z and\r
then transitioning in latent space (see Figure 4.1.1). Additionally, the\r
distance between the observed reward R(s) and the predicted reward\r
R¯\r
ζ (Zθ (s)) is minimized. We thus include a general reward loss term.\r
We write s\r
0\r
n = T(sn, an), zn = Zθ (sn), and minimize\r
L(θ, φ, ζ) = 1\r
N\r
N\r
∑\r
n=1\r
\u0014\r
d\r
\r
Zθ (s\r
0\r
n\r
), T¯\r
φ(zn, A¯φ(zn, an))\u0001\r
+d\r
\r
R(sn), R¯\r
ζ (zn)\r
\u0001\r
\u0015\r
(4.7)\r
by randomly sampling batches of experience tuples from D. In this\r
chapter, we use d(z, z\r
0\r
) = 1\r
2\r
(z − z\r
0\r
)\r
2\r
to model distances in Z ⊆ RD.\r
Here, T¯\r
φ is a function that maps a point in latent space z ∈ Z to a\r
new state z\r
0 ∈ Z by predicting an action-effect that acts on z. We adopt\r
earlier approaches of letting T¯\r
φ be of the form T¯φ(z, a¯) = z + A¯φ(z, a),\r
where A¯\r
φ(z, a) is a simple feedforward network [94, 51]. Thus A¯φ :\r
Z × A → A¯ is a function mapping from the original action space to

77\r
an abstract action space, and A¯\r
φ(z, a) approximates A¯s(a) (Eq. 4.4).\r
The resulting transition loss is a variant of the loss proposed in [94].\r
The function R¯\r
ζ\r
: Z → R predicts the reward from z. Since Z, K\r
s\r
g\r
and R¯ are neural networks optimized with SGD, Eq. 4.7 has a triv\u0002ial solution where all states are mapped to the same point, especially\r
in the sparse reward case. When the reward function is informative,\r
minimizing Eq. 4.7 can suffice, as is empirically demonstrated in [54].\r
However, when rewards are sparse, the representations may collapse\r
to the trivial embedding, and for more complex tasks [54] requires a\r
pixel reconstruction term. In practice, earlier works use a variety of\r
solutions to prevent the trivial map. Approaches based on pixel recon\u0002structions are common [182, 183, 36, 104, 67, 85, 66, 202, 54], but there\r
are also approaches based on self-supervision that use alternatives to\r
reconstruction of input states [6, 94, 51, 135, 3, 9, 200].\r
To prevent trivial solutions, we use a contrastive loss, maximizing the\r
distance between the latent next state and the embeddings of a set of\r
random other states, S¬ = {sj}\r
J\r
j=1\r
sampled from the same trajectory\r
on every epoch. Thus, the complete loss is\r
L(θ, φ, ζ) = 1\r
N\r
N\r
∑\r
n=1\r
\u0014\r
d\r
\r
Zθ (s\r
0\r
n\r
), T¯\r
φ(zn, A¯φ(zn, an))\u0001\r
+d\r
\r
R(sn), R¯\r
ζ (zn)\r
\u0001\r
+ ∑\r
s¬∈S¬\r
d¬\r
\r
Zθ (s¬), T¯\r
φ(zn, A¯φ(zn, an))\u0001\r
\u0015\r
(4.8)\r
where d¬ is a negative distance function. Similar to [94], we use the\r
hinge loss d¬(z, z\r
0\r
) = max(0, e − d(z, z\r
0\r
)) to prevent the negative dis\u0002tance from growing indefinitely. Here, e is a parameter that controls\r
the scale of the embeddings. To limit the scope of this chapter, we con\u0002sider domains where we can find a reasonable data set of transitions\r
without considering exploration. Changing the sampling policy will\r
introduce bias in the data set, influencing the representations. Here\r
we evaluate if we can find plannable MDP homomorphisms and leave\r
the exploration problem to future work.\r
Constructing the Abstract MDP\r
After learning a structured latent space, we find abstract MDP M¯ by\r
constructing reward and transition functions from Zθ, T¯\r
φ, R¯\r
ζ\r
.\r
Abstract States\r
Core to our approach is the idea that exploiting action-equivariance\r
constraints leads to nicely structured abstract spaces that can be planned"""

[[sections]]
number = "78"
title = "Figure 4.3.1: Schematic"
text = """
overview of our method.\r
We learn the map Z from S to\r
Z and discretize Z to obtain\r
X . We plan in X and use\r
interpolated Q-values to obtain\r
a policy in S.\r
in. Of course the space Z is still continuous, which requires either\r
more complex planning methods, or state discretization. In this chap\u0002ter we aim for the latter, simpler, option, by constructing a discrete\r
set X of (‘prototype’) latent states in Z over which we can perform\r
standard dynamic programming techniques. We will denote such pro\u0002totype states as x ∈ X , cf. Figure 4.3.1. Of course, we then also need\r
to construct discrete transition Tˆ\r
φ and reward Rˆ\r
ζ\r
functions. The next\r
sub-sections will outline methods to obtain these from Z, T¯\r
φ and R¯\r
ζ\r
.\r
To find a ‘plannable’ set of states, the abstract state space should be\r
sufficiently covered. To construct the set, we sample L states from\r
the replay memory and encode them, i.e. X = {Zθ (sl)|sl ∼ D}L\r
l=1\r
,\r
pruning duplicates.\r
Reward Function\r
In Eq. 4.8 we use a reward prediction loss to encourage the latent states\r
to contain information about the rewards. This helps separate distinct\r
states with comparable transition functions. During planning, we can\r
use this predicted reward R¯\r
ζ\r
. When the reward depends on a chang\u0002ing goal state, such as in the goal-conditioned tasks in Section 3.5,\r
Rˆ\r
ζ (Zθ (s)) = 1 if Zθ (s) = Zθ (sg) and 0 otherwise. We use this reward\r
function in planning, i.e. Rˆ\r
ζ (x) = 1 if x = Zθ (sg) and 0 otherwise.\r
Transition Function\r
We model the transitions on the basis of similarity in the abstract space.\r
We follow earlier work [53, 92] and assume that if two states are con\u0002nected by an action in the state space, they should be close after apply\u0002ing the latent action transition. The transition function is a distribu\u0002tion over next latent states. Therefore, we use a temperature softmax\r
to model transition probabilities between representations of abstract\r
states in X :\r
Tˆ\r
0\r
φ\r
(zj|zi, α) = e\r
−d(zj,zi+A¯\r
φ(zi\r
,α))/τ\r
∑k∈X e\r
−d(zk,zi+A¯\r
φ(zi\r
,α))/τ\r
(4.9)"""

[[sections]]
number = "79"
title = "Thus, for the transitions between abstract states:"
text = """
Tˆ\r
φ(x = j|x\r
0 = i, aˆ = α) = Tˆ0\r
φ\r
(zj|zi, α) (4.10)\r
where τ is a temperature parameter that determines how ‘soft’ the\r
edges are, and zjis the representation of abstract state j. Intuitively,\r
this means that if an action moves two states closer together, the weight\r
of their connection increases, and if it moves two states away from each\r
other, the weight of their connection decreases. For very small τ, the\r
transitions are deterministic.\r
Convergence to an MDP homomorphism\r
We now show that when combining optimization of our proposed loss\r
fuction equation 4.8 with the construction of an abstract MDP as de\u0002tailed in this subsection, we can approximate an MDP homomorphism.\r
Specifically, for deterministic MDPs, we show that when the loss func\u0002tion in Eq. 4.8 reaches zero, we have an MDP homomorphism of M.\r
Theorem 1 In a deterministic MDP M, assuming a training set that con\u0002tains all state, action pairs, and an exhaustively sampled set of abstract states\r
X we consider a sequence of losses in a successful training run, i.e. the losses\r
converge to 0. In the limit of the loss L in Eq. 4.8 approaching 0, i.e. L → 0\r
and 0 < τ \u001C 1, τ \u001C e, (σ, {αs|s ∈ S}) = (Zθ, A¯\r
φ) is an MDP homomor\u0002phism of M.\r
Proof 4 Fix 0 < τ \u001C 1 and write z = Zθ (s) and a¯ = A¯\r
φ(z, a). Consider\r
that learning converges, i.e. L → 0. This implies that the individual loss\r
terms d(T¯\r
φ(z, a¯), z\r
0\r
), d¬(T¯\r
φ(z, a¯), z¬) and d(R(s), R¯\r
ζ (z)) also go to zero for\r
all (s, a,r,s\r
0\r
,s¬) ∼ D.\r
Positive samples: As the distance for positive samples d+ =\r
d(T¯\r
φ(z, a¯), z\r
0\r
) → 0, then d+ \u001C τ. Since d+ \u001C τ, then e−d+/τ ≈ 1.\r
Negative samples: Because the negative distance d¬(T¯\r
φ(z, a¯), z¬) → 0,\r
d¬ ≤ e. This, in turn, implies that the distance to all negative samples\r
d− = d(T¯\r
φ(z, a¯), z¬) ≥ e and thus τ \u001C e ≤ d−, meaning that 1 \u001C d−\r
τ\r
and\r
thus e−d−/τ ≈ 0.\r
This means that when the loss approaches 0, Tˆ0\r
φ\r
(z\r
0\r
|z, a¯) = 1 where\r
T(s\r
0\r
|s, a) = 1 and Tˆ0\r
φ\r
(z¬|z, a¯) = 0 when T(s¬|s, a) = 0. Since M\r
is deterministic, T(s\r
0\r
|s, a) transitions to one state with probability 1, and\r
probability 0 for the others. Therefore, Tˆ0\r
φ\r
(Zθ (s\r
0\r
)|Zθ (s), A¯\r
φ(Zθ (s), a)) =\r
∑s\r
00∈[s0\r
]Z\r
T(s\r
00|s, a) and Eq. 4.4 holds. As the distance for rewards\r
d(R(s), R¯\r
ζ (z)) → 0, we have that R¯ζ (z) = R(s) and Eq. 4.5 holds. There\u0002fore, when the loss reaches zero we have an MDP homomorphism of M.\r
Note that Eq. 4.8 will not completely reach zero: negative samples are\r
drawn uniformly. Thus, a positive sample may occasionally be treated

80\r
as a negative sample. Refining the negative sampling can further im\u0002prove this approach.\r
Planning and Acting\r
After constructing the abstract MDP we plan with VI [140] and lift\r
the found policy to the original space by interpolating between Q\u0002value embeddings. Given Mˆ = (X , Aˆ, Tˆ\r
φ, Rˆ\r
ζ\r
, γ), VI finds a policy\r
πˆ that is optimal in Mˆ . For a new state s\r
∗ ∈ S, we embed it in\r
the representation space Z as z\r
∗ = Zθ (s∗\r
) and use a softmax over its\r
distance to each x ∈ X to interpolate between their Q-values, i.e.\r
Q(z\r
∗\r
, a) = ∑\r
x∈X\r
w(z\r
∗\r
, x)Q(x, a) (4.11)\r
w(z\r
∗\r
, x) = e\r
−d(zx,z\r
∗\r
)/η\r
∑k∈X e\r
−d(zk,z\r
∗)/η\r
(4.12)\r
where η is a temperature parameter that sets the ‘softness’ of the inter\u0002polation. We use the interpolated Q-values for greedy action selection\r
for s\r
∗\r
, transition to s\r
∗∗ and iterate until the episode ends."""

[[sections]]
number = "4.4"
title = "Experiments"
text = """
Here we show that in simple domains, our approach 1) succeeds at\r
finding plannable MDP homomorphisms for discrete and continuous\r
problems 2) requires less data than model-free approaches, 3) gener\u0002alizes to new reward functions and data and 4) trains faster than ap\u0002proaches based on reconstructions. We focus on deterministic MDPs.\r
While preliminary results on stochastic domains were promising, an\r
in-depth discussion is beyond the scope of this chapter.\r
Baselines\r
To evaluate our approach, we compare to a number of baselines:"""

[[sections]]
number = "1"
title = "WM-AE: An auto-encoder approach inspired by World Models [66]."
text = """
We follow their approach of training representations using a recon\u0002struction loss, then learning latent dynamics on fixed representa\u0002tions. We experimented with a VAE [91], which did not perform\r
well (see [94] for similar results). We thus use an auto-encoder to\r
learn an embedding, then train an MLP to predict the next state\r
from embedding and action."""

[[sections]]
number = "2"
title = "LD-AE: An auto-encoder with latent dynamics. We train an auto-"
text = """
81\r
encoder to reconstruct the input, and predict the next latent state.\r
We experimented with reconstructing the next state, but this re\u0002sulted in the model placing the next state embeddings in a different\r
location than the latent transitions.\r
3. DMDP-H: We evaluate the effectiveness of training without nega\u0002tive sampling. This is similar to DeepMDP [54]. However, unlike\r
DeepMDP, DMDP-H uses action-embeddings, for a fairer compari\u0002son.\r
4. GC-Model-Free: Finally, we compare to a goal-conditioned model\u0002free baseline (REINFORCE with state-value baseline), to contrast\r
our approach with directly optimizing the policy1. We include the"""

[[sections]]
number = "1"
title = "Deep reinforcement learning algo\u0002rithms such as our baseline may fail"
text = """
catastrophically depending on the ran\u0002dom seed [70]. For a fair compari\u0002son, we train the baseline on 6 random\r
seeds, then remove those seeds where\r
the method fails to converge for the train\r
setting.\r
goal state as input for a fair comparison.\r
To fairly compare the planning approaches, we perform a grid search\r
over the softness of the transition function by evaluating performance\r
on the train goals in τ ∈ [1, 0.1, 0.001, 0.0001, 0.00001, 1e − 20]. Unless\r
otherwise stated, the planning approaches are all trained on datasets\r
of 1000 trajectories, sampled with a random policy. The learning rate\r
is set to 0.001 and we use Adam [90]. For the hinge loss, we use e = 1.\r
The latent dimensionality is set to 50 everywhere. Our approach is\r
trained for 100 epochs. WM-AE is trained for 1000 epochs in total: 500\r
for the auto-encoder and 500 for the dynamics. LD-AE is trained for\r
1000 epochs. For constructing the abstract MDP we sample 1024 states\r
from D, project unto Z and prune duplicates. For planning we use VI\r
with discount factor γ = 0.9, 500 backups and interpolation parameter\r
(Eq. 4.12) η = 1e − 20. The learning rate for the model-free baseline\r
was chosen by fine-tuning on the training goals. For the model-free\r
baseline, we use a learning rate of 5e − 4 and we train for 500k steps\r
(more than five times the number of samples the planning approaches\r
use). Network Zθ has 2 convolutional layers (both 16 channels, 3 × 3\r
filters) and 3 fully connected layers (input→ 64 → 32 → |z|). Net\u0002works Tφ and Rξ each have 2 fully connected layers. We use ReLU\r
non-linearities between layers.\r
Object Collection\r
We test our approach on an object collection task inspired by the key\r
task in [51], with major differences: rather than searching for three\r
keys in a labyrinth, the agent is placed in a room with some objects.\r
Its task is to collect the key. On every time step, the agent receives a\r
state—a 3 × 48 × 48 pixel image (a channel per object, including the\r
agent), as shown in Figure 4.4.1—and a goal state of the same size."""

[[sections]]
number = "82"
title = "At train time, the agent receives reward of 1 on collection of the key"
text = """
object, and a reward of −1 if it grabs the wrong object, and a reward\r
of −0.1 on every time step. The episode ends if the agent picks up\r
one (or more) of the objects and delivers it to one of the four corners\r
(randomly sampled at episode start), receiving an additional delivery\r
reward of 1. At test time, the agent is tasked with retrieving one of\r
the objects chosen at random, and delivering to a randomly chosen\r
location, encoded as a desired goal state. This task will evaluate how\r
easily the trained agent adapts to new goals/reward functions. The\r
agent can interact with the environment until it reaches the goal or\r
100 steps have passed. For both tasks, we compare to the model-free\r
baseline. We also compare to the DMDP-H, WD-AE and LD-AE base\u0002lines. We additionally perform a grid search over the hinge, number of\r
Figure 4.4.1: Example states in\r
the object collection domain for\r
the single object and double ob\u0002ject tasks.\r
state samples for discretization and η hyperparameters for insight in\r
how these influence the performance. This showed that our approach\r
is robust with respect to the hinge parameter, but it influences the scale\r
of the embeddings. The results decrease only when using 256 or fewer\r
state samples. Lastly, η is robust for values lower than 1. We opt for\r
a low value of η, to assign most weight to the Q-value of the closest\r
state.\r
Single Object Task\r
We first evaluate a simple task with only one object (a key). The agent’s\r
task is to retrieve the key, and move to one of four delivery locations in\r
the corners of the room. The delivery location is knowledge supplied\r
to the agent in the form of a goal state that places the agent in the cor\u0002rect corner and shows that there is no key. These goal states are also\r
supplied to the baseline, during training and testing. Additionally, we\r
perform an ablation study on the effect of the reward loss. The average\r
episode lengths are shown in Table 4.4.1. Our approach outperforms"""

[[sections]]
number = "83"
title = "Avg. ep. length ↓"
text = """
Task Single Object Double Object\r
Goal Set Train Test Train Test\r
GC-Model-free 10.00 ± 0.11 67.25 ± 6.81 10.10 ± 0.69 38.25 ± 15.30\r
WM-AE 12.96 ± 8.93 10.03 ± 5.56 29.61 ± 19.42 22.53 ± 22.12\r
LD-AE 23.46 ± 27.10 21.04 ± 21.71 60.26 ± 29.14 52.72 ± 27.32\r
DMDP-H (J = 0) 82.88 ± 11.62 85.69 ± 7.98 81.24 ± 2.45 81.17 ± 2.69\r
Ours, J = 1, 8.61 ± 0.35 7.53 ± 0.24 8.53 ± 0.36 8.38 ± 0.07\r
Ours, J = 3 8.68 ± 0.27 7.63 ± 0.19 8.61 ± 0.38 8.95 ± 0.63\r
Ours, J = 5 8.57 ± 0.48 7.74 ± 0.22 8.26 ± 0.84 8.96 ± 1.15\r
Table 4.4.1: Comparing average\r
episode length of 100 episodes\r
on the object collection domain.\r
Reporting mean and standard\r
deviation over 5 random seeds\r
for the planning approaches.\r
The model free approach is av\u0002eraged over 4 random seeds for\r
the single object domain, 3 ran\u0002dom seeds for the double object\r
domain. all baselines, both at train and at test time. There is no clear prefer\u0002ence in terms of the number of negative samples — as long as J > 0\r
— the result for all values of J are quite close together. The DMDP-H\r
approach fails to find a reasonable policy, possibly due to the sparse\r
rewards in this task providing little pull against state collapse. Out of\r
the planning baselines, WM-AE performs best, probably because visu\u0002ally salient features are aligned with decision making features in this\r
task. Finally, the model-free approach is the best performing baseline\r
on the training goals, but does not generalize to test goals.\r
The results of the reward ablation are shown in Table 4.4.2. While\r
75\r
50\r
25\r
0\r
25\r
50\r
75\r
100\r
60 40 20 0 20 40 60 80\r
80\r
60\r
40\r
20\r
0\r
20\r
40\r
60\r
80\r
4\r
5\r
6\r
7\r
8"""

[[sections]]
number = "9"
title = "Value"
text = """
(a) WM-AE Base\u0002line\r
20 15 10 5\r
0\r
5\r
10 15 20\r
10\r
5\r
0\r
5\r
10\r
8\r
6\r
4\r
2\r
0\r
2\r
4\r
6\r
8\r
0\r
1\r
2\r
3\r
4"""

[[sections]]
number = "5"
title = "Value"
text = """
(b) LD-AE Base\u0002line\r
0.02\r
0.00\r
0.02\r
0.04\r
0.06\r
0.08\r
0.10 0.02 0.00 0.02 0.04 0.06\r
0.03\r
0.02\r
0.01\r
0.00\r
0.01\r
0.02\r
0.03\r
0.04\r
0\r
2\r
4\r
6"""

[[sections]]
number = "8"
title = "Value"
text = """
(c) DMDP-H Base\u0002line\r
6\r
4\r
2\r
0\r
2\r
4\r
6\r
4\r
2\r
0\r
2\r
4\r
0.75\r
0.50\r
0.25\r
0.00\r
0.25\r
0.50\r
0.75\r
1.00\r
3\r
4\r
5\r
6\r
7\r
8"""

[[sections]]
number = "9"
title = "Value"
text = """
(d) This chapter\r
Figure 4.4.2: Abstract MDP for\r
three approaches in the single\r
object room domain. Nodes\r
are PCA projections of abstract\r
states, edges are predicted T¯\r
φ,\r
colors are predicted values.\r
removing the reward loss does not influence performance much for\r
J = 0, J = 3 and J = 5, when J = 1 the reward prediction is needed\r
to separate the states. Without the reward, the single negative sample\r
does not provide enough pull for complete separation.\r
Avg. ep. length ↓\r
Reward Loss No Reward Loss\r
Goal Set Train Test Train Test\r
DMDP-H (J = 0) 82.88 ± 11.62 85.69 ± 7.98 87.03 ± 3.08 84.08 ± 3.02\r
Ours, J = 1 8.61 ± 0.35 7.53 ± 0.24 74.32 ± 19.90 68.54 ± 17.29\r
Ours, J = 3 8.68 ± 0.27 7.63 ± 0.19 8.54 ± 0.36 7.44 ± 0.21\r
Ours, J = 5 8.57 ± 0.48 7.74 ± 0.22 8.52 ± 0.19 7.53 ± 0.20\r
Table 4.4.2: Ablation study of\r
the effect of the reward loss.\r
Comparing average episode\r
length of 100 episodes for the\r
single object room domain.\r
Reporting mean and standard\r
deviation over 5 random seeds.\r
We show the latent spaces found for the baselines and our approach in\r
Figure 4.4.2. Our approach has found a double grid structure - repre-

84\r
senting the grid world before, and after picking up the key. The base\u0002lines are reasonably plannable after training for long enough, but the\r
latent spaces aren’t as nicely structured as our approach. This mirrors\r
results in earlier work [94]. Thus, while pixel reconstruction losses\r
may be able to find reasonable representations for certain problems,\r
these rely on arbitrarily complex transition functions. Moreover, due\r
to their need to train a pixel reconstruction loss they take much longer\r
to find useable representations. This is shown in Figure 4.4.3b, where\r
0 20 40 60 80 100\r
Training epoch\r
20\r
40\r
60\r
80"""

[[sections]]
number = "100"
title = "Average episode length"
text = """
J = 0\r
J = 1\r
J = 3\r
J = 5\r
(a) Comparison of different values of J.\r
0 200 400 600 800 1000\r
Training epoch\r
20\r
40\r
60\r
80"""

[[sections]]
number = "100"
title = "Average episode length"
text = """
LD-AE\r
WM-AE\r
This paper\r
(b) Comparison of this chapter and the\r
WM-AE and LD-AE baselines. WM\u0002AE can not be evaluated until the\r
auto-encoder has finished training and\r
training of the dynamics model begins.\r
Figure 4.4.3: Average episode\r
length per training epoch for the\r
single object domain. Reported\r
mean and standard error over 5\r
random seeds.\r
the performance after planning for each training epoch is plotted and\r
compared. Additionally, we observe state collapse for DMDP-H in Fig\u0002ure 4.4.2c, and this is reflected in a high average episode length after\r
planning.\r
Double Object Task\r
We now extend the task to two objects: a key and an envelope. The\r
agent’s task at train time is still to retrieve the key. At test time, the\r
agent has to pick up the key or the envelope (randomly chosen) and\r
deliver it to one of the corners. We show results in Table 4.4.1. Again,\r
our method performs well on both train and test set, having clearly\r
learned a useful abstract representation, that generalizes to new goals.\r
The WM-AE baseline again fares better than the LD-AE baseline, and\r
DMDP-H fails to find a plannable representation. The model-free base\u0002line performs slightly worse than our method on this task, even after\r
seeing much more data. Additionally, even though it performs rea\u0002sonably well on the training goals, it does not generalize to new goals\r
at all. The WM-AE performs worse on this task than our approach,\r
but generalizes much better than the model-free baseline, due to its\r
planning, while the LD-AE baseline does not find plannable represen\u0002tations of this task."""

[[sections]]
number = "85"
title = "Continuous State Spaces"
text = """
0\r
2\r
4\r
6\r
8\r
1.5\r
1.0\r
0.5\r
0.0\r
0.5\r
1.0\r
1.5\r
1\r
0\r
1\r
2\r
3\r
0.0\r
0.2\r
0.4\r
0.6\r
0.8"""

[[sections]]
number = "1.0"
title = "Value"
text = """
(a) WM-AE Baseline\r
2 1 0 1 2\r
0.2\r
0.0\r
0.2\r
0.4\r
0.6\r
0.8\r
1.0\r
1.2\r
1.0\r
0.5\r
0.0\r
0.5\r
1.0\r
0.0\r
0.2\r
0.4\r
0.6\r
0.8"""

[[sections]]
number = "1.0"
title = "Value"
text = """
(b) LD-AE Baseline\r
0.020.010.000.010.020.03 0.005\r
0.000\r
0.005\r
0.010\r
0.015\r
0.005\r
0.000\r
0.005\r
0.010\r
0.015\r
0.2\r
0.4\r
0.6\r
0.8"""

[[sections]]
number = "1.0"
title = "Value"
text = """
(c) DMDP-H Baseline\r
10\r
30 20 10 0 10 20 30 40\r
5\r
0\r
5\r
10\r
0.5\r
0.0\r
0.5\r
1.0\r
1.5\r
2.0\r
2.5\r
0.0\r
0.2\r
0.4\r
0.6\r
0.8"""

[[sections]]
number = "1.0"
title = "Value"
text = """
(d) This chapter\r
Figure 4.4.4: Abstract MDP for\r
four approaches in CartPole.\r
Nodes are PCA projections of\r
abstract states, edges are pre\u0002dicted T¯\r
φ, colors are predicted\r
values.\r
We evaluate whether we can use our method to learn plannable rep\u0002resentations for continuous state spaces. We use OpenAI’s CartPole\u0002v0 environment [24]. We include again a model-free baseline that is\r
trained until completion as a reference for the performance of a good\r
policy. We also compare DMDP-H, WD-AE and LD-AE. We expect\r
that the latter two would perform well here; after all, the representa\u0002tion that they reconstruct is already quite compact. We additionally\r
evaluate performance when the amount of data is limited to only 100\r
trajectories (and we limit the number of training epochs for all plan\u0002ning approaches to 100 epochs). We plot the found latent space for our\r
approach and the baselines in Figure 4.4.4. The goal in this problem is\r
to reach the all-zero reward vector, which we set as the goal state with\r
reward 1, and all other states to reward 0. For our approach and both\r
auto-encoder baselines, the latent space forms a bowl with the goal in\r
its center. The DMDP-H again shows a shrunk latent space, and does\r
not have this bowl structure.\r
Results are shown in Table 4.4.3. Our approach performs best out of\r
all planning approaches. When trained fully, the model-free approach\r
performs better. However, when we limit the number of environmen\u0002tal interactions to 100 trajectories, we see that the planning approach\r
still finds a reasonable policy, while the model-free approach fails com-"""

[[sections]]
number = "86"
title = "Average episode length ↑ Standard Only 100 trajectories"
text = """
GC-Model-free 197.85 ± 2.16 23.84 ± 0.88\r
WM-AE 150.61 ± 30.48 114.47 ± 17.32\r
LD-AE 157.10 ± 11.14 154.73 ± 50.49\r
DMDP-H (J = 0) 39.32 ± 9.02 72.81 ± 20.16\r
Ours, J = 1, 174.64 ± 22.43 127.37 ± 44.02\r
Ours, J = 3 166.05 ± 24.73 148.30 ± 67.27\r
Ours, J = 5 186.31 ± 12.28 171.53 ± 34.18\r
Table 4.4.3: CartPole results.\r
Comparing average episode\r
length over 100 episodes, re\u0002porting mean and standard\r
deviation over 5 random seeds.\r
The left column has standard\r
settings, in the right column\r
only 100 trajectories are encoun\u0002tered, and planning models are\r
trained for only 100 epochs.\r
pletely. This indicates that our approach is more data efficient.\r
Generalizing over Goals and Objects\r
Figure 4.4.5: Transitions in the\r
image manipulation task.\r
In many tasks we need to be able to generalize not only over goals, but\r
also object instances. We evaluate if our abstract state space generalizes\r
to unseen objects in a problem class. For this we construct an object\r
manipulation task. On each episode, an image of a piece of cloth\u0002ing is sampled from a set of training images in Fashion MNIST [196],\r
and a goal translation of the image is sampled from a set of train\r
goals (translations with negative x-offset: (−3, ·) up to and including\r
(−1, ·)). Thus, the underlying state space is a 7 × 7 grid. The trans\u0002lated image is provided to the agent as a goal state. The agent receives\r
a reward of +1 if she moves the clothing to the correct translation. See\r
Figure 4.4.5.\r
At test time, we evaluate performance on test goals (translations with\r
positive x-offset: (1, ·) up to and including (3, ·), seen before as states\r
for training images but never as goals) and test images. The latent\r
spaces for each of the four representation learning approaches are\r
shown in Figure 4.4.6. For DMDP-H, the latent space collapses to all\r
but a few points. For WD-AE and LD-AE, the latent space does not\r
exhibit clear structure. For our approach, there is a clear square grid\r
structure present in the latent space. However, the underlying trans-

87\r
lations for the images do not neatly align across images. Clustering\r
such states together is interesting future work.\r
20\r
10\r
0\r
10\r
20\r
30\r
40\r
30\r
20\r
10\r
0\r
10\r
20\r
30\r
40\r
30210\r
0010203040\r
0\r
2\r
4\r
6"""

[[sections]]
number = "8"
title = "Value"
text = """
(a) WM-AE Baseline\r
0.2\r
0.1\r
0.0\r
0.1\r
0.2\r
0.3\r
0.4\r
0.5\r
0.4\r
0.2\r
0.0\r
0.2\r
0.4\r
0.3 0.2 0.1 0.0 0.1 0.20.30.4\r
0\r
2\r
4\r
6"""

[[sections]]
number = "8"
title = "Value"
text = """
(b) LD-AE Baseline\r
0.00005 0.00000 0.00005 0.00010\r
0.00015\r
0.000200.00002\r
0.00001\r
0.00000\r
0.00001\r
0.00002\r
0.00003\r
0.000002\r
0.000001\r
0.000000\r
0.000001\r
0.000002\r
9.00\r
9.25\r
9.50\r
9.75\r
10.00\r
10.25\r
10.50"""

[[sections]]
number = "10.75"
title = "Value"
text = """
(c) DMDP-H Baseline\r
7.5\r
5.0\r
2.5\r
0.0\r
2.5\r
5.0\r
7.5\r
10.0\r
4\r
2\r
0\r
2\r
4\r
6\r
0.3 0. 0\r
0.1 0.2.100.20.30.4\r
1.5\r
2.0\r
2.5\r
3.0\r
3.5\r
4.0\r
4.5"""

[[sections]]
number = "5.0"
title = "Value"
text = """
(d) This chapter\r
Figure 4.4.6: Abstract MDP for\r
four approaches in planning in\r
Fashion MNIST. Nodes are PCA\r
projections of abstract states,\r
edges are predicted T¯\r
φ, colors\r
are predicted values.\r
Results are shown in Table 4.4.4. The goal-conditioned model-free\r
baseline has an easy time finding a good policy for the training set\u0002ting. It also generalizes well to unseen images. However, it has trouble\r
generalizing to new goal locations for both train and test images. Our\r
planning approach, on the other hand, loses some performance on\r
the training setting, but easily generalizes to both test images and test\r
goals. Neither WM-AE nor LD-AE find good policies in this problem.\r
They have a difficult time learning plannable representations because\r
their focus is on reconstructing individual images.\r
Avg. ep. length ↓\r
Dataset Train Test\r
Goal Set Train Test Train Test\r
GC-Model-free 4.82 ± 0.33 9.67 ± 5.01 4.75 ± 0.12 8.17 ± 2.67\r
WM-AE 59.95 ± 4.06 63.27 ± 3.36 64.27 ± 5.33 63.41 ± 2.04\r
LD-AE 56.39 ± 7.07 49.35 ± 4.05 51.45 ± 6.79 51.70 ± 3.97\r
DMDP-H (J = 0) 62.86 ± 3.87 66.68 ± 4.40 65.93 ± 4.98 64.86 ± 1.57\r
Ours, J = 1, 5.07 ± 0.87 5.27 ± 0.56 5.69 ± 0.93 5.63 ± 0.96\r
Ours, J = 3 5.60 ± 0.97 5.46 ± 0.97 6.44 ± 1.12 5.42 ± 0.89\r
Ours, J = 5 5.36 ± 0.71 5.67 ± 1.20 6.36 ± 1.21 5.34 ± 0.93\r
Table 4.4.4: Comparing average\r
episode length of 100 episodes\r
for planning in Fashion MNIST.\r
Reporting mean and standard\r
deviation over 5 random seeds.

88"""

[[sections]]
number = "4.5"
title = "Related Work"
text = """
This chapter proposes a method for learning action equivariant map\u0002pings of MDPs, and using these mappings for constructing plannable\r
abstract MDPs. We learn action equivariant maps by minimizing MDP\r
homomorphism metrics [165]. As a result, when the loss reaches zero\r
the learned mapping is an MDP homomorphism [143]. MDP homo\u0002morphism metrics are a generalization of bisimulation metrics [46,\r
113]. Other works [71, 34, 193] consider equivariance to symmetry\r
group actions in learning. Here, we use a more general version of\r
equivariance under MDP actions for learning representations of MDPs.\r
We learn representations of MDPs by 1) predicting the next latent state,\r
2) predicting the reward and 3) using negative sampling to prevent\r
state collapse. Much recent work has considered self-supervised rep\u0002resentation learning for MDPs. Certain works focus on predicting the\r
next state using a contrastive loss [94, 6, 135], disregarding the re\u0002ward function. However, certain states may be erronously grouped\r
together without a reward function to distinguish them. [54] include\r
both rewards and transitions to propose an objective based on stochas\u0002tic bisimulation metrics [57, 46, 113]. However, at training time they\r
focus on deterministically predicting the next latent state. Their pro\u0002posed objective does not account for the possibility of latent space col\u0002lapse, and for complex tasks they require a pixel reconstruction term.\r
This phenomenon is also observed by [51], who prevent it with two\r
entropy maximization losses.\r
Many approaches to representation learning in MDPs depend (par\u0002tially) on learning to reconstruct the input state [36, 182, 66, 76, 67,\r
168, 104, 202, 85, 183, 178, 8]. A disadvantage of reconstruction losses\r
is training a decoder, which is time consuming and usually not re\u0002quired for decision making tasks. Additionally, such losses emphasize\r
visually salient features over features relevant to decision making.\r
Other approaches that side-step the pixel reconstruction loss include\r
predicting which action caused the transition between two states [3],\r
predicting the number of time steps between two states [9] or predict\u0002ing objects in an MDP state using supervised learning [200].\r
[82] identify a set of priors about the world and uses them to formulate\r
self-supervised objectives. In [55], the similarity between two states is\r
the difference in goal-conditioned policies needed to reach them from\r
another state. [152] learn representations for tree-based search that\r
must predict among others a policy and value function, and are thus\r
not policy-independent. Earlier work on decoupling representation\r
learning and planning exists [36, 182, 200]. However, these works use\r
objectives that include a pixel reconstruction term [36, 182] or require

89\r
labeling of objects in states for use in supervised learning [200].\r
Other work on planning algorithms in deep learning either assumes\r
knowledge of the state graph [164, 130, 112, 86], builds a graph out of\r
observed transitions [96] or structures the neural network architecture\r
as a planner [132, 45, 51], which limits the search depth."""

[[sections]]
number = "4.6"
title = "Relation to Group Equivariance"
text = """
Action-equivariance is a generalization of the older notion of group\r
equivariance [34]. In group equivariance, we require that for a function\r
Z : X → Z, and a group G acting on a space X , the following holds:\r
Z(g · x) = g¯ · Z(x) ∀g ∈ G, x ∈ X (4.13)\r
where g¯ indicates an equivalent group element acting on Z. If we view\r
the original point x ∈ X as a state, and the transformed point x\r
0 = gx\r
as a next state, we can define g as a special case of an MDP action and\r
write:\r
Z(T(x, g)) = T¯(Z(x), g¯) ∀g ∈ G, x ∈ X (4.14)\r
which we immediately recognize as the equation in Figure 4.1.1. We\r
can further generalize this by letting T(·, ·) and T¯(·, ·) be stochastic\r
functions, and matching their distributions. This generalization sug\u0002gests that the notion of a group action and an MDP action are more\r
similar than they at first glance look. Additionally, we can view equiv\u0002ariance in the general case as a body of work that finds or defines\r
homomorphisms between spaces that respect some notion of acting\r
on a point (a state, or e.g. an image) in order to bring it to another\r
point (a next state, or a rotated image)."""

[[sections]]
number = "4.7"
title = "Conclusion"
text = """
This chapter proposes the use of ‘equivariance under actions’ for learn\u0002ing representations in deterministic MDPs. Action equivariance is en\u0002forced by the use of MDP homomorphism metrics in defining a loss\r
function. We also propose a method of constructing plannable abstract\r
MDPs from continuous latent spaces. We prove that for determinis\u0002tic MDPs, when our objective function is zero and our method for\r
constructing abstract MDP is used, the map we learn is an MDP ho\u0002momorphism. Additionally, we show empirically that our approach\r
is data-efficient and fast to train, and generalizes well to new goal

90\r
states and instances with the same environmental dynamics. Finally,\r
we show that action-equivariance is a generalization of group equivari\u0002ance. Potential future work includes an extension to stochastic MDPs\r
and clustering states on the basis of MDP metrics. Using a clustering\r
approach as part of model training, we can learn the prototypical states\r
rather than sampling them. This comes at the cost of having to back\u0002propagate through a discretization step, which in early experiments\r
(using Gumbel-Softmax [78]) led to instability.

91"""

[[sections]]
number = "5"
title = "Learning Factored"
text = """
Representations of Markov\r
Decision Processes"""

[[sections]]
number = "5.1"
title = "Introduction"
text = """
In the previous Chapter we presented work that learns to recover the\r
structure in an environment by using action-equivariance. In this Chap\u0002ter, we will learn the structure in individual states in order to improve\r
decision making in object-oriented decision making problems.\r
Many decision-making problems are inherently structured: manip\u0002ulating objects in a scene, steering agents in a multi-agent system,\r
placing chip blocks on a chip and attaching atoms in molecule de\u0002sign are all examples of problems where the full problem state is\r
decomposed into different factors, which are sometimes independent\r
of each other. By making use of this structure, we can often reduce\r
solving an exponentially complex problem to solving a series of sim\u0002pler problems. For example, if we have a structured representation\r
of states in a reinforcement learning problem, we can use factored\r
approaches to decision making [65, 63, 154, 88] or use similarly struc\u0002tured graph neural network architectures in deep reinforcement learn\u0002ing [102, 180, 75, 132, 130, 37]. In Chapter 3 we have assumed to\r
know the factorization of the environment. In this Chapter, we in-

92\r
vestigate the case where this assumption is loosened: we consider\r
a set of decision making problems with unknown structure. This\r
is an important research question, as structure is not always easily\r
given a priori. Oftentimes, observations of the world are unstructured\r
streams of e.g. image data, and structure in the state of the world\r
must be inferred by the agent itself. As such, there is a large body\r
of work on learning structured representations of scenes, resulting in\r
object-based representations [28, 11, 184, 174, 92, 161, 162, 197, 200].\r
Most works in this area require some form of human supervision,\r
but several works consider the fully self-supervised or unsupervised\r
setting [59, 128, 174, 79, 197, 25, 58, 44]. Such self-supervised meth\u0002ods are usually based on reconstructing the visual inputs from the\r
learned representations. There are a few issues with such approaches:\r
reconstruction-based methods need to be able to properly reconstruct\r
visually important yet potentially irrelevant features such as back\u0002grounds. This means a lot of training time and model capacity is\r
wasted on learning to accurately represent those features. Addition\u0002ally, one needs to spend training time and compute to learn a decoder\r
model which is not needed for the downstream task. Finally, such ap\u0002proaches tend to ignore visual features that are small but potentially\r
important to decision-making, such as a small ball in certain Atari\r
games. We therefore propose using a contrastive learning method\r
based on graph embedding approaches [19, 181], where states that\r
transition into each other are placed close together in latent space, and\r
random state pairs are pushed further from each other.\r
We introduce a method for learning factored representations of object\u0002oriented MDPs, where each state consists of a set of latent state vari\u0002ables, one per object in a scene. We model the latent transition model\r
as a graph neural network [149, 114, 95, 56, 12], with the nodes the la\u0002tent state variables. Due to the graph network transition model, we au\u0002tomatically obtain permutation equivariance on the object transitions.\r
We call this method Contrastively-trained Structured World Models\r
(C-SWMs). We also introduce a factored contrastive loss based on\r
learning translational graph embeddings [19, 181], and connect con\u0002trastive learning for state representations to relational graph embed\u0002dings [129]. Finally, we introduce a novel ranking-based evaluation\r
strategy which we use to demonstrate that C-SWMs learn object-level\r
state representations, combinatorially generalize to unseen states and\r
can identify objects from scenes without supervision in object manip\u0002ulation, 3D physics, and Atari settings.

93"""

[[sections]]
number = "5.2"
title = "Background"
text = """
A Markov Decision Process (MDP) is a tuple M = (S, A, R, T, γ) with\r
S the set of states, A the set of actions, R : S × A → R the reward\r
function, T : S × A × S → [0, 1] the transition function and γ ∈ [0, 1] a\r
discount factor. An agent acts in an MDP with a policy π : S × A →\r
[0, 1]. In this Chapter, we will not consider the reward function, which\r
we will leave out going forward.\r
A factored MDP is an MDP whose state is a multivariate random vari\u0002able X = (X1, · · · , Xn) and state instances are x = (x1, · · · , xn) with\r
for every i, xi ∈ Dom(Xi). In a factored MDP, the transition function\r
P(x\r
0\r
|x, a) can be written as a set of Dynamic Bayesian Networks (one\r
per action) [22, 88, 154, 63]. We can write the transition function using\r
xi’s parent set, xa,i.\r
P(x\r
0\r
|x, a) = ∏\r
i\r
P(x\r
0\r
i\r
|xa,i) (5.1)\r
We base our method on the graph embedding method TransE [19].\r
Consider a knowledge base K of entity-relation-entity triples K =\r
{(et,rt, ot)}\r
T\r
t=1\r
, with et ∈ E the subject, rt ∈ R the relation (not to\r
be confused with the reward in an MDP) and ot ∈ E the object of the\r
knowledge base fact. We can draw a parallel between such fact triples\r
and state transitions (st, at,st+1) in an MDP without rewards. In a\r
sense, the MDP’s action can be viewed as the relation between a state\r
st and next state st+1.\r
TransE embeds knowledge facts with maps F : E → RD and G : R →\r
RD and computes the energy of a triple as H = d(F(et) + G(rt), F(ot))\r
with d(·, ·) the squared Euclidean distance and F (and G) are embed\u0002ding functions that map discrete entities (and relations) to RD, where\r
D is the dimensionality of the embedding space. During training, an\r
energy-based hinge loss [110] is used."""

[[sections]]
number = "5.3"
title = "Structured World Models"
text = """
We wish to learn structured abstract representations of MDP states\r
and permutation equivariant transition functions which are consistent\r
with the factored nature of the problem."""

[[sections]]
number = "94"
title = "Learning Abstract Representations"
text = """
Assume we have a dataset of experience B = {(st, at,st+1)}\r
T\r
t=1\r
sam\u0002pled from for example an exploration policy π, with T the number of\r
tuples in the data set. We wish to train an encoder E : S → Z that maps\r
states st ∈ S to abstract representations zt ∈ Z = Z1 × . . . × ZK, with K\r
the number of object slots. We set Zk = RD for each k, with D a hyper\u0002parameter. The abstract representations zt should contain only infor\u0002mation needed for modeling the transitions in the environment, and\r
no superfluous information (such as for example background color in\r
a video game). Note that while in this Chapter we do not consider the\r
reward, we can include reward in learning representations, as we did\r
in Chapter 4.\r
Since we wish to find a structured representation of the state, we re\u0002quire that our encoder maps from unstructured (usually image-based)\r
inputs to a factored, object-oriented representation. Our encoder thus\r
consists of two modules: an object extractor Eext that maps from ob\u0002servations to K feature maps, and an object encoder Eenc that maps\r
from feature maps to object representations. The feature maps mk\r
t =\r
[Eext(st)]k are flattened and used to predict abstract representations\r
z\r
k\r
t = Eenc(mkt\r
) with z\r
k\r
t ∈ Zk\r
. The feature maps can be viewed as object\r
masks corresponding to an object slot. The object encoder Eenc shares\r
weights between objects.\r
In this chapter we will assume a factored action space A = A1 × . . . ×\r
AK, which provides an independent action for each object in the scene.\r
This is a strong inductive bias for learning factored representations.\r
The full architecture is shown in Figure 5.3.1.\r
Contrastive Learning\r
We can use contrastive coding on the transitions in an MDP without\r
rewards: (st, at,st+1). However, the same action can have different\r
outcomes in different states. We therefore base the "relation effect" on\r
both the state and the action, resulting in the latent transition model\r
T(zt, at). In essence, we therefore constrain the latent transition func\u0002tion to model the effects of actions as translations in latent space. The\r
energy is then H = d(zt + T(zt, at), zt+1).\r
For a single (st, at,st+1) with a negative sample z˜t = E(s˜t), and s˜t\r
randomly sampled from the dataset, the energy-based hinge loss is\r
L = d(zt + T(zt, at), zt+1) + max(0, e − d(z˜t, zt+1)), (5.2)\r
where e is the margin for which e = 1 was used in our experiments."""

[[sections]]
number = "95"
title = "The full loss is an expectation of Eq. 5.2 over samples from the dataset."
text = """
CNN\r
Object \r
extractor\r
MLP\r
Object \r
encoder\r
GNN\r
Transition\r
model\r
Contrastive\r
loss\r
st mt zt zt\r
+Δzt zt+1\r
Figure 5.3.1: Visualization of\r
general C-SWM architecture: a\r
CNN object extractor, MLP ob\u0002ject encoder and GNN transi\u0002tion model that uses the actions\r
to compute ∆zt, together with\r
an object-factorized contrastive\r
loss."""

[[sections]]
number = "5.4"
title = "Transition Model"
text = """
We model the transition function as a graph neural network (GNN) [149,\r
114, 95, 11, 56, 12]. When using a GNN, the nodes are the different ob\u0002jects in the scene and the model is able to learn different pairwise\r
interactions based on the object features, while being equivariant to\r
the order in which the objects are assigned to slots. The input for the\r
GNN are a set of nodes, and a set of edges. In this chapter we model\r
the graph as a fully connected graph, so that long range dependencies\r
do not require multiple message passing steps. The node inputs are\r
the object representations {z\r
t\r
k\r
}\r
K\r
k=1\r
extracted by the encoder and the ac\u0002tions at = (a\r
1\r
t\r
, · · · , a\r
K\r
t\r
). The actions are encoded as one-hot vectors for\r
discrete action spaces, but can be replaced by zero vectors in case of\r
no actions or continuous vectors in case of continuous action spaces.\r
The transition GNN T(·, ·) predicts the effect of taking at from zt:\r
∆zt = T(zt, at) = GNN({(z\r
k\r
t\r
, a\r
k\r
t\r
)}\r
K\r
k=1\r
). (5.3)\r
where ∆zt = (∆z\r
1\r
t\r
, · · · , ∆z\r
K\r
t\r
). The predicted next latent state is then\r
given by\r
zt+1 = (z\r
1\r
t + ∆z\r
1\r
t\r
, · · · , z\r
K\r
t + ∆z\r
K\r
t\r
). (5.4)\r
Thus, the transition function can also be viewed as predicting the in\u0002dividual translational effects of the action on each object in the state.\r
Message Passing The GNN consists of MLP node update functions\r
fnode and MLP edge update functions fedge which share parameters\r
between nodes and edges. A message passing round is given by\r
e\r
(i,j)\r
t = fedge([z\r
i\r
t\r
, z\r
j\r
t\r
]) (5.5)\r
∆z\r
j\r
t = fnode([z\r
j\r
t\r
, a\r
j\r
t\r
, ∑i6=je\r
(i,j)\r
t\r
]), (5.6)\r
where e\r
(i,j)\r
t\r
is a predicted edge representation between nodes i and j.\r
Multiple rounds of message passing are possible, but were not found\r
to be necessary, possibly due to the use of a fully connected graph."""

[[sections]]
number = "96"
title = "Message passing in a fully connected graph is O(K"
text = """
2\r
), but this may\r
be reduced to linear complexity if messages are only sent to nearest\r
neighbors in latent space. We leave this for future work.\r
Factored Contrastive Loss We now adapt the original contrastive loss\r
function in Eq. 5.2 to a factored loss, which is computed independently\r
for the different objects. Write the predicted effect of the action on the\r
k-th object as ∆z\r
k\r
t = T\r
k\r
(zt, at). Then, the energy H for positive samples\r
is\r
H =\r
1\r
K\r
K\r
∑\r
k=1\r
d(z\r
k\r
t + T\r
k\r
(zt, at), z\r
k\r
t+1\r
), (5.7)\r
and the energy H˜ for negative samples is\r
H˜ =\r
1\r
K\r
K\r
∑\r
k=1\r
d(z˜\r
k\r
t\r
, z\r
k\r
t+1\r
). (5.8)\r
Here, z˜t = E(s˜t) is the representation of the negative sample, and z˜\r
k\r
t\r
the representation of the k-th object. The full contrastive loss for a\r
single sample is then given by\r
L = H + max(0, e − H˜ ). (5.9)"""

[[sections]]
number = "5.5"
title = "Related Work"
text = """
Here we review related work on state representation learning.\r
State Representation Learning State representation learning is a very\r
active field. The general goal is to find representations of states where\r
similar states are close together in latent space. Stochastic bisimula\u0002tion [38, 57], lax bisimulation [165] or MDP homomorphisms [141, 142]\r
are formalisms on which much of the work on state similarity is built1.\r
1 For an overview, see [113]. For example, there is much work on bisimulation and other similarity\r
metrics [46, 26, 106, 49, 47, 48, 27] and on using bisimulation (or MDP\r
homomorphism) metrics to learn or evaluate representations [201, 170,\r
54, 89, 4, 20]. It is also very common to use reconstruction-based\r
losses [36, 182, 66, 67, 108, 54, 76, 168, 104, 202, 85, 183, 178, 8]. Other\r
self-supervised learning approaches are also common [167, 51, 82, 43,\r
3, 9, 55, 152].\r
Contrastive Learning Contrastive approaches are common in learn\u0002ing graph representations [19, 139, 61, 19, 150, 176], word represen\u0002tations [123, 121], and image representations [135, 41, 72, 29]. They are

97\r
also becoming more common in state representation learning [94, 170,\r
158, 2, 116, 6, 135]. Most of these works on state representation do not\r
focus on recovering the structure in individual states.\r
Structured Models of Environments Graph networks have been used to\r
take advantage of the structure in an environment [160, 11, 73, 180, 92,\r
146]. Such approaches usually assume that the structured nature of the\r
environment is already known. For problems where this structure is\r
unknown, there is a body of literature (see [60] for a review) focusing\r
on recovering objects from scenes directly from pixels [59, 128, 174,\r
79, 197, 25, 58, 44], using pixel-based losses. Recently, other forms\r
of structure have been gaining ground as well, for example by taking\r
symmetries into account while learning representations [126, 136, 189,\r
155]."""

[[sections]]
number = "5.6"
title = "Experiments"
text = """
We evaluated C-SWMs on different environments to see if they can\r
recover objects, predict transitions accurately, and generalize to new\r
combinations of objects in scenes. Code can be found at https://\r
github.com/tkipf/c-swm.\r
Evaluation and Training\r
We evaluate using rankings and compare to baselines. Settings, base\u0002lines, and evaluation metrics are described below. For more details\r
and experiments, see [94].\r
Evaluation Metrics Different models result in different latent spaces.\r
To compare their trajectory predictions in latent space, we compare\r
the different approaches based on ranking. This is done by encoding\r
the starting observation into latent space, followed by a prediction of\r
the next step(s). Then the target observation is encoded, as well as a\r
set of reference observations. The latent states are ranked based on\r
how close they are to the predicted latent target state. This allows\r
us to compare methods which learn very different latent spaces and\r
removes the need for comparing based on reconstruction error or to\r
do downstream planning (for planning performance of a related ap\u0002proach, see Chapter 4). We compare on Hits at Rank 1 (H@1) and\r
Mean Reciprocal Rank (MRR) (in %) after encoding the original state,\r
taking steps in latent space, and comparing to the encoding of the tar\u0002get state. We report mean and standard error on 4 runs of hold-out

98\r
environment instances.\r
Training and Evaluation We sample a training dataset by taking uni\u0002formly random actions in the environment and storing the interac\u0002tions. We similarly sample a separate evaluation data set for each\r
environment. We use Adam [90] with a learning rate of 5 · 10−4 and\r
a batch size of 1024. Details of architectures and hyperparameters are\r
included in the sections below and in [94].\r
Baselines For baselines we compare to auto-encoder based world mod\u0002els [66] (both AE and VAE [91]). For the 3-body physics environment\r
we additionally compare to Physics as Inverse Graphics (PAIG) [80].\r
Additionally, for 3D shapes we perform an ablation study where we\r
compare the effect of removing the latent GNN, the state factorization,\r
or the contrastive loss. For AE and VAE baselines we use a batch size\r
of 512 (due to higher memory demands) and for PAIG we use a batch\r
size of 100, as recommended by the authors.\r
(a) 3D blocks (b) Space Invaders (c) 3 Body Physics\r
Figure 5.6.1: Example observa\u0002tions from a) 3D shapes block\r
pushing world, b) Atari Space\r
Invaders, and c) 3 Body Physics.\r
3D Shapes\r
We evaluate on a novel 3D shapes environment, where each block is\r
moved by its own action. The shapes move in a 2D grid, but are\r
represented in the image as blocks in 3D, making it more difficult to\r
extract the underlying objects from visual information. The observa\u0002tions are 50 × 50 × 3 color images. Additional details in [94]. Exam\u0002ple observations are shown in Figure 5.6.1a. We sample 1000 training\r
episodes with 100 environment steps. We sample 10,000 evaluation\r
episodes with 10 steps each. Since the number of possible states is\r
large (approximately 6.4M unique states), a complete trajectory over\u0002lap between test and train data is unlikely. We train for 100 epochs.\r
Qualitative Results We visualize discovered object masks on held-out\r
test data in Figure 5.6.2a, which shows that the objects are masked out\r
separately. In Figure 5.6.2b we show the learned abstract transition\r
graph for one of the blocks. These results show a that the model is\r
able to recover the underlying grid structure of a given object’s transi\u0002tion graph. Note that no transitions are predicted where the block is

99\r
obstructed by another, even though the transition model does not have\r
access to the image inputs.\r
(a) Discovered object masks in a scene\r
from the 3D Cubes environment.\r
(b) Learned abstract state transition\r
graph of the yellow cube, others fixed.\r
Figure 5.6.2: Discovered object\r
masks (left) and abstract state\r
transition graphs for a single\r
object (right) in the 3D shapes\r
block pushing task. Each node\r
is an encoded state from a held\r
out test set, and each edge (color\r
coded by action type) is a transi\u0002tion predicted by the model.\r
Quantitative Results See Table 5.6.1 for quantitative results. C-SWMs\r
predict almost perfectly for short term (1 step), mid-term (5 steps), and\r
long term (10 steps) predictions, in terms of both H@1 and MRR. In\r
comparison, both the auto-encoder world model and the VAE world\r
model do quite well in the short term, but perform a lot worse on\r
mid-term and long term prediction. In terms of ablations of C-SWMs,\r
removing the latent GNN (replacing it by an object based MLP that\r
ignores interactions) slightly hurts long term prediction. Removing the\r
factored latent space hurts all predictions, and long term predictions\r
most. Removing the contrastive loss hurts all predictions, resulting in\r
a very low score for long term prediction especially.\r
Space Invaders\r
We also evaluate on Space Invaders, an Atari 2600 game which has\r
a lot of moving parts: a gun (the agent), the shields, and the aliens.\r
The observations are two consecutive frames, given as 50 × 50 × 6 ten\u0002sors. Example observations are shown in Figure 5.6.1b. We sample\r
1000 training episodes with 10 steps each. We sample 100 evaluation\r
episodes with 10 environment steps. To minimize train/test overlap,\r
we warm-start the data set by first taking random actions for 50 inter\u0002actions, discard them, and then start filling the data set. We addition\u0002ally take care that no trajectories from the test and train set overlap\r
completely. We train for 200 epochs.\r
Qualitative Results See Figure 5.6.3 for object-specific filters and a\r
state transition graph for Space Invaders. The transition graphs are\r
much harder to interpret than those for the 3D shapes grid world.\r
This can have multiple reasons. For one, actions are less indepen\u0002dent compared to the grid worlds. For example, shooting the bullet

100\r
now influences the aliens in the future. Additionally, objects cannot be\r
swapped out the way the objects can in the block world (i.e. the bullet\r
behaves differently than the aliens). Finally, there are many objects\r
with identical visual features (the aliens).\r
(a) Object-specific filters.\r
2 0 2\r
0.05\r
0.00\r
0.05\r
0.10\r
(b) Object slot 1.\r
2.5 0.0 2.5\r
0.0\r
0.5\r
1.0\r
(c) Object slot 2.\r
5 0 5\r
2\r
0\r
2\r
(d) Object slot 3.\r
Figure 5.6.3: Object filters (top)\r
and abstract state transition\r
graphs per object slot (bottom)\r
in Space Invaders. Each node\r
is an encoded state from an un\u0002seen test instance of the environ\u0002ment. Predictions from a trained\r
C-SWM model with K = 3 ob\u0002ject slots.\r
Quantitative Results See Table 5.6.1 for quantitative results. The pre\u0002diction is not as good as it was in the block world, possibly for similar\r
reasons as those listed above. Additionally, results can have a higher\r
variance. Compared to the AE and VAE baselines, C-SWMs perform\r
better at short, mid and long term prediction than the baselines, as\u0002suming we use the right number of slots (performance drops for K = 1\r
and is best for K = 5).\r
(a) Observations from 3-body gravitational\r
physics simulation (top) and learned filter\r
for one object (bottom).\r
2.5 0.0 2.5\r
1\r
0\r
1\r
2\r
(b) Abstract state transition\r
graph.\r
Figure 5.6.4: Object-specific fil\u0002ter (left) and embedded trajec\u0002tories (right) in 3-body physics.\r
Embeddings are projected from\r
four to two dimensions with\r
PCA. Orange nodes are starting\r
states, green edges are ground\r
truth transitions, purple edges\r
are predicted transitions.\r
3-Body Physics\r
Finally, we evaluate on 3-body physics, where the transitions of a sys\u0002tem of 3 objects have to be predicted. Notably, this environment does\r
not contain any actions. The observations are two consecutive frames,\r
given as 50 × 50 × 6 tensors. Example observations are shown in Fig\u0002ure 5.6.1c. We sample 5000 training episodes with 10 steps. We sample

101\r
1000 evaluation episodes with 10 steps. Since this environment has a\r
continuous state space, a full trajectory overlap between test and train\r
set is unlikely. We train for 100 epochs.\r
Qualitative Results We show learned filters for one of the objects, and\r
an abstract state graph for multiple trajectories in Figure 5.6.4. The\r
learned filters are able to separately encode each of the objects. In\r
the abstract state transition graph, we see that the model is able to\r
encode the relevant information about the object and predict smooth\r
trajectories, with the exception of one of the trajectories (in the center),\r
which deviates.\r
Quantitative Results See Table 5.6.1. Both C-SWMs and the AE and\r
VAE baselines are good at short and mid term prediction. C-SWM, as\r
well as the autoencoder baseline are also good at longer term predic\u0002tion. The PAIG model does not perform as well2\r
."""

[[sections]]
number = "2"
title = "Using the hyperparameter settings rec\u0002ommended by the authors."
text = """
Summary C-SWMs predict almost perfectly in the block pushing task,\r
and are able to recover the grid structure of the transition function\r
almost perfectly. The ablation study shows that all components of\r
C-SWMs, but particularly the contrastive loss, contribute to a greater\r
predictive performance, both for short and long term prediction. The\r
object factorization is an important component as well, with the in\u0002teraction (GNN) component providing a small final boost. For Space\r
Invaders, C-SWMs and baseline models have a harder time represent\u0002ing the states well. While C-SWMs perform better than the baselines,\r
the performance is not as good as in the 3D shapes environment. Ad\u0002ditionally, the number of slots (a hyperparameter) has a non-trivial in\u0002fluence on the performance of the model, and should be chosen with\r
care. In the Physics environment, C-SWMs and the strongest baseline\r
(AE world model) perform well, with C-SWMs being slightly better at\r
long term prediction."""

[[sections]]
number = "5.7"
title = "Conclusions"
text = """
We proposed C-SWMs, a method for learning structured representa\u0002tions of states in object-oriented MDPs that uses contrastive coding\r
and learns a graph neural network transition model. C-SWMs make\r
use of the learned structure, improving on multi-step prediction and\r
providing better generalization to unseen environment configurations\r
than models that use decoders, unstructured transitions, or unstruc\u0002tured representations.

102"""

[[sections]]
number = "1"
title = "Step 5 Steps 10 Steps"
text = """
Model H@1 MRR H@1 MRR H@1 MRR\r
3D BLOCKS\r
C-SWM 99.9±0.0 100±0.0 99.9±0.0 100±0.0 99.9±0.0 99.9±0.0\r
– latent GNN 99.9±0.0 99.9±0.0 96.3±0.4 97.7±0.3 86.0±1.8 90.2±1.5\r
– factored states 74.2±9.3 82.5±8.3 48.7±12.9 62.6±13.0 65.8±14.0 49.6±11.0\r
– contrastive loss 48.9±16.8 52.5±17.8 12.2±5.8 16.3±7.1 3.1±1.9 5.3±2.8\r
World Model (AE) 93.5±0.8 95.6±0.6 26.7±0.7 35.6±0.8 4.0±0.2 7.6±0.3\r
World Model (VAE) 90.9±0.7 94.2±0.6 31.3±2.3 41.8±2.3 7.2±0.9 12.9±1.3\r
SPACE\r
INVADERS\r
C-SWM (K = 5) 48.5±7.0 66.1±6.6 16.8±2.7 35.7±3.7 11.8±3.0 26.0±4.1\r
C-SWM (K = 3) 46.2±13.0 62.3±11.5 10.8±3.7 28.5±5.8 6.0±0.4 20.9±0.9\r
C-SWM (K = 1) 31.5±13.1 48.6±11.8 10.0±2.3 23.9±3.6 6.0±1.7 19.8±3.3\r
World Model (AE) 40.2±3.6 59.6±3.5 5.2±1.1 14.1±2.0 3.8±0.8 10.4±1.3\r
World Model (VAE) 1.0±0.0 5.3±0.1 0.8±0.2 5.2±0.0 1.0±0.0 5.2±0.0\r
3-BODY\r
PHYSICS\r
C-SWM 100±0.0 100±0.0 97.2±0.9 98.5±0.5 75.5±4.7 85.2±3.1\r
World Model (AE) 100±0.0 100±0.0 97.7±0.3 98.8±0.2 67.9±2.4 78.4±1.8\r
World Model (VAE) 100±0.0 100±0.0 83.1±2.5 90.3±1.6 23.6±4.2 37.5±4.8\r
Physics WM (PAIG) 89.2±3.5 90.7±3.4 57.7±12.0 63.1±11.1 25.1±13.0 33.1±13.4\r
Table 5.6.1: Ranking results for\r
multi-step prediction in latent\r
space. Reported mean and\r
standard error of scores over 4\r
runs on hold-out environment\r
instances. Highest (mean) scores\r
in bold.\r
Scope Our approach focuses on learning representations for deter\u0002ministic environments under the Markov assumption. We consider an\r
extension to stochastic transition functions and partially observable en\u0002vironments a useful avenue for future work. Additionally, C-SWMs in\r
their current form do not model the reward function, nor do they take\r
reward into account when learning representations. This results in\r
representations that may not contain all necessary information about\r
a given state. Another limitation is that we do not evaluate the repre\u0002sentations on downstream decision making tasks: their usefulness is\r
only evaluated in terms of predictive power. In Chapter 4 we looked at\r
unstructured representations learned in a similar way and show that\r
they are also plannable. Finally, C-SWMs are not able to tell different\r
instances of visually identical objects in a scene apart. For instance\r
disambiguation in a task such as Space Invaders, an approach based\r
on assigning objects into slots [83, 84, 145, 58, 44, 25, 93] could be a\r
useful future avenue.

103"""

[[sections]]
number = "6"
title = "Conclusion"
text = """
This thesis studied symmetry and structure in deep reinforcement\r
learning. In particular, we focused on the main research question:\r
How can we incorporate and extract symmetry and structure in reinforce\u0002ment learning?\r
We divided the thesis up into two main parts, symmetry and struc\u0002ture. Part 1 considered symmetries in the state-action space of deci\u0002sion making problems. We considered symmetries in single agent and\r
multi-agent reinforcement learning problems. In Part 2, we discussed\r
structure in environments and states. We proposed learning action\u0002equivariant representations of MDPs, and recovering object-oriented\r
structure from environment interactions.\r
We start from the first research question, How can we leverage knowl\u0002edge of symmetries in deep reinforcement learning? To answer this ques\u0002tion, Chapter 2 introduces MDP Homomorphic networks for deep re\u0002inforcement learning. MDP Homomorphic networks are neural net\u0002works that are equivariant under symmetries in the joint state-action\r
space of an MDP. We make explicit the connection between MDP Ho\u0002momorphisms and equivariant networks. Current approaches to deep\r
reinforcement learning do not usually exploit knowledge about such\r
structure. By building this prior knowledge into policy and value\r
networks using an equivariance constraint, we can reduce the size of\r
the solution space. We specifically focus on group-structured symme\u0002tries (invertible transformations). Additionally, we introduce an easy\r
method for constructing equivariant network layers numerically, so

104\r
the system designer need not solve the constraints by hand, as is typ\u0002ically done. We construct MDP homomorphic MLPs and CNNs that\r
are equivariant under either a group of reflections or rotations. We\r
show that such networks converge faster than unstructured baselines\r
on CartPole, a grid world and Pong.\r
After showing the improvement in data efficiency using symmetry\r
constraints in single agent policies, we next investigate the effect of\r
global symmetry constraints in distributed cooperative multi agent\r
systems. The second research question, How can we leverage knowledge\r
of global symmetries in distributed cooperative multi-agent systems?, can\r
now be discussed. Chapter 3 introduces Multi-Agent MDP Homomor\u0002phic Networks, a class of networks that allows distributed execution\r
using only local information, yet is able to share experience between\r
global symmetries in the joint state-action space of cooperative multi\u0002agent systems. In cooperative multi-agent systems, complex symme\u0002tries arise between different configurations of the agents and their local\r
observations. For example, consider a group of agents navigating: ro\u0002tating the state globally results in a permutation of the optimal joint\r
policy. Existing work on symmetries in single agent reinforcement\r
learning can only be generalized to the fully centralized setting, be\u0002cause such approaches rely on the global symmetry in the full state\u0002action spaces, and these can result in correspondences across agents.\r
To encode such symmetries while still allowing distributed execution\r
we propose a factorization that decomposes global symmetries into\r
local transformations. Our proposed factorization allows for distribut\u0002ing the computation that enforces global symmetries over local agents\r
and local interactions. We introduce a multi-agent equivariant policy\r
network based on this factorization. We show empirically on symmet\u0002ric multi-agent problems that globally symmetric distributable policies\r
improve data efficiency compared to non-equivariant baselines.\r
In Part 2, we considered structure in learning representations of en\u0002vironments and states. We considered learning plannable represen\u0002tations that are equivariant to actions taken in the original problem\r
using contrastive learning. We dicuss the third research question, How\r
can we learn representations of the world that capture the structure of the\r
environment? In Chapter 4, we exploit action equivariance for repre\u0002sentation learning in reinforcement learning. Equivariance under ac\u0002tions states that transitions in the input space are mirrored by equiva\u0002lent transitions in latent space, while the map and transition functions\r
should also commute. We introduce a contrastive loss function that\r
enforces action equivariance on the learned representations. We prove\r
that when our loss is zero, we have a homomorphism of a determinis-

105\r
tic Markov Decision Process (MDP). Learning equivariant maps leads\r
to structured latent spaces, allowing us to build a model on which we\r
plan through value iteration. We show experimentally that for deter\u0002ministic MDPs, the optimal policy in the abstract MDP can be success\u0002fully lifted to the original MDP. Moreover, the approach easily adapts\r
to changes in the goal states. Empirically, we show that in such MDPs,\r
we obtain better representations in fewer epochs compared to repre\u0002sentation learning approaches using reconstructions, while generaliz\u0002ing better to new goals than model-free approaches. Additionally, we\r
show that action-equivariance is a generalization of the older notion\r
of group equivariance, which suggests that work on equivariance can\r
be viewed from the perspective of homomorphisms that act to move\r
from one state (point in a space) to another state (point in a space).\r
Thus, learned representations that are action-equivariant are plannable\r
with exact planning methods. We next consider the problem of ex\u0002tracting structure within a state, in the form of object-oriented repre\u0002sentation learning. We can now discuss the final research question:\r
How can we learn representations of the world that capture the structure in\r
individual states? Learning structured representations of the world is\r
an important step towards better generalization in downstream tasks.\r
Chapter 5 discusses Contrastively-trained Structured World Models\r
(C-SWMs), an approach to representation learning that uses a con\u0002trastive method to learn compositional representations of world states.\r
We show that C-SWMs can recover structured representations from\r
unstructured image inputs, and can generalize to new scenes better\r
than reconstruction-based baselines and ablated versions.\r
General Conclusions\r
The focus of this thesis was to leverage symmetry and structure in\r
deep reinforcement learning problems. We considered two views on\r
this problem: incorporating prior information in the form of sym\u0002metries, and finding representations that capture the structure in the\r
world. We have shown that using symmetries in single and multi\r
agent systems can leverage outside knowledge to reduce the amount\r
of environment interactions necessary to finding solutions. We have\r
also shown that it is possible to recover plannable representations by\r
focusing on learning action-equivariant representations, and that in\r
object-oriented problems with factored action spaces, we are able to\r
recover the object structure, and use this to better predict future states."""

[[sections]]
number = "106"
title = "Limitations"
text = """
The work presented in this thesis is only the first step in this research\r
area. Many open challenges remain. In certain problems, we might\r
not be able to identify the symmetries a priori. In general, comput\u0002ing symmetries in MDPs is isomorphism complete (complexity class\r
of testing whether two graphs are isomorphic) when the dynamics of\r
the system are known [127]. Developing methods for learning these\r
symmetries from data is a promising emerging research area [189]. It\r
is important to correctly identify symmetries, as the approaches pre\u0002sented here are not suitable for asymmetric problems. For example,\r
consider a cartpole problem where the right side of the tracks has more\r
friction than the left side of the tracks. In such a case, the dynamics\r
of the problem are not symmetric, even if they appear that way to a\r
system designer. Another example is playing a game with symmetric\r
dynamics with an asymmetric opponent. In football one might expect\r
a symmetry to exist between the top and the bottom half of the field.\r
However, if the opposing team has a bias to playing through the top\r
half, the resulting dynamics are asymmetric. For problems that are not\r
symmetric, using a symmetric equivariant policy network such as the\r
methods presented here is suboptimal; we force the agent to use sym\u0002metric policies only. If the optimal policy is asymmetric, this means\r
we exclude the optimal policy from the set of possible policies.\r
Enforcing symmetries in neural networks tends to introduce additional\r
computational overhead. In practice, we need to balance computa\u0002tional speed with sample efficiency. Problems with a high cost per\r
sample will benefit from using equivariance, whereas in problems with\r
cheap samples and a necessity for fast decision making it might be\r
preferable to use faster neural network architecturess combined with\r
large numbers of samples.\r
In partially observable problems, symmetries can depend on the whole\r
history of a trajectory [74]. This means that any network that uses such\r
information needs to propagate geometry information through pro\u0002cessing the whole trajectory. In practice, this means developing meth\u0002ods that propagate equivariant information through recurrent networks.\r
Throughout this work, we have focused on exact symmetries. In many\r
problems, the symmetries may only be approximate. For example, in\r
continuously rotating a robot arm we may wish to treat rotations of\r
−90◦ and 90.5◦ as symmetric. Some notion of e-symmetry may be\r
useful in problems like these. Additionally, some symmetries may be\r
partial: opening the door on the left is symmetric to opening the door

107\r
on the right, except the door on the right has a different color handle.\r
Dealing with situations like these will require us to find state-action\r
representations that abstract away irrelevant differences, for example\r
through learning MDP homomorphisms (see Chapter 4).\r
For planning in learned representations, there are limits to using dy\u0002namic programming, as we cannot always consider the full state space\r
before acting. Additionally, as the world is dynamic, there is value in\r
developing methods that are dynamic as well, changing representa\u0002tions on the fly as the agent encounters changes in the environmental\r
dynamics. Finally, our representation learning methods have focused\r
primarily on deterministic and Markov problems. Generalizing these\r
methods to stochastic and partially observable problems is an impor\u0002tant step to later applications.

109"""

[[sections]]
number = "7"
title = "Acknowledgments"
text = """
I’d like to take this section to thank everyone who so generously gave\r
to me of their time, their support, or their insight. First of all, I’m\r
extremely grateful to my promotor, Max Welling, who beyond his bril\u0002liant intellect is a kind and creative person. Max, thank you for your\r
support, guidance, and the trust and freedom you gave me to pur\u0002sue the research that inspired me. I am furthermore deeply indebted\r
to my co-promotor Frans Oliehoek, who besides his sharp mind and\r
encyclopedic knowledge of the pre-deep learning RL literature, has\r
championed for me since before the PhD. Frans, I cannot express how\r
much your support has meant to me in finding my way as a researcher.\r
Further, I would like to express my deepest appreciation to my co\u0002promotor Herke van Hoof, whose insightful questions and conscien\u0002tious attitude have made my work stronger and my life easier. Herke,\r
thank you for being a supervisor I could always fall back on.\r
I’d like to extend my gratitude to the esteemed committee for my\r
PhD: Dr. Doina Precup, Dr. Jan Peters, Dr. Evangelos Kanoulas, Dr.\r
Maarten de Rijke, Dr. Erik Bekkers.\r
Thanks Yuge Shi and Karen Ullrich. Yuge, over the past few years you\r
have become one of my closest friends. It’s hard to describe what your\r
friendship means to me. Karen, you have been an incredible support\r
and cheerleader for me and I’m grateful for your wonderful friendship.\r
I furthermore want to extend my thanks to Rianne van den Berg and\r
Zeynep Akata for their time, advice, and friendship through the years."""

[[sections]]
number = "110"
title = "I’d also like to thank Bert Bredeweg, for encouraging me to continue"
text = """
my studies after undergrad, and Leo Dorst, for always being kind,\r
honest, and a wonderful teacher. Many thanks also to Cees Snoek,\r
Katja Hofmann, Jan-Willem van de Meent, Diederik Roijers, and Sam\r
Devlin for their time, advice, and support.\r
I would like to express my appreciation to my close collaborators\r
Daniel Worrall and Thomas Kipf. Thank you for being brilliant and a\r
delight to work with. Thanks as well to my other co-authors: Johannes\r
Brandstetter, Rob Hesselink, Ondrej Biza, Jose Gallego-Posada, Lau\u0002rens Weitkamp, Jakob Foerster, Darius Muglich, Christian Schroeder\r
de Witt, Shimon Whiteson, Tejaswi Kasarla, Gertjan Burghouts, Max\r
van Spengler, Rita Cucchiara, Rahul Savani, Roderich Groß.\r
Doing a PhD in AMLAB has been an incredible experience. A heart\u0002felt thanks to the many insightful, kind, creative, smart, and funny lab\u0002mates from AMLAB and beyond who I had the good fortune of doing a\r
PhD with. Thank you Sadaf, Maxi, Bas, Victor, Tim, Ivan, Sindy, Taco,\r
Jakub, Marco, Qi, Maartje, Gabriele, Maurice, Artem, Emiel, Tessa,\r
Matthias, Christos, Wendy, Sara, Sharvaree, Putri. Thank you as well\r
to Felice and Virginie for taking many tasks out of our hands, and to\r
Dennis for solving all of our computer problems.\r
Many thanks to the good people at DeepMind, who put a lot of effort\r
into turning a remote internship into a wonderful experience. Thanks\r
to my supervisors Ian Gemp, Richard Everett, and Yoram Bachrach.\r
Many thanks also to Danielle Belgrave, Feryal Behbahani, Luisa Zint\u0002graf and all the other kind folks at DeepMind. Additionally, I’d also\r
like to thank Michael Herman, Christian Daniel, and everyone else at\r
the Bosch Center for AI.\r
Finally, I’d like to thank my family and friends. Pascal, je bent mijn\r
steun en toeverlaat. Bedankt voor je grenzeloze enthousiasme, alle\r
middernachtelijke peptalks, en de cuba libres. Mam, bedankt voor je\r
standvastige vertrouwen in mijn kunnen, en bedankt dat je me hebt\r
opgevoed met het idee dat ik alles kan waar ik mijn best voor doe.\r
Emma, ik ben je zeer dankbaar voor je vriendschap van de afgelopen\r
16 jaar. Ik kan mijn leven niet voorstellen zonder jou. Ik ben heel\r
dankbaar voor mijn andere lieve vrienden, die een bron waren van\r
energie, comfort, en afleiding tijdens de PhD: Eszter, Bas, Tessa, Chiel,\r
Sander, Jelte, Willemijn, Jorn, Fabien. Bedankt familie Mazier, dat jullie\r
mij lang geleden opgevangen hebben. Bedankt familie Mettes, dat\r
jullie mij zo volledig in de familie opgenomen hebben."""

[[sections]]
number = "111"
title = "Bibliography"
text = """
[1] Farzad Abdolhosseini, Hung Yu Ling, Zhaoming Xie, Xue Bin\r
Peng, and Michiel van de Panne. On learning symmetric lo\u0002comotion. In ACM SIGGRAPH Motion, Interaction, and Games.\r
2019.\r
[2] Rishabh Agarwal, Marlos C. Machado, Pablo Samuel Castro,\r
and Marc G. Bellemare. Contrastive behavioral similarity em\u0002beddings for generalization in reinforcement learning. In Inter\u0002national Conference on Learning Representations, 2021.\r
[3] Pulkit Agrawal, Ashvin Nair, Pieter Abbeel, Jitendra Malik, and\r
Sergey Levine. Learning to poke by poking: Experiential learn\u0002ing of intuitive physics. In Advances in Neural Information Pro\u0002cessing Systems, 2016.\r
[4] Nele Albers, Miguel Suau, and Frans A. Oliehoek. Using bisim\u0002ulation metrics to analyze and evaluate latent state representa\u0002tions. In Belgian-Dutch Conference on Machine Learning, 2021.\r
[5] Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano,\r
John Schulman, and Dan Mané. Concrete problems in AI safety.\r
arXiv:1606.06565, 2016.\r
[6] Ankesh Anand, Evan Racah, Sherjil Ozair, Yoshua Bengio, Marc\u0002Alexandre Cote, and R. Devon Hjelm. Unsupervised state rep\u0002resentation learning in Atari. In Advances in Neural Information\r
Processing Systems, 2019.\r
[7] Brandon Anderson, Truong Son Hy, and Risi Kondor. Cor\u0002morant: Covariant molecular neural networks. In Advances in\r
Neural Information Processing Systems, 2019.\r
[8] Masataro Asai. Unsupervised grounding of plannable first\u0002order logic representation from images. In International Confer\u0002ence on Automated Planning and Scheduling, 2019.

112\r
[9] Yusuf Aytar, Tobias Pfaff, David Budden, Thomas Paine, Ziyu\r
Wang, and Nando de Freitas. Playing hard exploration games by\r
watching youtube. In Advances in Neural Information Processing\r
Systems, 2018.\r
[10] Andrew G. Barto, Richard S. Sutton, and Charles W. Anderson.\r
Neuronlike adaptive elements that can solve difficult learning\r
control problems. IEEE transactions on systems, man, and cybernet\u0002ics, 1983.\r
[11] Peter Battaglia, Razvan Pascanu, Matthew Lai, Danilo Jimenez\r
Rezende, and Koray Kavukcuoglu. Interaction networks for\r
learning about objects, relations and physics. In Advances in Neu\u0002ral Information Processing, 2016.\r
[12] Peter W. Battaglia, Jessica B. Hamrick, Victor Bapst, Alvaro\r
Sanchez-Gonzalez, Vinicius Zambaldi, Mateusz Malinowski,\r
Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner,\r
et al. Relational inductive biases, deep learning, and graph net\u0002works. arXiv preprint arXiv:1806.01261, 2018.\r
[13] Erik J. Bekkers. B-spline CNNs on Lie groups. In International\r
Conference on Learning Representations, 2019.\r
[14] Erik J. Bekkers, Maxime W. Lafarge, Mitko Veta, Koen A.J. Ep\u0002penhof, Josien P.W. Pluim, and Remco Duits. Roto-translation\r
covariant convolutional networks for medical image analy\u0002sis. In International Conference on Medical Image Computing and\r
Computer-Assisted Intervention, 2018.\r
[15] Richard E. Bellman. Dynamic Programming. 1957.\r
[16] Ondrej Biza and Robert Platt. Online abstraction with MDP ho\u0002momorphisms for deep learning. In International Conference on\r
Autonomous Agents and Multi-Agent Systems, 2019.\r
[17] Ondrej Biza, Elise van der Pol, and Thomas Kipf. The impact\r
of negative sampling on contrastive structured world models.\r
In ICML Workshop on Self-Supervised Learning for Reasoning and\r
Perception, 2021.\r
[18] Wendelin Böhmer, Vitaly Kurin, and Shimon Whiteson. Deep\r
coordination graphs. In International Conference on Machine Learn\u0002ing, 2020.\r
[19] Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason\r
Weston, and Oksana Yakhnenko. Translating embeddings for\r
modeling multi-relational data. In Advances in Neural Information\r
Processing, 2013.

113\r
[20] Nicolò Botteghi, Mannes Poel, Beril Sirmacek, and Christoph\r
Brune. Low-dimensional state and action representation\r
learning with MDP homomorphism metrics. arXiv preprint\r
arXiv:2107.01677, 2021.\r
[21] Craig Boutilier. Planning, learning and coordination in multi\u0002agent decision processes. In Conference on Theoretical aspects of\r
rationality and knowledge, 1996.\r
[22] Craig Boutilier, Richard Dearden, Moisés Goldszmidt, et al. Ex\u0002ploiting structure in policy construction. In International Joint\r
Conference on Artificial Intelligence, 1995.\r
[23] Johannes Brandstetter, Rob Hesselink, Elise van der Pol, Erik J.\r
Bekkers, and Max Welling. Geometric and physical quantities\r
improve E(3) equivariant message passing. In International Con\u0002ference on Learning Representations, 2021.\r
[24] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas\r
Schneider, John Schulman, Jie Tang, and Wojciech Zaremba.\r
OpenAI gym. arXiv preprint arXiv:1606.01540, 2016.\r
[25] Christopher P Burgess, Loic Matthey, Nicholas Watters, Rishabh\r
Kabra, Irina Higgins, Matt Botvinick, and Alexander Lerchner.\r
Monet: Unsupervised scene decomposition and representation.\r
arXiv preprint arXiv:1901.11390, 2019.\r
[26] Pablo Samuel Castro. Scalable methods for computing state sim\u0002ilarity in deterministic Markov decision processes. In AAAI Con\u0002ference on Artificial Intelligence, 2020.\r
[27] Pablo Samuel Castro, Tyler Kastner, Prakash Panangaden, and\r
Mark Rowland. MICo: Improved representations via sampling\u0002based state similarity for Markov decision processes. In Advances\r
in Neural Information Processing Systems, 2021.\r
[28] Michael B. Chang, Tomer Ullman, Antonio Torralba, and\r
Joshua B. Tenenbaum. A compositional object-based approach to\r
learning physical dynamics. In International Conference on Learn\u0002ing Representations, 2017.\r
[29] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geof\u0002frey Hinton. A simple framework for contrastive learning of vi\u0002sual representations. In International Conference on Machine Learn\u0002ing, 2020.\r
[30] Christopher Clark and Amos Storkey. Teaching deep convolu\u0002tional neural networks to play Go. In International Conference on\r
Machine Learning, 2015.

114\r
[31] Karl Cobbe, Oleg Klimov, Chris Hesse, Taehoon Kim, and John\r
Schulman. Quantifying generalization in reinforcement learn\u0002ing. In International Conference on Machine Learning, 2019.\r
[32] Taco S. Cohen, Mario Geiger, Jonas Koehler, and Max Welling.\r
Spherical CNNs. In International Conference on Learning Represen\u0002tations, 2018.\r
[33] Taco S. Cohen, Mario Geiger, and Maurice Weiler. A general the\u0002ory of equivariant CNNs on homogeneous spaces. In Advances\r
in Neural Information Processing Systems. 2019.\r
[34] Taco S. Cohen and Max Welling. Group equivariant convolu\u0002tional networks. In International Conference on Machine Learning,\r
2016.\r
[35] Taco S. Cohen and Max Welling. Steerable CNNs. In International\r
Conference on Learning Representations, 2017.\r
[36] Dane Corneil, Wulfram Gerstner, and Johanni Brea. Efficient\r
model-based deep reinforcement learning with variational state\r
tabulation. In International Conference on Machine Learning, 2018.\r
[37] Andreea Deac, Petar Veliˇckovi´c, Ognjen Milinkovi´c, Pierre-Luc\r
Bacon, Jian Tang, and Mladen Nikoli´c. XLVIN: executed latent\r
value iteration nets. arXiv preprint arXiv:2010.13146, 2020.\r
[38] Thomas Dean and Robert Givan. Model minimization in\r
Markov decision processes. In AAAI Conference on Artifical In\u0002telligence/IAAI Conference on Innovative Applications of Artificial In\u0002telligence, 1997.\r
[39] Neel Dey, Antong Chen, and Soheil Ghafurian. Group equivari\u0002ant generative adversarial networks. In International Conference\r
on Learning Representations, 2021.\r
[40] Nichita Diaconu and Daniel E. Worrall. Learning to convolve:\r
A generalized weight-tying approach. In International Conference\r
on Machine Learning, 2019.\r
[41] Alexey Dosovitskiy, Jost Tobias Springenberg, Martin Ried\u0002miller, and Thomas Brox. Discriminative unsupervised feature\r
learning with convolutional neural networks. In Advances in\r
Neural Information Processing Systems, 2014.\r
[42] David Steven Dummit and Richard M. Foote. Abstract Algebra.\r
Wiley, 2004.

115\r
[43] Sebastien Ehrhardt, Aron Monszpart, Niloy Mitra, and Andrea\r
Vedaldi. Unsupervised intuitive physics from visual observa\u0002tions. In Asian Conference on Computer Vision, 2018.\r
[44] Martin Engelcke, Adam R Kosiorek, Oiwi Parker Jones, and Ing\u0002mar Posner. GENESIS: Generative scene inference and sampling\r
with object-centric latent representations. International Conference\r
on Learning Representations, 2020.\r
[45] Gregory Farquhar, Tim Rocktäschel, Maximilian Igl, and\r
SA Whiteson. TreeQN and ATreeC: Differentiable tree\u0002structured models for deep reinforcement learning. In Interna\u0002tional Conference on Learning Representations, 2018.\r
[46] Norm Ferns, Prakash Panangaden, and Doina Precup. Metrics\r
for finite Markov decision processes. In Conference on Uncertainty\r
in Artificial Intelligence, 2004.\r
[47] Norm Ferns, Prakash Panangaden, and Doina Precup. Bisimu\u0002lation metrics for continuous Markov decision processes. SIAM\r
Journal on Computing, 2011.\r
[48] Norman Ferns, Pablo Samuel Castro, Doina Precup, and\r
Prakash Panangaden. Methods for computing state similarity\r
in Markov decision processes. In Conference on Uncertainty in\r
Artificial Intelligence, 2012.\r
[49] Norman Ferns and Doina Precup. Bisimulation metrics are op\u0002timal value functions. In Conference on Uncertainty in Artificial\r
Intelligence, 2014.\r
[50] Marc Finzi, Samuel Stanton, Pavel Izmailov, and Andrew Gor\u0002don Wilson. Generalizing convolutional neural networks for\r
equivariance to Lie groups on arbitrary continuous data. In In\u0002ternational Conference on Machine Learning, 2020.\r
[51] Vincent François-Lavet, Yoshua Bengio, Doina Precup, and Joelle\r
Pineau. Combined reinforcement learning via abstract represen\u0002tations. In AAAI Conference on Artificial Intelligence, 2019.\r
[52] Fabian B Fuchs, Daniel E Worrall, Volker Fischer, and Max\r
Welling. SE (3)-transformers: 3D roto-translation equivariant at\u0002tention networks. Advances in Neural Information Processing Sys\u0002tems, 2020.\r
[53] Victor Garcia Satorras and Joan Bruna. Few-shot learning with\r
graph neural networks. In International Conference on Learning\r
Representations, 2018.

116\r
[54] Carles Gelada, Saurabh Kumar, Jacob Buckman, Ofir Nachum,\r
and Marc G. Bellemare. DeepMDP: Learning continuous latent\r
space models for representation learning. In International Con\u0002ference on Machine Learning, 2019.\r
[55] Dibya Ghosh, Abhishek Gupta, and Sergey Levine. Learning\r
actionable representations with goal conditioned policies. In In\u0002ternational Conference on Learning Representations, 2019.\r
[56] Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol\r
Vinyals, and George E Dahl. Neural message passing for quan\u0002tum chemistry. In International Conference on Machine Learning,\r
2017.\r
[57] Robert Givan, Thomas Dean, and Matthew Greig. Equivalence\r
notions and model minimization in Markov decision processes.\r
In Artificial Intelligence, 2003.\r
[58] Klaus Greff, Raphaël Lopez Kaufman, Rishabh Kabra, Nick Wat\u0002ters, Christopher Burgess, Daniel Zoran, Loic Matthey, Matthew\r
Botvinick, and Alexander Lerchner. Multi-object representation\r
learning with iterative variational inference. In International Con\u0002ference on Machine Learning, 2019.\r
[59] Klaus Greff, Sjoerd van Steenkiste, and Jürgen Schmidhuber.\r
Neural expectation maximization. In Advances in Neural Infor\u0002mation Processing, 2017.\r
[60] Klaus Greff, Sjoerd van Steenkiste, and Jürgen Schmidhuber. On\r
the binding problem in artificial neural networks. arXiv preprint\r
arXiv:2012.05208, 2020.\r
[61] Aditya Grover and Jure Leskovec. node2vec: Scalable feature\r
learning for networks. In International Conference on Knowledge\r
Discovery and Data Mining, 2016.\r
[62] Carlos Guestrin, Daphne Koller, and Ronald Parr. Multiagent\r
planning with factored MDPs. In Advances in Neural Information\r
Processing Systems, 2002.\r
[63] Carlos Guestrin, Daphne Koller, Ronald Parr, and Shobha\r
Venkataraman. Efficient solution algorithms for factored MDPs.\r
Journal of Artificial Intelligence Research, 2003.\r
[64] Carlos Guestrin, Michail Lagoudakis, and Ronald Parr. Coordi\u0002nated reinforcement learning. In International Conference on Ma\u0002chine Learning, 2002.

117\r
[65] Carlos Guestrin, Shobha Venkataraman, and Daphne Koller.\r
Context-specific multiagent coordination and planning with fac\u0002tored MDPs. In AAAI Conference on Artifical Intelligence/IAAI\r
Conference on Innovative Applications of Artificial Intelligence, 2002.\r
[66] David Ha and Jürgen Schmidhuber. World models. arxiv preprint\r
arXiv:1803.10122, 2018.\r
[67] Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas,\r
David Ha, Honglak Lee, and James Davidson. Learning latent\r
dynamics for planning from pixels. In International Conference on\r
Machine Learning, 2019.\r
[68] Charles R. Harris, K. Jarrod Millman, Stéfan J van der Walt,\r
Ralf Gommers, Pauli Virtanen, David Cournapeau, Eric Wieser,\r
Julian Taylor, Sebastian Berg, Nathaniel J. Smith, Robert Kern,\r
Matti Picus, Stephan Hoyer, Marten H. van Kerkwijk, Matthew\r
Brett, Allan Haldane, Jaime Fernández del Río, Mark Wiebe,\r
Pearu Peterson, Pierre Gérard-Marchant, Kevin Sheppard, Tyler\r
Reddy, Warren Weckesser, Hameer Abbasi, Christoph Gohlke,\r
and Travis E. Oliphant. Array programming with NumPy. Na\u0002ture, 2020.\r
[69] Juris Hartmanis and R. E. Stearns. Algebraic Structure Theory Of\r
Sequential Machines. Prentice-Hall, Inc., 1966.\r
[70] Peter Henderson, Riashat Islam, Joelle Pineau, David Meger,\r
Doina Precup, and Philip Bachman. Deep reinforcement learn\u0002ing that matters. In AAAI Conference on Artificial Intelligence,\r
2018.\r
[71] Irina Higgins, David Amos, David Pfau, Sebastien Racaniere,\r
Loic Matthey, Danilo Rezende, and Alexander Lerchner. To\u0002wards a definition of disentangled representations. arxiv preprint\r
arXiv:1812.02230, 2018.\r
[72] R. Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon,\r
Karan Grewal, Adam Trischler, and Yoshua Bengio. Learn\u0002ing deep representations by mutual information estimation and\r
maximization. In International Conference on Learning Representa\u0002tions, 2019.\r
[73] Yedid Hoshen. VAIN: Attentional multi-agent predictive mod\u0002eling. In Advances in Neural Information Processing, 2017.\r
[74] Hengyuan Hu, Adam Lerer, Alex Peysakhovich, and Jakob Fo\u0002erster. "Other-play" for zero-shot coordination. In International\r
Conference on Machine Learning, 2020.

118\r
[75] Wenlong Huang, Igor Mordatch, and Deepak Pathak. One\r
policy to control them all: Shared modular policies for agent\u0002agnostic control. In International Conference on Machine Learning,\r
2020.\r
[76] Maximilian Igl, Luisa Zintgraf, Tuan Anh Le, Frank Wood, and\r
Shimon Whiteson. Deep variational reinforcement learning for\r
POMDPs. In International Conference on Machine Learning, 2018.\r
[77] Max Jaderberg, Volodymyr Mnih, Wojciech Marian Czar\u0002necki, Tom Schaul, Joel Z. Leibo, David Silver, and Koray\r
Kavukcuoglu. Reinforcement learning with unsupervised auxil\u0002iary tasks. In International Conference on Learning Representations,\r
2017.\r
[78] Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparame\u0002terization with Gumbel-Softmax. In International Conference on\r
Learning Representations, 2017.\r
[79] Michael Janner, Sergey Levine, William T Freeman, Joshua B\r
Tenenbaum, Chelsea Finn, and Jiajun Wu. Reasoning about\r
physical interactions with object-oriented prediction and plan\u0002ning. In International Conference on Learning Representations, 2019.\r
[80] Miguel Jaques, Michael Burke, and Timothy Hospedales.\r
Physics-as-inverse-graphics: Joint unsupervised learning of ob\u0002jects and physics from video. arXiv preprint arXiv:1905.11169,\r
2019.\r
[81] Jiechuan Jiang, Chen Dun, Tiejun Huang, and Zongqing Lu.\r
Graph convolutional reinforcement learning. In International\r
Conference on Learning Representations, 2020.\r
[82] Rico Jonschkowski and Oliver Brock. Learning state representa\u0002tions with robotic priors. In Autonomous Robots, 2015.\r
[83] Daniel Kahneman and Anne Treisman. Changing views of Atten\u0002tion and Automaticity. Academic Press, Inc., 1984.\r
[84] Daniel Kahneman, Anne Treisman, and Brian J Gibbs. The re\u0002viewing of object files: Object-specific integration of informa\u0002tion. Cognitive psychology, 1992.\r
[85] Lukasz Kaiser, Mohammad Babaeizadeh, Piotr Milos, Blazej Os\u0002inski, Roy H. Campbell, Konrad Czechowski, Dumitru Erhan,\r
Chelsea Finn, Piotr Kozakowski, Sergey Levine, et al. Model\u0002based reinforcement learning for Atari. In International Confer\u0002ence on Learning Representations, 2020.

119\r
[86] Peter Karkus, David Hsu, and Wee Sun Lee. QMDP-net: Deep\r
learning for planning under partial observability. In Advances in\r
Neural Information Processing Systems, 2017.\r
[87] Tejaswi Kasarla, Gertjan J Burghouts, Max van Spengler, Elise\r
van der Pol, Rita Cucchiara, and Pascal Mettes. Maximum\r
class separation as inductive bias in one matrix. arXiv preprint\r
arXiv:2206.08704, 2022.\r
[88] Michael Kearns and Daphne Koller. Efficient reinforcement\r
learning in factored MDPs. In International Joint Conference on\r
Artificial Intelligence, 1999.\r
[89] Mete Kemertas and Tristan Aumentado-Armstrong. Towards ro\u0002bust bisimulation metric learning. Advances in Neural Information\r
Processing Systems, 2021.\r
[90] Diederik P. Kingma and Jimmy Lei Ba. Adam: A method for\r
stochastic optimization. In International Conference on Learning\r
Representations, 2015.\r
[91] Diederik P. Kingma and Max Welling. Auto-encoding varia\u0002tional bayes. In International Conference on Learning Representa\u0002tions, 2014.\r
[92] Thomas Kipf, Ethan Fetaya, Kuan-Chieh Wang, Max Welling,\r
and Richard Zemel. Neural relational inference for interacting\r
systems. In International Conference on Machine Learning, 2018.\r
[93] Thomas Kipf, Yujia Li, Hanjun Dai, Vinicius Zambaldi, Alvaro\r
Sanchez-Gonzalez, Edward Grefenstette, Pushmeet Kohli, and\r
Peter Battaglia. Compile: Compositional imitation learning and\r
execution. In International Conference on Machine Learning, 2019.\r
[94] Thomas Kipf, Elise van der Pol, and Max Welling. Contrastive\r
learning of structured world models. In International Conference\r
on Learning Representations, 2020.\r
[95] Thomas N. Kipf and Max Welling. Semi-supervised classifica\u0002tion with graph convolutional networks. In International Confer\u0002ence on Learning Representations, 2017.\r
[96] Martin Klissarov and Doina Precup. Diffusion-based approxi\u0002mate value functions. In ICML Workshop on Efficient Credit As\u0002signment in Deep Learning and Deep Reinforcement Learning, 2018.\r
[97] Jelle R Kok and Nikos Vlassis. Collaborative multiagent rein\u0002forcement learning by payoff propagation. Journal of Machine\r
Learning Research, 2006.

120\r
[98] Risi Kondor and Shubhendu Trivedi. On the generalization of\r
equivariance and convolution in neural networks to the action of\r
compact groups. In International Conference on Machine Learning,\r
2018.\r
[99] Ilya Kostrikov, Denis Yarats, and Rob Fergus. Image augmenta\u0002tion is all you need: Regularizing deep reinforcement learning\r
from pixels. In International Conference on Learning Representa\u0002tions, 2021.\r
[100] Landon Kraemer and Bikramjit Banerjee. Multi-agent reinforce\u0002ment learning as a rehearsal for decentralized planning. Neuro\u0002computing, 2016.\r
[101] Schütt Kristof, Kindermans Pieter-Jan, Sauceda Huziel, Chmiela\r
Stefan, Tkatchenko Alexandre, and Klaus-Robert Müller. Schnet:\r
a continuous-filter convolutional neural network for modeling\r
quantum interactions. In Advances in Neural Information Process\u0002ing Systems, 2017.\r
[102] Vitaly Kurin, Maximilian Igl, Tim Rocktäschel, Wendelin\r
Boehmer, and Shimon Whiteson. My body is a cage: the role\r
of morphology in graph-based incompatible control. In Interna\u0002tional Conference on Learning Representations, 2021.\r
[103] Thanard Kurutach, Ignasi Clavera, Yan Duan, Aviv Tamar, and\r
Pieter Abbeel. Model-ensemble trust-region policy optimization.\r
In International Conference on Learning Representations, 2018.\r
[104] Thanard Kurutach, Aviv Tamar, Ge Yang, Stuart Russell, and\r
Pieter Abbeel. Learning plannable representations with causal\r
infogan. In Advances in Neural Information Processing Systems,\r
2018.\r
[105] Lior Kuyer, Shimon Whiteson, Bram Bakker, and Nikos Vlassis.\r
Multiagent reinforcement learning for urban traffic control us\u0002ing coordination graphs. In Joint European Conference on Machine\r
Learning and Knowledge Discovery in Databases, 2008.\r
[106] Charline Le Lan, Marc G Bellemare, and Pablo Samuel Castro.\r
Metrics and continuity in reinforcement learning. In AAAI Con\u0002ference on Artificial Intelligence, 2021.\r
[107] Michael Laskin, Kimin Lee, Adam Stooke, Lerrel Pinto, Pieter\r
Abbeel, and Aravind Srinivas. Reinforcement learning with aug\u0002mented data. In Advances in Neural Information Processing Sys\u0002tems, 2020.

121\r
[108] Adrien Laversanne-Finot, Alexandre Péré, and Pierre-Yves\r
Oudeyer. Curiosity driven exploration of learned disentangled\r
goal spaces. In Conference on Robot Learning, 2018.\r
[109] Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner.\r
Gradient-based learning applied to document recognition. IEEE,\r
1998.\r
[110] Yann LeCun, Sumit Chopra, Raia Hadsell, M Ranzato, and\r
F Huang. A tutorial on energy-based learning. Predicting struc\u0002tured data, 2006.\r
[111] Kimin Lee, Kibok Lee, Jinwoo Shin, and Honglak Lee. Network\r
randomization: A simple technique for generalization in deep\r
reinforcement learning. In International Conference on Learning\r
Representations, 2020.\r
[112] Lisa Lee, Emilio Parisotto, Devendra Singh Chaplot, Eric P. Xing,\r
and Ruslan Salakhutdinov. Gated path planning networks. In\r
International Conference on Machine Learning, 2018.\r
[113] Lihong Li, Thomas J. Walsh, and Michael L. Littman. Towards\r
a unified theory of state abstraction for MDPs. In International\r
Symposium on Artificial Intelligence and Mathematics, 2006.\r
[114] Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard\r
Zemel. Gated graph sequence neural networks. arXiv preprint\r
arXiv:1511.05493, 2015.\r
[115] Yijiong Lin, Jiancong Huang, Matthieu Zimmer, Yisheng Guan,\r
Juan Rojas, and Paul Weng. Invariant transform experience re\u0002play: Data augmentation for deep reinforcement learning. IEEE\r
Robotics and Automation Letters, 2020.\r
[116] Guoqing Liu, Chuheng Zhang, Li Zhao, Tao Qin, Jinhua Zhu,\r
Jian Li, Nenghai Yu, and Tie-Yan Liu. Return-based contrastive\r
representation learning for reinforcement learning. In Interna\u0002tional Conference on Learning Representations, 2021.\r
[117] Iou-Jen Liu, Raymond A. Yeh, and Alexander G. Schwing. PIC:\r
Permutation invariant critic for multi-agent deep reinforcement\r
learning. In Conference on Robot Learning, 2019.\r
[118] Anuj Mahajan and Theja Tulabandhula. Symmetry learn\u0002ing for function approximation in reinforcement learning.\r
arXiv:1706.02999, 2017.\r
[119] Aditi Mavalankar. Goal-conditioned batch reinforcement learn\u0002ing for rotation invariant locomotion. In ICLR Workshop Beyond\r
Tabula Rasa in RL, 2020.

122\r
[120] Pascal Mettes, Elise van der Pol, and Cees Snoek. Hyperspheri\u0002cal prototype networks. Advances in Neural Information Processing\r
Systems, 2019.\r
[121] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Ef\u0002ficient estimation of word representations in vector space. arXiv\r
preprint arXiv:1301.3781, 2013.\r
[122] Shruti Mishra, Abbas Abdolmaleki, Arthur Guez, Piotr Trochim,\r
and Doina Precup. Augmenting learning using symmetry in\r
a biologically-inspired domain. arXiv preprint arXiv:1910.00528,\r
2019.\r
[123] Andriy Mnih and Yee Whye Teh. A fast and simple algorithm for\r
training neural probabilistic language models. In International\r
Conference on Machine Learning, 2012.\r
[124] Volodymyr Mnih, Adrià Puigdomènech Badia, Mehdi Mirza,\r
Alex Graves, Tim Harley, Timothy P. Lillicrap, David Silver, and\r
Koray Kavukcuoglu. Asynchronous methods for deep reinforce\u0002ment learning. In International Conference on Machine Learning,\r
2016.\r
[125] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A.\r
Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin Ried\u0002miller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen,\r
Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King,\r
Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis\r
Hassabis. Human-level control through deep reinforcement\r
learning. In Nature, 2015.\r
[126] Arnab Kumar Mondal, Vineet Jain, Kaleem Siddiqi, and Sia\u0002mak Ravanbakhsh. EqR: Equivariant representations for data\u0002efficient reinforcement learning. In International Conference on\r
Machine Learning, 2021.\r
[127] Shravan Matthur Narayanamurthy and Balaraman Ravindran.\r
On the hardness of finding symmetries in Markov decision pro\u0002cesses. In International Conference on Machine learning, 2008.\r
[128] Charlie Nash, Ali Eslami, Chris Burgess, Irina Higgins, Daniel\r
Zoran, Theophane Weber, and Peter Battaglia. The multi-entity\r
variational autoencoder. In Advances in Neural Information Pro\u0002cessing Workshops, 2017.\r
[129] Maximilian Nickel, Kevin Murphy, Volker Tresp, and Evgeniy\r
Gabrilovich. A review of relational machine learning for knowl\u0002edge graphs. IEEE, 2016.

123\r
[130] Sufeng Niu, Siheng Chen, Colin Targonski, Melissa Smith, Je\u0002lena Kova evi, and Hanyu Guo. Generalized value iteration\r
networks: Life beyond lattices. In AAAI Conference on Artificial\r
Intelligence, 2018.\r
[131] Future of Life Institute. Autonomous weapons: An open letter\r
from AI & robotics researchers, 2015.\r
[132] Junhyuk Oh, Satinder Singh, and Honglak Lee. Value prediction\r
network. In Advances in Neural Information Processing Systems,\r
2017.\r
[133] Frans A. Oliehoek, Matthijs T.J. Spaan, and Nikos Vlassis.\r
Optimal and approximate Q-value functions for decentralized\r
POMDPs. Journal of Artificial Intelligence Research, 2008.\r
[134] Rahul Oliehoek, Frans A.and Savani, Jose Gallego, Elise van der\r
Pol, and Roderich Groß. Beyond local Nash equilibria for ad\u0002versarial networks. In Benelux Conference on Artificial Intelligence,\r
2018.\r
[135] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representa\u0002tion learning with contrastive predictive coding. arXiv preprint\r
arXiv:1807.03748, 2018.\r
[136] Jung Yeon Park, Ondrej Biza, Linfeng Zhao, Jan-Willem van de\r
Meent, and Robin Walters. Learning symmetric embeddings for\r
equivariant world models. In International Conference on Machine\r
Learning, 2021.\r
[137] Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan,\r
Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison,\r
Luca Antiga, and Adam Lerer. Automatic differentiation in py\u0002torch. In Advances in Neural Information Processing Autodiff Work\u0002shop, 2017.\r
[138] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James\r
Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Na\u0002talia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf,\r
Edward Yang, Zachary DeVito, Martin Raison, Alykhan Te\u0002jani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai,\r
and Soumith Chintala. Pytorch: An imperative style, high\u0002performance deep learning library. In Advances in Neural Infor\u0002mation Processing Systems, 2019.\r
[139] Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. Deepwalk:\r
Online learning of social representations. In International Confer\u0002ence on Knowledge Discovery and Data Mining, 2014.

124\r
[140] Martin L Puterman. Markov Decision Processes: Discrete Stochastic\r
Dynamic Programming. John Wiley & Sons, Inc., 1994.\r
[141] Balaraman Ravindran and Andrew G. Barto. Symmetries and\r
model minimization in Markov decision processes. Technical\r
report, University of Massachusetts, 2001.\r
[142] Balaraman Ravindran and Andrew G. Barto. SMDP homomor\u0002phisms: An algebraic approach to abstraction in semi-Markov\r
decision processes. In International Joint Conference on Artificial\r
Intelligence, 2003.\r
[143] Balaraman Ravindran and Andrew G. Barto. Approximate ho\u0002momorphisms: A framework for non-exact minimization in\r
Markov decision processes. In International Conference on Knowl\u0002edge Based Computer Systems, 2004.\r
[144] Philipp Robbel, Frans A. Oliehoek, and Mykel J. Kochender\u0002fer. Exploiting anonymity in approximate linear programming:\r
Scaling to large multiagent MDPs. In AAAI Conference on Artifi\u0002cial Intelligence, 2016.\r
[145] Sara Sabour, Nicholas Frosst, and Geoffrey E Hinton. Dynamic\r
routing between capsules. In Advances in Neural Information Pro\u0002cessing, 2017.\r
[146] Alvaro Sanchez-Gonzalez, Nicolas Heess, Jost Tobias Springen\u0002berg, Josh Merel, Martin Riedmiller, Raia Hadsell, and Peter\r
Battaglia. Graph networks as learnable physics engines for infer\u0002ence and control. In International Conference on Machine Learning,\r
2018.\r
[147] Victor Garcia Satorras, Emiel Hoogeboom, and Max Welling.\r
E(n) equivariant graph neural networks. In International Con\u0002ference on Machine Learning, 2021.\r
[148] Victor Garcia Satorras and Max Welling. Neural enhanced be\u0002lief propagation on factor graphs. In International Conference on\r
Artificial Intelligence and Statistics, 2021.\r
[149] Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagen\u0002buchner, and Gabriele Monfardini. The graph neural network\r
model. IEEE Transactions on Neural Networks, 2009.\r
[150] Michael Schlichtkrull, Thomas N Kipf, Peter Bloem, Rianne Van\r
Den Berg, Ivan Titov, and Max Welling. Modeling relational data\r
with graph convolutional networks. 2017.

125\r
[151] Nicol N. Schraudolph, Peter Dayan, and Terrence J. Sejnowski.\r
Temporal difference learning of position evaluation in the game\r
of Go. In Advances in Neural Information Processing Systems, 1994.\r
[152] Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert,\r
Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez,\r
Edward Lockhart, Demis Hassabis, Thore Graepel, Timothy P.\r
Lillicrap, and David Silver. Mastering Atari, Go, chess and Shogi\r
by planning with a learned model. 2019.\r
[153] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford,\r
and Oleg Klimov. Proximal policy optimization algorithms. In\r
arXiv:1707.06347, 2017.\r
[154] Dale Schuurmans and Relu Patrascu. Direct value\u0002approximation for factored MDPs. Advances in Neural Informa\u0002tion Processing Systems, 2001.\r
[155] Dhruv Sharma, Alihusein Kuwajerwala, and Florian Shkurti.\r
Augmenting imitation experience via equivariant representa\u0002tions. In International Conference on Robotics and Automation, 2022.\r
[156] David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Lau\u0002rent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioan\u0002nis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mas\u0002tering the game of Go with deep neural networks and tree\r
search. In Nature, 2016.\r
[157] Gregor N.C. Simm, Robert Pinsler, Gábor Csányi, and\r
José Miguel Hernández-Lobato. Symmetry-aware actor-critic for\r
3D molecular design. In International Conference on Learning Rep\u0002resentations, 2021.\r
[158] Aravind Srinivas, Michael Laskin, and Pieter Abbeel. CURL:\r
Contrastive unsupervised representations for reinforcement\r
learning. In International Conference on Machine Learning, 2020.\r
[159] Adam Stooke and Pieter Abbeel. rlpyt: A research code\r
base for deep reinforcement learning in Pytorch. arxiv preprint\r
arXiv:1909.01500, 2019.\r
[160] Sainbayar Sukhbaatar, Arthur Szlam, and Rob Fergus. Learning\r
multiagent communication with backpropagation. In Advances\r
in Neural Information Processing Systems, 2016.\r
[161] Chen Sun, Abhinav Shrivastava, Carl Vondrick, Kevin Murphy,\r
Rahul Sukthankar, and Cordelia Schmid. Actor-centric relation\r
network. In European Conference on Computer Vision, 2018.

126\r
[162] Chen Sun, Abhinav Shrivastava, Carl Vondrick, Rahul Suk\u0002thankar, Kevin Murphy, and Cordelia Schmid. Relational action\r
forecasting. In IEEE Conference on Computer Vision and Pattern\r
Recognition, 2019.\r
[163] Peter Sunehag, Guy Lever, Audrunas Gruslys, Wojciech Mar\u0002ian Czarnecki, Vinicius Zambaldi, Max Jaderberg, Marc Lanc\u0002tot, Nicolas Sonnerat, Joel Z Leibo, Karl Tuyls, et al. Value\u0002decomposition networks for cooperative multi-agent learning.\r
In International Conference on Autonomous Agents and Multi-Agent\r
Systems, 2018.\r
[164] Aviv Tamar, Yi Wu, Garrett Thomas, Sergey Levine, and Pieter\r
Abbeel. Value iteration networks. In Advances in Neural Informa\u0002tion Processing Systems, 2016.\r
[165] Jonathan Taylor, Doina Precup, and Prakash Panagaden. Bound\u0002ing performance loss in approximate MDP homomorphisms. In\r
Advances in Neural Information Processing Systems, 2008.\r
[166] Nathaniel Thomas, Tess Smidt, Steven Kearnes, Lusann Yang,\r
Li Li, Kai Kohlhoff, and Patrick Riley. Tensor field networks:\r
Rotation-and translation-equivariant neural networks for 3D\r
point clouds. arXiv preprint arXiv:1802.08219, 2018.\r
[167] Valentin Thomas, Emmanuel Bengio, William Fedus, Jules Pon\u0002dard, Philippe Beaudoin, Hugo Larochelle, Joelle Pineau, Doina\r
Precup, and Yoshua Bengio. Disentangling the independently\r
controllable factors of variation by interacting with the world.\r
arXiv preprint arXiv:1802.09484, 2018.\r
[168] Valentin Thomas, Jules Pondard, Emmanuel Bengio, Marc Sar\u0002fati, Philippe Beaudoin, Marie-Jean Meurs, Joelle Pineau, Doina\r
Precup, and Yoshua Bengio. Independently controllable factors.\r
arxiv preprint arXiv:1708.01289, 2017.\r
[169] Elise van der Pol, Ian Gemp, Yoram Bachrach, and Richard Ev\u0002erett. Stochastic parallelizable eigengap dilation for large graph\r
clustering. arXiv preprint arXiv:2207.14589, 2022.\r
[170] Elise van der Pol, Thomas Kipf, Frans A. Oliehoek, and Max\r
Welling. Plannable approximations to MDP homomorphisms:\r
Equivariance under actions. In International Conference on Au\u0002tonomous Agents and Multi-Agent Systems, 2020.\r
[171] Elise van der Pol and Frans A. Oliehoek. Coordinated deep rein\u0002forcement learners for traffic light control. In NeurIPS Workshop\r
on Learning, Inference and Control of Multi-Agent Systems, 2016.

127\r
[172] Elise van der Pol, Herke van Hoof, Frans A. Oliehoek, and Max\r
Welling. Multi-agent MDP homomorphic networks. In Interna\u0002tional Conference on Learning Representations, 2022.\r
[173] Elise van der Pol, Daniel E. Worrall, Herke van Hoof, Frans A.\r
Oliehoek, and Max Welling. MDP homomorphic networks:\r
Group symmetries in reinforcement learning. In Advances in\r
Neural Information Processing Systems, 2020.\r
[174] Sjoerd van Steenkiste, Michael Chang, Klaus Greff, and Jürgen\r
Schmidhuber. Relational neural expectation maximization: Un\u0002supervised discovery of objects and their interactions. In Inter\u0002national Conference on Learning Representations, 2018.\r
[175] Pradeep Varakantham, Yossiri Adulyasak, and Patrick Jaillet.\r
Decentralized stochastic planning with anonymity in interac\u0002tions. In AAAI Conference on Artificial Intelligence, 2014.\r
[176] Petar Veliˇckovi´c, William Fedus, William L. Hamilton, Pietro\r
Liò, Yoshua Bengio, and R. Devon Hjelm. Deep graph infomax.\r
In International Conference on Learning Representations, 2019.\r
[177] K.P. Wabersich and M.N. Zeilinger. Linear model predictive\r
safety certification for learning-based control. In IEEE Confer\u0002ence on Decision and Control, 2018.\r
[178] Angelina Wang, Thanard Kurutach, Kara Liu, Pieter Abbeel, and\r
Aviv Tamar. Learning robotic manipulation through visual plan\u0002ning and acting. In Robotics: Science and Systems, 2019.\r
[179] Dian Wang, Robin Walters, and Robert Platt. SO(2)-equivariant\r
reinforcement learning. In International Conference on Learning\r
Representations, 2022.\r
[180] Tingwu Wang, Renjie Liao, Jimmy Ba, and Sanja Fidler. Ner\u0002venet: Learning structured policy with graph neural networks.\r
In International Conference on Learning Representations, 2018.\r
[181] Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng Chen.\r
Knowledge graph embedding by translating on hyperplanes. In\r
AAAI Conference on Artificial Intelligence, 2014.\r
[182] Manuel Watter, Jost Tobias Springenberg, Joschka Boedecker,\r
and Martin Riedmiller. Embed to control: a locally linear la\u0002tent dynamics model for control from raw images. In Advances\r
in Neural Information Processing Systems, 2015.

128\r
[183] Nicholas Watters, Loic Matthey, Matko Bosnjak, Christopher P.\r
Burgess, and Alexander Lerchner. COBRA: data-efficient model\u0002based RL through unsupervised object discovery and curiosity\u0002driven exploration. arxiv preprint arXiv:1905.09275, 2019.\r
[184] Nicholas Watters, Daniel Zoran, Theophane Weber, Peter\r
Battaglia, Razvan Pascanu, and Andrea Tacchetti. Visual inter\u0002action networks: Learning a physics simulator from video. In\r
Advances in Neural Information Processing, 2017.\r
[185] Hua Wei, Guanjie Zheng, Vikash Gayah, and Zhenhui Li.\r
A survey on traffic signal control methods. arXiv preprint\r
arXiv:1904.08117, 2019.\r
[186] Maurice Weiler and Gabriele Cesa. General E(2)-equivariant\r
steerable CNNs. In Advances in Neural Information Processing Sys\u0002tems, 2019.\r
[187] Maurice Weiler, Mario Geiger, Max Welling, Wouter Boomsma,\r
and Taco S Cohen. 3D steerable CNNs: Learning rotationally\r
equivariant features in volumetric data. In Advances in Neural\r
Information Processing Systems, 2018.\r
[188] Maurice Weiler, Fred A. Hamprecht, and Martin Storath. Learn\u0002ing steerable filters for rotation equivariant CNNs. In IEEE Con\u0002ference on Computer Vision and Pattern Recognition, 2018.\r
[189] Matthias Weissenbacher, Samarth Sinha, Animesh Garg, and\r
Yoshinobu Kawahara. Koopman Q-learning: Offline reinforce\u0002ment learning via symmetries of dynamics. In International Con\u0002ference on Learning Representations, 2022.\r
[190] Laurens Weitkamp, Elise van der Pol, and Zeynep Akata. Visual\r
rationalizations in deep reinforcement learning for Atari games.\r
In Benelux Conference on Artificial Intelligence, 2018.\r
[191] Marysia Winkels and Taco S. Cohen. 3D G-CNNs for pulmonary\r
nodule detection. In Medical Imaging with Deep Learning Confer\u0002ence, 2018.\r
[192] Daniel E. Worrall and Gabriel J. Brostow. CubeNet: Equivari\u0002ance to 3D rotation and translation. In European Conference on\r
Computer Vision, 2018.\r
[193] Daniel E. Worrall, Stephan J. Garbin, Daniyar Turmukhambetov,\r
and Gabriel J. Brostow. Harmonic networks: Deep translation\r
and rotation equivariance. In IEEE Conference on Computer Vision\r
and Pattern Recognition, 2017.

129\r
[194] Daniel E. Worrall and Max Welling. Deep scale-spaces: Equiv\u0002ariance over scale. In Advances in Neural Information Processing\r
Systems, 2019.\r
[195] Wenxuan Wu, Zhongang Qi, and Li Fuxin. Pointconv: Deep\r
convolutional networks on 3D point clouds. In IEEE Conference\r
on Computer Vision and Pattern Recognition, 2019.\r
[196] Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-MNIST:\r
A novel image dataset for benchmarking machine learning algo\u0002rithms. arxiv preprint arXiv:1708.07747, 2017.\r
[197] Zhenjia Xu, Zhijian Liu, Chen Sun, Kevin Murphy, William T\r
Freeman, Joshua B Tenenbaum, and Jiajun Wu. Unsupervised\r
discovery of parts, structure, and dynamics. In International Con\u0002ference on Learning Representations, 2019.\r
[198] Dmitry Yarotsky. Universal approximations of invariant maps\r
by neural networks. arxiv preprint arXiv:1804.10306, 2018.\r
[199] KiJung Yoon, Renjie Liao, Yuwen Xiong, Lisa Zhang, Ethan Fe\u0002taya, Raquel Urtasun, Richard Zemel, and Xaq Pitkow. Inference\r
in probabilistic graphical models by graph neural networks. In\r
ICLR Workshop Track, 2018.\r
[200] Amy Zhang, Adam Lerer, Sainbayar Sukhbaatar, Rob Fergus,\r
and Arthur Szlam. Composable planning with attributes. In\r
International Conference on Machine Learning, 2018.\r
[201] Amy Zhang, Rowan McAllister, Roberto Calandra, Yarin Gal,\r
and Sergey Levine. Learning invariant representations for rein\u0002forcement learning without reconstruction. In International Con\u0002ference on Learning Representations, 2020.\r
[202] Marvin Zhang, Sharad Vikram, Laura Smith, Pieter Abbeel,\r
Matthew Johnson, and Sergey Levine. SOLAR: Deep structured\r
representations for model-based reinforcement learning. In In\u0002ternational Conference on Machine Learning, 2019.\r
[203] Xupeng Zhu, Dian Wang, Ondrej Biza, Guanang Su, Robin Wal\u0002ters, and Robert Platt. Sample efficient grasp learning using\r
equivariant models. In Robotics: Science and Systems (RSS), 2022.\r
[204] Martin Zinkevich and Tucker Balch. Symmetry in Markov de\u0002cision processes and its implications for single agent and multi\r
agent learning. In International Conference on Machine Learning,\r
2001.

131"""

[[sections]]
number = "8"
title = "Summary"
text = """
In this thesis, we study symmetry and structure in deep reinforcement\r
learning. We divide the thesis into two different parts. In the first, we\r
explore how to leverage knowledge of symmetries in reinforcement\r
learning problems. In the second, we propose methods to learning\r
about the structure of an agent’s environment and individual states.\r
Our contributions are as follows: In Part 1 we use existing knowl\u0002edge of symmetries to gain improvements in data efficiency in MDPs\r
(Chapter 2) and knowledge of symmetries and structure to improve\r
data efficiency in multi-agent MDPs (Chapter 3).\r
• We propose MDP Homomorphic Networks (Chapter 2) [173]. MDP\r
homomorphic networks are neural networks that are equivariant\r
under symmetries in the joint state-action space of an MDP. Due to\r
their equivariance, we find improved data efficiency compared to\r
non-equivariant baselines.\r
• We propose Multi-Agent MDP Homomorphic Networks (Chapter 3) [172].\r
Multi-Agent MDP Homomorphic Networks form a class of net\u0002works that allows distributed execution using only local informa\u0002tion, yet is able to share experience between global symmetries in\r
the joint state-action space of cooperative multi-agent systems. We\r
show that global equivariance improves data efficiency compared\r
to non-equivariant distributed networks on symmetric coordination\r
problems.

132\r
In Part 2 we consider learning the underlying graphs of MDPs (Chap\u0002ter 4) and structure in individual states (Chapter 5).\r
• We propose PRAE (Chapter 4) [170]. PRAE exploits action equivari\u0002ance for representation learning in reinforcement learning. Equiv\u0002ariance under actions states that transitions in the input space are\r
mirrored by equivalent transitions in latent space, while the map\r
and transition functions should also commute. We prove that under\r
certain assumptions, the map we learn is an MDP homomorphism\r
and show empirically that the approach is data-efficient and fast to\r
train, generalizing well to new goal states and instances with the\r
same environmental dynamics.\r
• We propose C-SWMs (Chapter 5) [94]. C-SWMs find object-oriented\r
representations of states from pixels, using contrastive coding and\r
graph neural network transition functions. We show improvement\r
in multi-step prediction and generalization to unseen environment\r
configurations compared to models that use decoders, unstructured\r
transitions, or unstructured representations.

133"""

[[sections]]
number = "9"
title = "Samenvatting - Dutch"
text = """
Summary\r
In deze dissertatie bestuderen we symmetrieën en structuur in deep\r
reinforcement learning. De dissertatie bestaat uit twee delen. In het\r
eerste deel onderzoeken we hoe we kennis over symmetrieën kun\u0002nen gebruiken bij het oplossen van taken in reinforcement learning. In\r
het tweede deel stellen we methoden voor om over de structuur van\r
de omgeving van een agent en de structuur van individuele omgev\u0002ingstoestanden te leren.\r
Onze bijdragen zijn als volgt: In Deel 1 gebruiken we voorafgaande\r
kennis over symmetrieen om verbeteringen wat betreft data efficiëntie\r
in MDPs (Hoofdstuk 2) en multi-agent MDPs (Hoofdstuk 3) te verkrij\u0002gen.\r
• We stellen MDP Homomorphic Networks (Hoofdstuk 2) [173] voor.\r
MDP homomorphic networks zijn neurale netwerken die equivariant\r
zijn onder symmetrieen in de gezamenlijke ruimte van omgevingstoe\u0002standen en acties in een MDP. Door deze equivariantie ontdekken\r
we een verbetering in data efficiëntie vergeleken met netwerken die\r
niet equivariant zijn.\r
• We stellen Multi-Agent MDP Homomorphic Networks (Hoofdstuk 3) [172]\r
voor. Multi-Agent MDP Homomorphic Networks zijn een klasse neu\u0002rale netwerken die gedistribueerde uitvoering toestaan en daarvoor\r
alleen lokale informatie nodig hebben, doch alsnog ervaringen kan

134\r
delen tussen globale symmetrieen in de gezamenlijke ruimte van\r
omgevingstoestanden en acties in cooperatieve multi-agent syste\u0002men. We laten zien dat in symmetrische coordinatie problemen het\r
gebruik van globale equivariantie de data efficientie verbetert ten\r
opzichte van niet-equivariante gedistribueerde netwerken.\r
In Deel 2 beschouwen we het leren van de onderliggende grafen van\r
MDPs (Hoofdstuk 4) en het leren van de structuur in individuele\r
omgevingstoestanden (Hoofdstuk 5).\r
• We stellen PRAE (Hoofdstuk 4) [170] voor. PRAE maakt gebruik\r
van actie-equivariantie voor het leren van representaties in reinforce\u0002ment learning. Equivariantie onder acties betekent dat transities in\r
de invoerruimte gespiegeld worden door equivalente transities in\r
de latente ruimte, terwijl de functie naar de latente ruimte en de\r
transitiefunctie commutatief moeten zijn. We bewijzen dat de func\u0002tie naar de latente ruimte die we leren onder bepaalde aannames\r
een MDP homomorphisme is. Verder laten we zien dat de aan\u0002pak data-efficient is en snel leert, en goed generaliseert naar nieuwe\r
doelen en instanties met dezelfde omgevingsdynamiek.\r
• We stellen C-SWMs (Hoofdstuk 5) [94] voor. C-SWMs ontdekken\r
object-georienteerde representaties van omgevingstoestanden vanuit\r
pixels, daarbij gebruik makende van contrastive coding en graph neu\u0002ral network transitiefuncties. We laten vergeleken met modellen met\r
decoders, ongestructureerde transities of ongestructureerde repre\u0002sentaties verbetering zien in voorspellingen over meerdere stappen\r
en in generalisatie naar nieuwe configuraties van de omgeving."""

[[figures]]
label = "fig:1"
page_num = 52
image_path = "images/image_p52_1.png"

[[figures]]
label = "fig:2"
page_num = 55
image_path = "images/image_p55_2.png"

[[figures]]
label = "fig:3"
page_num = 58
image_path = "images/image_p58_3.png"

[[figures]]
label = "fig:4"
page_num = 65
image_path = "images/image_p65_4.png"

[[figures]]
label = "fig:5"
page_num = 96
image_path = "images/image_p96_5.png"

[[figures]]
label = "fig:6"
page_num = 108
image_path = "images/image_p108_6.png"

[[figures]]
label = "fig:7"
page_num = 108
image_path = "images/image_p108_7.png"

[[figures]]
label = "fig:8"
page_num = 109
image_path = "images/image_p109_8.png"

[[figures]]
label = "fig:9"
page_num = 109
image_path = "images/image_p109_9.png"

[[figures]]
label = "fig:10"
page_num = 110
image_path = "images/image_p110_10.png"
