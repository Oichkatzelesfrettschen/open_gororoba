# Experiments registry -- strict TOML schema (Wave 5 batch 4).
# Generated by src/scripts/analysis/build_wave5_batch4_registries.py.

[experiments]
updated = "2026-02-10"
authoritative = true
experiment_count = 14
deterministic_count = 8
gpu_count = 1
seeded_count = 6
status_allowlist = ["active", "deprecated", "planned", "blocked"]

[[experiment]]
id = "E-001"
title = "Cayley-Dickson Motif Census"
binary = "motif-census"
binary_registered = true
binary_experiment_declared = "E-001"
method = "Exact enumeration of connected-component structure of diagonal zero-product graph for cross-assessor pairs at each CD doubling (dim=16,32,64,128,256)."
input = "None (purely algebraic)"
output = ["data/csv/motif_census_dim{N}.csv", "data/csv/motif_census_summary.csv"]
run = "cargo run --release --bin motif-census -- --dims 16,32,64,128,256 --details"
run_command_sha256 = "0ed18c7e96c0fdb05e6cddfe47a84d53d2e3541ba5afffac75daff84def69e53"
claims = ["C-100", "C-101", "C-102", "C-103", "C-104", "C-105", "C-106", "C-107", "C-108", "C-109", "C-110"]
claim_refs = ["C-100", "C-101", "C-102", "C-103", "C-104", "C-105", "C-106", "C-107", "C-108", "C-109", "C-110"]
deterministic = true
gpu = false
status = "active"
status_token = "ACTIVE"
lineage_id = "XL-001"
input_path_refs = []
output_path_refs = ["data/csv/motif_census_dim{N}.csv", "data/csv/motif_census_summary.csv"]
dataset_refs = []
reproducibility_class = "deterministic_replay"

[[experiment]]
id = "E-002"
title = "Multi-Dataset GPU Ultrametric Sweep"
binary = "multi-dataset-ultrametric"
binary_registered = true
binary_experiment_declared = "E-002"
method = "For 9 catalogs: normalize, compute Euclidean distances, ultrametric fraction test with column-shuffled null. 10M triples x 1000 permutations. BH-FDR correction."
input = "data/external/ (CHIME/FRB, ATNF, GWOSC, Pantheon+, Gaia DR3, SDSS DR18, Fermi GBM, Hipparcos, McGill)"
output = ["data/csv/c071g_multi_dataset_ultrametric.csv"]
run = "cargo run --release --bin multi-dataset-ultrametric -- --explore --n-triples 10000000 --n-permutations 1000"
run_command_sha256 = "e59ef06282507cc0fb1674275f790190b68117da9cd1d56c88234535d1be4d23"
claims = ["C-071", "C-436", "C-437", "C-438", "C-439", "C-440"]
claim_refs = ["C-071", "C-436", "C-437", "C-438", "C-439", "C-440"]
deterministic = false
seed = 42
gpu = true
status = "active"
status_token = "ACTIVE"
lineage_id = "XL-002"
input_path_refs = ["data/external/"]
output_path_refs = ["data/csv/c071g_multi_dataset_ultrametric.csv"]
dataset_refs = ["PC-0017"]
reproducibility_class = "seeded_stochastic_replay"

[[experiment]]
id = "E-003"
title = "Real Cosmological Fit (Pantheon+ / DESI BAO)"
binary = "real-cosmo-fit"
binary_registered = true
binary_experiment_declared = "E-003"
method = "Joint chi-square over 1578 Pantheon+ SNe + 7 DESI DR1 BAO bins. Analytic M_B marginalization. Nelder-Mead. Lambda-CDM vs w0-CDM via delta-BIC."
input = "data/external/Pantheon+SH0ES.dat"
output = ["stdout"]
run = "cargo run --release --bin real-cosmo-fit"
run_command_sha256 = "419b37ec0b4ed9616f8b4a6b9a8a61ab46d40205ddffb339284c5068b95d4197"
claims = ["C-200", "C-201", "C-202", "C-203", "C-204", "C-205", "C-206", "C-207", "C-208", "C-209", "C-210"]
claim_refs = ["C-200", "C-201", "C-202", "C-203", "C-204", "C-205", "C-206", "C-207", "C-208", "C-209", "C-210"]
deterministic = true
gpu = false
status = "active"
status_token = "ACTIVE"
lineage_id = "XL-003"
input_path_refs = ["data/external/Pantheon+SH0ES.dat"]
output_path_refs = []
dataset_refs = []
reproducibility_class = "deterministic_replay"

[[experiment]]
id = "E-004"
title = "Kerr Shadow Boundaries"
binary = "kerr-shadow"
binary_registered = true
binary_experiment_declared = "E-004"
method = "Bardeen shadow boundary (alpha, beta) for Kerr BH at given spin and inclination."
input = "None (analytic)"
output = ["stdout or --output file"]
run = "cargo run --release --bin kerr-shadow -- --spin 0.998 --n-points 1000 --inclination 17"
run_command_sha256 = "b8cceed694706bbb72a5d18f683846dd723aec40790c7c35c9f340318dc94182"
claims = ["C-301", "C-302", "C-303", "C-304", "C-305", "C-306", "C-307", "C-308", "C-309", "C-310"]
claim_refs = ["C-301", "C-302", "C-303", "C-304", "C-305", "C-306", "C-307", "C-308", "C-309", "C-310"]
deterministic = true
gpu = false
status = "active"
status_token = "ACTIVE"
lineage_id = "XL-004"
input_path_refs = []
output_path_refs = []
dataset_refs = []
reproducibility_class = "deterministic_replay"

[[experiment]]
id = "E-005"
title = "Zero-Divisor Graph Invariants"
binary = "zd-search"
binary_registered = true
binary_experiment_declared = "E-005"
method = "Build sedenion ZD interaction graph, compute graph-theoretic invariants. Extend to dim=32."
input = "None (purely algebraic)"
output = ["stdout"]
run = "cargo run --release --bin zd-search -- --dim 16 --max-pairs 5000"
run_command_sha256 = "cc836cab950db27c227d6bd4407dfb5e5b4fccc93ecc7b5f96c6c38599781254"
claims = ["C-050", "C-051", "C-052", "C-053", "C-054", "C-055", "C-056", "C-057", "C-058", "C-059", "C-060"]
claim_refs = ["C-050", "C-051", "C-052", "C-053", "C-054", "C-055", "C-056", "C-057", "C-058", "C-059", "C-060"]
deterministic = true
gpu = false
status = "active"
status_token = "ACTIVE"
lineage_id = "XL-005"
input_path_refs = []
output_path_refs = []
dataset_refs = []
reproducibility_class = "deterministic_replay"

[[experiment]]
id = "E-006"
title = "Gravastar TOV Parameter Sweep"
binary = "gravastar-sweep"
binary_registered = true
binary_experiment_declared = "E-006"
method = "Solve TOV for three-layer gravastar across polytropic indices and target masses."
input = "None (parametric sweep)"
output = ["data/csv/gravastar_sweep.csv"]
run = "cargo run --release --bin gravastar-sweep -- --n-gamma 32 --n-mass 32 --output data/csv/gravastar_sweep.csv"
run_command_sha256 = "cb2d95828c12d251641f827220bf3295a0efc4cdbaabb7ca8591ed31a2150b32"
claims = ["C-400", "C-401", "C-402", "C-403", "C-404", "C-405", "C-406", "C-407", "C-408", "C-409", "C-410"]
claim_refs = ["C-400", "C-401", "C-402", "C-403", "C-404", "C-405", "C-406", "C-407", "C-408", "C-409", "C-410"]
deterministic = true
gpu = false
status = "active"
status_token = "ACTIVE"
lineage_id = "XL-006"
input_path_refs = []
output_path_refs = ["data/csv/gravastar_sweep.csv"]
dataset_refs = []
reproducibility_class = "deterministic_replay"

[[experiment]]
id = "E-007"
title = "Tensor Network / PEPS Entropy"
binary = "tensor-network"
binary_registered = true
binary_experiment_declared = "E-007"
method = "Classical tensor network simulator: Bell/GHZ states, random circuits, SVD entropy, PEPS boundary MPS."
input = "None (synthetic circuits)"
output = ["data/csv/entropy_scaling.csv"]
run = "cargo run --release --bin tensor-network -- scaling --n-min 2 --n-max 12 --output data/csv/entropy_scaling.csv"
run_command_sha256 = "3bdfb56d9470cbd4e3ab78c30ba39b3c9fdb08c6f24f5bbdb0f8b6e1d07d265b"
claims = ["C-350", "C-351", "C-352", "C-353", "C-354", "C-355", "C-356", "C-357", "C-358", "C-359", "C-360"]
claim_refs = ["C-350", "C-351", "C-352", "C-353", "C-354", "C-355", "C-356", "C-357", "C-358", "C-359", "C-360"]
deterministic = false
seed = 42
gpu = false
status = "active"
status_token = "ACTIVE"
lineage_id = "XL-007"
input_path_refs = []
output_path_refs = ["data/csv/entropy_scaling.csv"]
dataset_refs = []
reproducibility_class = "seeded_stochastic_replay"

[[experiment]]
id = "E-008"
title = "GWTC-3 Mass Clumping (Dip Test)"
binary = "mass-clumping"
binary_registered = true
binary_experiment_declared = "E-008"
method = "Hartigan dip test for multimodality of BBH mass distribution. Permutation-based p-value."
input = "data/external/gwosc_all_events.csv"
output = ["data/csv/gwtc3_mass_clumping_dip.csv"]
run = "cargo run --release --bin mass-clumping -- --n-permutations 10000 --seed 42"
run_command_sha256 = "ed3c2b072a68930190dad3e602856382bddef01b367b2addb6806c8893fb39f9"
claims = ["C-007"]
claim_refs = ["C-007"]
deterministic = false
seed = 42
gpu = false
status = "active"
status_token = "ACTIVE"
lineage_id = "XL-008"
input_path_refs = ["data/external/gwosc_all_events.csv"]
output_path_refs = ["data/csv/gwtc3_mass_clumping_dip.csv"]
dataset_refs = ["PC-0042"]
reproducibility_class = "seeded_stochastic_replay"

[[experiment]]
id = "E-009"
title = "Negative-Dimension Eigenvalue Convergence"
binary = "neg-dim-eigen"
binary_registered = true
binary_experiment_declared = "E-009"
method = "Eigenvalues of H = T(k) + V(x) with fractional kinetic operator, imaginary-time evolution, epsilon->0 sweep."
input = "None (parametric)"
output = ["data/csv/neg_dim_convergence.csv"]
run = "cargo run --release --bin neg-dim-eigen -- sweep --alpha -1.5 --eps-start 0.5 --eps-end 0.01 --eps-steps 20 --output data/csv/neg_dim_convergence.csv"
run_command_sha256 = "2bf71ceb3ae9163bb4c2ffefdebf1c52eb9e054be93459f046faa5ebff6dd81d"
claims = ["C-420", "C-421", "C-422", "C-423", "C-424", "C-425"]
claim_refs = ["C-420", "C-421", "C-422", "C-423", "C-424", "C-425"]
deterministic = true
gpu = false
status = "active"
status_token = "ACTIVE"
lineage_id = "XL-009"
input_path_refs = []
output_path_refs = ["data/csv/neg_dim_convergence.csv"]
dataset_refs = []
reproducibility_class = "deterministic_replay"

[[experiment]]
id = "E-010"
title = "Materials Science Baselines (JARVIS + AFLOW)"
binary = "materials-baseline"
binary_registered = true
binary_experiment_declared = "E-010"
method = "JARVIS-DFT and AFLOW datasets, Magpie-style featurization, OLS regression for formation energy and band gap."
input = "data/external/ (JARVIS JSON, AFLOW CSV)"
output = ["stdout"]
run = "cargo run --release --bin materials-baseline -- --data-dir data/external --seed 42"
run_command_sha256 = "35772f3c6cc257b32ee9edea6e592050bb8c803cc6a01a1f3e22d5b377a395ee"
claims = []
claim_refs = []
deterministic = false
seed = 42
gpu = false
status = "active"
status_token = "ACTIVE"
lineage_id = "XL-010"
input_path_refs = ["data/external/"]
output_path_refs = []
dataset_refs = []
reproducibility_class = "seeded_stochastic_replay"

[[experiment]]
id = "E-011"
title = "Cross-Stack Locality Comparison (Experiment A)"
binary = "cross-stack-locality"
binary_registered = true
binary_experiment_declared = "E-011"
method = "Compare adjacency-locality metrics across three independent constraint systems: Stack 1 (E10 Billiard): Wall-transition sequence from HyperbolicBilliard, E10 Dynkin graph. Metric: r_e8 = fraction of consecutive pairs that are E8-adjacent. Null: ColumnIndependentNull (uniform random generator selection). Stack 2 (ET DMZ Walk): Navigation sequences through de Marrais Emanation Table. Metric: r_dmz = fraction of consecutive ET cells that share a DMZ edge. Null: uniform random cell selection in the ET grid. Stack 3 (CD ZD Catamaran/Twist): Twist-product navigation across zero-divisor graph. Metric: r_zd = fraction of consecutive basis-pair transitions that are ZD-adjacent. Null: uniform random basis-pair selection. All three use the common abstract model: generator-driven piecewise geodesic dynamics (generator set G, constraint graph Gamma subset G x G, sequence s_1..s_n, locality ratio r = |{i : (s_i, s_{i+1}) in Gamma}| / (n-1)). Prediction (ALP, C-476): all three r values significantly exceed their nulls. Falsifier: one or more shows r consistent with uniform/random."
input = "None (purely algebraic / simulation)"
output = ["data/csv/cross_stack_locality_comparison.csv"]
run = "cargo run --release --bin cross-stack-locality -- --n-bounces 10000 --n-permutations 1000 --seed 42"
run_command_sha256 = "43d8015e68ae9ead4fc208d09f43bbef2a64890082224cf3529e6463dca298df"
claims = ["C-476"]
claim_refs = ["C-476"]
deterministic = false
seed = 42
gpu = false
status = "active"
status_token = "ACTIVE"
lineage_id = "XL-011"
input_path_refs = []
output_path_refs = ["data/csv/cross_stack_locality_comparison.csv"]
dataset_refs = ["PC-0031"]
reproducibility_class = "seeded_stochastic_replay"

[[experiment]]
id = "E-012"
title = "ET Discrete Billiard (Experiment B)"
binary = "et-billiard"
binary_registered = true
binary_experiment_declared = "E-012"
method = "Treat the Emanation Table as a discrete billiard wall set. Walls: ET constraints define forbidden/allowed transitions in (S, row, col) space. Reflections: when a trajectory hits an ET \"wall\" (DMZ boundary), apply the ET rule update (fill/hide involution or strut-constant increment). Symbolic dynamics: record the sequence of wall types hit (DMZ, label-line, empty). Phase structure: compute Lyapunov exponent (or mixing time) as a function of strut constant S. Compare against spectroscopy band classification (Dense/Sparse/Mixed from spectroscopy_bands()). Prediction: spectroscopy band transitions correspond to qualitative phase changes in the toy billiard (e.g., Dense bands -> low Lyapunov, Sparse -> high Lyapunov). Falsifier: no correlation between spectroscopy classification and billiard dynamics."
input = "None (ET computation from emanation.rs)"
output = ["data/csv/et_billiard_phase_structure.csv"]
run = "cargo run --release --bin et-billiard -- --n-levels 4,5,6 --n-steps 10000 --seed 42"
run_command_sha256 = "486e4fdc996d9d591999f9b99f4fb913d77e4e05f0e8ee177561695f6116e5fc"
claims = ["C-476", "C-477"]
claim_refs = ["C-476", "C-477"]
deterministic = false
seed = 42
gpu = false
status = "active"
status_token = "ACTIVE"
lineage_id = "XL-012"
input_path_refs = []
output_path_refs = ["data/csv/et_billiard_phase_structure.csv"]
dataset_refs = ["PC-0032"]
reproducibility_class = "seeded_stochastic_replay"

[[experiment]]
id = "E-013"
title = "Sky-Limit-Set Correspondence (Experiment C)"
binary = "sky-limit-set"
binary_registered = true
binary_experiment_declared = "E-013"
method = "Compare ET skybox pattern invariants to Coxeter group limit set invariants. Step 1: For each CD level N=4,5,6, compute the skybox (G x G grid) and extract: - box-kite count (= N-1) - emptiness clustering exponent (connected-component size distribution) - DMZ density (fraction of cells that are DMZ) - fill/hide period-doubling structure (from Four Corners rule) Step 2: For candidate Coxeter groups (A_{N-1}, B_{N-1}, D_{N-1}), compute limit set invariants from the Coxeter matrix: - rank - fractal dimension of limit set - invariant measure density Step 3: Compare skybox invariants (Step 1) to limit set invariants (Step 2) via correlation and matching tests. Prediction (C-477): systematic correspondence between skybox pattern invariants and Coxeter limit set invariants at matching rank. Falsifier: no Coxeter group at any rank produces invariants matching the ET skybox."
input = "None (ET computation + Coxeter matrix computation)"
output = ["data/csv/sky_limit_set_comparison.csv"]
run = "cargo run --release --bin sky-limit-set -- --n-levels 4,5,6 --coxeter-types A,B,D"
run_command_sha256 = "37973a488de5e55a2b2de39142b5f51c3bfaf4ba184779a2c13fe48ace1022bf"
claims = ["C-477"]
claim_refs = ["C-477"]
deterministic = true
gpu = false
status = "active"
status_token = "ACTIVE"
lineage_id = "XL-013"
input_path_refs = []
output_path_refs = ["data/csv/sky_limit_set_comparison.csv"]
dataset_refs = ["PC-0056"]
reproducibility_class = "deterministic_replay"

[[experiment]]
id = "E-014"
title = "Dimensional Ladder APT Census (4D-4096D)"
binary = "dimensional-census"
binary_registered = true
binary_experiment_declared = ""
method = "Exhaustive and Monte Carlo Anti-Diagonal Parity Theorem verification across the full Cayley-Dickson dimensional ladder from dim=4 to dim=4096. Exhaustive mode: Enumerate ALL graph triangles in each zero-divisor component. For each triangle (a,b,c), compute eta via anti-diagonal parity formula: eta(a,b) = psi(lo_a, hi_b) XOR psi(hi_a, lo_b). Classify as pure (eta constant across 3 edges) or mixed (eta non-constant). Verify 1:3 pure:mixed ratio (Quarter Rule: pure*4 = total exactly). Verify Klein-four fiber symmetry: F(0,0) = F(0,1) = F(1,0) = F(1,1). Monte Carlo mode: Rejection-sample random node triples within components, accept only graph triangles (all 3 edges present), classify via APT. Deterministic with seed=42. Tier classification: dims 4-32 (TIER 0-1, <15s), dims 64-256 (TIER 2, ~5s release), dims 512-1024 (TIER 3, minutes-hours CPU), dims 2048-4096 (TIER 4, GPU Monte Carlo)."
input = "None (Cayley-Dickson structure tables computed on the fly)"
output = ["data/csv/apt_dimensional_census_summary.csv", "data/csv/apt_dimensional_census_details.csv"]
run = "cargo run --release --bin dimensional-census -- --slow --details"
run_command_sha256 = "ed72a3d2f1cac08e2a1787c6be706b5db2bda1acf02bb0f493ebc7107a7cd1be"
claims = ["C-570", "C-571", "C-572"]
claim_refs = ["C-570", "C-571", "C-572"]
deterministic = true
gpu = false
status = "active"
status_token = "ACTIVE"
lineage_id = "XL-014"
input_path_refs = []
output_path_refs = ["data/csv/apt_dimensional_census_summary.csv", "data/csv/apt_dimensional_census_details.csv"]
dataset_refs = ["PC-0002"]
reproducibility_class = "deterministic_replay"

