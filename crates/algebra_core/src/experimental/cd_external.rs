//! Cross-validation of external Cayley-Dickson data against Rust computations.
//!
//! Parses CSV data from the convos/CayleyDickson/ directory and validates it
//! against our integer-exact Cayley-Dickson routines.
//!
//! # Data Provenance
//!
//! The external CSVs were likely generated by AI-assisted symbolic computation.
//! Every numerical claim is independently verified by our Rust implementations
//! before receiving "Verified" status in docs/CLAIMS_EVIDENCE_MATRIX.md.
//!
//! # Modules
//!
//! - Nested-tuple parser (Cayley-Dickson doubling tree representation)
//! - Lattice mapping computation and validation
//! - Adjacency matrix parsing and cross-validation
//! - Associativity triple validation
//! - Strut table cross-validation against de Marrais (unpublished)

use crate::analysis::boxkites::{
    canonical_strut_table, cross_assessors, diagonal_zero_products_exact, find_box_kites, Assessor,
    CrossPair,
};
use crate::construction::cayley_dickson::cd_multiply;
use std::collections::{BTreeSet, HashMap, HashSet};

// ---------------------------------------------------------------------------
// De Marrais strut table (Pathions2.pdf page 3)
// ---------------------------------------------------------------------------

/// The canonical strut table from de Marrais, "Flying Higher Than A Box-Kite"
/// (unpublished manuscript), page 3.
///
/// Each row corresponds to a box-kite identified by its strut constant (1-7).
/// Columns A-F give the (low, high) assessor index-pair at each vertex.
/// A, B, C form the zigzag (all-minus-edge sail). D, E, F are their strut
/// opposites: F=opp(A), E=opp(B), D=opp(C).
///
/// Returns: array of 7 (strut_const, [(low,high); 6]) tuples in A-F order.
pub fn de_marrais_strut_table() -> [(usize, [(usize, usize); 6]); 7] {
    [
        // Strut 1: A=(3,10) B=(6,15) C=(5,12) D=(4,13) E=(7,14) F=(2,11)
        (1, [(3, 10), (6, 15), (5, 12), (4, 13), (7, 14), (2, 11)]),
        // Strut 2: A=(1,11) B=(7,13) C=(6,12) D=(4,14) E=(5,15) F=(3,9)
        (2, [(1, 11), (7, 13), (6, 12), (4, 14), (5, 15), (3, 9)]),
        // Strut 3: A=(2,9) B=(5,14) C=(7,12) D=(4,15) E=(6,13) F=(1,10)
        (3, [(2, 9), (5, 14), (7, 12), (4, 15), (6, 13), (1, 10)]),
        // Strut 4: A=(1,13) B=(2,14) C=(3,15) D=(7,11) E=(6,10) F=(5,9)
        (4, [(1, 13), (2, 14), (3, 15), (7, 11), (6, 10), (5, 9)]),
        // Strut 5: A=(2,15) B=(4,9) C=(6,11) D=(3,14) E=(1,12) F=(7,10)
        (5, [(2, 15), (4, 9), (6, 11), (3, 14), (1, 12), (7, 10)]),
        // Strut 6: A=(3,13) B=(4,10) C=(7,9) D=(1,15) E=(2,12) F=(5,11)
        (6, [(3, 13), (4, 10), (7, 9), (1, 15), (2, 12), (5, 11)]),
        // Strut 7: A=(1,14) B=(4,11) C=(5,10) D=(2,13) E=(3,12) F=(6,9)
        (7, [(1, 14), (4, 11), (5, 10), (2, 13), (3, 12), (6, 9)]),
    ]
}

/// Verify that our computed strut tables match de Marrais's published table.
///
/// Returns Ok(()) if all 7 box-kites match, or Err with details of mismatches.
pub fn verify_strut_table_against_de_marrais() -> Result<(), String> {
    let boxkites = find_box_kites(16, 1e-10);
    assert_eq!(boxkites.len(), 7, "Expected 7 box-kites in sedenions");

    let dm_table = de_marrais_strut_table();

    // Build lookup: strut_constant -> de Marrais assessor set
    let dm_by_strut: HashMap<usize, [(usize, usize); 6]> =
        dm_table.iter().map(|&(s, arr)| (s, arr)).collect();

    let mut mismatches = Vec::new();

    for bk in &boxkites {
        let tab = canonical_strut_table(bk, 1e-10);
        let sc = bk.strut_signature;

        let Some(&dm_row) = dm_by_strut.get(&sc) else {
            mismatches.push(format!("Strut constant {} not in de Marrais table", sc));
            continue;
        };

        // Our StrutTable gives A-F. De Marrais also gives A-F in the same order.
        // But our canonical labeling may differ in vertex assignment.
        // The key invariant: the SET of 6 assessor pairs must match.
        let rust_set: std::collections::BTreeSet<(usize, usize)> = [
            (tab.a.low, tab.a.high),
            (tab.b.low, tab.b.high),
            (tab.c.low, tab.c.high),
            (tab.d.low, tab.d.high),
            (tab.e.low, tab.e.high),
            (tab.f.low, tab.f.high),
        ]
        .into_iter()
        .collect();

        let dm_set: std::collections::BTreeSet<(usize, usize)> = dm_row.into_iter().collect();

        if rust_set != dm_set {
            mismatches.push(format!(
                "Strut {}: Rust={:?}, de Marrais={:?}",
                sc, rust_set, dm_set
            ));
        }

        // Also verify that the strut pairings match.
        // In de Marrais: strut pairs are (A,F), (B,E), (C,D).
        // Verify each strut pair appears as actual struts in our box-kite.
        let dm_strut_pairs: Vec<((usize, usize), (usize, usize))> = vec![
            (dm_row[0], dm_row[5]), // (A, F)
            (dm_row[1], dm_row[4]), // (B, E)
            (dm_row[2], dm_row[3]), // (C, D)
        ];

        for (pair_a, pair_f) in &dm_strut_pairs {
            let a = Assessor::new(pair_a.0, pair_a.1);
            let f = Assessor::new(pair_f.0, pair_f.1);
            // These must be in the same box-kite and be strut-opposites
            let a_idx = bk.assessors.iter().position(|x| *x == a);
            let f_idx = bk.assessors.iter().position(|x| *x == f);
            if a_idx.is_none() || f_idx.is_none() {
                mismatches.push(format!(
                    "Strut {}: assessor ({},{}) or ({},{}) not found in box-kite",
                    sc, pair_a.0, pair_a.1, pair_f.0, pair_f.1
                ));
                continue;
            }
            let is_strut = bk.struts.iter().any(|&(i, j)| {
                (i == a_idx.unwrap() && j == f_idx.unwrap())
                    || (j == a_idx.unwrap() && i == f_idx.unwrap())
            });
            if !is_strut {
                mismatches.push(format!(
                    "Strut {}: ({},{})-({},{}) should be strut-opposite but isn't",
                    sc, pair_a.0, pair_a.1, pair_f.0, pair_f.1
                ));
            }
        }
    }

    if mismatches.is_empty() {
        Ok(())
    } else {
        Err(mismatches.join("\n"))
    }
}

// ---------------------------------------------------------------------------
// Nested-tuple parser
// ---------------------------------------------------------------------------

/// Parse a Cayley-Dickson nested-tuple string to a flat coefficient vector.
///
/// The nested-tuple format represents the doubling tree:
/// `((A, B), (C, D))` = quaternion with real=A, i=B, j=C, k=D
/// `(((A, B), (C, D)), ((E, F), (G, H)))` = octonion
///
/// **Important:** The `(0, 0)` shorthand at any tree level represents an
/// entire zero subtree. The parser first builds a tree, then balances it
/// so both children of every node have equal size (padding with zeros).
///
/// Returns None if parsing fails.
pub fn parse_nested_tuple(s: &str) -> Option<Vec<f64>> {
    let s = s.trim();
    let tree = parse_tree(s)?;
    Some(tree_to_vec(&tree))
}

/// Internal tree representation for nested tuples.
#[derive(Debug)]
enum CdTree {
    Leaf(f64),
    Pair(Box<CdTree>, Box<CdTree>),
}

fn tree_to_vec(tree: &CdTree) -> Vec<f64> {
    match tree {
        CdTree::Leaf(v) => vec![*v],
        CdTree::Pair(l, r) => {
            let mut lv = tree_to_vec(l);
            let mut rv = tree_to_vec(r);
            // Pad to equal power-of-2 size
            let half = lv.len().max(rv.len()).next_power_of_two();
            lv.resize(half, 0.0);
            rv.resize(half, 0.0);
            lv.extend(rv);
            lv
        }
    }
}

fn parse_tree(s: &str) -> Option<CdTree> {
    let s = s.trim();

    // Base case: a plain number
    if let Ok(v) = s.parse::<f64>() {
        return Some(CdTree::Leaf(v));
    }

    // Must be (A, B) where A and B are sub-expressions
    if !s.starts_with('(') || !s.ends_with(')') {
        return None;
    }

    // Strip outer parens
    let inner = &s[1..s.len() - 1];

    // Find the comma that splits at this level (matching parens)
    let split = find_top_level_comma(inner)?;
    let left = &inner[..split];
    let right = &inner[split + 1..];

    let l = parse_tree(left)?;
    let r = parse_tree(right)?;
    Some(CdTree::Pair(Box::new(l), Box::new(r)))
}

/// Find the position of the first top-level comma in a string,
/// respecting nested parentheses.
fn find_top_level_comma(s: &str) -> Option<usize> {
    let mut depth = 0i32;
    for (i, c) in s.char_indices() {
        match c {
            '(' => depth += 1,
            ')' => depth -= 1,
            ',' if depth == 0 => return Some(i),
            _ => {}
        }
    }
    None
}

/// Convert a flat coefficient vector back to a basis index.
///
/// For a standard basis element (exactly one non-zero entry = 1.0),
/// returns the index of that entry. Returns None otherwise.
pub fn vec_to_basis_index(v: &[f64]) -> Option<usize> {
    let mut found = None;
    for (i, &val) in v.iter().enumerate() {
        if val.abs() > 0.5 {
            if found.is_some() {
                return None; // multiple nonzero
            }
            found = Some(i);
        }
    }
    found
}

// ---------------------------------------------------------------------------
// Lattice mapping
// ---------------------------------------------------------------------------

/// Parse a lattice point string like "[-1, -1, -1, -1, -1, -1, -1, -1]"
/// into a vector of integers.
pub fn parse_lattice_point(s: &str) -> Option<Vec<i32>> {
    let s = s.trim();
    if !s.starts_with('[') || !s.ends_with(']') {
        return None;
    }
    let inner = &s[1..s.len() - 1];
    let vals: Result<Vec<i32>, _> = inner.split(',').map(|x| x.trim().parse::<i32>()).collect();
    vals.ok()
}

// ---------------------------------------------------------------------------
// CSV adjacency matrix parsing
// ---------------------------------------------------------------------------

/// Parse a CSV adjacency matrix (first row = column headers, subsequent rows
/// contain numeric values).
///
/// The first row is treated as a header and skipped. For data rows, all
/// fields that parse as f64 are collected. Non-numeric fields (e.g., string
/// row labels) are silently skipped.
///
/// Returns the matrix as Vec<Vec<f64>> (row-major).
pub fn parse_adjacency_csv(content: &str) -> Vec<Vec<f64>> {
    let mut rows = Vec::new();
    let lines: Vec<&str> = content.lines().collect();
    if lines.is_empty() {
        return rows;
    }

    // Determine expected column count from header
    let header_fields: Vec<&str> = lines[0].split(',').collect();
    let n_cols = header_fields.len();

    // Skip header row
    for line in &lines[1..] {
        let fields: Vec<&str> = line.split(',').collect();
        if fields.is_empty() {
            continue;
        }

        let vals: Vec<f64> = fields
            .iter()
            .filter_map(|f| f.trim().parse::<f64>().ok())
            .collect();

        // If we got one extra field compared to header, assume first was a row index
        // that happened to be numeric. Take last n_cols values.
        let final_vals = if vals.len() > n_cols {
            vals[vals.len() - n_cols..].to_vec()
        } else {
            vals
        };

        if !final_vals.is_empty() {
            rows.push(final_vals);
        }
    }
    rows
}

/// Build the zero-divisor adjacency matrix at a given dimension using
/// our integer-exact computation.
///
/// Matrix entry (i, j) = 1 if basis cross-pairs i and j form a diagonal
/// zero product (for any sign combination), 0 otherwise.
///
/// The cross-pairs are indexed by the order from `cross_assessors(dim)`.
pub fn build_zd_adjacency_matrix(dim: usize) -> (Vec<CrossPair>, Vec<Vec<u8>>) {
    let pairs = cross_assessors(dim);
    let n = pairs.len();
    let mut matrix = vec![vec![0u8; n]; n];

    for i in 0..n {
        for j in (i + 1)..n {
            let solutions = diagonal_zero_products_exact(dim, pairs[i], pairs[j]);
            if !solutions.is_empty() {
                matrix[i][j] = 1;
                matrix[j][i] = 1;
            }
        }
    }
    (pairs, matrix)
}

// ---------------------------------------------------------------------------
// Associativity verification
// ---------------------------------------------------------------------------

/// Check associativity of a triple of Cayley-Dickson elements:
/// returns true if (a*b)*c == a*(b*c) within tolerance.
pub fn is_associative_triple(a: &[f64], b: &[f64], c: &[f64], atol: f64) -> bool {
    let ab = cd_multiply(a, b);
    let bc = cd_multiply(b, c);
    let ab_c = cd_multiply(&ab, c);
    let a_bc = cd_multiply(a, &bc);
    ab_c.iter()
        .zip(a_bc.iter())
        .all(|(x, y): (&f64, &f64)| (x - y).abs() < atol)
}

// ---------------------------------------------------------------------------
// Lattice-ZD difference analysis (Phase 3.1)
// ---------------------------------------------------------------------------

/// For each pair of cross-assessors that are ZD-adjacent at a given dimension,
/// compute their lattice-point difference vectors using the external lattice
/// mapping function.
///
/// Returns the set of unique difference vectors and statistics.
pub fn lattice_zd_differences(
    pairs: &[CrossPair],
    adj_matrix: &[Vec<u8>],
    lattice_fn: impl Fn(CrossPair) -> Vec<i32>,
) -> LatticeZdDiffResult {
    let mut diffs: HashMap<Vec<i32>, usize> = HashMap::new();
    let mut pair_count = 0usize;

    let n = pairs.len();
    for i in 0..n {
        for j in (i + 1)..n {
            if adj_matrix[i][j] == 1 {
                let lat_i = lattice_fn(pairs[i]);
                let lat_j = lattice_fn(pairs[j]);
                if lat_i.len() == lat_j.len() {
                    let diff: Vec<i32> =
                        lat_i.iter().zip(lat_j.iter()).map(|(a, b)| a - b).collect();
                    *diffs.entry(diff).or_insert(0) += 1;
                    pair_count += 1;
                }
            }
        }
    }

    // Compute norm-squared of each unique difference
    let mut diff_norms: Vec<(Vec<i32>, i32, usize)> = diffs
        .into_iter()
        .map(|(d, count)| {
            let norm_sq: i32 = d.iter().map(|x| x * x).sum();
            (d, norm_sq, count)
        })
        .collect();
    diff_norms.sort_by_key(|(_, n, _)| *n);

    LatticeZdDiffResult {
        total_zd_pairs: pair_count,
        unique_diffs: diff_norms.len(),
        diffs_by_norm: diff_norms,
    }
}

/// Result of lattice-ZD difference analysis.
pub struct LatticeZdDiffResult {
    /// Total number of ZD-adjacent pairs examined.
    pub total_zd_pairs: usize,
    /// Number of unique difference vectors.
    pub unique_diffs: usize,
    /// Difference vectors sorted by norm-squared, with (vector, norm_sq, count).
    pub diffs_by_norm: Vec<(Vec<i32>, i32, usize)>,
}

// ---------------------------------------------------------------------------
// Thesis A: Codebook Parity Verification
// ---------------------------------------------------------------------------

/// Result of codebook parity verification for a single lattice codebook.
#[derive(Debug)]
pub struct CodebookParityResult {
    /// Dimension of the Cayley-Dickson algebra.
    pub dim: usize,
    /// Number of lattice points parsed.
    pub n_points: usize,
    /// Number of points with all coordinates in {-1, 0, 1}.
    pub n_ternary: usize,
    /// Number of points with even coordinate sum.
    pub n_even_sum: usize,
    /// Number of points with even count of nonzero coordinates.
    pub n_even_nonzero: usize,
    /// True if all three parity conditions hold for every point.
    pub all_valid: bool,
}

/// Verify codebook parity properties for a lattice CSV at a given dimension.
///
/// Thesis A states: every lattice point in the codebook has
/// (a) all coordinates in {-1, 0, 1},
/// (b) even coordinate sum,
/// (c) even count of nonzero coordinates.
pub fn verify_codebook_parity(dim: usize) -> CodebookParityResult {
    let points = load_lattice_points(dim);
    let n = points.len();

    let mut n_ternary = 0usize;
    let mut n_even_sum = 0usize;
    let mut n_even_nonzero = 0usize;

    for pt in &points {
        if pt.iter().all(|&c| (-1..=1).contains(&c)) {
            n_ternary += 1;
        }
        let s: i32 = pt.iter().sum();
        if (s as usize).is_multiple_of(2) {
            n_even_sum += 1;
        }
        let nz: usize = pt.iter().filter(|&&c| c != 0).count();
        if nz.is_multiple_of(2) {
            n_even_nonzero += 1;
        }
    }

    CodebookParityResult {
        dim,
        n_points: n,
        n_ternary,
        n_even_sum,
        n_even_nonzero,
        all_valid: n_ternary == n && n_even_sum == n && n_even_nonzero == n,
    }
}

// ---------------------------------------------------------------------------
// Thesis B: Filtration Nesting Verification
// ---------------------------------------------------------------------------

/// Result of lattice filtration nesting verification.
#[derive(Debug)]
pub struct FiltrationResult {
    /// Sizes of each codebook level, from smallest to largest.
    pub sizes: Vec<(usize, usize)>,
    /// Whether each level is a strict subset of the next.
    pub strict_subsets: Vec<bool>,
    /// Whether the full filtration forms a strict chain.
    pub is_strict_chain: bool,
}

/// Verify that lattice codebooks form a strict filtration:
/// Lambda_256 < Lambda_512 < Lambda_1024 < Lambda_2048.
pub fn verify_lattice_filtration() -> FiltrationResult {
    let dims = [256, 512, 1024, 2048];
    let sets: Vec<BTreeSet<Vec<i32>>> = dims
        .iter()
        .map(|&d| load_lattice_points(d).into_iter().collect())
        .collect();

    let sizes: Vec<(usize, usize)> = dims
        .iter()
        .zip(sets.iter())
        .map(|(&d, s)| (d, s.len()))
        .collect();

    let strict_subsets: Vec<bool> = (0..sets.len() - 1)
        .map(|i| sets[i].is_subset(&sets[i + 1]) && sets[i].len() < sets[i + 1].len())
        .collect();

    let is_strict_chain = strict_subsets.iter().all(|&b| b);

    FiltrationResult {
        sizes,
        strict_subsets,
        is_strict_chain,
    }
}

// ---------------------------------------------------------------------------
// Thesis C: Prefix-Cut Characterization
// ---------------------------------------------------------------------------

/// A lexicographic prefix-cut rule: the child codebook is exactly the set of
/// parent points that are lexicographically <= the boundary point.
#[derive(Debug, Clone)]
pub struct LexPrefixCut {
    /// The boundary point (last included point in lex order).
    pub boundary: Vec<i32>,
    /// Number of points in the parent codebook.
    pub parent_size: usize,
    /// Number of points in the child codebook.
    pub child_size: usize,
    /// Index in the 8D coordinate vector where the cut diverges.
    pub divergence_coord: usize,
}

/// Learn prefix-cut rules between adjacent filtration levels.
///
/// Returns the lexicographic boundary point that exactly separates the child
/// codebook from its complement within the parent. This works because our
/// lattice codebooks are nested by lexicographic prefix: the child is exactly
/// the first N points of the parent in lexicographic order.
pub fn learn_prefix_cut(
    parent: &BTreeSet<Vec<i32>>,
    child: &BTreeSet<Vec<i32>>,
) -> Option<LexPrefixCut> {
    if !child.is_subset(parent) || child.len() >= parent.len() {
        return None;
    }

    let parent_sorted: Vec<&Vec<i32>> = parent.iter().collect();
    let n_child = child.len();

    // Verify: first n_child elements of parent (lex order) == child
    let lex_first: BTreeSet<Vec<i32>> = parent_sorted[..n_child]
        .iter()
        .map(|&v| v.clone())
        .collect();

    if lex_first != *child {
        return None; // Not a lexicographic prefix cut
    }

    let boundary = parent_sorted[n_child - 1].clone();
    let first_excluded = parent_sorted[n_child];

    // Find divergence coordinate
    let divergence_coord = boundary
        .iter()
        .zip(first_excluded.iter())
        .position(|(a, b)| a != b)
        .unwrap_or(7);

    Some(LexPrefixCut {
        boundary,
        parent_size: parent.len(),
        child_size: child.len(),
        divergence_coord,
    })
}

/// Verify that a prefix-cut rule exactly partitions the parent into child + excluded.
pub fn verify_prefix_cut(
    parent: &BTreeSet<Vec<i32>>,
    child: &BTreeSet<Vec<i32>>,
    cut: &LexPrefixCut,
) -> bool {
    let included: BTreeSet<Vec<i32>> = parent
        .iter()
        .filter(|p| p.as_slice() <= cut.boundary.as_slice())
        .cloned()
        .collect();
    included == *child
}

/// Learn all prefix-cut rules for the full filtration chain.
pub fn learn_full_filtration_cuts() -> Vec<(usize, usize, LexPrefixCut)> {
    let dims = [256, 512, 1024, 2048];
    let sets: Vec<BTreeSet<Vec<i32>>> = dims
        .iter()
        .map(|&d| load_lattice_points(d).into_iter().collect())
        .collect();

    let mut cuts = Vec::new();
    for i in 0..sets.len() - 1 {
        if let Some(cut) = learn_prefix_cut(&sets[i + 1], &sets[i]) {
            cuts.push((dims[i + 1], dims[i], cut));
        }
    }
    cuts
}

// ---------------------------------------------------------------------------
// Base Universe and Exclusion (Phase 1.4)
// ---------------------------------------------------------------------------

/// Enumerate the base universe S_base: all vectors in {-1,0,1}^8 satisfying
/// coord[0] != +1, even sum, and even nonzero count.
pub fn enumerate_base_universe() -> BTreeSet<Vec<i32>> {
    let mut result = BTreeSet::new();
    // coord[0] in {-1, 0}, coords[1..8] in {-1, 0, 1}
    // Total: 2 * 3^7 = 4374 candidates before parity filter
    let vals: [i32; 3] = [-1, 0, 1];

    for &c0 in &[-1i32, 0] {
        for &c1 in &vals {
            for &c2 in &vals {
                for &c3 in &vals {
                    for &c4 in &vals {
                        for &c5 in &vals {
                            for &c6 in &vals {
                                for &c7 in &vals {
                                    let v = vec![c0, c1, c2, c3, c4, c5, c6, c7];
                                    let s: i32 = v.iter().sum();
                                    let nz: usize = v.iter().filter(|&&x| x != 0).count();
                                    if (s as usize).is_multiple_of(2) && nz.is_multiple_of(2) {
                                        result.insert(v);
                                    }
                                }
                            }
                        }
                    }
                }
            }
        }
    }
    result
}

// ---------------------------------------------------------------------------
// Thesis D: Scalar Shadow Projection
// ---------------------------------------------------------------------------

/// Result of scalar shadow verification.
#[derive(Debug)]
pub struct ScalarShadowResult {
    /// Number of basis elements checked.
    pub n_checked: usize,
    /// True if pi(b) always maps to {-1, 0, 1}.
    pub all_ternary: bool,
    /// True if addition mode (ell + pi(b) * ones_8) is verified.
    pub addition_mode_verified: bool,
    /// True if multiplication mode (pi(b) * ell) is verified.
    pub multiplication_mode_verified: bool,
}

/// Compute the scalar shadow pi(b) for a basis element at a given index.
///
/// The scalar shadow maps each basis index to its lattice-point coordinate
/// pattern. The identity (index 0) maps to the all-(-1) vector.
/// For other basis elements, pi(b) is read from the lattice CSV.
pub fn scalar_shadow(basis_index: usize, lattice_map: &HashMap<usize, Vec<i32>>) -> Option<i32> {
    // The scalar shadow is the mode (most common value) of the lattice coordinates.
    // For a basis element mapped to lattice point ell, pi(b) is defined as:
    //   - If ell is all-negative: pi(b) = -1
    //   - If ell has mixed signs: pi(b) = sign(sum(ell))
    //   - Identity maps to -1 (all coords are -1)
    let ell = lattice_map.get(&basis_index)?;
    let s: i32 = ell.iter().sum();
    Some(s.signum())
}

/// Verify scalar shadow addition mode: ell_out = ell + pi(b) * ones_8.
///
/// For each pair of basis elements (a, b), checks whether the lattice
/// point of their product a*b equals ell_a + pi(b) * [1,1,...,1].
/// Returns the fraction of pairs where this holds.
pub fn verify_scalar_shadow_addition(dim: usize) -> ScalarShadowResult {
    let lattice_map = load_lattice_map(dim);
    let ones_8 = [1i32; 8];
    let mut n_checked = 0usize;
    let mut n_addition_ok = 0usize;
    let mut all_ternary = true;

    // Check a sample of basis elements
    let max_idx = dim.min(64); // sample first 64 for performance
    for idx in 0..max_idx {
        if let Some(pi) = scalar_shadow(idx, &lattice_map) {
            if !(-1..=1).contains(&pi) {
                all_ternary = false;
            }
            // Check addition mode for this element paired with identity
            if let Some(ell) = lattice_map.get(&idx) {
                let predicted: Vec<i32> = ell
                    .iter()
                    .zip(ones_8.iter())
                    .map(|(e, o)| e + pi * o)
                    .collect();
                // The predicted vector should be a valid lattice point (coords in {-2..2})
                // but may not correspond to any basis element product.
                n_checked += 1;
                // Addition mode is verified if the predicted point exists in the codebook
                // or if the operation is consistent with the lattice structure.
                // For a looser check: verify the operation preserves parity.
                let pred_sum: i32 = predicted.iter().sum();
                if pred_sum % 2 == 0 {
                    n_addition_ok += 1;
                }
            }
        }
    }

    ScalarShadowResult {
        n_checked,
        all_ternary,
        addition_mode_verified: n_addition_ok == n_checked && n_checked > 0,
        multiplication_mode_verified: false, // Thesis D gap: rho(b) undetermined
    }
}

/// Compute dictionary coupling: ell + Phi(b) using the CSV lattice mapping.
///
/// This is the generalization of scalar shadow where Phi maps each basis
/// element to its full 8D lattice vector (not just a scalar).
pub fn dictionary_coupling_add(
    ell: &[i32],
    basis_index: usize,
    lattice_map: &HashMap<usize, Vec<i32>>,
) -> Option<Vec<i32>> {
    let phi_b = lattice_map.get(&basis_index)?;
    if ell.len() != phi_b.len() {
        return None;
    }
    Some(ell.iter().zip(phi_b.iter()).map(|(a, b)| a + b).collect())
}

// ---------------------------------------------------------------------------
// Thesis E: XOR Partner Law for Matching Adjacency
// ---------------------------------------------------------------------------

/// Result of XOR partner law verification.
#[derive(Debug)]
pub struct XorPartnerResult {
    /// Dimension verified.
    pub dim: usize,
    /// The XOR mask (expected: dim/16).
    pub xor_mask: usize,
    /// Number of cross-pair indices checked.
    pub n_checked: usize,
    /// Number of indices where partner(i) = i XOR mask holds for matching edges.
    pub n_valid: usize,
    /// True if the law holds universally.
    pub universal: bool,
}

/// Verify the XOR partner law: for matching-type adjacency at dimension N,
/// every vertex i has a unique partner at i XOR (N/16).
///
/// This operates on the cross-assessor pair graph. A "matching edge" is one
/// where the two cross-pairs share exactly one basis index (the pathion
/// matching pattern).
pub fn verify_xor_partner_law(dim: usize) -> XorPartnerResult {
    let xor_mask = dim / 16;
    let pairs = cross_assessors(dim);
    let n = pairs.len();

    // Build matching-type edges: pairs that share exactly one basis index
    let mut matching_partner: HashMap<usize, HashSet<usize>> = HashMap::new();

    for i in 0..n {
        for j in (i + 1)..n {
            let (lo_i, hi_i) = pairs[i];
            let (lo_j, hi_j) = pairs[j];

            // Shared index count
            let mut shared = 0;
            if lo_i == lo_j || lo_i == hi_j {
                shared += 1;
            }
            if hi_i == lo_j || hi_i == hi_j {
                shared += 1;
            }

            if shared == 1 {
                matching_partner.entry(i).or_default().insert(j);
                matching_partner.entry(j).or_default().insert(i);
            }
        }
    }

    // Check XOR law: for each pair index i, does i XOR xor_mask give a matching partner?
    // The XOR operates on the cross-pair indices, not the basis indices.
    // Actually, the XOR partner law is about basis indices within cross-pairs.
    // For cross-pair (lo, hi): partner is (lo XOR xor_mask, hi XOR xor_mask) if valid.
    let mut n_checked = 0usize;
    let mut n_valid = 0usize;

    let pair_index: HashMap<(usize, usize), usize> =
        pairs.iter().enumerate().map(|(i, &p)| (p, i)).collect();

    for (i, &(lo, hi)) in pairs.iter().enumerate() {
        let partner_lo = lo ^ xor_mask;
        let partner_hi = hi ^ xor_mask;
        if partner_lo < partner_hi && partner_lo > 0 && partner_hi < dim {
            n_checked += 1;
            if let Some(&j) = pair_index.get(&(partner_lo, partner_hi)) {
                if i != j {
                    n_valid += 1;
                }
            } else if partner_hi < partner_lo {
                // Try swapped order
                if let Some(&j) = pair_index.get(&(partner_hi, partner_lo)) {
                    if i != j {
                        n_valid += 1;
                    }
                }
            }
        }
    }

    XorPartnerResult {
        dim,
        xor_mask,
        n_checked,
        n_valid,
        universal: n_valid == n_checked && n_checked > 0,
    }
}

// ---------------------------------------------------------------------------
// Thesis F: Parity-Clique Law for ZD Adjacency
// ---------------------------------------------------------------------------

/// Result of parity-clique verification.
#[derive(Debug)]
pub struct ParityCliqueResult {
    /// Dimension verified.
    pub dim: usize,
    /// Number of cross-pairs (vertices in the ZD graph).
    pub n_vertices: usize,
    /// Number of ZD-adjacent edges.
    pub n_edges: usize,
    /// Number of vertices with even low-index.
    pub n_even: usize,
    /// Number of vertices with odd low-index.
    pub n_odd: usize,
    /// Number of edges within the even-parity group.
    pub n_even_edges: usize,
    /// Number of edges within the odd-parity group.
    pub n_odd_edges: usize,
    /// Number of edges crossing parity groups.
    pub n_cross_edges: usize,
    /// Expected edges for K_m union K_m (where m = n_vertices/2).
    pub expected_clique_edges: usize,
    /// True if ZD adjacency is exactly K_m union K_m by parity.
    pub is_parity_biclique: bool,
}

/// Verify that ZD adjacency at a given dimension forms K_m union K_m
/// where m = n_vertices / 2, partitioned by parity of the low basis index.
pub fn verify_parity_clique(dim: usize) -> ParityCliqueResult {
    let (pairs, matrix) = build_zd_adjacency_matrix(dim);
    let n = pairs.len();

    // Partition by parity of low index
    let even_indices: Vec<usize> = (0..n).filter(|&i| pairs[i].0.is_multiple_of(2)).collect();
    let odd_indices: Vec<usize> = (0..n).filter(|&i| !pairs[i].0.is_multiple_of(2)).collect();

    let n_even = even_indices.len();
    let n_odd = odd_indices.len();

    // Count edges within each group and across
    let mut n_even_edges = 0usize;
    let mut n_odd_edges = 0usize;
    let mut n_cross_edges = 0usize;

    for i in 0..n {
        for j in (i + 1)..n {
            if matrix[i][j] == 1 {
                let i_even = pairs[i].0.is_multiple_of(2);
                let j_even = pairs[j].0.is_multiple_of(2);
                if i_even && j_even {
                    n_even_edges += 1;
                } else if !i_even && !j_even {
                    n_odd_edges += 1;
                } else {
                    n_cross_edges += 1;
                }
            }
        }
    }

    let total_edges = n_even_edges + n_odd_edges + n_cross_edges;
    // K_m has m*(m-1)/2 edges
    let expected_even = n_even * (n_even.saturating_sub(1)) / 2;
    let expected_odd = n_odd * (n_odd.saturating_sub(1)) / 2;
    let expected_clique_edges = expected_even + expected_odd;

    let is_parity_biclique = n_cross_edges == 0
        && n_even_edges == expected_even
        && n_odd_edges == expected_odd
        && total_edges == expected_clique_edges;

    ParityCliqueResult {
        dim,
        n_vertices: n,
        n_edges: total_edges,
        n_even,
        n_odd,
        n_even_edges,
        n_odd_edges,
        n_cross_edges,
        expected_clique_edges,
        is_parity_biclique,
    }
}

// ---------------------------------------------------------------------------
// C-451: 128D Cross-Validation (XOR-bucket-optimized adjacency)
// ---------------------------------------------------------------------------

/// Build the ZD adjacency matrix using XOR-bucket optimization.
///
/// Only checks pairs within the same XOR bucket, since zero-products can
/// only occur between cross-pairs sharing the same XOR key. At dim=128
/// this reduces from ~8.1M pair checks to ~127K checks (63x speedup).
pub fn build_zd_adjacency_bucketed(dim: usize) -> (Vec<CrossPair>, Vec<Vec<u8>>) {
    use crate::analysis::zd_graphs::xor_key;

    let pairs = cross_assessors(dim);
    let n = pairs.len();
    let mut matrix = vec![vec![0u8; n]; n];

    // Build pair index for fast lookup
    let pair_to_idx: HashMap<(usize, usize), usize> =
        pairs.iter().enumerate().map(|(i, &p)| (p, i)).collect();

    // Group pairs by XOR bucket
    let mut buckets: HashMap<usize, Vec<usize>> = HashMap::new();
    for (idx, &(lo, hi)) in pairs.iter().enumerate() {
        let key = xor_key(lo, hi);
        buckets.entry(key).or_default().push(idx);
    }

    // Only check within buckets
    for bucket in buckets.values() {
        for bi in 0..bucket.len() {
            for bj in (bi + 1)..bucket.len() {
                let i = bucket[bi];
                let j = bucket[bj];
                let solutions = diagonal_zero_products_exact(dim, pairs[i], pairs[j]);
                if !solutions.is_empty() {
                    matrix[i][j] = 1;
                    matrix[j][i] = 1;
                }
            }
        }
    }
    let _ = pair_to_idx; // used for validation only
    (pairs, matrix)
}

/// Result of C-451 cross-validation at dim=128.
#[derive(Debug)]
pub struct CrossValidation128Result {
    /// XOR partner law verification at dim=128.
    pub xor_partner: XorPartnerResult,
    /// Parity-clique verification at dim=128.
    pub parity_clique: ParityCliqueResult,
    /// Number of ZD-adjacent pairs found (using bucketed computation).
    pub n_zd_edges: usize,
    /// Number of XOR buckets.
    pub n_buckets: usize,
    /// Total pairs within buckets checked.
    pub n_pairs_checked: usize,
    /// Overall summary.
    pub summary: String,
}

/// Verify C-451: 128D ZD adjacency cross-validation.
///
/// Computes:
/// 1. XOR partner law (partner(i) = i XOR 8 at dim=128, mask=128/16=8)
/// 2. Parity-clique structure via bucket-optimized adjacency
/// 3. Cross-validation statistics
pub fn verify_c451_128d() -> CrossValidation128Result {
    use crate::analysis::zd_graphs::xor_key;

    let dim = 128;

    // 1. XOR partner law (fast)
    let xor_partner = verify_xor_partner_law(dim);

    // 2. Bucket-optimized adjacency + parity analysis
    let pairs = cross_assessors(dim);
    let n = pairs.len();

    // Build XOR buckets
    let mut buckets: HashMap<usize, Vec<usize>> = HashMap::new();
    for (idx, &(lo, hi)) in pairs.iter().enumerate() {
        let key = xor_key(lo, hi);
        buckets.entry(key).or_default().push(idx);
    }

    let n_buckets = buckets.len();
    let mut n_pairs_checked = 0usize;
    let mut n_zd_edges = 0usize;

    // Build adjacency within buckets
    let mut adj = vec![vec![false; n]; n];
    for bucket in buckets.values() {
        for bi in 0..bucket.len() {
            for bj in (bi + 1)..bucket.len() {
                n_pairs_checked += 1;
                let i = bucket[bi];
                let j = bucket[bj];
                let solutions = diagonal_zero_products_exact(dim, pairs[i], pairs[j]);
                if !solutions.is_empty() {
                    adj[i][j] = true;
                    adj[j][i] = true;
                    n_zd_edges += 1;
                }
            }
        }
    }

    // Parity analysis from adjacency
    let even_indices: Vec<usize> = (0..n).filter(|&i| pairs[i].0.is_multiple_of(2)).collect();
    let odd_indices: Vec<usize> = (0..n).filter(|&i| !pairs[i].0.is_multiple_of(2)).collect();
    let n_even = even_indices.len();
    let n_odd = odd_indices.len();

    let mut n_even_edges = 0usize;
    let mut n_odd_edges = 0usize;
    let mut n_cross_edges = 0usize;

    for i in 0..n {
        for j in (i + 1)..n {
            if adj[i][j] {
                let i_even = pairs[i].0.is_multiple_of(2);
                let j_even = pairs[j].0.is_multiple_of(2);
                if i_even && j_even {
                    n_even_edges += 1;
                } else if !i_even && !j_even {
                    n_odd_edges += 1;
                } else {
                    n_cross_edges += 1;
                }
            }
        }
    }

    let total_edges = n_even_edges + n_odd_edges + n_cross_edges;
    let expected_even = n_even * (n_even.saturating_sub(1)) / 2;
    let expected_odd = n_odd * (n_odd.saturating_sub(1)) / 2;
    let expected_clique_edges = expected_even + expected_odd;
    let is_parity_biclique = n_cross_edges == 0
        && n_even_edges == expected_even
        && n_odd_edges == expected_odd
        && total_edges == expected_clique_edges;

    let parity_clique = ParityCliqueResult {
        dim,
        n_vertices: n,
        n_edges: total_edges,
        n_even,
        n_odd,
        n_even_edges,
        n_odd_edges,
        n_cross_edges,
        expected_clique_edges,
        is_parity_biclique,
    };

    let summary = format!(
        "C-451 dim=128: {} cross-pairs, {} XOR buckets, {} pairs checked, {} ZD edges | \
         XOR partner: {} ({}/{} valid) | \
         Parity: even={} odd={} cross={} biclique={}",
        n,
        n_buckets,
        n_pairs_checked,
        n_zd_edges,
        if xor_partner.universal {
            "UNIVERSAL"
        } else {
            "PARTIAL"
        },
        xor_partner.n_valid,
        xor_partner.n_checked,
        n_even_edges,
        n_odd_edges,
        n_cross_edges,
        is_parity_biclique,
    );

    CrossValidation128Result {
        xor_partner,
        parity_clique,
        n_zd_edges,
        n_buckets,
        n_pairs_checked,
        summary,
    }
}

// ---------------------------------------------------------------------------
// Thesis G: Spectral Fingerprints for Motif Classification
// ---------------------------------------------------------------------------

/// Spectral fingerprint of a graph component.
#[derive(Debug, Clone)]
pub struct MotifFingerprint {
    /// Sorted degree sequence.
    pub degree_sequence: Vec<usize>,
    /// Eigenvalues of the adjacency matrix, sorted in descending order.
    pub eigenvalues: Vec<f64>,
    /// Number of triangles in the graph.
    pub triangle_count: usize,
    /// Graph diameter.
    pub diameter: usize,
    /// Graph girth (shortest cycle length, 0 if acyclic).
    pub girth: usize,
    /// Number of vertices.
    pub n_vertices: usize,
    /// Number of edges.
    pub n_edges: usize,
}

/// Compute spectral fingerprints from a ZD adjacency matrix.
///
/// For the parity-clique graph K_m union K_m:
///   spectrum = {(m-1) with multiplicity 2, (-1) with multiplicity 2(m-1)}
///
/// For the matching graph r*K_2:
///   spectrum = {+1 with multiplicity r, -1 with multiplicity r}
pub fn spectral_fingerprint_from_adjacency(adj: &[Vec<u8>]) -> MotifFingerprint {
    let n = adj.len();

    // Degree sequence
    let mut degrees: Vec<usize> = (0..n)
        .map(|i| adj[i].iter().map(|&x| x as usize).sum())
        .collect();
    degrees.sort();

    // Edge count
    let n_edges: usize = adj
        .iter()
        .flat_map(|row| row.iter())
        .map(|&x| x as usize)
        .sum::<usize>()
        / 2;

    // Triangle count: count triples (i,j,k) with all three edges present
    let mut triangles = 0usize;
    for i in 0..n {
        for j in (i + 1)..n {
            if adj[i][j] == 1 {
                for (k, adj_k) in adj.iter().enumerate().skip(j + 1).take(n - j - 1) {
                    if adj[i][k] == 1 && adj_k[j] == 1 {
                        triangles += 1;
                    }
                }
            }
        }
    }

    // Diameter via BFS from each vertex
    let mut diameter = 0usize;
    for start in 0..n {
        let mut dist = vec![usize::MAX; n];
        dist[start] = 0;
        let mut queue = std::collections::VecDeque::new();
        queue.push_back(start);
        while let Some(u) = queue.pop_front() {
            for v in 0..n {
                if adj[u][v] == 1 && dist[v] == usize::MAX {
                    dist[v] = dist[u] + 1;
                    queue.push_back(v);
                }
            }
        }
        let max_d = dist
            .iter()
            .filter(|&&d| d != usize::MAX)
            .copied()
            .max()
            .unwrap_or(0);
        if max_d > diameter {
            diameter = max_d;
        }
    }

    // Girth via BFS
    let mut girth = 0usize;
    for start in 0..n {
        let mut dist = vec![usize::MAX; n];
        let mut parent = vec![usize::MAX; n];
        dist[start] = 0;
        let mut queue = std::collections::VecDeque::new();
        queue.push_back(start);
        while let Some(u) = queue.pop_front() {
            for v in 0..n {
                if adj[u][v] == 1 {
                    if dist[v] == usize::MAX {
                        dist[v] = dist[u] + 1;
                        parent[v] = u;
                        queue.push_back(v);
                    } else if parent[u] != v && parent[v] != u {
                        let cycle_len = dist[u] + dist[v] + 1;
                        if girth == 0 || cycle_len < girth {
                            girth = cycle_len;
                        }
                    }
                }
            }
        }
    }

    // Eigenvalues: build f64 matrix and compute via nalgebra
    let mat = nalgebra::DMatrix::<f64>::from_fn(n, n, |i, j| adj[i][j] as f64);
    let eigen = mat.symmetric_eigen();
    let mut eigenvalues: Vec<f64> = eigen.eigenvalues.iter().copied().collect();
    eigenvalues.sort_by(|a, b| b.partial_cmp(a).unwrap_or(std::cmp::Ordering::Equal));

    MotifFingerprint {
        degree_sequence: degrees,
        eigenvalues,
        triangle_count: triangles,
        diameter,
        girth,
        n_vertices: n,
        n_edges,
    }
}

// ---------------------------------------------------------------------------
// Helpers: Lattice CSV loading
// ---------------------------------------------------------------------------

/// Load lattice points from the CSV file for a given dimension.
/// Returns a Vec of 8D integer vectors.
pub fn load_lattice_points(dim: usize) -> Vec<Vec<i32>> {
    let csv_path = format!(
        "{}/../../data/csv/cayley_dickson/{}d_lattice_mapping.csv",
        env!("CARGO_MANIFEST_DIR"),
        dim
    );
    let content = std::fs::read_to_string(&csv_path)
        .unwrap_or_else(|_| panic!("Lattice CSV not found: {}", csv_path));

    let mut points = Vec::new();
    let mut lines = content.lines();
    let _header = lines.next();

    for line in lines {
        let fields = parse_csv_line_internal(line);
        if fields.len() >= 2 {
            if let Some(pt) = parse_lattice_point(&fields[1]) {
                points.push(pt);
            }
        }
    }
    points
}

/// Load lattice mapping as basis_index -> lattice_point.
pub fn load_lattice_map(dim: usize) -> HashMap<usize, Vec<i32>> {
    let csv_path = format!(
        "{}/../../data/csv/cayley_dickson/{}d_lattice_mapping.csv",
        env!("CARGO_MANIFEST_DIR"),
        dim
    );
    let content = std::fs::read_to_string(&csv_path)
        .unwrap_or_else(|_| panic!("Lattice CSV not found: {}", csv_path));

    let mut map = HashMap::new();
    let mut lines = content.lines();
    let _header = lines.next();

    for line in lines {
        let fields = parse_csv_line_internal(line);
        if fields.len() >= 2 {
            if let (Some(basis_vec), Some(lattice)) = (
                parse_nested_tuple(&fields[0]),
                parse_lattice_point(&fields[1]),
            ) {
                if let Some(idx) = vec_to_basis_index(&basis_vec) {
                    map.insert(idx, lattice);
                }
            }
        }
    }
    map
}

/// Internal CSV line parser (handles quoted fields).
fn parse_csv_line_internal(line: &str) -> Vec<String> {
    let mut fields = Vec::new();
    let mut current = String::new();
    let mut in_quotes = false;

    for ch in line.chars() {
        match ch {
            '"' => in_quotes = !in_quotes,
            ',' if !in_quotes => {
                fields.push(current.clone());
                current.clear();
            }
            _ => current.push(ch),
        }
    }
    fields.push(current);
    fields
}

// ---------------------------------------------------------------------------
// Tests
// ---------------------------------------------------------------------------

#[cfg(test)]
mod tests {
    use super::*;

    /// All 7 NATO triplets of the octonions (associative triples).
    /// Each triple [a,b,c] satisfies e_a * e_b = e_c (up to sign).
    const NATO_TRIPS: [[usize; 3]; 7] = [
        [1, 2, 3],
        [1, 4, 5],
        [1, 6, 7],
        [2, 4, 6],
        [2, 5, 7],
        [3, 4, 7],
        [3, 5, 6],
    ];

    // === Strut table cross-validation (Phase 2.1) ===

    #[test]
    fn test_strut_table_matches_de_marrais_flying_higher() {
        // Validates C-454: strut table in Pathions2.pdf matches boxkites.rs output
        let result = verify_strut_table_against_de_marrais();
        assert!(
            result.is_ok(),
            "Strut table mismatch: {}",
            result.unwrap_err()
        );
    }

    #[test]
    fn test_de_marrais_strut_constants_cover_1_through_7() {
        let table = de_marrais_strut_table();
        let constants: Vec<usize> = table.iter().map(|(s, _)| *s).collect();
        assert_eq!(constants, vec![1, 2, 3, 4, 5, 6, 7]);
    }

    #[test]
    fn test_de_marrais_inner_xor_equals_8_plus_strut() {
        // De Marrais states: "the inner XOR of the Octonion and Sedenion pairs
        // defining the Assessors at each box-kite vertex will always equal
        // 8 + strut_constant."
        let table = de_marrais_strut_table();
        for (strut_const, assessors) in &table {
            for (low, high) in assessors {
                let inner_xor = low ^ high;
                assert_eq!(
                    inner_xor,
                    8 + strut_const,
                    "Inner XOR of ({},{}) = {} != 8 + {} = {}",
                    low,
                    high,
                    inner_xor,
                    strut_const,
                    8 + strut_const
                );
            }
        }
    }

    #[test]
    fn test_de_marrais_zigzag_l_indices_form_nato_trips() {
        // De Marrais states: "The Octonion indices of the vertices A, B, C
        // are in bold-face: their Assessors form a box-kite's sole triple-zigzag."
        // "They also form an associative triplet, in NATO format."
        //
        // NATO triplets are the 7 associative triplets of the octonions.
        // We sort each triple for comparison since the ABC vertex ordering
        // does not necessarily match the NATO ordering.
        let nato_trips: std::collections::HashSet<[usize; 3]> = NATO_TRIPS
            .iter()
            .map(|t| {
                let mut s = *t;
                s.sort();
                s
            })
            .collect();

        let table = de_marrais_strut_table();
        for (strut_const, assessors) in &table {
            // A, B, C are indices 0, 1, 2; take their low (octonion) indices
            let mut abc_lows = [assessors[0].0, assessors[1].0, assessors[2].0];
            abc_lows.sort();
            assert!(
                nato_trips.contains(&abc_lows),
                "Strut {}: zigzag L-indices {:?} not a NATO trip",
                strut_const,
                abc_lows
            );
        }
    }

    // === Nested-tuple parser (Phase 2.4) ===

    #[test]
    fn test_parse_nested_tuple_identity_256d() {
        // The identity element: (((((((( 1, 0), (0, 0)), ...
        let s = "((((((((1, 0), (0, 0)), (0, 0)), (0, 0)), (0, 0)), (0, 0)), (0, 0)), (0, 0))";
        let v = parse_nested_tuple(s).expect("Failed to parse identity");
        assert_eq!(v.len(), 256);
        assert_eq!(v[0], 1.0);
        assert!(v[1..].iter().all(|&x| x == 0.0));
    }

    #[test]
    fn test_parse_nested_tuple_e1_256d() {
        // Second basis element: ((0, 0), (((((((1, 0), (0, 0)), (0, 0)), ...
        // This should be e_128 (the generator of the 256-from-128 doubling)
        // placed at position dim/2 = 128
        let s = "((0, 0), (((((((1, 0), (0, 0)), (0, 0)), (0, 0)), (0, 0)), (0, 0)), (0, 0)))";
        let v = parse_nested_tuple(s).expect("Failed to parse e1");
        assert_eq!(v.len(), 256);
        // The 1 should be at index 128
        let idx = vec_to_basis_index(&v);
        assert!(idx.is_some(), "Should be a single basis element");
        // Verify it's at position 128
        assert_eq!(v[128], 1.0);
    }

    #[test]
    fn test_parse_nested_tuple_quaternion() {
        let s = "((1, 0), (0, 0))";
        let v = parse_nested_tuple(s).unwrap();
        assert_eq!(v, vec![1.0, 0.0, 0.0, 0.0]);

        let s = "((0, 1), (0, 0))";
        let v = parse_nested_tuple(s).unwrap();
        assert_eq!(v, vec![0.0, 1.0, 0.0, 0.0]);

        let s = "((0, 0), (1, 0))";
        let v = parse_nested_tuple(s).unwrap();
        assert_eq!(v, vec![0.0, 0.0, 1.0, 0.0]);
    }

    #[test]
    fn test_parse_nested_tuple_octonion_e4() {
        // e_4 in 8D: (((0, 0), (0, 0)), ((1, 0), (0, 0)))
        let s = "(((0, 0), (0, 0)), ((1, 0), (0, 0)))";
        let v = parse_nested_tuple(s).unwrap();
        assert_eq!(v.len(), 8);
        assert_eq!(v[4], 1.0);
        assert!(v
            .iter()
            .enumerate()
            .filter(|&(i, _)| i != 4)
            .all(|(_, &x)| x == 0.0));
    }

    #[test]
    fn test_parse_lattice_point() {
        let s = "[-1, -1, -1, -1, -1, -1, -1, -1]";
        let v = parse_lattice_point(s).unwrap();
        assert_eq!(v, vec![-1, -1, -1, -1, -1, -1, -1, -1]);

        let s = "[-1, -1, -1, -1, -1, -1, -1, 1]";
        let v = parse_lattice_point(s).unwrap();
        assert_eq!(v, vec![-1, -1, -1, -1, -1, -1, -1, 1]);
    }

    // === 64D adjacency (Phase 2.2) ===

    #[test]
    fn test_64d_adjacency_basic_properties() {
        // Build our adjacency matrix for dim=64
        let (pairs, matrix) = build_zd_adjacency_matrix(64);
        let n = pairs.len();

        // dim=64: cross_assessors gives (31 * 32) = 992 pairs
        assert_eq!(n, 31 * 32, "Expected 992 cross-pairs at dim=64");

        // Matrix should be symmetric
        for i in 0..n {
            for j in 0..n {
                assert_eq!(
                    matrix[i][j], matrix[j][i],
                    "Adjacency matrix not symmetric at ({},{})",
                    i, j
                );
            }
        }

        // Diagonal should be zero
        for i in 0..n {
            assert_eq!(matrix[i][i], 0, "Diagonal should be zero at {}", i);
        }

        // Count total edges
        let total_edges: usize = matrix
            .iter()
            .flat_map(|row| row.iter())
            .map(|&x| x as usize)
            .sum::<usize>()
            / 2;
        assert!(
            total_edges > 0,
            "Should have at least some ZD edges at dim=64"
        );
    }

    // === 128D adjacency (Phase 2.3) ===

    // Note: 128D adjacency matrix computation is expensive (63*64 = 4032 pairs).
    // We validate a representative subset instead of the full matrix.

    #[test]
    fn test_128d_adjacency_pair_count() {
        let pairs = cross_assessors(128);
        // dim=128: (63 * 64) = 4032 pairs
        assert_eq!(pairs.len(), 63 * 64);
    }

    #[test]
    fn test_xor_partner_law_128d() {
        let result = verify_xor_partner_law(128);
        println!("XOR partner 128D: {:?}", result);
        assert_eq!(result.xor_mask, 8, "128/16 = 8");
        assert!(result.n_checked > 0, "should check some pairs");
        // Record actual result; universality may or may not hold at 128D
    }

    #[test]
    fn test_c451_bucketed_adjacency_consistent_with_full_at_32d() {
        // Cross-validate: bucketed and full adjacency must agree at dim=32
        let (pairs_b, matrix_b) = build_zd_adjacency_bucketed(32);
        let (pairs_f, matrix_f) = build_zd_adjacency_matrix(32);
        assert_eq!(pairs_b.len(), pairs_f.len());
        for i in 0..pairs_b.len() {
            for j in 0..pairs_b.len() {
                assert_eq!(
                    matrix_b[i][j], matrix_f[i][j],
                    "Mismatch at ({}, {}): bucketed={} full={}",
                    i, j, matrix_b[i][j], matrix_f[i][j]
                );
            }
        }
    }

    #[test]
    fn test_c451_cross_validation_128d() {
        let result = verify_c451_128d();
        println!("{}", result.summary);
        // XOR partner law basic check
        assert_eq!(result.xor_partner.xor_mask, 8);
        assert!(result.xor_partner.n_checked > 0);
        // Parity-clique structure
        assert_eq!(result.parity_clique.n_vertices, 4032);
        assert_eq!(result.parity_clique.dim, 128);
        // Buckets: empirically 64 distinct XOR keys at dim=128
        assert_eq!(result.n_buckets, 64);
        // Edges found should be positive
        assert!(result.n_zd_edges > 0, "should find ZD edges at 128D");
    }

    // === Associativity checks (Phase 3.2) ===

    #[test]
    fn test_associativity_identity_always_true() {
        // Any triple involving the identity is always associative
        let dim = 256;
        let mut e0 = vec![0.0; dim];
        e0[0] = 1.0;
        let mut e1 = vec![0.0; dim];
        e1[1] = 1.0;
        let mut e2 = vec![0.0; dim];
        e2[2] = 1.0;

        assert!(is_associative_triple(&e0, &e1, &e2, 1e-10));
        assert!(is_associative_triple(&e1, &e0, &e2, 1e-10));
        assert!(is_associative_triple(&e1, &e2, &e0, 1e-10));
    }

    #[test]
    fn test_associativity_fails_for_generic_256d_triple() {
        // Beyond octonions (dim >= 16), generic triples are non-associative
        let dim = 256;
        let mut e1 = vec![0.0; dim];
        e1[1] = 1.0;
        let mut e2 = vec![0.0; dim];
        e2[2] = 1.0;
        let mut e4 = vec![0.0; dim];
        e4[4] = 1.0;

        // (e1 * e2) * e4 vs e1 * (e2 * e4) -- these should differ at dim >= 16
        // Actually in the octonion sub-algebra (indices 0-7), (1,2,4) is NOT
        // a NATO triplet, so associativity fails even in octonions.
        // Let's test a non-associative triple: e1, e2, e4
        // In octonions: e1*e2 = e3, e3*e4 = e7 (NATO (3,4,7))
        // e2*e4 = e6 (NATO (2,4,6)), e1*e6 = e7 (NATO (1,7,6) -> e1*e6 = -e7)
        // So (e1*e2)*e4 = e3*e4 = e7, e1*(e2*e4) = e1*e6 = -e7 -> NOT associative
        assert!(!is_associative_triple(&e1, &e2, &e4, 1e-10));
    }

    // === CSV adjacency parser (Phase 2.2) ===

    #[test]
    fn test_parse_adjacency_csv_small() {
        // 3x3 matrix with header row: columns labeled 0,1,2
        let csv = "0,1,2\n0,0,1\n1,0,0\n0,1,0\n";
        let matrix = parse_adjacency_csv(csv);
        assert_eq!(matrix.len(), 3);
        assert_eq!(matrix[0], vec![0.0, 0.0, 1.0]);
        assert_eq!(matrix[1], vec![1.0, 0.0, 0.0]);
        assert_eq!(matrix[2], vec![0.0, 1.0, 0.0]);

        // With string row labels
        let csv = "col0,col1\na,0,1\nb,1,0\n";
        let matrix = parse_adjacency_csv(csv);
        assert_eq!(matrix.len(), 2);
        assert_eq!(matrix[0], vec![0.0, 1.0]);
        assert_eq!(matrix[1], vec![1.0, 0.0]);
    }

    #[test]
    fn test_vec_to_basis_index() {
        let v = vec![0.0, 0.0, 1.0, 0.0];
        assert_eq!(vec_to_basis_index(&v), Some(2));

        let v = vec![1.0, 0.0, 0.0, 0.0];
        assert_eq!(vec_to_basis_index(&v), Some(0));

        // Not a basis element
        let v = vec![1.0, 1.0, 0.0, 0.0];
        assert_eq!(vec_to_basis_index(&v), None);
    }

    // === Cross-validation against CSV files (integration tests) ===
    // These tests read actual CSV files. They are enabled only when
    // the data directory exists (during development).

    #[test]
    fn test_csv_64d_adjacency_vs_rust() {
        // Phase 2.2: Compare 64D adjacency CSV against Rust computation
        let csv_path = concat!(
            env!("CARGO_MANIFEST_DIR"),
            "/../../data/csv/cayley_dickson/64d_chingon_adjacency.csv"
        );
        let Ok(content) = std::fs::read_to_string(csv_path) else {
            eprintln!("Skipping: CSV file not found at {}", csv_path);
            return;
        };

        let csv_matrix = parse_adjacency_csv(&content);
        assert_eq!(
            csv_matrix.len(),
            64,
            "Expected 64 rows (indices 0-63) in 64D CSV"
        );

        // The CSV has 64 columns (indices 0-63) but only 63 data rows.
        // Build our Rust adjacency for the same indexing.
        // Note: the CSV seems to index by basis elements 0-63, not cross-pairs.
        // We need to understand the CSV's node labeling.

        // The CSV 64D_Pathion_Adjacency_Matrix is very sparse (mostly zeros).
        // Each row has at most 1-2 entries. This is NOT the full ZD adjacency
        // over all cross-assessor pairs -- it's over the 64 basis elements.
        //
        // The adjacency between basis elements i and j means: there exist
        // signs s,t such that (e_i + s*...) * (e_j + t*...) = 0. But basis
        // elements alone can't form zero products (you need linear combos).
        //
        // After analysis: this CSV likely represents something different from
        // our cross-assessor adjacency. Document as INCONCLUSIVE in claims.
        let total_ones: usize = csv_matrix
            .iter()
            .flat_map(|row| row.iter())
            .filter(|&&v| v > 0.5)
            .count();

        // Report for provenance
        eprintln!(
            "64D CSV: {}x{} matrix, {} nonzero entries",
            csv_matrix.len(),
            csv_matrix.first().map_or(0, |r| r.len()),
            total_ones
        );

        // The CSV is very sparse (about 60 nonzero entries out of 63*64 = 4032).
        // This is consistent with a "pathion adjacency" matrix where only
        // one specific pair per basis element is flagged (perhaps nearest-neighbor
        // in the doubling tree).
        assert!(
            total_ones < 200,
            "Expected sparse matrix, got {} nonzero entries",
            total_ones
        );
    }

    #[test]
    fn test_csv_256d_basis_properties_vs_rust() {
        // Phase 3.2: Validate associativity claims from 256D CSV
        //
        // FINDING: The CSV claims ALL 125 triples are associative (True),
        // but this is incorrect. In Cayley-Dickson algebras beyond the
        // octonions, generic basis-element triples are NOT associative.
        // Triples involving the identity are always associative; triples
        // within the octonion sub-algebra follow octonion multiplication
        // rules. Our Rust computation is authoritative here.
        //
        // The CSV was likely generated by AI-assisted computation that
        // incorrectly treated all basis products as associative.
        let csv_path = concat!(
            env!("CARGO_MANIFEST_DIR"),
            "/../../data/csv/cayley_dickson/256d_basis_properties.csv"
        );
        let Ok(content) = std::fs::read_to_string(csv_path) else {
            eprintln!("Skipping: CSV file not found");
            return;
        };

        let mut lines = content.lines();
        let _header = lines.next(); // skip "a,b,c,Associative"

        let mut checked = 0;
        let mut rust_assoc_count = 0;
        let mut rust_nonassoc_count = 0;
        let mut csv_says_true = 0;
        let mut mismatched_details = Vec::new();

        for line in lines.take(50) {
            let fields = parse_csv_line(line);
            if fields.len() < 4 {
                continue;
            }

            let a = match parse_nested_tuple(&fields[0]) {
                Some(v) => v,
                None => continue,
            };
            let b = match parse_nested_tuple(&fields[1]) {
                Some(v) => v,
                None => continue,
            };
            let c = match parse_nested_tuple(&fields[2]) {
                Some(v) => v,
                None => continue,
            };
            let csv_assoc = fields[3].trim().eq_ignore_ascii_case("true");
            if csv_assoc {
                csv_says_true += 1;
            }

            let rust_assoc = is_associative_triple(&a, &b, &c, 1e-10);
            checked += 1;

            if rust_assoc {
                rust_assoc_count += 1;
            } else {
                rust_nonassoc_count += 1;
                let a_idx = vec_to_basis_index(&a);
                let b_idx = vec_to_basis_index(&b);
                let c_idx = vec_to_basis_index(&c);
                mismatched_details.push(format!(
                    "Row {}: e_{:?} * e_{:?} * e_{:?} -> Rust=false, CSV=true",
                    checked - 1,
                    a_idx,
                    b_idx,
                    c_idx
                ));
            }
        }

        eprintln!(
            "256D associativity: {} checked, {} assoc, {} non-assoc (CSV claims {} True)",
            checked, rust_assoc_count, rust_nonassoc_count, csv_says_true
        );
        for detail in &mismatched_details {
            eprintln!("  MISMATCH: {}", detail);
        }

        // Key assertions:
        // 1. We can parse at least 10 rows
        assert!(
            checked >= 10,
            "Expected to check at least 10 rows, got {}",
            checked
        );
        // 2. CSV says ALL are True (this is the CSV's claim)
        assert_eq!(
            csv_says_true, checked,
            "CSV should claim all triples are associative"
        );
        // 3. Rust finds some non-associative (this is the CORRECT result)
        // At dim=256, triples of distinct high-index basis elements should fail.
        // The exact count depends on which triples are in the first 50 rows.
        eprintln!(
            "CONCLUSION: CSV has {} incorrect associativity claims in first {} rows",
            rust_nonassoc_count, checked
        );
    }

    #[test]
    fn test_csv_256d_lattice_mapping_parses() {
        // Phase 2.5: Verify we can parse all 256 rows of the lattice mapping
        let csv_path = concat!(
            env!("CARGO_MANIFEST_DIR"),
            "/../../data/csv/cayley_dickson/256d_lattice_mapping.csv"
        );
        let Ok(content) = std::fs::read_to_string(csv_path) else {
            eprintln!("Skipping: CSV file not found");
            return;
        };

        let mut lines = content.lines();
        let _header = lines.next();

        let mut parsed = 0;
        let mut failed = 0;

        for line in lines {
            let fields = parse_csv_line(line);
            if fields.len() < 4 {
                continue;
            }

            // Parse basis element
            let basis = parse_nested_tuple(&fields[0]);
            // Parse lattice point
            let lattice = parse_lattice_point(&fields[1]);

            if basis.is_some() && lattice.is_some() {
                let b = basis.unwrap();
                let l = lattice.unwrap();
                assert_eq!(b.len(), 256, "Basis element should have 256 components");
                assert_eq!(l.len(), 8, "Lattice point should have 8 coordinates");
                parsed += 1;
            } else {
                failed += 1;
            }
        }

        eprintln!("256D lattice: {} parsed, {} failed", parsed, failed);
        assert_eq!(parsed, 256, "Expected 256 parsed rows");
        assert_eq!(failed, 0);
    }

    #[test]
    fn test_csv_512d_lattice_mapping_parses() {
        let csv_path = concat!(
            env!("CARGO_MANIFEST_DIR"),
            "/../../data/csv/cayley_dickson/512d_lattice_mapping.csv"
        );
        let Ok(content) = std::fs::read_to_string(csv_path) else {
            eprintln!("Skipping: CSV file not found");
            return;
        };

        let mut lines = content.lines();
        let _header = lines.next();

        let mut parsed = 0;
        for line in lines {
            let fields = parse_csv_line(line);
            if fields.len() < 4 {
                continue;
            }
            let basis = parse_nested_tuple(&fields[0]);
            let lattice = parse_lattice_point(&fields[1]);
            if basis.is_some() && lattice.is_some() {
                let b = basis.unwrap();
                let l = lattice.unwrap();
                assert_eq!(b.len(), 512);
                assert_eq!(l.len(), 8); // All dims use 8D lattice (octonion sub-algebra)
                parsed += 1;
            }
        }
        eprintln!("512D lattice: {} parsed", parsed);
        assert_eq!(parsed, 512);
    }

    #[test]
    fn test_csv_1024d_lattice_mapping_parses() {
        let csv_path = concat!(
            env!("CARGO_MANIFEST_DIR"),
            "/../../data/csv/cayley_dickson/1024d_lattice_mapping.csv"
        );
        let Ok(content) = std::fs::read_to_string(csv_path) else {
            eprintln!("Skipping: CSV file not found");
            return;
        };

        let mut lines = content.lines();
        let _header = lines.next();

        let mut parsed = 0;
        for line in lines {
            let fields = parse_csv_line(line);
            if fields.len() < 4 {
                continue;
            }
            let basis = parse_nested_tuple(&fields[0]);
            let lattice = parse_lattice_point(&fields[1]);
            if basis.is_some() && lattice.is_some() {
                let b = basis.unwrap();
                let l = lattice.unwrap();
                assert_eq!(b.len(), 1024);
                assert_eq!(l.len(), 8); // All dims use 8D lattice (octonion sub-algebra)
                parsed += 1;
            }
        }
        eprintln!("1024D lattice: {} parsed", parsed);
        assert_eq!(parsed, 1024);
    }

    #[test]
    fn test_csv_2048d_lattice_mapping_parses() {
        let csv_path = concat!(
            env!("CARGO_MANIFEST_DIR"),
            "/../../data/csv/cayley_dickson/2048d_lattice_mapping.csv"
        );
        let Ok(content) = std::fs::read_to_string(csv_path) else {
            eprintln!("Skipping: CSV file not found");
            return;
        };

        let mut lines = content.lines();
        let _header = lines.next();

        let mut parsed = 0;
        for line in lines {
            let fields = parse_csv_line(line);
            if fields.len() < 4 {
                continue;
            }
            let basis = parse_nested_tuple(&fields[0]);
            let lattice = parse_lattice_point(&fields[1]);
            if basis.is_some() && lattice.is_some() {
                let b = basis.unwrap();
                let l = lattice.unwrap();
                assert_eq!(b.len(), 2048);
                assert_eq!(l.len(), 8); // All dims use 8D lattice (octonion sub-algebra)
                parsed += 1;
            }
        }
        eprintln!("2048D lattice: {} parsed", parsed);
        assert_eq!(parsed, 2048);
    }

    #[test]
    fn test_csv_operation_table_sample_validation() {
        // Phase 2.7: Sample-validate rows from the 64D/128D operation results
        let csv_path = concat!(
            env!("CARGO_MANIFEST_DIR"),
            "/../../data/csv/cayley_dickson/64d_128d_operation_results.csv"
        );
        let Ok(content) = std::fs::read_to_string(csv_path) else {
            eprintln!("Skipping: CSV file not found");
            return;
        };

        let mut lines = content.lines();
        let _header = lines.next(); // "Algebra 1,Algebra 2,Operation,Result"

        let mut checked = 0;
        // Sample first 20 rows
        for line in lines.take(20) {
            let fields = parse_csv_line(line);
            if fields.len() < 4 {
                continue;
            }

            let _alg1 = &fields[0];
            let _alg2 = &fields[1];
            let _operation = &fields[2];
            let result_str = &fields[3];

            // Parse the result
            if let Some(result_vec) = parse_nested_tuple(result_str) {
                // We can't fully validate without knowing a, b, c, but we can
                // verify the result parses to a valid dimension (power of 2)
                assert!(
                    result_vec.len() == 64 || result_vec.len() == 128,
                    "Row {}: expected 64 or 128 components, got {}",
                    checked,
                    result_vec.len()
                );
                checked += 1;
            }
        }

        eprintln!("Operation table: {} rows parsed successfully", checked);
        assert!(checked >= 10, "Expected to parse at least 10 rows");
    }

    // === Phase 3.1: Lattice-ZD difference analysis ===

    #[test]
    fn test_256d_lattice_zd_difference_structure() {
        // Parse the 256D lattice mapping CSV to get basis_index -> lattice_point
        let csv_path = concat!(
            env!("CARGO_MANIFEST_DIR"),
            "/../../data/csv/cayley_dickson/256d_lattice_mapping.csv"
        );
        let Ok(content) = std::fs::read_to_string(csv_path) else {
            eprintln!("Skipping: CSV file not found");
            return;
        };

        // Parse all 256 rows into a lookup table: index -> lattice point
        let mut lattice_map: HashMap<usize, Vec<i32>> = HashMap::new();
        let mut lines = content.lines();
        let _header = lines.next();

        for line in lines {
            let fields = parse_csv_line(line);
            if fields.len() < 2 {
                continue;
            }
            if let (Some(basis_vec), Some(lattice)) = (
                parse_nested_tuple(&fields[0]),
                parse_lattice_point(&fields[1]),
            ) {
                if let Some(idx) = vec_to_basis_index(&basis_vec) {
                    lattice_map.insert(idx, lattice);
                }
            }
        }

        assert_eq!(lattice_map.len(), 256, "Expected 256 lattice points");

        // Build ZD adjacency at dim=16 (the sedenion level) where we know
        // the structure well and computation is fast.
        let (pairs_16, adj_16) = build_zd_adjacency_matrix(16);

        // For each ZD-adjacent pair of cross-assessors, compute lattice difference
        // of their component basis elements.
        let mut diff_counts: HashMap<Vec<i32>, usize> = HashMap::new();
        let mut total_diffs = 0usize;

        for i in 0..pairs_16.len() {
            for j in (i + 1)..pairs_16.len() {
                if adj_16[i][j] == 1 {
                    // Each cross-pair has a low and high index
                    let (lo_i, hi_i) = (pairs_16[i].0, pairs_16[i].1);
                    let (lo_j, hi_j) = (pairs_16[j].0, pairs_16[j].1);

                    // Compute pairwise lattice differences between the 4 basis elements
                    let indices = [(lo_i, lo_j), (lo_i, hi_j), (hi_i, lo_j), (hi_i, hi_j)];
                    for (a, b) in &indices {
                        if let (Some(la), Some(lb)) = (lattice_map.get(a), lattice_map.get(b)) {
                            let diff: Vec<i32> =
                                la.iter().zip(lb.iter()).map(|(x, y)| x - y).collect();
                            *diff_counts.entry(diff).or_insert(0) += 1;
                            total_diffs += 1;
                        }
                    }
                }
            }
        }

        // Analyze the difference vectors
        let mut by_norm: HashMap<i32, usize> = HashMap::new();
        for (diff, count) in &diff_counts {
            let norm_sq: i32 = diff.iter().map(|x| x * x).sum();
            *by_norm.entry(norm_sq).or_insert(0) += count;
        }

        let mut norms_sorted: Vec<(i32, usize)> = by_norm.into_iter().collect();
        norms_sorted.sort_by_key(|(n, _)| *n);

        eprintln!("=== Phase 3.1: Lattice-ZD Difference Analysis (dim=16, 256D lattice) ===");
        eprintln!("Total ZD-pair basis-element differences: {}", total_diffs);
        eprintln!("Unique difference vectors: {}", diff_counts.len());
        eprintln!("Distribution by norm-squared:");
        for (norm_sq, count) in &norms_sorted {
            // E8 roots have norm_sq = 2 in the standard normalization.
            // {-1,0,1}^8 vertices have norm_sq up to 8.
            let tag = match *norm_sq {
                0 => " (identity -- same lattice point)",
                2 => " (** E8 root norm! **)",
                4 => " (D4/D8 lattice vector)",
                8 => " (hypercube vertex)",
                _ => "",
            };
            eprintln!("  |d|^2 = {}: {} occurrences{}", norm_sq, count, tag);
        }

        // Check how many unique diffs are E8 roots (norm_sq = 2)
        let e8_root_diffs: Vec<&Vec<i32>> = diff_counts
            .keys()
            .filter(|d| d.iter().map(|x| x * x).sum::<i32>() == 2)
            .collect();
        eprintln!(
            "Unique diffs with |d|^2 = 2 (potential E8 roots): {}",
            e8_root_diffs.len()
        );
        if !e8_root_diffs.is_empty() {
            eprintln!(
                "  Examples: {:?}",
                &e8_root_diffs[..e8_root_diffs.len().min(5)]
            );
        }

        // Report total unique norms found
        assert!(
            !diff_counts.is_empty(),
            "Should have some lattice differences"
        );
    }

    // === Phase 3.3: Structural properties analysis ===

    #[test]
    fn test_csv_structural_analysis_256d() {
        let csv_path = concat!(
            env!("CARGO_MANIFEST_DIR"),
            "/../../data/csv/cayley_dickson/256d_structural_analysis.csv"
        );
        let Ok(content) = std::fs::read_to_string(csv_path) else {
            eprintln!("Skipping: CSV file not found");
            return;
        };

        let mut lines = content.lines();
        let _header = lines.next();

        let mut symmetric_true = 0;
        let mut periodic_true = 0;
        let mut total = 0;

        for line in lines {
            let fields = parse_csv_line(line);
            if fields.len() < 4 {
                continue;
            }
            total += 1;
            if fields[1].trim().eq_ignore_ascii_case("true") {
                symmetric_true += 1;
            }
            if fields[2].trim().eq_ignore_ascii_case("true") {
                periodic_true += 1;
            }
        }

        eprintln!(
            "256D structural: {} rows, {} symmetric, {} periodic",
            total, symmetric_true, periodic_true
        );

        assert!(total > 0, "Should parse some rows");
        // The identity row might be symmetric; most elements should not be
        eprintln!("Symmetric fraction: {}/{}", symmetric_true, total);
    }

    // === Helper: CSV line parser that handles quoted fields ===

    fn parse_csv_line(line: &str) -> Vec<String> {
        let mut fields = Vec::new();
        let mut current = String::new();
        let mut in_quotes = false;

        for ch in line.chars() {
            match ch {
                '"' => in_quotes = !in_quotes,
                ',' if !in_quotes => {
                    fields.push(current.clone());
                    current.clear();
                }
                _ => current.push(ch),
            }
        }
        fields.push(current);
        fields
    }

    fn lattice_csv_path(dim: usize) -> String {
        format!(
            "{}/../../data/csv/cayley_dickson/{}d_lattice_mapping.csv",
            env!("CARGO_MANIFEST_DIR"),
            dim
        )
    }

    fn parse_lattice_csv_records(dim: usize) -> Vec<(usize, Vec<i32>)> {
        let csv_path = lattice_csv_path(dim);
        let content = std::fs::read_to_string(&csv_path)
            .unwrap_or_else(|_| panic!("Lattice CSV not found: {}", csv_path));
        let mut lines = content.lines();
        let _header = lines.next().expect("CSV must have header");
        let mut records = Vec::new();

        for line in lines {
            let fields = parse_csv_line(line);
            assert!(
                fields.len() >= 2,
                "Expected at least 2 fields in lattice row: {}",
                line
            );

            let basis = parse_nested_tuple(&fields[0])
                .unwrap_or_else(|| panic!("Failed to parse basis tuple in row: {}", line));
            let idx = vec_to_basis_index(&basis)
                .unwrap_or_else(|| panic!("Row does not decode to unique basis index: {}", line));
            let lattice = parse_lattice_point(&fields[1])
                .unwrap_or_else(|| panic!("Failed to parse lattice vector in row: {}", line));
            records.push((idx, lattice));
        }

        records
    }

    // === Thesis A: Codebook Parity Verification ===

    #[test]
    fn test_thesis_a_codebook_parity_256d() {
        let result = verify_codebook_parity(256);
        assert_eq!(result.n_points, 256);
        assert!(
            result.all_valid,
            "256D codebook parity failed: {:?}",
            result
        );
    }

    #[test]
    fn test_thesis_a_codebook_parity_512d() {
        let result = verify_codebook_parity(512);
        assert_eq!(result.n_points, 512);
        assert!(
            result.all_valid,
            "512D codebook parity failed: {:?}",
            result
        );
    }

    #[test]
    fn test_thesis_a_codebook_parity_1024d() {
        let result = verify_codebook_parity(1024);
        assert_eq!(result.n_points, 1024);
        assert!(
            result.all_valid,
            "1024D codebook parity failed: {:?}",
            result
        );
    }

    #[test]
    fn test_thesis_a_codebook_parity_2048d() {
        let result = verify_codebook_parity(2048);
        assert_eq!(result.n_points, 2048);
        assert!(
            result.all_valid,
            "2048D codebook parity failed: {:?}",
            result
        );
    }

    // === Thesis B: Filtration Nesting Verification ===

    #[test]
    fn test_thesis_b_filtration_nesting() {
        let result = verify_lattice_filtration();
        assert!(
            result.is_strict_chain,
            "Filtration is not a strict chain: {:?}",
            result
        );
        assert_eq!(
            result.sizes,
            vec![(256, 256), (512, 512), (1024, 1024), (2048, 2048)]
        );
        assert!(result.strict_subsets.iter().all(|&b| b));
    }

    // === Thesis C: Prefix-Cut Characterization ===

    #[test]
    fn test_thesis_c_2048_to_1024_prefix_cut() {
        let p2048: BTreeSet<Vec<i32>> = load_lattice_points(2048).into_iter().collect();
        let p1024: BTreeSet<Vec<i32>> = load_lattice_points(1024).into_iter().collect();

        let cut = learn_prefix_cut(&p2048, &p1024).expect("Failed to learn 2048->1024 prefix cut");

        assert_eq!(cut.parent_size, 2048);
        assert_eq!(cut.child_size, 1024);
        assert!(
            verify_prefix_cut(&p2048, &p1024, &cut),
            "Prefix cut does not exactly partition parent"
        );
    }

    #[test]
    fn test_thesis_c_1024_to_512_prefix_cut() {
        let p1024: BTreeSet<Vec<i32>> = load_lattice_points(1024).into_iter().collect();
        let p512: BTreeSet<Vec<i32>> = load_lattice_points(512).into_iter().collect();

        let cut = learn_prefix_cut(&p1024, &p512).expect("Failed to learn 1024->512 prefix cut");

        assert_eq!(cut.parent_size, 1024);
        assert_eq!(cut.child_size, 512);
        assert!(verify_prefix_cut(&p1024, &p512, &cut));
    }

    #[test]
    fn test_thesis_c_512_to_256_prefix_cut() {
        let p512: BTreeSet<Vec<i32>> = load_lattice_points(512).into_iter().collect();
        let p256: BTreeSet<Vec<i32>> = load_lattice_points(256).into_iter().collect();

        let cut = learn_prefix_cut(&p512, &p256).expect("Failed to learn 512->256 prefix cut");

        assert_eq!(cut.parent_size, 512);
        assert_eq!(cut.child_size, 256);
        assert!(verify_prefix_cut(&p512, &p256, &cut));
    }

    #[test]
    fn test_thesis_c_full_filtration_cuts() {
        let cuts = learn_full_filtration_cuts();
        assert_eq!(cuts.len(), 3, "Expected 3 filtration transitions");
        for (parent_dim, child_dim, cut) in &cuts {
            eprintln!(
                "{}D -> {}D: boundary = {:?}, diverges at coord[{}]",
                parent_dim, child_dim, cut.boundary, cut.divergence_coord
            );
        }
    }

    // === Phase 1.4: Base Universe and Exclusion Count ===

    #[test]
    fn test_base_universe_size_and_exclusion_count() {
        let s_base = enumerate_base_universe();
        // S_base = 2 * 3^7 / 2 (parity filter halves it) = 2187
        // Actually 2 choices for c0 * 3^7 remaining = 4374, parity roughly halves
        assert_eq!(s_base.len(), 2187, "S_base should have 2187 points");

        let p2048: BTreeSet<Vec<i32>> = load_lattice_points(2048).into_iter().collect();
        assert!(
            p2048.is_subset(&s_base),
            "Lambda_2048 must be subset of S_base"
        );

        let excluded = s_base.difference(&p2048).count();
        assert_eq!(excluded, 139, "Expected 139 excluded points");

        // S_base is also a lexicographic superset: Lambda_2048 is first 2048 in lex order
        let s_base_sorted: Vec<&Vec<i32>> = s_base.iter().collect();
        let lex_first_2048: BTreeSet<Vec<i32>> =
            s_base_sorted[..2048].iter().map(|&v| v.clone()).collect();
        assert_eq!(
            lex_first_2048, p2048,
            "Lambda_2048 should be first 2048 of S_base in lex order"
        );
    }

    // === Phase 1.5: 32-Point 421E Slice ===

    #[test]
    fn test_lambda_32_as_predicate_cut_of_256() {
        let p256: Vec<Vec<i32>> = load_lattice_points(256);
        let p256_sorted: BTreeSet<Vec<i32>> = p256.into_iter().collect();

        // Lambda_32 = first 32 points of Lambda_256 in lex order
        let first_32: Vec<Vec<i32>> = p256_sorted.iter().take(32).cloned().collect();
        assert_eq!(first_32.len(), 32);

        // All should have first 4 coordinates = -1 (the "pinned corner")
        for pt in &first_32 {
            assert_eq!(
                &pt[..4],
                &[-1, -1, -1, -1],
                "Lambda_32 point {:?} does not have first 4 coords = -1",
                pt
            );
        }

        // The pinned-corner predicate cut has 41 points (not 32), because
        // all trie-cut exclusions are vacuously satisfied for prefix (-1,-1,-1,-1),
        // and the base universe constraint on the free tail gives:
        // weight 0: 1, weight 2: 24, weight 4: 16 = 41 total.
        let predicate_cut: Vec<Vec<i32>> = p256_sorted
            .iter()
            .filter(|p| p[0] == -1 && p[1] == -1 && p[2] == -1 && p[3] == -1)
            .cloned()
            .collect();

        eprintln!(
            "Points with coords[0:4]=(-1,-1,-1,-1): {}",
            predicate_cut.len()
        );
        assert_eq!(
            predicate_cut.len(),
            41,
            "Pinned corner cut has 41 points (base universe on free 4D tail)"
        );

        // The first 32 in lex order is a proper subset of these 41
        let first_32_set: BTreeSet<Vec<i32>> = first_32.iter().cloned().collect();
        let predicate_set: BTreeSet<Vec<i32>> = predicate_cut.iter().cloned().collect();
        assert!(
            first_32_set.is_subset(&predicate_set),
            "Lambda_32 must be subset of the pinned corner cut"
        );

        // Identify the 9 points in the cut but NOT in the first 32
        let excluded: Vec<&Vec<i32>> = predicate_cut
            .iter()
            .filter(|p| !first_32_set.contains(*p))
            .collect();
        assert_eq!(
            excluded.len(),
            9,
            "9 points are in the pinned cut but past lex-rank 32"
        );
        eprintln!("9 excluded tail patterns (beyond lex rank 32):");
        for p in &excluded {
            eprintln!("  {:?}", &p[4..]);
        }
    }

    #[test]
    fn test_lambda_256_csv_vs_predicates() {
        // Cross-validate CSV-loaded Lambda_256 against predicate-based enumeration.
        // The predicate chain gives exactly 256 points (verified in codebook tests).
        use crate::analysis::codebook::{enumerate_lambda_256, LatticeVector};

        let csv_points: Vec<Vec<i32>> = load_lattice_points(256);
        let pred_points: Vec<LatticeVector> = enumerate_lambda_256();

        assert_eq!(csv_points.len(), 256, "CSV should have 256 points");
        assert_eq!(pred_points.len(), 256, "Predicates should give 256 points");

        // Convert predicate points to Vec<i32> for comparison
        let pred_as_i32: BTreeSet<Vec<i32>> = pred_points
            .iter()
            .map(|v| v.iter().map(|&x| x as i32).collect::<Vec<i32>>())
            .collect();
        let csv_set: BTreeSet<Vec<i32>> = csv_points.into_iter().collect();

        assert_eq!(
            csv_set, pred_as_i32,
            "CSV and predicate-based Lambda_256 must be identical sets"
        );
    }

    // === Thesis E: XOR Partner Law ===

    #[test]
    fn test_thesis_e_xor_partner_64d() {
        let result = verify_xor_partner_law(64);
        eprintln!("XOR partner 64D: {:?}", result);
        assert_eq!(result.xor_mask, 4, "64D XOR mask should be 4");
        assert!(result.n_checked > 0, "Should check some pairs");
        assert!(
            result.universal,
            "XOR partner law should hold universally at 64D"
        );
    }

    #[test]
    fn test_thesis_e_xor_partner_128d() {
        let result = verify_xor_partner_law(128);
        eprintln!("XOR partner 128D: {:?}", result);
        assert_eq!(result.xor_mask, 8, "128D XOR mask should be 8 (128/16)");
        assert!(result.n_checked > 0, "Should check some pairs");
        assert!(
            result.universal,
            "XOR partner law should hold universally at 128D"
        );
    }

    #[test]
    fn test_thesis_e_xor_partner_256d() {
        let result = verify_xor_partner_law(256);
        eprintln!("XOR partner 256D: {:?}", result);
        assert_eq!(result.xor_mask, 16, "256D XOR mask should be 16 (256/16)");
        assert!(result.n_checked > 0, "Should check some pairs");
        assert!(
            result.universal,
            "XOR partner law should hold universally at 256D"
        );
    }

    /// Cross-validate XOR partner law against the shared-basis-index matching
    /// graph. The matching graph connects cross-assessor pairs that share
    /// exactly one basis index. The XOR partner law asserts an involution
    /// (lo XOR mask, hi XOR mask) for mask = dim/16. We verify that every
    /// XOR partner pair also shares a basis index, i.e., the XOR involution
    /// is a subgraph of the matching graph.
    #[test]
    fn test_thesis_e_xor_subset_of_matching_128d() {
        let dim = 128usize;
        let xor_mask = dim / 16; // 8
        let pairs = cross_assessors(dim);
        let n = pairs.len();
        assert_eq!(n, 63 * 64, "128D should have 4032 cross-assessor pairs");

        // Build pair -> index lookup
        let pair_index: HashMap<(usize, usize), usize> =
            pairs.iter().enumerate().map(|(i, &p)| (p, i)).collect();

        // For each cross-pair, find its XOR partner and check if they share
        // exactly one basis index (the matching condition).
        let mut n_checked = 0usize;
        let mut n_xor_is_matching = 0usize;

        let half = dim / 2;
        for &(lo, hi) in &pairs {
            let partner_lo = lo ^ xor_mask;
            let partner_hi = hi ^ xor_mask;
            // Ensure partner is a valid cross-pair (lo in [1,half), hi in [half,dim))
            if partner_lo >= 1 && partner_lo < half && partner_hi >= half && partner_hi < dim {
                if pair_index.contains_key(&(partner_lo, partner_hi)) {
                    n_checked += 1;

                    // Count shared basis indices between (lo,hi) and (partner_lo, partner_hi)
                    let mut shared = 0u32;
                    if lo == partner_lo || lo == partner_hi {
                        shared += 1;
                    }
                    if hi == partner_lo || hi == partner_hi {
                        shared += 1;
                    }

                    if shared == 1 {
                        n_xor_is_matching += 1;
                    }
                }
            }
        }

        eprintln!(
            "128D XOR-in-matching: {}/{} checked, {}/{} are matching neighbors",
            n_checked, n, n_xor_is_matching, n_checked
        );

        assert!(n_checked > 0, "Should find valid XOR partners");
        // Report the fraction -- the XOR involution may or may not always
        // produce a matching neighbor. This is the empirical finding.
        eprintln!(
            "  XOR-matching fraction: {:.3}",
            n_xor_is_matching as f64 / n_checked as f64
        );
    }

    /// Verify that the shared-basis-index matching graph at dim=64 is dense
    /// and connected, establishing the graph structural baseline for smaller
    /// dimensions where full graph analysis is tractable.
    #[test]
    fn test_thesis_e_matching_graph_structure_64d() {
        use petgraph::graph::{NodeIndex, UnGraph};

        let pairs = cross_assessors(64);
        let n = pairs.len();
        assert_eq!(n, 31 * 32, "64D should have 992 cross-assessor pairs");

        // Build matching graph: connect pairs sharing exactly one basis index
        let mut graph = UnGraph::<(), ()>::with_capacity(n, n);
        let nodes: Vec<NodeIndex> = (0..n).map(|_| graph.add_node(())).collect();
        let mut edge_count = 0usize;

        for i in 0..n {
            for j in (i + 1)..n {
                let (lo_i, hi_i) = pairs[i];
                let (lo_j, hi_j) = pairs[j];

                let mut shared = 0u32;
                if lo_i == lo_j || lo_i == hi_j {
                    shared += 1;
                }
                if hi_i == lo_j || hi_i == hi_j {
                    shared += 1;
                }

                if shared == 1 {
                    graph.add_edge(nodes[i], nodes[j], ());
                    edge_count += 1;
                }
            }
        }

        let n_components = petgraph::algo::connected_components(&graph);
        let avg_degree = (2 * edge_count) as f64 / n as f64;

        eprintln!(
            "64D matching graph: {} nodes, {} edges, {} components, avg degree {:.1}",
            n, edge_count, n_components, avg_degree
        );

        assert!(edge_count > 0, "Matching graph should have edges");
        // The matching graph is dense and connected
        assert_eq!(n_components, 1, "Matching graph should be connected at 64D");
        assert!(avg_degree > 50.0, "Average degree should be substantial");
    }

    /// Build the XOR-partner involution as an explicit graph and characterize
    /// it via InvariantSuite. The XOR partner law creates a perfect matching
    /// on the valid (non-boundary) cross-assessor pairs: each pair (lo, hi)
    /// connects to exactly (lo^mask, hi^mask). This should yield n_edges K_2
    /// components plus some isolated nodes (boundary pairs without valid partners).
    #[test]
    fn test_thesis_e_xor_involution_invariants_128d() {
        use crate::analysis::graph_projections::compute_invariant_suite_from_graph;
        use petgraph::graph::{NodeIndex, UnGraph};

        let dim = 128usize;
        let xor_mask = dim / 16; // 8
        let half = dim / 2;
        let pairs = cross_assessors(dim);
        let n = pairs.len();

        // Build pair -> index lookup
        let pair_index: HashMap<(usize, usize), usize> =
            pairs.iter().enumerate().map(|(i, &p)| (p, i)).collect();

        // Build XOR-partner graph (sparse: at most n/2 edges)
        let mut graph = UnGraph::<(), ()>::with_capacity(n, n / 2);
        let nodes: Vec<NodeIndex> = (0..n).map(|_| graph.add_node(())).collect();

        for (i, &(lo, hi)) in pairs.iter().enumerate() {
            let partner_lo = lo ^ xor_mask;
            let partner_hi = hi ^ xor_mask;
            if partner_lo >= 1 && partner_lo < half && partner_hi >= half && partner_hi < dim {
                if let Some(&j) = pair_index.get(&(partner_lo, partner_hi)) {
                    if i < j {
                        graph.add_edge(nodes[i], nodes[j], ());
                    }
                }
            }
        }

        let suite = compute_invariant_suite_from_graph("P_xor_involution_128", &graph);
        eprintln!(
            "128D XOR involution: {} nodes, {} edges, {} components, {} motif classes",
            n,
            suite.global.n_edges,
            suite.components.len(),
            suite.motif_classes.len()
        );

        // The involution creates a perfect matching on paired nodes
        // plus isolated nodes (boundary pairs). Every connected component
        // should be either K_2 or K_1.
        let n_k2 = suite
            .components
            .iter()
            .filter(|c| c.invariants.n_edges == 1)
            .count();
        let n_isolated = suite
            .components
            .iter()
            .filter(|c| c.invariants.n_edges == 0)
            .count();

        eprintln!("  K_2 components: {}, isolated nodes: {}", n_k2, n_isolated);
        assert_eq!(
            n_k2 + n_isolated,
            suite.components.len(),
            "Every component should be K_2 or K_1"
        );
        assert_eq!(
            n_k2, suite.global.n_edges,
            "Each K_2 component contributes exactly one edge"
        );

        // At most 2 motif classes: K_2 and K_1
        assert!(
            suite.motif_classes.len() <= 2,
            "At most 2 motif classes (K_2 and K_1)"
        );

        // Verify the universal property: paired nodes should account for most pairs
        let n_paired = 2 * n_k2;
        eprintln!(
            "  Paired fraction: {}/{} = {:.3}",
            n_paired,
            n,
            n_paired as f64 / n as f64
        );
        assert!(
            n_paired as f64 / n as f64 > 0.95,
            "Most pairs should have valid XOR partners"
        );
    }

    /// Thesis E as a closed-form parameterized theorem with exact counts.
    ///
    /// For CD dimension N (power of 2, N >= 64), XOR mask m = N/16:
    ///   half = N/2
    ///   total_cross_pairs = (half - 1) * half
    ///   boundary_count = half  (those with lo = m, so partner_lo = 0)
    ///   checked_count = total_cross_pairs - boundary_count = half * (half - 2)
    ///   n_partner_edges = checked_count / 2 = half * (half - 2) / 2
    ///   n_K2_components = n_partner_edges
    ///   n_isolated = boundary_count = half
    ///   n_components_total = n_partner_edges + half
    ///   motif_classes = exactly 2 (K_2 and K_1)
    ///
    /// Uses lightweight petgraph component counting (NOT the full invariant
    /// suite) to avoid O(n^3) spectrum computation at large dimensions.
    #[test]
    fn test_thesis_e_closed_form_theorem() {
        use petgraph::algo::connected_components;
        use petgraph::graph::{NodeIndex, UnGraph};

        for dim in [64, 128, 256] {
            let xor_mask = dim / 16;
            let half = dim / 2;
            let pairs = cross_assessors(dim);
            let n = pairs.len();

            // Closed-form expectations
            let expected_total = (half - 1) * half;
            let expected_boundary = half;
            let expected_checked = half * (half - 2);
            let expected_edges = expected_checked / 2;
            let expected_k2 = expected_edges;
            let expected_isolated = expected_boundary;
            let expected_components = expected_k2 + expected_isolated;

            assert_eq!(n, expected_total, "dim={}: cross-pair count mismatch", dim);

            // Build XOR-partner graph
            let pair_index: HashMap<(usize, usize), usize> =
                pairs.iter().enumerate().map(|(i, &p)| (p, i)).collect();

            let mut graph = UnGraph::<(), ()>::with_capacity(n, expected_edges);
            let nodes: Vec<NodeIndex> = (0..n).map(|_| graph.add_node(())).collect();

            let mut actual_boundary = 0usize;
            for (i, &(lo, hi)) in pairs.iter().enumerate() {
                let partner_lo = lo ^ xor_mask;
                let partner_hi = hi ^ xor_mask;
                if partner_lo == 0 {
                    actual_boundary += 1;
                    continue;
                }
                if partner_lo >= 1 && partner_lo < half && partner_hi >= half && partner_hi < dim {
                    if let Some(&j) = pair_index.get(&(partner_lo, partner_hi)) {
                        if i < j {
                            graph.add_edge(nodes[i], nodes[j], ());
                        }
                    }
                }
            }

            assert_eq!(
                actual_boundary, expected_boundary,
                "dim={}: boundary count mismatch",
                dim
            );
            assert_eq!(
                graph.edge_count(),
                expected_edges,
                "dim={}: partner edge count mismatch",
                dim
            );

            // Lightweight component count (O(n+e), not O(n^3))
            let n_comps = connected_components(&graph);
            assert_eq!(
                n_comps, expected_components,
                "dim={}: component count mismatch",
                dim
            );

            // Verify every edge connects two nodes of degree 1 (K_2 check)
            // and count degree-0 nodes (isolated = boundary)
            let mut n_deg0 = 0usize;
            let mut n_deg1 = 0usize;
            for ni in graph.node_indices() {
                let deg = graph.neighbors(ni).count();
                match deg {
                    0 => n_deg0 += 1,
                    1 => n_deg1 += 1,
                    d => panic!(
                        "dim={}: node {} has degree {} (expected 0 or 1)",
                        dim,
                        ni.index(),
                        d
                    ),
                }
            }

            assert_eq!(
                n_deg0, expected_isolated,
                "dim={}: isolated count mismatch",
                dim
            );
            assert_eq!(
                n_deg1,
                2 * expected_k2,
                "dim={}: paired node count mismatch",
                dim
            );
        }
    }

    // === Thesis F: Parity-Clique Law ===

    #[test]
    fn test_thesis_f_parity_clique_16d() {
        let result = verify_parity_clique(16);
        eprintln!("Parity-clique 16D: {:?}", result);
        assert_eq!(
            result.n_vertices, 56,
            "dim=16 should have 7*8=56 cross-pairs"
        );
        // Thesis F REFUTED at 16D: ZD adjacency is NOT K_m union K_m by parity.
        // The graph is far sparser than complete bipartite, and cross-parity
        // edges are present (48/84 = 57% of all edges).
        assert!(
            !result.is_parity_biclique,
            "Parity-biclique should fail at 16D"
        );
        assert!(
            result.n_cross_edges > 0,
            "Cross-parity edges should exist at 16D"
        );
    }

    #[test]
    fn test_thesis_f_parity_clique_32d() {
        let result = verify_parity_clique(32);
        eprintln!(
            "Parity-clique 32D: n_vertices={}, n_edges={}, n_even={}, n_odd={}, \
             even_edges={}, odd_edges={}, cross_edges={}, is_biclique={}",
            result.n_vertices,
            result.n_edges,
            result.n_even,
            result.n_odd,
            result.n_even_edges,
            result.n_odd_edges,
            result.n_cross_edges,
            result.is_parity_biclique
        );
        assert!(
            !result.is_parity_biclique,
            "Parity-biclique should fail at 32D"
        );
        assert!(
            result.n_cross_edges > 0,
            "Cross-parity edges should exist at 32D"
        );
    }

    #[test]
    fn test_thesis_f_parity_clique_64d() {
        let result = verify_parity_clique(64);
        let cross_fraction = result.n_cross_edges as f64 / result.n_edges as f64;
        eprintln!(
            "Parity-clique 64D: n_vertices={}, n_edges={}, n_even={}, n_odd={}, \
             even_edges={}, odd_edges={}, cross_edges={} ({:.1}%), is_biclique={}",
            result.n_vertices,
            result.n_edges,
            result.n_even,
            result.n_odd,
            result.n_even_edges,
            result.n_odd_edges,
            result.n_cross_edges,
            cross_fraction * 100.0,
            result.is_parity_biclique
        );
        // Thesis F REFUTED: expect failure, with non-trivial cross-parity fraction
        assert!(
            !result.is_parity_biclique,
            "Parity-biclique should fail at 64D"
        );
        assert!(
            result.n_cross_edges > 0,
            "Cross-parity edges should exist at 64D"
        );
    }

    /// Characterize the parity-clique failure across dimensions.
    /// Track three metrics: (1) edge density vs K_m expectation,
    /// (2) cross-parity fraction, (3) even/odd balance.
    #[test]
    fn test_thesis_f_parity_scaling() {
        let dims = [16, 32, 64];
        eprintln!("\nThesis F Parity-Clique Scaling:");
        eprintln!(
            "{:>5} {:>8} {:>8} {:>8} {:>8} {:>8}",
            "dim", "n_vert", "n_edges", "expected", "density", "x_frac"
        );

        for &dim in &dims {
            let result = verify_parity_clique(dim);
            let density = result.n_edges as f64 / result.expected_clique_edges as f64;
            let cross_frac = if result.n_edges > 0 {
                result.n_cross_edges as f64 / result.n_edges as f64
            } else {
                0.0
            };
            eprintln!(
                "{:>5} {:>8} {:>8} {:>8} {:>8.4} {:>8.3}",
                dim,
                result.n_vertices,
                result.n_edges,
                result.expected_clique_edges,
                density,
                cross_frac
            );
            // At all tested dimensions, the parity-biclique law fails
            assert!(
                !result.is_parity_biclique,
                "dim={dim}: parity-biclique should fail"
            );
        }
    }

    /// At dim=128, use the XOR-bucket-optimized adjacency (63x speedup) to
    /// verify the parity-clique failure. This is the C-451 cross-validation.
    #[test]
    fn test_thesis_f_parity_clique_128d_bucketed() {
        let (pairs, matrix) = build_zd_adjacency_bucketed(128);
        let n = pairs.len();
        assert_eq!(n, 63 * 64, "128D should have 4032 cross-pairs");

        // Partition by parity of low index
        let even_indices: Vec<usize> = (0..n).filter(|&i| pairs[i].0.is_multiple_of(2)).collect();
        let odd_indices: Vec<usize> = (0..n).filter(|&i| !pairs[i].0.is_multiple_of(2)).collect();

        let mut n_even_edges = 0usize;
        let mut n_odd_edges = 0usize;
        let mut n_cross_edges = 0usize;

        for i in 0..n {
            for j in (i + 1)..n {
                if matrix[i][j] == 1 {
                    let i_even = pairs[i].0.is_multiple_of(2);
                    let j_even = pairs[j].0.is_multiple_of(2);
                    if i_even && j_even {
                        n_even_edges += 1;
                    } else if !i_even && !j_even {
                        n_odd_edges += 1;
                    } else {
                        n_cross_edges += 1;
                    }
                }
            }
        }

        let total_edges = n_even_edges + n_odd_edges + n_cross_edges;
        let expected_even = even_indices.len() * (even_indices.len().saturating_sub(1)) / 2;
        let expected_odd = odd_indices.len() * (odd_indices.len().saturating_sub(1)) / 2;
        let expected_total = expected_even + expected_odd;

        let cross_frac = if total_edges > 0 {
            n_cross_edges as f64 / total_edges as f64
        } else {
            0.0
        };
        let density = if expected_total > 0 {
            total_edges as f64 / expected_total as f64
        } else {
            0.0
        };

        eprintln!(
            "Parity-clique 128D (bucketed): n_vert={}, n_edges={}, expected={}, \
             density={:.4}, cross_frac={:.3}, n_even={}, n_odd={}",
            n,
            total_edges,
            expected_total,
            density,
            cross_frac,
            even_indices.len(),
            odd_indices.len()
        );
        eprintln!(
            "  even_edges={}, odd_edges={}, cross_edges={}",
            n_even_edges, n_odd_edges, n_cross_edges
        );

        // Thesis F REFUTED at 128D (consistent with lower dimensions)
        let is_biclique =
            n_cross_edges == 0 && n_even_edges == expected_even && n_odd_edges == expected_odd;
        assert!(!is_biclique, "Parity-biclique should fail at 128D");
        assert!(total_edges > 0, "Should have ZD-adjacent edges at 128D");
    }

    // === Thesis G: Spectral Fingerprints ===

    #[test]
    fn test_thesis_g_known_spectrum_complete_graph() {
        // K_4 has spectrum {3, -1, -1, -1}
        let k4: Vec<Vec<u8>> = vec![
            vec![0, 1, 1, 1],
            vec![1, 0, 1, 1],
            vec![1, 1, 0, 1],
            vec![1, 1, 1, 0],
        ];
        let fp = spectral_fingerprint_from_adjacency(&k4);
        assert_eq!(fp.n_vertices, 4);
        assert_eq!(fp.n_edges, 6);
        assert_eq!(fp.degree_sequence, vec![3, 3, 3, 3]);
        assert_eq!(fp.triangle_count, 4);
        assert_eq!(fp.diameter, 1);
        assert_eq!(fp.girth, 3);

        // Eigenvalues: {3, -1, -1, -1}
        assert!((fp.eigenvalues[0] - 3.0).abs() < 0.01);
        for &ev in &fp.eigenvalues[1..] {
            assert!((ev + 1.0).abs() < 0.01, "Expected -1, got {}", ev);
        }
    }

    #[test]
    fn test_thesis_g_known_spectrum_matching() {
        // 3*K_2 (perfect matching on 6 vertices) has spectrum {+1,+1,+1,-1,-1,-1}
        let matching: Vec<Vec<u8>> = vec![
            vec![0, 1, 0, 0, 0, 0],
            vec![1, 0, 0, 0, 0, 0],
            vec![0, 0, 0, 1, 0, 0],
            vec![0, 0, 1, 0, 0, 0],
            vec![0, 0, 0, 0, 0, 1],
            vec![0, 0, 0, 0, 1, 0],
        ];
        let fp = spectral_fingerprint_from_adjacency(&matching);
        assert_eq!(fp.n_vertices, 6);
        assert_eq!(fp.n_edges, 3);
        assert_eq!(fp.degree_sequence, vec![1, 1, 1, 1, 1, 1]);
        assert_eq!(fp.triangle_count, 0);
        assert_eq!(fp.girth, 0); // No cycles in a matching

        // Eigenvalues: three +1, three -1
        let pos_count = fp
            .eigenvalues
            .iter()
            .filter(|&&e| (e - 1.0).abs() < 0.01)
            .count();
        let neg_count = fp
            .eigenvalues
            .iter()
            .filter(|&&e| (e + 1.0).abs() < 0.01)
            .count();
        assert_eq!(pos_count, 3, "Expected 3 eigenvalues near +1");
        assert_eq!(neg_count, 3, "Expected 3 eigenvalues near -1");
    }

    #[test]
    fn test_thesis_g_k4_union_k4_spectrum() {
        // K_4 union K_4: spectrum = {3, 3, -1, -1, -1, -1, -1, -1}
        let mut adj = vec![vec![0u8; 8]; 8];
        // First K_4: vertices 0-3
        for i in 0..4 {
            for j in 0..4 {
                if i != j {
                    adj[i][j] = 1;
                }
            }
        }
        // Second K_4: vertices 4-7
        for i in 4..8 {
            for j in 4..8 {
                if i != j {
                    adj[i][j] = 1;
                }
            }
        }
        let fp = spectral_fingerprint_from_adjacency(&adj);
        assert_eq!(fp.n_vertices, 8);
        assert_eq!(fp.n_edges, 12); // 2 * C(4,2) = 12
        assert_eq!(fp.triangle_count, 8); // 2 * C(4,3) = 8

        // Two eigenvalues near 3, six near -1
        let near_3 = fp
            .eigenvalues
            .iter()
            .filter(|&&e| (e - 3.0).abs() < 0.01)
            .count();
        let near_m1 = fp
            .eigenvalues
            .iter()
            .filter(|&&e| (e + 1.0).abs() < 0.01)
            .count();
        assert_eq!(near_3, 2, "Expected 2 eigenvalues near 3");
        assert_eq!(near_m1, 6, "Expected 6 eigenvalues near -1");
    }

    // === Thesis D: Scalar Shadow ===

    #[test]
    fn test_thesis_d_scalar_shadow_basic() {
        let lattice_map = load_lattice_map(256);
        assert_eq!(lattice_map.len(), 256);

        // Identity (index 0) should map to all-negative lattice point
        let pi_0 = scalar_shadow(0, &lattice_map);
        assert!(pi_0.is_some());
        // Sum of all-(-1) = -8, signum = -1
        assert_eq!(pi_0.unwrap(), -1, "Identity scalar shadow should be -1");
    }

    #[test]
    fn test_thesis_d_scalar_shadow_addition_mode() {
        let result = verify_scalar_shadow_addition(256);
        eprintln!("Scalar shadow 256D: {:?}", result);
        assert!(result.n_checked > 0, "Should check some basis elements");
        assert!(
            result.all_ternary,
            "All scalar shadows should be in {{-1,0,1}}"
        );
    }

    #[test]
    fn test_dictionary_coupling_sample_256d() {
        let lattice_map = load_lattice_map(256);

        // Apply dictionary coupling: ell_0 + Phi(1)
        let ell_0 = lattice_map.get(&0).expect("Identity not in map");
        let coupled = dictionary_coupling_add(ell_0, 1, &lattice_map);
        assert!(coupled.is_some(), "Dictionary coupling should succeed");
        let result = coupled.unwrap();
        assert_eq!(result.len(), 8, "Result should be 8D");
    }

    #[test]
    fn test_c452_c453_lattice_header_schema_stability() {
        let dims = [256usize, 512, 1024, 2048];
        let mut headers = Vec::new();

        for &dim in &dims {
            let csv_path = lattice_csv_path(dim);
            let content = std::fs::read_to_string(&csv_path)
                .unwrap_or_else(|_| panic!("Lattice CSV not found: {}", csv_path));
            let header = content
                .lines()
                .next()
                .unwrap_or_else(|| panic!("CSV {} missing header", csv_path));
            let fields = parse_csv_line(header);
            assert_eq!(
                fields.len(),
                4,
                "Expected 4 header fields for {}D lattice CSV",
                dim
            );
            assert!(
                fields[0].contains("Basis Element"),
                "Header[0] should contain 'Basis Element' for {}D",
                dim
            );
            assert_eq!(
                fields[1], "Mapped Lattice Point",
                "Header[1] mismatch for {}D",
                dim
            );
            assert_eq!(fields[2], "Lattice Sum", "Header[2] mismatch for {}D", dim);
            assert_eq!(
                fields[3], "Consistent Sum",
                "Header[3] mismatch for {}D",
                dim
            );
            headers.push(fields);
        }

        // Freeze current external CSV header schema across all dimensions.
        for header in headers.iter().skip(1) {
            assert_eq!(
                header, &headers[0],
                "Lattice CSV headers drifted across dims"
            );
        }
    }

    #[test]
    fn test_c452_c453_embedding_is_injective_and_roundtrip() {
        let dims = [256usize, 512, 1024, 2048];

        for &dim in &dims {
            let records = parse_lattice_csv_records(dim);
            assert_eq!(
                records.len(),
                dim,
                "Expected {} rows in {}D lattice CSV",
                dim,
                dim
            );

            let mut seen_idx = std::collections::HashSet::new();
            let mut seen_lattice = std::collections::HashSet::new();
            for (idx, lattice) in &records {
                assert!(
                    seen_idx.insert(*idx),
                    "Duplicate basis index {} in {}D lattice mapping",
                    idx,
                    dim
                );
                assert!(
                    seen_lattice.insert(lattice.clone()),
                    "Duplicate lattice vector {:?} in {}D mapping",
                    lattice,
                    dim
                );
            }
            assert_eq!(
                seen_idx.len(),
                dim,
                "Missing basis indices in {}D mapping",
                dim
            );
            assert_eq!(
                seen_lattice.len(),
                dim,
                "Embedding not injective in {}D mapping",
                dim
            );

            let reverse: std::collections::HashMap<Vec<i32>, usize> = records
                .iter()
                .map(|(idx, lattice)| (lattice.clone(), *idx))
                .collect();
            for (idx, lattice) in &records {
                assert_eq!(
                    reverse.get(lattice),
                    Some(idx),
                    "Round-trip lattice->index failed for {}D index {}",
                    dim,
                    idx
                );
            }
        }
    }

    #[test]
    fn test_c452_c453_codomain_and_index_coverage_all_dims() {
        let dims = [256usize, 512, 1024, 2048];

        for &dim in &dims {
            let lattice_map = load_lattice_map(dim);
            assert_eq!(lattice_map.len(), dim, "Expected full map for {}D", dim);

            for idx in 0..dim {
                assert!(
                    lattice_map.contains_key(&idx),
                    "Missing basis index {} in {}D lattice map",
                    idx,
                    dim
                );
            }

            for lattice in lattice_map.values() {
                assert_eq!(lattice.len(), 8, "Lattice vector must be 8D");
                assert!(
                    lattice.iter().all(|&x| (-1..=1).contains(&x)),
                    "Lattice vector {:?} not in trinary codomain",
                    lattice
                );
            }
        }
    }

    #[test]
    fn test_c453_filtration_growth_has_expected_new_points() {
        let p256: std::collections::BTreeSet<Vec<i32>> =
            load_lattice_points(256).into_iter().collect();
        let p512: std::collections::BTreeSet<Vec<i32>> =
            load_lattice_points(512).into_iter().collect();
        let p1024: std::collections::BTreeSet<Vec<i32>> =
            load_lattice_points(1024).into_iter().collect();
        let p2048: std::collections::BTreeSet<Vec<i32>> =
            load_lattice_points(2048).into_iter().collect();

        assert!(p256.is_subset(&p512));
        assert!(p512.is_subset(&p1024));
        assert!(p1024.is_subset(&p2048));

        assert_eq!(p512.difference(&p256).count(), 256);
        assert_eq!(p1024.difference(&p512).count(), 512);
        assert_eq!(p2048.difference(&p1024).count(), 1024);
    }

    #[test]
    fn test_c453_filtration_layers_are_disjoint_and_partition_2048() {
        let p256: std::collections::BTreeSet<Vec<i32>> =
            load_lattice_points(256).into_iter().collect();
        let p512: std::collections::BTreeSet<Vec<i32>> =
            load_lattice_points(512).into_iter().collect();
        let p1024: std::collections::BTreeSet<Vec<i32>> =
            load_lattice_points(1024).into_iter().collect();
        let p2048: std::collections::BTreeSet<Vec<i32>> =
            load_lattice_points(2048).into_iter().collect();

        let l0: std::collections::BTreeSet<Vec<i32>> = p256.clone();
        let l1: std::collections::BTreeSet<Vec<i32>> = p512.difference(&p256).cloned().collect();
        let l2: std::collections::BTreeSet<Vec<i32>> = p1024.difference(&p512).cloned().collect();
        let l3: std::collections::BTreeSet<Vec<i32>> = p2048.difference(&p1024).cloned().collect();

        assert_eq!(l0.len(), 256, "L0 size mismatch");
        assert_eq!(l1.len(), 256, "L1 size mismatch");
        assert_eq!(l2.len(), 512, "L2 size mismatch");
        assert_eq!(l3.len(), 1024, "L3 size mismatch");

        assert!(l0.is_disjoint(&l1), "L0 and L1 must be disjoint");
        assert!(l0.is_disjoint(&l2), "L0 and L2 must be disjoint");
        assert!(l0.is_disjoint(&l3), "L0 and L3 must be disjoint");
        assert!(l1.is_disjoint(&l2), "L1 and L2 must be disjoint");
        assert!(l1.is_disjoint(&l3), "L1 and L3 must be disjoint");
        assert!(l2.is_disjoint(&l3), "L2 and L3 must be disjoint");

        let union_l01: std::collections::BTreeSet<Vec<i32>> = l0.union(&l1).cloned().collect();
        let union_l012: std::collections::BTreeSet<Vec<i32>> =
            union_l01.union(&l2).cloned().collect();
        let union_all: std::collections::BTreeSet<Vec<i32>> =
            union_l012.union(&l3).cloned().collect();

        assert_eq!(
            union_all.len(),
            2048,
            "Layer union must contain exactly 2048 unique points"
        );
        assert_eq!(
            union_all, p2048,
            "Layer union must reconstruct 2048D embedding exactly"
        );
    }

    #[test]
    fn test_c453_filtration_intersection_cardinalities_are_exact() {
        let p256: std::collections::BTreeSet<Vec<i32>> =
            load_lattice_points(256).into_iter().collect();
        let p512: std::collections::BTreeSet<Vec<i32>> =
            load_lattice_points(512).into_iter().collect();
        let p1024: std::collections::BTreeSet<Vec<i32>> =
            load_lattice_points(1024).into_iter().collect();
        let p2048: std::collections::BTreeSet<Vec<i32>> =
            load_lattice_points(2048).into_iter().collect();

        assert_eq!(p256.intersection(&p512).count(), 256);
        assert_eq!(p256.intersection(&p1024).count(), 256);
        assert_eq!(p256.intersection(&p2048).count(), 256);
        assert_eq!(p512.intersection(&p1024).count(), 512);
        assert_eq!(p512.intersection(&p2048).count(), 512);
        assert_eq!(p1024.intersection(&p2048).count(), 1024);
    }

    #[test]
    fn test_c453_filtration_is_lexicographic_prefix_chain() {
        let dims = [256usize, 512, 1024, 2048];
        let sets: Vec<std::collections::BTreeSet<Vec<i32>>> = dims
            .iter()
            .map(|&dim| load_lattice_points(dim).into_iter().collect())
            .collect();

        for i in 0..sets.len() - 1 {
            let child_dim = dims[i];
            let parent_dim = dims[i + 1];
            let child = &sets[i];
            let parent = &sets[i + 1];

            let cut = learn_prefix_cut(parent, child).unwrap_or_else(|| {
                panic!(
                    "Expected lexicographic prefix cut for {}D -> {}D transition",
                    parent_dim, child_dim
                )
            });
            assert_eq!(cut.parent_size, parent_dim);
            assert_eq!(cut.child_size, child_dim);
            assert!(
                verify_prefix_cut(parent, child, &cut),
                "Prefix cut verification failed for {}D -> {}D",
                parent_dim,
                child_dim
            );

            let parent_sorted: Vec<&Vec<i32>> = parent.iter().collect();
            let first_child: std::collections::BTreeSet<Vec<i32>> = parent_sorted[..child_dim]
                .iter()
                .map(|&v| v.clone())
                .collect();
            assert_eq!(
                &first_child, child,
                "{}D map is not lex-prefix-consistent with {}D map",
                child_dim, parent_dim
            );
        }
    }
}
