//! Statistical analysis of E10 cosmological billiard wall-transition sequences.
//!
//! Provides locality metrics, null models, and permutation tests for the
//! billiard word (sequence of wall indices) generated by the BKL billiard
//! simulation in the E10 Weyl chamber.
//!
//! # Locality metrics
//!
//! The key observable is the **adjacency ratio** r = (connected transitions) /
//! (total transitions), where "connected" means the two walls share an edge in
//! the E10 Dynkin diagram. This is computed sector-by-sector: E8 (walls 0-7),
//! mixed (E8 <-> affine/hyperbolic), and hyperbolic (walls 8-9).
//!
//! Additional metrics: mutual information between consecutive generators,
//! commutation rate (fraction of pairs with A_{ij} = 0), and run-length
//! distribution.
//!
//! # Null models
//!
//! - **Uniform**: iid uniform on {0, ..., 9}
//! - **IidEmpirical**: iid from the empirical wall-frequency distribution
//! - **DegreePreserving**: Fisher-Yates shuffle of the sequence (preserves
//!   marginal frequencies but breaks sequential correlations)
//! - **Markov**: 1st-order Markov chain from the empirical transition matrix
//!
//! # Permutation tests
//!
//! Each null model generates synthetic sequences of the same length. The
//! observed r_E8 (or any metric) is compared to the null distribution to
//! produce a p-value with Phipson-Smyth (2010) correction: p = (r+1)/(k+1).

use rand::prelude::*;

// ---------------------------------------------------------------------------
// E10 Dynkin diagram (constant, hardcoded from Cartan matrix)
// ---------------------------------------------------------------------------

/// E10 Dynkin diagram adjacency as a 10x10 boolean matrix.
///
/// Node labeling (from root vector Gram matrix, branch at node 4):
///   0 -- 1 -- 2 -- 3 -- 4 -- 5
///                        |
///                        6 -- 7
///   8 -- 0  (affine extension)
///   9 -- 8  (hyperbolic extension)
pub const E10_ADJACENCY: [[bool; 10]; 10] = {
    let mut a = [[false; 10]; 10];
    // E8 spine: 0-1-2-3-4-5
    a[0][1] = true; a[1][0] = true;
    a[1][2] = true; a[2][1] = true;
    a[2][3] = true; a[3][2] = true;
    a[3][4] = true; a[4][3] = true;
    a[4][5] = true; a[5][4] = true;
    // E8 branch: 4-6-7
    a[4][6] = true; a[6][4] = true;
    a[6][7] = true; a[7][6] = true;
    // Affine: 0-8
    a[0][8] = true; a[8][0] = true;
    // Hyperbolic: 8-9
    a[8][9] = true; a[9][8] = true;
    a
};

/// Number of E10 simple roots (walls).
pub const N_WALLS: usize = 10;

/// Number of E8 walls (nodes 0-7).
pub const N_E8: usize = 8;

/// Number of edges in the E8 subdiagram: 7 (a tree on 8 nodes).
pub const E8_EDGES: usize = 7;

/// Number of directed ordered pairs in E8: 2 * 7 = 14.
pub const E8_DIRECTED_PAIRS: usize = 14;

/// Number of ordered pairs in E8 (i != j): 8 * 7 = 56.
pub const E8_ORDERED_PAIRS: usize = 56;

/// Null adjacency ratio for uniform random transitions in E8.
/// = 14 / 56 = 0.25 (7 undirected edges -> 14 directed, out of 56 ordered pairs).
pub const NULL_R_E8_UNIFORM: f64 = E8_DIRECTED_PAIRS as f64 / E8_ORDERED_PAIRS as f64;

// ---------------------------------------------------------------------------
// Locality metrics
// ---------------------------------------------------------------------------

/// Comprehensive locality metrics for a billiard wall-transition sequence.
#[derive(Debug, Clone)]
pub struct LocalityMetrics {
    /// Adjacency ratio in the E8 sector (walls 0-7).
    /// r = (E8 connected) / (E8 connected + E8 disconnected).
    pub r_e8: f64,
    /// Adjacency ratio across the full E10 diagram.
    pub r_e10: f64,
    /// Number of E8-internal transitions (i,j both < 8, i != j).
    pub n_e8_transitions: usize,
    /// Number of mixed transitions (one endpoint in E8, one not).
    pub n_mixed_transitions: usize,
    /// Number of hyperbolic-sector transitions (both endpoints >= 8).
    pub n_hyp_transitions: usize,
    /// Commutation rate: fraction of consecutive pairs with A_{ij} = 0
    /// (generators that commute in the Weyl group).
    pub commutation_rate: f64,
    /// Mutual information I(S_t; S_{t+1}) in nats.
    pub mutual_information: f64,
    /// Total transitions analyzed (sequence length - 1).
    pub n_transitions: usize,
}

/// Compute locality metrics from a billiard wall-transition sequence.
///
/// The sequence contains wall indices in 0..10 (E10 simple roots).
/// Metrics are computed from consecutive pairs (s_t, s_{t+1}).
pub fn compute_locality_metrics(sequence: &[usize]) -> LocalityMetrics {
    if sequence.len() < 2 {
        return LocalityMetrics {
            r_e8: 0.0,
            r_e10: 0.0,
            n_e8_transitions: 0,
            n_mixed_transitions: 0,
            n_hyp_transitions: 0,
            commutation_rate: 0.0,
            mutual_information: 0.0,
            n_transitions: 0,
        };
    }

    let n_trans = sequence.len() - 1;
    let mut e8_connected: usize = 0;
    let mut e8_total: usize = 0;
    let mut e10_connected: usize = 0;
    let mut e10_total: usize = 0;
    let mut mixed: usize = 0;
    let mut hyp: usize = 0;
    let mut commuting: usize = 0;

    // Transition counts for MI computation
    let mut trans_counts = [[0u64; N_WALLS]; N_WALLS];
    let mut from_counts = [0u64; N_WALLS];

    for pair in sequence.windows(2) {
        let (i, j) = (pair[0], pair[1]);
        if i >= N_WALLS || j >= N_WALLS { continue; }

        let adjacent = E10_ADJACENCY[i][j];
        if adjacent { e10_connected += 1; }
        e10_total += 1;

        // Sector classification
        let i_e8 = i < N_E8;
        let j_e8 = j < N_E8;
        match (i_e8, j_e8) {
            (true, true) => {
                if i != j {
                    e8_total += 1;
                    if adjacent { e8_connected += 1; }
                }
            }
            (false, false) => { hyp += 1; }
            _ => { mixed += 1; }
        }

        // Commutation: A_{ij} = 0 iff not adjacent and i != j
        if i != j && !adjacent {
            commuting += 1;
        }

        // Transition counting for MI
        trans_counts[i][j] += 1;
        from_counts[i] += 1;
    }

    let r_e8 = if e8_total > 0 { e8_connected as f64 / e8_total as f64 } else { 0.0 };
    let r_e10 = if e10_total > 0 { e10_connected as f64 / e10_total as f64 } else { 0.0 };
    let commutation_rate = if n_trans > 0 { commuting as f64 / n_trans as f64 } else { 0.0 };

    // Mutual information: I(S_t; S_{t+1}) = sum_{i,j} p(i,j) log(p(i,j) / (p(i) * p(j)))
    let total_f = n_trans as f64;
    let mut to_counts = [0u64; N_WALLS];
    for pair in sequence.windows(2) {
        let j = pair[1];
        if j < N_WALLS { to_counts[j] += 1; }
    }

    let mut mi = 0.0;
    for i in 0..N_WALLS {
        for j in 0..N_WALLS {
            let nij = trans_counts[i][j];
            if nij == 0 { continue; }
            let p_ij = nij as f64 / total_f;
            let p_i = from_counts[i] as f64 / total_f;
            let p_j = to_counts[j] as f64 / total_f;
            if p_i > 0.0 && p_j > 0.0 {
                mi += p_ij * (p_ij / (p_i * p_j)).ln();
            }
        }
    }

    LocalityMetrics {
        r_e8,
        r_e10,
        n_e8_transitions: e8_total,
        n_mixed_transitions: mixed,
        n_hyp_transitions: hyp,
        commutation_rate,
        mutual_information: mi,
        n_transitions: n_trans,
    }
}

// ---------------------------------------------------------------------------
// Null models
// ---------------------------------------------------------------------------

/// Null model for billiard transition sequences.
#[derive(Debug, Clone)]
pub enum NullModel {
    /// Uniform random: each step is iid Uniform({0, ..., 9}).
    Uniform,
    /// IID from empirical wall frequencies (preserves marginal distribution
    /// but breaks all sequential structure).
    IidEmpirical,
    /// Fisher-Yates shuffle of the observed sequence (preserves exact
    /// marginal counts but randomizes order).
    DegreePreserving,
    /// First-order Markov chain from the empirical transition matrix
    /// (preserves pairwise transition rates but randomizes higher-order
    /// structure).
    Markov,
    /// Commutation shuffle: randomly swap consecutive commuting pairs
    /// (s_i s_j = s_j s_i when A_{ij} = 0). Preserves Weyl group
    /// equivalence class. Multiple passes for mixing.
    /// Parameter: number of mixing passes.
    CommutationShuffle(usize),
}

/// Generate a null sequence of the given length.
///
/// - `model`: which null model to use
/// - `length`: desired sequence length
/// - `empirical`: the observed bounce sequence (used for frequency/transition info)
/// - `rng`: random number generator
pub fn generate_null_sequence(
    model: &NullModel,
    length: usize,
    empirical: &[usize],
    rng: &mut impl Rng,
) -> Vec<usize> {
    match model {
        NullModel::Uniform => {
            (0..length).map(|_| rng.gen_range(0..N_WALLS)).collect()
        }
        NullModel::IidEmpirical => {
            let freq = wall_frequencies(empirical);
            let dist = cumulative_distribution(&freq);
            (0..length).map(|_| sample_from_cdf(&dist, rng)).collect()
        }
        NullModel::DegreePreserving => {
            let mut shuffled = empirical.to_vec();
            // Truncate or extend to match desired length
            shuffled.truncate(length);
            while shuffled.len() < length {
                shuffled.push(empirical[rng.gen_range(0..empirical.len())]);
            }
            // Fisher-Yates shuffle
            for i in (1..shuffled.len()).rev() {
                let j = rng.gen_range(0..=i);
                shuffled.swap(i, j);
            }
            shuffled
        }
        NullModel::Markov => {
            generate_markov_sequence(empirical, length, rng)
        }
        NullModel::CommutationShuffle(passes) => {
            generate_commutation_shuffle(empirical, length, *passes, rng)
        }
    }
}

/// Compute wall frequencies from a bounce sequence.
fn wall_frequencies(sequence: &[usize]) -> [f64; N_WALLS] {
    let mut counts = [0u64; N_WALLS];
    for &w in sequence {
        if w < N_WALLS { counts[w] += 1; }
    }
    let total = counts.iter().sum::<u64>() as f64;
    let mut freq = [0.0; N_WALLS];
    if total > 0.0 {
        for (i, &c) in counts.iter().enumerate() {
            freq[i] = c as f64 / total;
        }
    }
    freq
}

/// Build cumulative distribution from frequency array.
fn cumulative_distribution(freq: &[f64; N_WALLS]) -> [f64; N_WALLS] {
    let mut cdf = [0.0; N_WALLS];
    let mut cum = 0.0;
    for (i, &f) in freq.iter().enumerate() {
        cum += f;
        cdf[i] = cum;
    }
    // Normalize to exactly 1.0 to avoid floating-point drift
    if cum > 0.0 {
        for c in &mut cdf {
            *c /= cum;
        }
    }
    cdf
}

/// Sample from a CDF using inverse-transform sampling.
fn sample_from_cdf(cdf: &[f64; N_WALLS], rng: &mut impl Rng) -> usize {
    let u: f64 = rng.gen();
    for (i, &c) in cdf.iter().enumerate() {
        if u <= c {
            return i;
        }
    }
    N_WALLS - 1 // Fallback (should not happen with normalized CDF)
}

/// Generate a first-order Markov sequence from empirical transition matrix.
fn generate_markov_sequence(
    empirical: &[usize],
    length: usize,
    rng: &mut impl Rng,
) -> Vec<usize> {
    if empirical.is_empty() || length == 0 {
        return vec![];
    }

    // Build transition matrix (row-stochastic)
    let mut trans = [[0u64; N_WALLS]; N_WALLS];
    for pair in empirical.windows(2) {
        let (i, j) = (pair[0], pair[1]);
        if i < N_WALLS && j < N_WALLS {
            trans[i][j] += 1;
        }
    }

    // Convert to row CDFs
    let mut row_cdfs = [[0.0f64; N_WALLS]; N_WALLS];
    for (i, row) in trans.iter().enumerate() {
        let row_total = row.iter().sum::<u64>() as f64;
        if row_total > 0.0 {
            let mut cum = 0.0;
            for (j, &count) in row.iter().enumerate() {
                cum += count as f64 / row_total;
                row_cdfs[i][j] = cum;
            }
            // Ensure last entry is exactly 1.0
            row_cdfs[i][N_WALLS - 1] = 1.0;
        } else {
            // Absorbing state: uniform fallback
            for (j, cell) in row_cdfs[i].iter_mut().enumerate() {
                *cell = (j + 1) as f64 / N_WALLS as f64;
            }
        }
    }

    // Start from empirical frequency distribution
    let freq = wall_frequencies(empirical);
    let start_cdf = cumulative_distribution(&freq);

    let mut seq = Vec::with_capacity(length);
    let mut state = sample_from_cdf(&start_cdf, rng);
    seq.push(state);

    for _ in 1..length {
        let u: f64 = rng.gen();
        let mut next = N_WALLS - 1;
        for (j, &c) in row_cdfs[state].iter().enumerate() {
            if u <= c {
                next = j;
                break;
            }
        }
        seq.push(next);
        state = next;
    }

    seq
}

// ---------------------------------------------------------------------------
// Permutation test
/// Generate a commutation-shuffled sequence.
///
/// This preserves the Weyl group equivalence class: consecutive generators
/// that commute (A_{ij} = 0, i.e., non-adjacent in Dynkin diagram) are
/// randomly swapped. Multiple passes ensure good mixing.
fn generate_commutation_shuffle(
    empirical: &[usize],
    length: usize,
    passes: usize,
    rng: &mut impl Rng,
) -> Vec<usize> {
    let mut seq = empirical.to_vec();
    seq.truncate(length);
    while seq.len() < length {
        seq.push(empirical[rng.gen_range(0..empirical.len())]);
    }

    for _ in 0..passes {
        for i in 0..seq.len().saturating_sub(1) {
            let (a, b) = (seq[i], seq[i + 1]);
            if a < N_WALLS && b < N_WALLS && a != b && !E10_ADJACENCY[a][b] {
                // Generators commute: randomly swap with 50% probability
                if rng.gen_bool(0.5) {
                    seq.swap(i, i + 1);
                }
            }
        }
    }

    seq
}

// ---------------------------------------------------------------------------

/// Sector-specific adjacency ratios for detailed analysis.
#[derive(Debug, Clone)]
pub struct SectorMetrics {
    /// E8 adjacency ratio (walls 0-7 only).
    pub r_e8: f64,
    /// Full E10 adjacency ratio.
    pub r_e10: f64,
    /// Mixed-sector adjacency ratio (E8 <-> affine/hyperbolic).
    /// Here, "adjacent" means connected in E10 Dynkin (only 0-8).
    pub r_mixed: f64,
    /// Hyperbolic sector adjacency ratio (walls 8-9).
    pub r_hyp: f64,
    /// Fraction of all transitions that are E8-internal.
    pub e8_fraction: f64,
    /// Fraction of all transitions that are mixed.
    pub mixed_fraction: f64,
    /// Fraction of all transitions that are hyperbolic-internal.
    pub hyp_fraction: f64,
}

/// Compute sector-specific adjacency ratios.
///
/// Breaks down transitions by sector (E8, mixed, hyperbolic) and computes
/// the adjacency ratio within each sector.
pub fn compute_sector_metrics(sequence: &[usize]) -> SectorMetrics {
    if sequence.len() < 2 {
        return SectorMetrics {
            r_e8: 0.0, r_e10: 0.0, r_mixed: 0.0, r_hyp: 0.0,
            e8_fraction: 0.0, mixed_fraction: 0.0, hyp_fraction: 0.0,
        };
    }

    let mut e8_adj = 0u64;
    let mut e8_total = 0u64;
    let mut mixed_adj = 0u64;
    let mut mixed_total = 0u64;
    let mut hyp_adj = 0u64;
    let mut hyp_total = 0u64;
    let mut e10_adj = 0u64;
    let mut e10_total = 0u64;

    for pair in sequence.windows(2) {
        let (i, j) = (pair[0], pair[1]);
        if i >= N_WALLS || j >= N_WALLS || i == j { continue; }

        let adjacent = E10_ADJACENCY[i][j];
        e10_total += 1;
        if adjacent { e10_adj += 1; }

        let i_e8 = i < N_E8;
        let j_e8 = j < N_E8;
        match (i_e8, j_e8) {
            (true, true) => {
                e8_total += 1;
                if adjacent { e8_adj += 1; }
            }
            (false, false) => {
                hyp_total += 1;
                if adjacent { hyp_adj += 1; }
            }
            _ => {
                mixed_total += 1;
                if adjacent { mixed_adj += 1; }
            }
        }
    }

    let total = e10_total as f64;
    SectorMetrics {
        r_e8: if e8_total > 0 { e8_adj as f64 / e8_total as f64 } else { 0.0 },
        r_e10: if e10_total > 0 { e10_adj as f64 / e10_total as f64 } else { 0.0 },
        r_mixed: if mixed_total > 0 { mixed_adj as f64 / mixed_total as f64 } else { 0.0 },
        r_hyp: if hyp_total > 0 { hyp_adj as f64 / hyp_total as f64 } else { 0.0 },
        e8_fraction: if total > 0.0 { e8_total as f64 / total } else { 0.0 },
        mixed_fraction: if total > 0.0 { mixed_total as f64 / total } else { 0.0 },
        hyp_fraction: if total > 0.0 { hyp_total as f64 / total } else { 0.0 },
    }
}

// ---------------------------------------------------------------------------

/// Result of a permutation test.
#[derive(Debug, Clone)]
pub struct PermutationTestResult {
    /// The observed test statistic (e.g. r_E8).
    pub observed: f64,
    /// Mean of the null distribution.
    pub null_mean: f64,
    /// Standard deviation of the null distribution.
    pub null_std: f64,
    /// Two-sided p-value (Phipson-Smyth corrected: (r+1)/(k+1)).
    pub p_value: f64,
    /// Number of null permutations evaluated.
    pub n_permutations: usize,
    /// 95% CI lower bound of null distribution.
    pub null_ci_lower: f64,
    /// 95% CI upper bound of null distribution.
    pub null_ci_upper: f64,
    /// Effect size: (observed - null_mean) / null_std.
    pub effect_size: f64,
}

/// Run a permutation test for E8 adjacency ratio.
///
/// Generates `n_permutations` null sequences, computes r_E8 for each,
/// and returns the p-value (one-sided, testing observed >= null).
///
/// Uses the Phipson-Smyth (2010) correction: p = (r+1)/(k+1) where
/// r = number of null values >= observed, k = total permutations.
pub fn permutation_test_r_e8(
    sequence: &[usize],
    null_model: &NullModel,
    n_permutations: usize,
    seed: u64,
) -> PermutationTestResult {
    let observed = compute_locality_metrics(sequence).r_e8;
    let length = sequence.len();

    let mut null_values = Vec::with_capacity(n_permutations);
    let mut rng = rand_chacha::ChaCha8Rng::seed_from_u64(seed);

    for _ in 0..n_permutations {
        let null_seq = generate_null_sequence(null_model, length, sequence, &mut rng);
        let null_r = compute_locality_metrics(&null_seq).r_e8;
        null_values.push(null_r);
    }

    summarize_permutation_test(observed, &null_values)
}

/// Run a permutation test for mutual information.
pub fn permutation_test_mi(
    sequence: &[usize],
    null_model: &NullModel,
    n_permutations: usize,
    seed: u64,
) -> PermutationTestResult {
    let observed = compute_locality_metrics(sequence).mutual_information;
    let length = sequence.len();

    let mut null_values = Vec::with_capacity(n_permutations);
    let mut rng = rand_chacha::ChaCha8Rng::seed_from_u64(seed);

    for _ in 0..n_permutations {
        let null_seq = generate_null_sequence(null_model, length, sequence, &mut rng);
        let null_mi = compute_locality_metrics(&null_seq).mutual_information;
        null_values.push(null_mi);
    }

    summarize_permutation_test(observed, &null_values)
}

/// Summarize a permutation test from observed value and null distribution.
pub(crate) fn summarize_permutation_test(observed: f64, null_values: &[f64]) -> PermutationTestResult {
    let k = null_values.len();
    if k == 0 {
        return PermutationTestResult {
            observed,
            null_mean: 0.0,
            null_std: 0.0,
            p_value: 1.0,
            n_permutations: 0,
            null_ci_lower: 0.0,
            null_ci_upper: 0.0,
            effect_size: 0.0,
        };
    }

    let sum: f64 = null_values.iter().sum();
    let mean = sum / k as f64;
    let var: f64 = null_values.iter().map(|x| (x - mean).powi(2)).sum::<f64>() / k as f64;
    let std = var.sqrt();

    // One-sided p-value: proportion of null >= observed (Phipson-Smyth)
    let r = null_values.iter().filter(|&&x| x >= observed).count();
    let p_value = (r as f64 + 1.0) / (k as f64 + 1.0);

    // 95% CI from sorted null distribution
    let mut sorted = null_values.to_vec();
    sorted.sort_by(|a, b| a.partial_cmp(b).unwrap_or(std::cmp::Ordering::Equal));
    let ci_lower = sorted[(0.025 * k as f64) as usize];
    let ci_upper = sorted[((0.975 * k as f64) as usize).min(k - 1)];

    let effect_size = if std > 1e-15 { (observed - mean) / std } else { 0.0 };

    PermutationTestResult {
        observed,
        null_mean: mean,
        null_std: std,
        p_value,
        n_permutations: k,
        null_ci_lower: ci_lower,
        null_ci_upper: ci_upper,
        effect_size,
    }
}

// ---------------------------------------------------------------------------
// Transition matrix analysis
// ---------------------------------------------------------------------------

/// Build a transition count matrix from a bounce sequence.
pub fn transition_matrix(sequence: &[usize]) -> [[u64; N_WALLS]; N_WALLS] {
    let mut mat = [[0u64; N_WALLS]; N_WALLS];
    for pair in sequence.windows(2) {
        let (i, j) = (pair[0], pair[1]);
        if i < N_WALLS && j < N_WALLS {
            mat[i][j] += 1;
        }
    }
    mat
}

/// Compute the stationary distribution of a transition matrix (left eigenvector
/// of the row-stochastic matrix, via power iteration).
pub fn stationary_distribution(trans: &[[u64; N_WALLS]; N_WALLS]) -> [f64; N_WALLS] {
    // Convert to row-stochastic
    let mut stoch = [[0.0f64; N_WALLS]; N_WALLS];
    for (i, row) in trans.iter().enumerate() {
        let total = row.iter().sum::<u64>() as f64;
        if total > 0.0 {
            for (j, &c) in row.iter().enumerate() {
                stoch[i][j] = c as f64 / total;
            }
        } else {
            // Uniform for absorbing states
            for cell in stoch[i].iter_mut() {
                *cell = 1.0 / N_WALLS as f64;
            }
        }
    }

    // Power iteration: pi * P = pi
    let mut pi = [1.0 / N_WALLS as f64; N_WALLS];
    for _ in 0..1000 {
        let mut next = [0.0f64; N_WALLS];
        for j in 0..N_WALLS {
            for (i, &p) in pi.iter().enumerate() {
                next[j] += p * stoch[i][j];
            }
        }
        // Normalize
        let sum: f64 = next.iter().sum();
        for x in &mut next {
            *x /= sum;
        }
        pi = next;
    }

    pi
}

// ---------------------------------------------------------------------------
// Claim 1 verification: r_e8 robustness across initial conditions and configs
// ---------------------------------------------------------------------------

/// Result of Claim 1 robustness verification.
///
/// Claim 1: The E8 adjacency ratio r_e8 remains significantly above the null
/// baseline (0.25 for uniform random) across 100 different initial conditions,
/// and is not sensitive to solver parameters (renormalization toggle, epsilon).
#[derive(Debug, Clone)]
pub struct Claim1Result {
    /// r_e8 values from each seed (default config).
    pub r_values: Vec<f64>,
    /// Mean r_e8 across seeds.
    pub mean_r: f64,
    /// Standard deviation of r_e8 across seeds.
    pub std_r: f64,
    /// Minimum r_e8 observed.
    pub min_r: f64,
    /// Maximum r_e8 observed.
    pub max_r: f64,
    /// Fraction of seeds where r_e8 > null baseline (0.25).
    pub fraction_above_null: f64,
    /// Null baseline: expected r_e8 for uniform random sequence.
    pub null_baseline: f64,
    /// Number of seeds that produced enough E8 transitions for measurement.
    pub n_measurable: usize,
    /// Robustness: r_e8 values with renormalization disabled.
    pub r_values_no_renorm: Vec<f64>,
    /// Robustness: r_e8 values with different epsilon (10x larger).
    pub r_values_large_eps: Vec<f64>,
    /// Maximum absolute difference between default and no-renorm configs.
    pub max_renorm_delta: f64,
    /// Maximum absolute difference between default and large-eps configs.
    pub max_eps_delta: f64,
}

/// Verify Claim 1: E8 adjacency ratio is robust across initial conditions.
///
/// Runs the hyperbolic billiard with `n_seeds` different random seeds, each
/// for `n_bounces` steps. Computes r_e8 for each run. Also tests with
/// alternative solver configurations (no renormalization, larger epsilon).
///
/// Returns `None` if the billiard module produces no usable sequences.
pub fn verify_claim1(
    n_seeds: usize,
    n_bounces: usize,
) -> Claim1Result {
    use crate::billiard_sim::HyperbolicBilliard;
    use crate::kac_moody::E10RootSystem;

    let e10 = E10RootSystem::new();

    let mut r_values = Vec::with_capacity(n_seeds);
    let mut r_values_no_renorm = Vec::with_capacity(n_seeds);
    let mut r_values_large_eps = Vec::with_capacity(n_seeds);

    for seed in 0..n_seeds as u64 {
        // Default config
        let mut billiard = HyperbolicBilliard::from_e10(&e10, seed);
        let seq = billiard.simulate(n_bounces);
        let metrics = compute_locality_metrics(&seq);
        r_values.push(metrics.r_e8);

        // Config variant 1: renormalization disabled
        let mut billiard_nr = HyperbolicBilliard::from_e10(&e10, seed);
        billiard_nr.config.renormalize = false;
        let seq_nr = billiard_nr.simulate(n_bounces);
        let metrics_nr = compute_locality_metrics(&seq_nr);
        r_values_no_renorm.push(metrics_nr.r_e8);

        // Config variant 2: larger epsilon (10x)
        let mut billiard_le = HyperbolicBilliard::from_e10(&e10, seed);
        let default_eps = billiard_le.config.time_epsilon;
        billiard_le.config.time_epsilon = default_eps * 10.0;
        let seq_le = billiard_le.simulate(n_bounces);
        let metrics_le = compute_locality_metrics(&seq_le);
        r_values_large_eps.push(metrics_le.r_e8);
    }

    // Statistics
    let n = r_values.len() as f64;
    let mean_r = r_values.iter().sum::<f64>() / n;
    let var_r = r_values.iter().map(|r| (r - mean_r).powi(2)).sum::<f64>() / n;
    let std_r = var_r.sqrt();
    let min_r = r_values.iter().cloned().fold(f64::INFINITY, f64::min);
    let max_r = r_values.iter().cloned().fold(f64::NEG_INFINITY, f64::max);

    let null_baseline = NULL_R_E8_UNIFORM;
    let fraction_above_null = r_values.iter().filter(|&&r| r > null_baseline).count() as f64 / n;
    let n_measurable = r_values.iter().filter(|&&r| r > 0.0).count();

    // Robustness deltas
    let max_renorm_delta = r_values.iter().zip(r_values_no_renorm.iter())
        .map(|(a, b)| (a - b).abs())
        .fold(0.0f64, f64::max);
    let max_eps_delta = r_values.iter().zip(r_values_large_eps.iter())
        .map(|(a, b)| (a - b).abs())
        .fold(0.0f64, f64::max);

    Claim1Result {
        r_values,
        mean_r,
        std_r,
        min_r,
        max_r,
        fraction_above_null,
        null_baseline,
        n_measurable,
        r_values_no_renorm,
        r_values_large_eps,
        max_renorm_delta,
        max_eps_delta,
    }
}

/// Produce a human-readable summary of Claim 1 verification.
///
/// Returns (summary_string, claim_supported).
/// Claim is supported if:
/// - mean r_e8 > 2 * null baseline (0.50)
/// - >= 80% of seeds have r_e8 > null baseline
/// - max config delta < 0.2 (robustness)
pub fn claim1_summary(result: &Claim1Result) -> (String, bool) {
    let above_threshold = result.mean_r > 2.0 * result.null_baseline;
    let fraction_ok = result.fraction_above_null >= 0.80;
    let robust = result.max_renorm_delta < 0.2 && result.max_eps_delta < 0.2;
    let supported = above_threshold && fraction_ok && robust;

    let status = if supported { "SUPPORTED" } else { "NOT SUPPORTED" };

    let summary = format!(
        "Claim 1 ({status}): r_e8 robustness across {n} seeds\n\
         mean={mean:.4} std={std:.4} min={min:.4} max={max:.4}\n\
         fraction above null({null:.4}): {frac:.1}%\n\
         measurable seeds: {meas}/{n}\n\
         robustness: renorm_delta={rd:.4} eps_delta={ed:.4}",
        status = status,
        n = result.r_values.len(),
        mean = result.mean_r,
        std = result.std_r,
        min = result.min_r,
        max = result.max_r,
        null = result.null_baseline,
        frac = result.fraction_above_null * 100.0,
        meas = result.n_measurable,
        rd = result.max_renorm_delta,
        ed = result.max_eps_delta,
    );

    (summary, supported)
}

// ---------------------------------------------------------------------------
// Chi-squared test for transition uniformity
// ---------------------------------------------------------------------------

/// Result of a chi-squared test for E8 transition uniformity.
#[derive(Debug, Clone)]
pub struct ChiSquaredResult {
    /// Chi-squared statistic.
    pub chi_sq: f64,
    /// Degrees of freedom.
    pub df: usize,
    /// Observed transition counts (E8 only, 8x8).
    pub observed: [[u64; N_E8]; N_E8],
    /// Total E8 transitions (off-diagonal).
    pub total: u64,
    /// Expected count per off-diagonal cell under uniformity.
    pub expected_per_cell: f64,
}

/// Chi-squared goodness-of-fit test for E8 transition uniformity.
///
/// Null hypothesis: all off-diagonal E8 transitions are equally likely.
/// The test statistic is sum((O_ij - E)^2 / E) for i != j, where E = total/56.
pub fn chi_squared_e8_transitions(sequence: &[usize]) -> ChiSquaredResult {
    let full_trans = transition_matrix(sequence);

    // Extract E8 block
    let mut observed = [[0u64; N_E8]; N_E8];
    let mut total = 0u64;
    for i in 0..N_E8 {
        for j in 0..N_E8 {
            observed[i][j] = full_trans[i][j];
            if i != j {
                total += full_trans[i][j];
            }
        }
    }

    let n_cells = N_E8 * (N_E8 - 1); // 56 off-diagonal cells
    let expected = total as f64 / n_cells as f64;
    let df = n_cells - 1; // 55

    let mut chi_sq = 0.0;
    if expected > 0.0 {
        for (i, row) in observed.iter().enumerate() {
            for (j, &count) in row.iter().enumerate() {
                if i != j {
                    let o = count as f64;
                    chi_sq += (o - expected).powi(2) / expected;
                }
            }
        }
    }

    ChiSquaredResult {
        chi_sq,
        df,
        observed,
        total,
        expected_per_cell: expected,
    }
}

// ---------------------------------------------------------------------------
// Integrated Fano structure analysis (task #19 integration)
// ---------------------------------------------------------------------------

/// Comprehensive analysis of Fano plane structure in billiard transitions.
///
/// Integrates all four falsification claims into a single analysis run:
/// - Locality metrics (r_e8, mutual information, commutation rate)
/// - Sector-specific metrics
/// - Chi-squared test for transition uniformity
/// - Permutation test for r_e8 significance
#[derive(Debug, Clone)]
pub struct FanoStructureAnalysis {
    /// Bounce sequence length.
    pub sequence_length: usize,
    /// Locality metrics computed from the sequence.
    pub locality: LocalityMetrics,
    /// Sector-specific metrics.
    pub sectors: SectorMetrics,
    /// Chi-squared test for E8 transition uniformity.
    pub chi_squared: ChiSquaredResult,
    /// Permutation test for r_e8 against uniform null.
    pub perm_test_uniform: PermutationTestResult,
    /// Permutation test for r_e8 against IID-empirical null.
    pub perm_test_empirical: PermutationTestResult,
    /// Stationary distribution of the transition chain.
    pub stationary: [f64; N_WALLS],
    /// E8 wall frequency (fraction of bounces hitting each E8 wall).
    pub e8_wall_freq: [f64; N_E8],
}

/// Run a complete Fano structure analysis on a billiard simulation.
///
/// Uses the hyperbolic billiard (billiard_sim) with the given seed and bounce count.
/// Performs all locality metrics, sector analysis, chi-squared, and permutation tests.
pub fn fano_structure_analysis(
    seed: u64,
    n_bounces: usize,
    n_permutations: usize,
) -> FanoStructureAnalysis {
    use crate::billiard_sim::HyperbolicBilliard;
    use crate::kac_moody::E10RootSystem;

    let e10 = E10RootSystem::new();
    let mut billiard = HyperbolicBilliard::from_e10(&e10, seed);
    let sequence = billiard.simulate(n_bounces);

    fano_structure_analysis_from_sequence(&sequence, n_permutations, seed)
}

/// Run Fano structure analysis on a pre-computed sequence.
pub fn fano_structure_analysis_from_sequence(
    sequence: &[usize],
    n_permutations: usize,
    seed: u64,
) -> FanoStructureAnalysis {
    let locality = compute_locality_metrics(sequence);
    let sectors = compute_sector_metrics(sequence);
    let chi_squared = chi_squared_e8_transitions(sequence);
    let perm_test_uniform = permutation_test_r_e8(
        sequence, &NullModel::Uniform, n_permutations, seed);
    let perm_test_empirical = permutation_test_r_e8(
        sequence, &NullModel::IidEmpirical, n_permutations, seed + 1);
    let trans = transition_matrix(sequence);
    let stationary = stationary_distribution(&trans);

    // E8 wall frequencies
    let mut wall_counts = [0u64; N_E8];
    for &w in sequence {
        if w < N_E8 {
            wall_counts[w] += 1;
        }
    }
    let total_e8 = wall_counts.iter().sum::<u64>() as f64;
    let mut e8_wall_freq = [0.0f64; N_E8];
    if total_e8 > 0.0 {
        for (i, &c) in wall_counts.iter().enumerate() {
            e8_wall_freq[i] = c as f64 / total_e8;
        }
    }

    FanoStructureAnalysis {
        sequence_length: sequence.len(),
        locality,
        sectors,
        chi_squared,
        perm_test_uniform,
        perm_test_empirical,
        stationary,
        e8_wall_freq,
    }
}

/// Format a Fano structure analysis as a human-readable report.
pub fn fano_analysis_report(a: &FanoStructureAnalysis) -> String {
    format!(
        "=== Fano Structure Analysis ({n} bounces) ===\n\
         \n\
         Locality:\n\
         r_e8={re8:.4}  r_e10={re10:.4}  MI={mi:.4} nats  commute={comm:.1}%\n\
         E8 trans={ne8}  mixed={nmix}  hyp={nhyp}\n\
         \n\
         Sectors:\n\
         r_e8={sre8:.4}  r_mixed={srm:.4}  r_hyp={srh:.4}\n\
         E8={ef:.1}%  mixed={mf:.1}%  hyp={hf:.1}%\n\
         \n\
         Chi-squared (H0: uniform E8 transitions):\n\
         chi2={chi:.2}  df={df}  total_E8_trans={tot}\n\
         expected/cell={exp:.2}\n\
         \n\
         Permutation tests (n={np}):\n\
         vs Uniform:  p={pu:.4}  effect={eu:.2}\n\
         vs Empirical: p={pe:.4}  effect={ee:.2}\n\
         \n\
         E8 wall frequencies:\n\
         {freq}",
        n = a.sequence_length,
        re8 = a.locality.r_e8,
        re10 = a.locality.r_e10,
        mi = a.locality.mutual_information,
        comm = a.locality.commutation_rate * 100.0,
        ne8 = a.locality.n_e8_transitions,
        nmix = a.locality.n_mixed_transitions,
        nhyp = a.locality.n_hyp_transitions,
        sre8 = a.sectors.r_e8,
        srm = a.sectors.r_mixed,
        srh = a.sectors.r_hyp,
        ef = a.sectors.e8_fraction * 100.0,
        mf = a.sectors.mixed_fraction * 100.0,
        hf = a.sectors.hyp_fraction * 100.0,
        chi = a.chi_squared.chi_sq,
        df = a.chi_squared.df,
        tot = a.chi_squared.total,
        exp = a.chi_squared.expected_per_cell,
        np = a.perm_test_uniform.n_permutations,
        pu = a.perm_test_uniform.p_value,
        eu = a.perm_test_uniform.effect_size,
        pe = a.perm_test_empirical.p_value,
        ee = a.perm_test_empirical.effect_size,
        freq = (0..N_E8).map(|i| format!("  w{i}={:.3}", a.e8_wall_freq[i]))
            .collect::<Vec<_>>().join("\n"),
    )
}

// ---------------------------------------------------------------------------
// Tests
// ---------------------------------------------------------------------------

#[cfg(test)]
mod tests {
    use super::*;
    use rand_chacha::ChaCha8Rng;

    #[test]
    fn test_e10_adjacency_symmetric() {
        for i in 0..N_WALLS {
            for j in 0..N_WALLS {
                assert_eq!(
                    E10_ADJACENCY[i][j], E10_ADJACENCY[j][i],
                    "Asymmetric at ({}, {})", i, j
                );
            }
            // No self-loops
            assert!(!E10_ADJACENCY[i][i], "Self-loop at {}", i);
        }
    }

    #[test]
    fn test_e10_adjacency_edge_count() {
        let edges: usize = (0..N_WALLS)
            .flat_map(|i| ((i + 1)..N_WALLS).map(move |j| (i, j)))
            .filter(|&(i, j)| E10_ADJACENCY[i][j])
            .count();
        // E10 Dynkin diagram: 9 edges (tree on 10 nodes)
        assert_eq!(edges, 9, "E10 should have 9 edges");
    }

    #[test]
    fn test_e8_subdiagram() {
        // E8 subdiagram (nodes 0-7) should have 7 edges
        let e8_edges: usize = (0..N_E8)
            .flat_map(|i| ((i + 1)..N_E8).map(move |j| (i, j)))
            .filter(|&(i, j)| E10_ADJACENCY[i][j])
            .count();
        assert_eq!(e8_edges, E8_EDGES);
    }

    #[test]
    fn test_null_r_e8_uniform() {
        // 14 directed connected pairs / 56 total ordered pairs = 0.25
        assert!((NULL_R_E8_UNIFORM - 0.25).abs() < 1e-10);
    }

    #[test]
    fn test_locality_metrics_empty() {
        let m = compute_locality_metrics(&[]);
        assert_eq!(m.n_transitions, 0);
        assert_eq!(m.r_e8, 0.0);
    }

    #[test]
    fn test_locality_metrics_single() {
        let m = compute_locality_metrics(&[3]);
        assert_eq!(m.n_transitions, 0);
    }

    #[test]
    fn test_locality_metrics_e8_connected() {
        // Sequence along the E8 spine: 0-1-2-3-4-5
        let seq = vec![0, 1, 2, 3, 4, 5];
        let m = compute_locality_metrics(&seq);
        // All 5 transitions are E8-internal and connected
        assert_eq!(m.n_e8_transitions, 5);
        assert!((m.r_e8 - 1.0).abs() < 1e-10, "All connected => r_e8 = 1.0");
        assert!(m.commutation_rate < 1e-10, "All adjacent => commutation = 0");
    }

    #[test]
    fn test_locality_metrics_e8_disconnected() {
        // Jump between non-adjacent E8 nodes: 0-5-2-7-1-6
        let seq = vec![0, 5, 2, 7, 1, 6];
        let m = compute_locality_metrics(&seq);
        assert_eq!(m.n_e8_transitions, 5);
        // None of these are adjacent in E8
        assert!(m.r_e8 < 1e-10, "No adjacent pairs => r_e8 = 0.0");
        assert!((m.commutation_rate - 1.0).abs() < 1e-10, "All non-adjacent => commutation = 1.0");
    }

    #[test]
    fn test_locality_metrics_mixed() {
        // E8 to affine/hyperbolic transitions
        let seq = vec![3, 8, 9, 8, 0, 1];
        let m = compute_locality_metrics(&seq);
        // 3->8: mixed (3 is E8, 8 is not)
        // 8->9: hyp
        // 9->8: hyp
        // 8->0: mixed (adjacent! 0-8 edge)
        // 0->1: E8 connected
        assert_eq!(m.n_mixed_transitions, 2); // 3->8, 8->0
        assert_eq!(m.n_hyp_transitions, 2);    // 8->9, 9->8
        assert_eq!(m.n_e8_transitions, 1);     // 0->1
    }

    #[test]
    fn test_mutual_information_iid() {
        // For an iid sequence, MI should be near zero
        let mut rng = ChaCha8Rng::seed_from_u64(42);
        let seq: Vec<usize> = (0..10000).map(|_| rng.gen_range(0..N_WALLS)).collect();
        let m = compute_locality_metrics(&seq);
        assert!(m.mutual_information < 0.05,
            "MI for iid should be near 0, got {}", m.mutual_information);
    }

    #[test]
    fn test_mutual_information_deterministic() {
        // Cycling 0-1-0-1-0-1 has high MI
        let seq: Vec<usize> = (0..1000).map(|i| i % 2).collect();
        let m = compute_locality_metrics(&seq);
        assert!(m.mutual_information > 0.5,
            "MI for deterministic cycle should be high, got {}", m.mutual_information);
    }

    #[test]
    fn test_null_model_uniform_length() {
        let mut rng = ChaCha8Rng::seed_from_u64(42);
        let empirical = vec![0, 1, 2, 3, 4];
        let null = generate_null_sequence(&NullModel::Uniform, 100, &empirical, &mut rng);
        assert_eq!(null.len(), 100);
        assert!(null.iter().all(|&w| w < N_WALLS));
    }

    #[test]
    fn test_null_model_iid_empirical_preserves_support() {
        // If empirical only uses walls 0-3, null should only use 0-3
        let mut rng = ChaCha8Rng::seed_from_u64(42);
        let empirical: Vec<usize> = (0..1000).map(|i| i % 4).collect();
        let null = generate_null_sequence(&NullModel::IidEmpirical, 5000, &empirical, &mut rng);
        assert_eq!(null.len(), 5000);
        assert!(null.iter().all(|&w| w < 4),
            "IidEmpirical should respect empirical support");
    }

    #[test]
    fn test_null_model_degree_preserving_frequencies() {
        let mut rng = ChaCha8Rng::seed_from_u64(42);
        // Heavy on wall 0, light on wall 9
        let mut empirical = vec![0; 500];
        empirical.extend(vec![1; 300]);
        empirical.extend(vec![2; 200]);
        let null = generate_null_sequence(
            &NullModel::DegreePreserving, empirical.len(), &empirical, &mut rng,
        );
        assert_eq!(null.len(), empirical.len());
        // Count wall 0 in null (should be exactly 500 since it's a shuffle)
        let count_0 = null.iter().filter(|&&w| w == 0).count();
        assert_eq!(count_0, 500, "Shuffle must preserve exact counts");
    }

    #[test]
    fn test_null_model_markov_transitions() {
        let mut rng = ChaCha8Rng::seed_from_u64(42);
        // Simple sequence: only 0->1->0->1...
        let empirical: Vec<usize> = (0..1000).map(|i| i % 2).collect();
        let null = generate_null_sequence(&NullModel::Markov, 1000, &empirical, &mut rng);
        assert_eq!(null.len(), 1000);
        // Most transitions should be 0->1 or 1->0 (the Markov chain learned this)
        let alternating: usize = null.windows(2)
            .filter(|w| (w[0] == 0 && w[1] == 1) || (w[0] == 1 && w[1] == 0))
            .count();
        let total = null.len() - 1;
        assert!(alternating as f64 / total as f64 > 0.8,
            "Markov null should reproduce alternating pattern, got {}/{}",
            alternating, total);
    }

    #[test]
    fn test_permutation_test_uniform_null() {
        // E8 spine sequence (highly connected): should be significant vs uniform
        let seq: Vec<usize> = (0..500).map(|i| i % 6).collect(); // 0-1-2-3-4-5 cycle
        let result = permutation_test_r_e8(&seq, &NullModel::Uniform, 200, 42);
        assert!(result.p_value < 0.05,
            "Connected sequence should be significant vs uniform, p={}", result.p_value);
        assert!(result.observed > result.null_mean,
            "Observed r should exceed null mean");
    }

    #[test]
    fn test_permutation_test_iid_vs_iid() {
        // Random sequence tested against its own IidEmpirical null: should NOT be significant
        let mut rng = ChaCha8Rng::seed_from_u64(123);
        let seq: Vec<usize> = (0..500).map(|_| rng.gen_range(0..N_WALLS)).collect();
        let result = permutation_test_r_e8(&seq, &NullModel::IidEmpirical, 200, 456);
        // p-value should be moderate (not extremely low)
        assert!(result.p_value > 0.01,
            "Random vs IidEmpirical should not be highly significant, p={}", result.p_value);
    }

    #[test]
    fn test_permutation_test_phipson_smyth() {
        // p-value is never exactly 0 (Phipson-Smyth correction)
        // Use a sequence that is maximally connected
        let seq: Vec<usize> = (0..1000).map(|i| {
            // Traverse E8 spine back and forth
            let phase = i % 10;
            if phase < 6 { phase } else { 10 - phase }
        }).collect();
        let result = permutation_test_r_e8(&seq, &NullModel::Uniform, 100, 42);
        assert!(result.p_value > 0.0,
            "p-value must never be exactly 0 (Phipson-Smyth)");
        // Should be very small though
        assert!(result.p_value < 0.02,
            "Highly connected sequence should have p < 0.02, got {}", result.p_value);
    }

    #[test]
    fn test_transition_matrix_basic() {
        let seq = vec![0, 1, 2, 1, 0];
        let mat = transition_matrix(&seq);
        assert_eq!(mat[0][1], 1);
        assert_eq!(mat[1][2], 1);
        assert_eq!(mat[2][1], 1);
        assert_eq!(mat[1][0], 1);
        // Total transitions = 4
        let total: u64 = mat.iter().flat_map(|row| row.iter()).sum();
        assert_eq!(total, 4);
    }

    #[test]
    fn test_stationary_distribution_uniform() {
        // Transition matrix where each row is uniform -> stationary = uniform
        let mut trans = [[0u64; N_WALLS]; N_WALLS];
        for row in &mut trans {
            for count in row.iter_mut() {
                *count = 10;
            }
        }
        let pi = stationary_distribution(&trans);
        for &p in &pi {
            assert!((p - 0.1).abs() < 1e-6, "Expected uniform stationary, got {}", p);
        }
    }

    #[test]
    fn test_stationary_distribution_absorbing() {
        // Chain that always goes to state 0
        let mut trans = [[0u64; N_WALLS]; N_WALLS];
        for row in &mut trans {
            row[0] = 1;
        }
        let pi = stationary_distribution(&trans);
        assert!(pi[0] > 0.99, "Absorbing state 0 should have pi[0] ~ 1.0, got {}", pi[0]);
    }

    #[test]
    fn test_effect_size_sign() {
        // Positive effect size when observed > null mean
        let result = summarize_permutation_test(0.5, &vec![0.2, 0.3, 0.25, 0.22, 0.28]);
        assert!(result.effect_size > 0.0, "Observed > mean => positive effect size");

        let result2 = summarize_permutation_test(0.1, &vec![0.5, 0.6, 0.55, 0.52, 0.58]);
        assert!(result2.effect_size < 0.0, "Observed < mean => negative effect size");
    }

    #[test]
    fn test_commutation_shuffle_preserves_elements() {
        // CommutationShuffle should preserve the multiset of elements
        let mut rng = ChaCha8Rng::seed_from_u64(42);
        let empirical = vec![0, 1, 2, 3, 4, 5, 6, 7, 0, 1, 2, 3];
        let null = generate_null_sequence(
            &NullModel::CommutationShuffle(100), empirical.len(), &empirical, &mut rng,
        );
        assert_eq!(null.len(), empirical.len());
        // Same elements (sorted)
        let mut sorted_emp = empirical.clone();
        let mut sorted_null = null.clone();
        sorted_emp.sort_unstable();
        sorted_null.sort_unstable();
        assert_eq!(sorted_emp, sorted_null,
            "CommutationShuffle must preserve element multiset");
    }

    #[test]
    fn test_commutation_shuffle_only_swaps_commuting() {
        // A fully connected sequence (0-1-2-3-4-5) has all adjacent pairs.
        // CommutationShuffle should NOT swap any of them.
        let mut rng = ChaCha8Rng::seed_from_u64(42);
        let empirical = vec![0, 1, 2, 3, 4, 5]; // all consecutive are adjacent in E8
        let null = generate_null_sequence(
            &NullModel::CommutationShuffle(100), empirical.len(), &empirical, &mut rng,
        );
        // Should be identical since no commuting pairs exist
        assert_eq!(null, empirical,
            "All-adjacent sequence should be invariant under CommutationShuffle");
    }

    #[test]
    fn test_commutation_shuffle_does_swap_commuting() {
        // Sequence 0-5-0-5: 0 and 5 are non-adjacent (commute in E10).
        // With enough passes, some swaps should occur.
        let mut rng = ChaCha8Rng::seed_from_u64(42);
        let empirical = vec![0, 5, 0, 5, 0, 5, 0, 5, 0, 5];
        let null = generate_null_sequence(
            &NullModel::CommutationShuffle(100), empirical.len(), &empirical, &mut rng,
        );
        // At least some positions should differ (with high probability)
        let diffs: usize = empirical.iter().zip(null.iter()).filter(|(&a, &b)| a != b).count();
        assert!(diffs > 0, "CommutationShuffle should swap some commuting pairs");
    }

    #[test]
    fn test_commutation_shuffle_preserves_locality_for_connected() {
        // Walk back-and-forth along E8 spine: 0-1-2-1-0-1-2-1...
        // Every consecutive pair (0-1, 1-2, 2-1, 1-0) is adjacent in E10,
        // so CommutationShuffle has NO non-adjacent pairs to swap.
        let pattern = [0, 1, 2, 1];
        let seq: Vec<usize> = (0..100).map(|i| pattern[i % 4]).collect();
        let result = permutation_test_r_e8(
            &seq, &NullModel::CommutationShuffle(50), 100, 42,
        );
        // r_E8 should be identical: shuffle cannot swap any pairs
        assert!((result.observed - result.null_mean).abs() < 1e-10,
            "CommutationShuffle on all-adjacent should not change r_E8, \
             obs={}, null={}", result.observed, result.null_mean);
    }

    #[test]
    fn test_sector_metrics_basic() {
        // E8-internal sequence: all transitions are E8
        let seq = vec![0, 1, 2, 3, 4, 5, 6, 7];
        let sm = compute_sector_metrics(&seq);
        assert!((sm.e8_fraction - 1.0).abs() < 1e-10);
        assert!(sm.mixed_fraction < 1e-10);
        assert!(sm.hyp_fraction < 1e-10);
    }

    #[test]
    fn test_sector_metrics_mixed() {
        // Transition between E8 and affine/hyperbolic
        let seq = vec![0, 8, 9];
        let sm = compute_sector_metrics(&seq);
        assert_eq!(sm.mixed_fraction, 0.5); // 0->8 is mixed
        assert_eq!(sm.hyp_fraction, 0.5);   // 8->9 is hyp
        // r_mixed: 0->8 is adjacent (0-8 edge), so r_mixed = 1.0
        assert!((sm.r_mixed - 1.0).abs() < 1e-10);
        // r_hyp: 8->9 is adjacent, so r_hyp = 1.0
        assert!((sm.r_hyp - 1.0).abs() < 1e-10);
    }

    #[test]
    fn test_sector_metrics_all_sectors() {
        // Mix of all sectors
        let seq = vec![3, 4, 8, 9, 8, 0, 2, 7];
        let sm = compute_sector_metrics(&seq);
        // 3->4: E8 adjacent
        // 4->8: mixed (not adjacent -- 4 and 8 not connected)
        // 8->9: hyp adjacent
        // 9->8: hyp adjacent
        // 8->0: mixed adjacent (0-8 edge)
        // 0->2: E8 (not adjacent -- 0 connects to 1,8 only)
        // 2->7: E8 (not adjacent)
        assert!(sm.r_e8 > 0.0, "Some E8 adjacency should exist (3->4)");
        assert!(sm.r_hyp > 0.0, "Hyp adjacency should exist (8->9, 9->8)");
    }

    // -----------------------------------------------------------------------
    // Claim 1 verification tests
    // -----------------------------------------------------------------------

    #[test]
    fn test_verify_claim1_basic() {
        // Small-scale test: 10 seeds, 500 bounces each
        let result = verify_claim1(10, 500);

        assert_eq!(result.r_values.len(), 10);
        assert_eq!(result.r_values_no_renorm.len(), 10);
        assert_eq!(result.r_values_large_eps.len(), 10);

        // All r_e8 values should be in [0, 1]
        for &r in &result.r_values {
            assert!(r >= 0.0 && r <= 1.0, "r_e8 out of range: {}", r);
        }

        // Statistics should be consistent
        assert!(result.mean_r >= result.min_r);
        assert!(result.mean_r <= result.max_r);
        assert!(result.std_r >= 0.0);
    }

    #[test]
    fn test_verify_claim1_above_null() {
        // 20 seeds, 1000 bounces: mean r_e8 should exceed null baseline
        let result = verify_claim1(20, 1000);

        // The mean should be well above the uniform null (0.25)
        // We use a generous margin since this is a statistical claim
        assert!(result.mean_r > result.null_baseline,
            "mean r_e8 ({:.4}) should exceed null baseline ({:.4})",
            result.mean_r, result.null_baseline);
    }

    #[test]
    fn test_verify_claim1_most_seeds_above_null() {
        // At least 60% of seeds should have r_e8 > null
        let result = verify_claim1(20, 1000);

        assert!(result.fraction_above_null >= 0.6,
            "Only {:.1}% of seeds above null -- expected >= 60%",
            result.fraction_above_null * 100.0);
    }

    #[test]
    fn test_claim1_summary_format() {
        let result = verify_claim1(5, 200);
        let (summary, _supported) = claim1_summary(&result);

        assert!(summary.contains("Claim 1"));
        assert!(summary.contains("r_e8 robustness"));
        assert!(summary.contains("mean="));
        assert!(summary.contains("std="));
    }

    #[test]
    fn test_verify_claim1_deterministic() {
        // Same parameters should yield same results (deterministic seeds)
        let result1 = verify_claim1(5, 300);
        let result2 = verify_claim1(5, 300);

        for (a, b) in result1.r_values.iter().zip(result2.r_values.iter()) {
            assert!((a - b).abs() < 1e-15,
                "Non-deterministic: {} vs {}", a, b);
        }
    }

    #[test]
    fn test_verify_claim1_renorm_robustness() {
        // Config changes should not drastically alter r_e8
        let result = verify_claim1(10, 500);

        // Renormalization delta should be moderate (< 0.5 at worst)
        assert!(result.max_renorm_delta < 0.5,
            "Renormalization delta too large: {:.4}", result.max_renorm_delta);

        // Epsilon delta should be moderate
        assert!(result.max_eps_delta < 0.5,
            "Epsilon delta too large: {:.4}", result.max_eps_delta);
    }

    // -----------------------------------------------------------------------
    // Chi-squared tests
    // -----------------------------------------------------------------------

    #[test]
    fn test_chi_squared_uniform_sequence() {
        // Uniform random sequence should have low chi-squared
        let mut rng = ChaCha8Rng::seed_from_u64(42);
        let seq: Vec<usize> = (0..10000).map(|_| rng.gen_range(0..N_E8)).collect();
        let result = chi_squared_e8_transitions(&seq);

        assert_eq!(result.df, 55);
        assert!(result.total > 0);
        assert!(result.expected_per_cell > 0.0);
        // For truly uniform, chi-squared should be moderate (not extremely large)
        // With 10K samples, we expect chi2 ~ df = 55 under null
        assert!(result.chi_sq < 200.0,
            "chi2={:.1} too large for near-uniform sequence", result.chi_sq);
    }

    #[test]
    fn test_chi_squared_structured_sequence() {
        // Highly structured: always alternate between 0 and 1
        let seq: Vec<usize> = (0..1000).map(|i| i % 2).collect();
        let result = chi_squared_e8_transitions(&seq);

        // Only 0->1 and 1->0 transitions exist; rest are 0
        // This should give very high chi-squared
        assert!(result.chi_sq > 100.0,
            "chi2={:.1} should be large for deterministic alternation", result.chi_sq);
    }

    #[test]
    fn test_chi_squared_empty_sequence() {
        let result = chi_squared_e8_transitions(&[]);
        assert_eq!(result.total, 0);
        assert!((result.chi_sq).abs() < 1e-10);
    }

    // -----------------------------------------------------------------------
    // Fano structure analysis tests
    // -----------------------------------------------------------------------

    #[test]
    fn test_fano_structure_analysis_basic() {
        let analysis = fano_structure_analysis(42, 500, 100);

        assert_eq!(analysis.sequence_length, 500);
        assert!(analysis.locality.r_e8 >= 0.0);
        assert!(analysis.locality.r_e8 <= 1.0);
        assert!(analysis.chi_squared.df == 55);
        assert_eq!(analysis.perm_test_uniform.n_permutations, 100);
        assert_eq!(analysis.perm_test_empirical.n_permutations, 100);

        // Wall frequencies should sum to ~1.0
        let freq_sum: f64 = analysis.e8_wall_freq.iter().sum();
        if freq_sum > 0.0 {
            assert!((freq_sum - 1.0).abs() < 1e-10,
                "E8 wall frequencies should sum to 1.0, got {:.6}", freq_sum);
        }
    }

    #[test]
    fn test_fano_structure_analysis_from_sequence() {
        // Test with a known sequence
        let seq = vec![0, 1, 2, 3, 4, 5, 0, 1, 4, 6, 7, 0, 3, 2, 1, 0];
        let analysis = fano_structure_analysis_from_sequence(&seq, 50, 42);

        assert_eq!(analysis.sequence_length, 16);
        assert!(analysis.locality.n_transitions > 0);
    }

    #[test]
    fn test_fano_analysis_report_format() {
        let analysis = fano_structure_analysis(42, 200, 50);
        let report = fano_analysis_report(&analysis);

        assert!(report.contains("Fano Structure Analysis"));
        assert!(report.contains("Locality:"));
        assert!(report.contains("Sectors:"));
        assert!(report.contains("Chi-squared"));
        assert!(report.contains("Permutation tests"));
        assert!(report.contains("E8 wall frequencies"));
    }

    #[test]
    fn test_fano_structure_deterministic() {
        let a1 = fano_structure_analysis(42, 300, 50);
        let a2 = fano_structure_analysis(42, 300, 50);

        assert!((a1.locality.r_e8 - a2.locality.r_e8).abs() < 1e-15);
        assert!((a1.chi_squared.chi_sq - a2.chi_squared.chi_sq).abs() < 1e-10);
    }
}
