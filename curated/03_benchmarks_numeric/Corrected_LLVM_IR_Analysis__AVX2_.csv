LLVM IR Output
"{(Array(float64, 1, 'A', False, aligned=True), Array(float64, 1, 'A', False, aligned=True)): '; ModuleID = \'avx2_vectorized\'\nsource_filename = ""<string>""\ntarget datalayout = ""e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128""\ntarget triple = ""x86_64-unknown-linux-gnu""\n\n@.const.pickledata.139580212392336 = internal constant [115 x i8] c""\\80\\04\\95h\\00\\00\\00\\00\\00\\00\\00\\8C\\08builtins\\94\\8C\\0EAssertionError\\94\\93\\94\\8CASizes of a, b do not match on /tmp/ipykernel_12/2224780964.py (4)\\94\\85\\94N\\87\\94.""\n@.const.pickledata.139580212392336.sha1 = internal constant [20 x i8] c""\\B1\\CEh\\12~\\1DM\\89\\91\\A7\\CD#\\DD]yV0\\15\\1B4""\n@.const.picklebuf.139580212392336 = internal constant { i8*, i32, i8*, i8*, i32 } { i8* getelementptr inbounds ([115 x i8], [115 x i8]* @.const.pickledata.139580212392336, i32 0, i32 0), i32 115, i8* getelementptr inbounds ([20 x i8], [20 x i8]* @.const.pickledata.139580212392336.sha1, i32 0, i32 0), i8* null, i32 0 }\n@printf_format = internal constant [17 x i8] c""num_threads: %d\\0A\\00""\n@.const.pickledata.139579935905616 = internal constant [112 x i8] c""\\80\\04\\95e\\00\\00\\00\\00\\00\\00\\00\\8C\\08builtins\\94\\8C\\0CRuntimeError\\94\\93\\94\\8C@Invalid number of threads. This likely indicates a bug in Numba.\\94\\85\\94N\\87\\94.""\n@.const.pickledata.139579935905616.sha1 = internal constant [20 x i8] c""\\9D\\8B\\D6\\D5A\\B3\\E36\\FDy\\17\\99I@x\\1B\\C6\\8A:\\8A""\n@.const.picklebuf.139579935905616 = internal constant { i8*, i32, i8*, i8*, i32 } { i8* getelementptr inbounds ([112 x i8], [112 x i8]* @.const.pickledata.139579935905616, i32 0, i32 0), i32 112, i8* getelementptr inbounds ([20 x i8], [20 x i8]* @.const.pickledata.139579935905616.sha1, i32 0, i32 0), i8* null, i32 0 }\n@.const.avx2_vectorized = internal constant [16 x i8] c""avx2_vectorized\\00""\n@_ZN08NumbaEnv8__main__15avx2_vectorizedB3v14B110c8tJTC_2fWQAliW1xhDEoY6EEMEUOEMISPGsAQMVj4QniQ4IXKQEMXwoMGLoQDDVsQR1NHAS2hQ9XgStYw86ABbYse0tXqICn1WqDBwFiD2AEAE5ArrayIdLi1E1A7mutable7alignedE5ArrayIdLi1E1A7mutable7alignedE = common local_unnamed_addr global i8* null\n@"".const.missing Environment: _ZN08NumbaEnv8__main__15avx2_vectorizedB3v14B110c8tJTC_2fWQAliW1xhDEoY6EEMEUOEMISPGsAQMVj4QniQ4IXKQEMXwoMGLoQDDVsQR1NHAS2hQ9XgStYw86ABbYse0tXqICn1WqDBwFiD2AEAE5ArrayIdLi1E1A7mutable7alignedE5ArrayIdLi1E1A7mutable7alignedE"" = internal constant [243 x i8] c""missing Environment: _ZN08NumbaEnv8__main__15avx2_vectorizedB3v14B110c8tJTC_2fWQAliW1xhDEoY6EEMEUOEMISPGsAQMVj4QniQ4IXKQEMXwoMGLoQDDVsQR1NHAS2hQ9XgStYw86ABbYse0tXqICn1WqDBwFiD2AEAE5ArrayIdLi1E1A7mutable7alignedE5ArrayIdLi1E1A7mutable7alignedE\\00""\n@"".const.can\'t unbox array from PyObject into native value.  The object maybe of a different type"" = internal constant [89 x i8] c""can\'t unbox array from PyObject into native value.  The object maybe of a different type\\00""\n@"".const.`env.consts` is NULL in `read_const`"" = internal constant [37 x i8] c""`env.consts` is NULL in `read_const`\\00""\n@.const.pickledata.139583502599616 = internal constant [32 x i8] c""\\80\\04\\95\\15\\00\\00\\00\\00\\00\\00\\00\\8C\\05numpy\\94\\8C\\07ndarray\\94\\93\\94.""\n@.const.pickledata.139583502599616.sha1 = internal constant [20 x i8] c""\\DF\\BC\\FD\\D3\\9F\\CB&\\F4\\D0\\C6\\80\\95D\\87\\B8\\C0\\B5;\\B8\\A3""\n@"".const.Error creating Python tuple from runtime exception arguments"" = internal constant [61 x i8] c""Error creating Python tuple from runtime exception arguments\\00""\n@"".const.unknown error when calling native function"" = internal constant [43 x i8] c""unknown error when calling native function\\00""\n@"".const.Error creating Python tuple from runtime exception arguments.1"" = internal constant [61 x i8] c""Error creating Python tuple from runtime exception arguments\\00""\n@"".const.unknown error when calling native function.2"" = internal constant [43 x i8] c""unknown error when calling native function\\00""\n@"".const.<numba.core.cpu.CPUContext object at 0x7ef27be99f60>"" = internal constant [53 x i8] c""<numba.core.cpu.CPUContext object at 0x7ef27be99f60>\\00""\n@_ZN08NumbaEnv5numba2np8arrayobj11ol_np_empty12_3clocals_3e4implB3v15B60c8tJTIeFIjxB2IKSgI4CrvQClYb5wBbdC9XqICn1Wk1gKGLEM1QzMJzALE0AEx18class_28float64_29 = common local_unnamed_addr global i8* null\n@.const.pickledata.139579935483472 = internal constant [77 x i8] c""\\80\\04\\95B\\00\\00\\00\\00\\00\\00\\00\\8C\\08builtins\\94\\8C\\0AValueError\\94\\93\\94\\8C\\1Fnegative dimensions not allowed\\94\\85\\94N\\87\\94.""\n@.const.pickledata.139579935483472.sha1 = internal constant [20 x i8] c""3\\1B\\85c\\BD\\B9\\DA\\C8\\1B8B\\22s\\05,Ho\\C1pk""\n@.const.picklebuf.139579935483472 = internal constant { i8*, i32, i8*, i8*, i32 } { i8* getelementptr inbounds ([77 x i8], [77 x i8]* @.const.pickledata.139579935483472, i32 0, i32 0), i32 77, i8* getelementptr inbounds ([20 x i8], [20 x i8]* @.const.pickledata.139579935483472.sha1, i32 0, i32 0), i8* null, i32 0 }\n@.const.pickledata.139580256691920 = internal constant [137 x i8] c""\\80\\04\\95~\\00\\00\\00\\00\\00\\00\\00\\8C\\08builtins\\94\\8C\\0AValueError\\94\\93\\94\\8C[array is too big; `arr.size * arr.dtype.itemsize` is larger than the maximum possible size.\\94\\85\\94N\\87\\94.""\n@.const.pickledata.139580256691920.sha1 = internal constant [20 x i8] c""X\\E1N\\CC\\B5\\07\\B1\\E0 i\\81t\\02#\\E6\\85\\CB\\8C<W""\n@.const.picklebuf.139580256691920 = internal constant { i8*, i32, i8*, i8*, i32 } { i8* getelementptr inbounds ([137 x i8], [137 x i8]* @.const.pickledata.139580256691920, i32 0, i32 0), i32 137, i8* getelementptr inbounds ([20 x i8], [20 x i8]* @.const.pickledata.139580256691920.sha1, i32 0, i32 0), i8* null, i32 0 }\n@PyExc_SystemError = external global i8\n@_ZN08NumbaEnv5numba2np8arrayobj15_call_allocatorB2v3B42c8tJTC_2fWQA93W1AaAIYBPIqRBFCjDSZRVAJmaQIAEN29typeref_5b_3cclass_20_27numba4core5types8npytypes14Array_27_3e_5dExj = common local_unnamed_addr global i8* null\n@_ZN08NumbaEnv5numba2np8arrayobj18_ol_array_allocate12_3clocals_3e4implB2v4B42c8tJTIeFIjxB2IKSgI4CrvQClcaMQ5hEEUSJJgA_3dEN29typeref_5b_3cclass_20_27numba4core5types8npytypes14Array_27_3e_5dExj = common local_unnamed_addr global i8* null\n@.const.pickledata.139580232182160 = internal constant [86 x i8] c""\\80\\04\\95K\\00\\00\\00\\00\\00\\00\\00\\8C\\08builtins\\94\\8C\\0BMemoryError\\94\\93\\94\\8C\'Allocation failed (probably too large).\\94\\85\\94N\\87\\94.""\n@.const.pickledata.139580232182160.sha1 = internal constant [20 x i8] c""\\BA(\\9D\\81\\F0\\\\p \\F3G|\\15sH\\04\\DFe\\AB\\E2\\09""\n@.const.picklebuf.139580232182160 = internal constant { i8*, i32, i8*, i8*, i32 } { i8* getelementptr inbounds ([86 x i8], [86 x i8]* @.const.pickledata.139580232182160, i32 0, i32 0), i32 86, i8* getelementptr inbounds ([20 x i8], [20 x i8]* @.const.pickledata.139580232182160.sha1, i32 0, i32 0), i8* null, i32 0 }\n@_ZN08NumbaEnv13_3cdynamic_3e36__numba_parfor_gufunc_0x7ef28ec1b9a0B3v16B128c8tJTC_2fWQAliW1xhDEoY6EEMEUOEMISPGsAQMVj4QniQ4IXKQEMXwoMGLoQDDVsQR1NHAS2hQ9XgStYwaUTI24JtBVoBDXtb9MCvVgdJqdcC7YLGAMy5RRDjNAE_3dE5ArrayIyLi1E1C7mutable7alignedE5ArrayIdLi1E1A7mutable7alignedE5ArrayIdLi1E1A7mutable7alignedE5ArrayIdLi1E1C7mutable7alignedE = common local_unnamed_addr global i8* null\n@PyExc_TypeError = external global i8\n@PyExc_RuntimeError = external global i8\n\ndefine i32 @_ZN8__main__15avx2_vectorizedB3v14B110c8tJTC_2fWQAliW1xhDEoY6EEMEUOEMISPGsAQMVj4QniQ4IXKQEMXwoMGLoQDDVsQR1NHAS2hQ9XgStYw86ABbYse0tXqICn1WqDBwFiD2AEAE5ArrayIdLi1E1A7mutable7alignedE5ArrayIdLi1E1A7mutable7alignedE({ i8*, i8*, i64, i64, double*, [1 x i64], [1 x i64] }* noalias nocapture writeonly %retptr, { i8*, i32, i8*, i8*, i32 }** noalias nocapture writeonly %excinfo, i8* %arg.a.0, i8* nocapture readnone %arg.a.1, i64 %arg.a.2, i64 %arg.a.3, double* %arg.a.4, i64 %arg.a.5.0, i64 %arg.a.6.0, i8* %arg.b.0, i8* nocapture readnone %arg.b.1, i64 %arg.b.2, i64 %arg.b.3, double* %arg.b.4, i64 %arg.b.5.0, i64 %arg.b.6.0) local_unnamed_addr {\nentry:\n  %.7.i = alloca i32, align 4\n  %dim_starts = alloca i64, align 8\n  %dim_stops = alloca i64, align 8\n  %pargs2 = alloca [4 x i8*], align 8\n  %pshape3 = alloca [3 x i64], align 8\n  %psteps4 = alloca [8 x i64], align 8\n  tail call void @NRT_incref(i8* %arg.a.0)\n  tail call void @NRT_incref(i8* %arg.b.0)\n  %.89 = icmp eq i64 %arg.a.5.0, %arg.b.5.0\n  br i1 %.89, label %B0.endif, label %B0.else\n\ncommon.ret:                                       ; preds = %B0.endif.if, %B0.endif.endif.endif, %B0.endif.endif.if, %B0.else\n  %common.ret.op = phi i32 [ 1, %B0.else ], [ 1, %B0.endif.endif.if ], [ 0, %B0.endif.endif.endif ], [ 1, %B0.endif.if ]\n  ret i32 %common.ret.op\n\nB0.else:                                          ; preds = %entry\n  store { i8*, i32, i8*, i8*, i32 }* @.const.picklebuf.139580212392336, { i8*, i32, i8*, i8*, i32 }** %excinfo, align 8, !numba_exception_output !0\n  br label %common.ret\n\nB0.endif:                                         ; preds = %entry\n  %.15.i = icmp slt i64 %arg.a.5.0, 0\n  br i1 %.15.i, label %B0.endif.if, label %B0.endif.endif.i, !prof !1\n\nB0.endif.endif.i:                                 ; preds = %B0.endif\n  %.29.i = tail call { i64, i1 } @llvm.smul.with.overflow.i64(i64 %arg.a.5.0, i64 8)\n  %.31.i = extractvalue { i64, i1 } %.29.i, 1\n  br i1 %.31.i, label %B0.endif.if, label %B0.endif.endif.endif.i, !prof !1\n\nB0.endif.endif.endif.i:                           ; preds = %B0.endif.endif.i\n  %.30.i = extractvalue { i64, i1 } %.29.i, 0\n  %.7.i.i.i = tail call i8* @NRT_MemInfo_alloc_aligned(i64 %.30.i, i32 32), !noalias !2\n  %.8.i.i.i = icmp eq i8* %.7.i.i.i, null\n  br i1 %.8.i.i.i, label %B0.endif.if, label %B0.endif.endif, !prof !1\n\nB0.endif.if:                                      ; preds = %B0.endif, %B0.endif.endif.i, %B0.endif.endif.endif.i\n  %excinfo.1.0.ph = phi { i8*, i32, i8*, i8*, i32 }* [ @.const.picklebuf.139580232182160, %B0.endif.endif.endif.i ], [ @.const.picklebuf.139580256691920, %B0.endif.endif.i ], [ @.const.picklebuf.139579935483472, %B0.endif ]\n  store { i8*, i32, i8*, i8*, i32 }* %excinfo.1.0.ph, { i8*, i32, i8*, i8*, i32 }** %excinfo, align 8\n  br label %common.ret\n\nB0.endif.endif:                                   ; preds = %B0.endif.endif.endif.i\n  %.5.i.i = getelementptr i8, i8* %.7.i.i.i, i64 24\n  %0 = bitcast i8* %.5.i.i to double**\n  %.6.i1.i = load double*, double** %0, align 8, !noalias !12\n  %.173 = add nsw i64 %arg.a.5.0, -1\n  store i64 0, i64* %dim_starts, align 8\n  store i64 %.173, i64* %dim_stops, align 8\n  %.178 = tail call i64 @get_num_threads()\n  %.179 = tail call i64 @get_parallel_chunksize()\n  %.180 = icmp slt i64 %.178, 1\n  br i1 %.180, label %B0.endif.endif.if, label %B0.endif.endif.endif, !prof !1\n\nB0.endif.endif.if:                                ; preds = %B0.endif.endif\n  %.183 = tail call i32 (i8*, ...) @printf(i8* noundef nonnull dereferenceable(1) getelementptr inbounds ([17 x i8], [17 x i8]* @printf_format, i64 0, i64 0), i64 %.178)\n  store { i8*, i32, i8*, i8*, i32 }* @.const.picklebuf.139579935905616, { i8*, i32, i8*, i8*, i32 }** %excinfo, align 8, !numba_exception_output !0\n  br label %common.ret\n\nB0.endif.endif.endif:                             ; preds = %B0.endif.endif\n  %1 = bitcast [8 x i64]* %psteps4 to i64*\n  %2 = bitcast [3 x i64]* %pshape3 to i64*\n  %.189 = call i64 @get_sched_size(i64 %.178, i64 1, i64* nonnull %dim_starts, i64* nonnull %dim_stops)\n  call void @set_parallel_chunksize(i64 0)\n  %.191 = shl i64 %.189, 1\n  %.192 = call i64* @allocate_sched(i64 %.191)\n  %.197 = call i64* @do_scheduling_unsigned(i64 1, i64* nonnull %dim_starts, i64* nonnull %dim_stops, i64 %.189, i64* %.192, i64 0)\n  %3 = bitcast [4 x i8*]* %pargs2 to i64**\n  store i64* %.192, i64** %3, align 8\n  %.205 = getelementptr inbounds [4 x i8*], [4 x i8*]* %pargs2, i64 0, i64 1\n  %4 = bitcast i8** %.205 to double**\n  store double* %arg.a.4, double** %4, align 8\n  %.217 = getelementptr inbounds [4 x i8*], [4 x i8*]* %pargs2, i64 0, i64 2\n  %5 = bitcast i8** %.217 to double**\n  store double* %arg.b.4, double** %5, align 8\n  %.229 = getelementptr inbounds [4 x i8*], [4 x i8*]* %pargs2, i64 0, i64 3\n  %6 = bitcast i8** %.229 to double**\n  store double* %.6.i1.i, double** %6, align 8\n  store i64 %.189, i64* %2, align 8\n  %.270 = getelementptr inbounds [3 x i64], [3 x i64]* %pshape3, i64 0, i64 1\n  store i64 2, i64* %.270, align 8\n  %.272 = getelementptr inbounds [3 x i64], [3 x i64]* %pshape3, i64 0, i64 2\n  store i64 %arg.a.5.0, i64* %.272, align 8\n  store i64 16, i64* %1, align 8\n  %.276 = getelementptr inbounds [8 x i64], [8 x i64]* %psteps4, i64 0, i64 1\n  %.282 = getelementptr inbounds [8 x i64], [8 x i64]* %psteps4, i64 0, i64 4\n  %7 = bitcast i64* %.276 to i8*\n  call void @llvm.memset.p0i8.i64(i8* noundef nonnull align 8 dereferenceable(24) %7, i8 0, i64 24, i1 false)\n  store i64 8, i64* %.282, align 8\n  %.284 = getelementptr inbounds [8 x i64], [8 x i64]* %psteps4, i64 0, i64 5\n  store i64 %arg.a.6.0, i64* %.284, align 8\n  %.286 = getelementptr inbounds [8 x i64], [8 x i64]* %psteps4, i64 0, i64 6\n  store i64 %arg.b.6.0, i64* %.286, align 8\n  %.288 = getelementptr inbounds [8 x i64], [8 x i64]* %psteps4, i64 0, i64 7\n  store i64 8, i64* %.288, align 8\n  %8 = bitcast i32* %.7.i to i8*\n  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %8)\n  store i32 0, i32* %.7.i, align 4\n  call void @numba_gil_ensure(i32* nonnull %.7.i)\n  %.10.i = call i8* @PyEval_SaveThread()\n  %.11.i = call i64 @get_num_threads()\n  %.13.i = bitcast [4 x i8*]* %pargs2 to i8*\n  %.14.i = bitcast [3 x i64]* %pshape3 to i8*\n  %.15.i1 = bitcast [8 x i64]* %psteps4 to i8*\n  call void @numba_parallel_for(i8* bitcast (void (i8**, i64*, i64*, i8*)* @__gufunc__._ZN13_3cdynamic_3e36__numba_parfor_gufunc_0x7ef28ec1b9a0B3v16B128c8tJTC_2fWQAliW1xhDEoY6EEMEUOEMISPGsAQMVj4QniQ4IXKQEMXwoMGLoQDDVsQR1NHAS2hQ9XgStYwaUTI24JtBVoBDXtb9MCvVgdJqdcC7YLGAMy5RRDjNAE_3dE5ArrayIyLi1E1C7mutable7alignedE5ArrayIdLi1E1A7mutable7alignedE5ArrayIdLi1E1A7mutable7alignedE5ArrayIdLi1E1C7mutable7alignedE to i8*), i8* nonnull %.13.i, i8* nonnull %.14.i, i8* nonnull %.15.i1, i8* null, i64 2, i64 4, i64 %.11.i)\n  call void @PyEval_RestoreThread(i8* %.10.i)\n  call void @numba_gil_release(i32* nonnull %.7.i)\n  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %8)\n  call void @set_parallel_chunksize(i64 %.179)\n  call void @deallocate_sched(i64* %.192)\n  %retptr.repack30 = bitcast { i8*, i8*, i64, i64, double*, [1 x i64], [1 x i64] }* %retptr to i8**\n  store i8* %.7.i.i.i, i8** %retptr.repack30, align 8\n  %retptr.repack5 = getelementptr inbounds { i8*, i8*, i64, i64, double*, [1 x i64], [1 x i64] }, { i8*, i8*, i64, i64, double*, [1 x i64], [1 x i64] }* %retptr, i64 0, i32 1\n  store i8* null, i8** %retptr.repack5, align 8\n  %retptr.repack7 = getelementptr inbounds { i8*, i8*, i64, i64, double*, [1 x i64], [1 x i64] }, { i8*, i8*, i64, i64, double*, [1 x i64], [1 x i64] }* %retptr, i64 0, i32 2\n  store i64 %arg.a.5.0, i64* %retptr.repack7, align 8\n  %retptr.repack9 = getelementptr inbounds { i8*, i8*, i64, i64, double*, [1 x i64], [1 x i64] }, { i8*, i8*, i64, i64, double*, [1 x i64], [1 x i64] }* %retptr, i64 0, i32 3\n  store i64 8, i64* %retptr.repack9, align 8\n  %retptr.repack11 = getelementptr inbounds { i8*, i8*, i64, i64, double*, [1 x i64], [1 x i64] }, { i8*, i8*, i64, i64, double*, [1 x i64], [1 x i64] }* %retptr, i64 0, i32 4\n  store double* %.6.i1.i, double** %retptr.repack11, align 8\n  %9 = getelementptr inbounds { i8*, i8*, i64, i64, double*, [1 x i64], [1 x i64] }, { i8*, i8*, i64, i64, double*, [1 x i64], [1 x i64] }* %retptr, i64 0, i32 5, i64 0\n  store i64 %arg.a.5.0, i64* %9, align 8\n  %10 = getelementptr inbounds { i8*, i8*, i64, i64, double*, [1 x i64], [1 x i64] }, { i8*, i8*, i64, i64, double*, [1 x i64], [1 x i64] }* %retptr, i64 0, i32 6, i64 0\n  store i64 8, i64* %10, align 8\n  call void @NRT_decref(i8* %arg.b.0)\n  call void @NRT_decref(i8* %arg.a.0)\n  br label %common.ret\n}\n\ndeclare i64 @get_num_threads() local_unnamed_addr\n\ndeclare i64 @get_parallel_chunksize() local_unnamed_addr\n\n; Function Attrs: nofree nounwind\ndeclare noundef i32 @printf(i8* nocapture noundef readonly, ...) local_unnamed_addr #0\n\ndeclare i64 @get_sched_size(i64, i64, i64*, i64*) local_unnamed_addr\n\ndeclare void @set_parallel_chunksize(i64) local_unnamed_addr\n\ndeclare i64* @allocate_sched(i64) local_unnamed_addr\n\ndeclare i64* @do_scheduling_unsigned(i64, i64*, i64*, i64, i64*, i64) local_unnamed_addr\n\ndeclare void @deallocate_sched(i64*) local_unnamed_addr\n\ndefine i8* @_ZN7cpython8__main__15avx2_vectorizedB3v14B110c8tJTC_2fWQAliW1xhDEoY6EEMEUOEMISPGsAQMVj4QniQ4IXKQEMXwoMGLoQDDVsQR1NHAS2hQ9XgStYw86ABbYse0tXqICn1WqDBwFiD2AEAE5ArrayIdLi1E1A7mutable7alignedE5ArrayIdLi1E1A7mutable7alignedE(i8* nocapture readnone %py_closure, i8* %py_args, i8* nocapture readnone %py_kws) local_unnamed_addr {\nentry:\n  %.5 = alloca i8*, align 8\n  %.6 = alloca i8*, align 8\n  %.7 = call i32 (i8*, i8*, i64, i64, ...) @PyArg_UnpackTuple(i8* %py_args, i8* getelementptr inbounds ([16 x i8], [16 x i8]* @.const.avx2_vectorized, i64 0, i64 0), i64 2, i64 2, i8** nonnull %.5, i8** nonnull %.6)\n  %.8 = icmp eq i32 %.7, 0\n  %.22 = alloca { i8*, i8*, i64, i64, double*, [1 x i64], [1 x i64] }, align 8\n  %.45 = alloca { i8*, i8*, i64, i64, double*, [1 x i64], [1 x i64] }, align 8\n  %.67 = alloca { i8*, i8*, i64, i64, double*, [1 x i64], [1 x i64] }, align 8\n  %excinfo = alloca { i8*, i32, i8*, i8*, i32 }*, align 8\n  store { i8*, i32, i8*, i8*, i32 }* null, { i8*, i32, i8*, i8*, i32 }** %excinfo, align 8\n  %.131 = alloca { i8*, i8*, i64, i64, double*, [1 x i64], [1 x i64] }, align 8\n  br i1 %.8, label %common.ret, label %entry.endif, !prof !1\n\ncommon.ret:                                       ; preds = %entry.endif.endif.endif.thread, %entry.endif.endif.endif.endif.endif.endif.endif.if.endif, %entry.endif.endif.endif.e...if.1, %entry.endif.endif.endif.e...endif.2.endif, %entry.endif.endif.endif.endif.endif.if, %entry, %entry.endif.endif.endif.e...if, %entry.endif.endif.endif.endif.endif.endif.if.endif.endif, %entry.endif.if\n  %common.ret.op = phi i8* [ null, %entry.endif.if ], [ %.135, %entry.endif.endif.endif.endif.endif.endif.if.endif.endif ], [ null, %entry.endif.endif.endif.e...if ], [ null, %entry ], [ null, %entry.endif.endif.endif.endif.endif.if ], [ null, %entry.endif.endif.endif.e...endif.2.endif ], [ null, %entry.endif.endif.endif.e...if.1 ], [ null, %entry.endif.endif.endif.endif.endif.endif.endif.if.endif ], [ null, %entry.endif.endif.endif.thread ]\n  ret i8* %common.ret.op\n\nentry.endif:                                      ; preds = %entry\n  %.12 = load i8*, i8** @_ZN08NumbaEnv8__main__15avx2_vectorizedB3v14B110c8tJTC_2fWQAliW1xhDEoY6EEMEUOEMISPGsAQMVj4QniQ4IXKQEMXwoMGLoQDDVsQR1NHAS2hQ9XgStYw86ABbYse0tXqICn1WqDBwFiD2AEAE5ArrayIdLi1E1A7mutable7alignedE5ArrayIdLi1E1A7mutable7alignedE, align 8\n  %.17 = icmp eq i8* %.12, null\n  br i1 %.17, label %entry.endif.if, label %entry.endif.endif, !prof !1\n\nentry.endif.if:                                   ; preds = %entry.endif\n  call void @PyErr_SetString(i8* nonnull @PyExc_RuntimeError, i8* getelementptr inbounds ([243 x i8], [243 x i8]* @"".const.missing Environment: _ZN08NumbaEnv8__main__15avx2_vectorizedB3v14B110c8tJTC_2fWQAliW1xhDEoY6EEMEUOEMISPGsAQMVj4QniQ4IXKQEMXwoMGLoQDDVsQR1NHAS2hQ9XgStYw86ABbYse0tXqICn1WqDBwFiD2AEAE5ArrayIdLi1E1A7mutable7alignedE5ArrayIdLi1E1A7mutable7alignedE"", i64 0, i64 0))\n  br label %common.ret\n\nentry.endif.endif:                                ; preds = %entry.endif\n  %.21 = load i8*, i8** %.5, align 8\n  %.25 = bitcast { i8*, i8*, i64, i64, double*, [1 x i64], [1 x i64] }* %.22 to i8*\n  %0 = bitcast { i8*, i8*, i64, i64, double*, [1 x i64], [1 x i64] }* %.22 to i8*\n  call void @llvm.memset.p0i8.i64(i8* noundef nonnull align 8 dereferenceable(56) %0, i8 0, i64 56, i1 false)\n  %.26 = call i32 @NRT_adapt_ndarray_from_python(i8* %.21, i8* nonnull %.25)\n  %1 = bitcast { i8*, i8*, i64, i64, double*, [1 x i64], [1 x i64] }* %.22 to i8*\n  %sunkaddr = getelementptr inbounds i8, i8* %1, i64 24\n  %2 = bitcast i8* %sunkaddr to i64*\n  %.30 = load i64, i64* %2, align 8\n  %.31 = icmp ne i64 %.30, 8\n  %.32 = icmp ne i32 %.26, 0\n  %.33 = or i1 %.32, %.31\n  br i1 %.33, label %entry.endif.endif.endif.thread, label %entry.endif.endif.endif.endif, !prof !1\n\nentry.endif.endif.endif.thread:                   ; preds = %entry.endif.endif\n  call void @PyErr_SetString(i8* nonnull @PyExc_TypeError, i8* getelementptr inbounds ([89 x i8], [89 x i8]* @"".const.can\'t unbox array from PyObject into native value.  The object maybe of a different type"", i64 0, i64 0))\n  br label %common.ret\n\nentry.endif.endif.endif.endif:                    ; preds = %entry.endif.endif\n  %3 = bitcast { i8*, i8*, i64, i64, double*, [1 x i64], [1 x i64] }* %.22 to i8**\n  %.37.fca.0.load = load i8*, i8** %3, align 8\n  %4 = bitcast { i8*, i8*, i64, i64, double*, [1 x i64], [1 x i64] }* %.22 to i8*\n  %sunkaddr10 = getelementptr inbounds i8, i8* %4, i64 32\n  %5 = bitcast i8* %sunkaddr10 to double**\n  %.37.fca.4.load = load double*, double** %5, align 8\n  %6 = bitcast { i8*, i8*, i64, i64, double*, [1 x i64], [1 x i64] }* %.22 to i8*\n  %sunkaddr11 = getelementptr inbounds i8, i8* %6, i64 40\n  %7 = bitcast i8* %sunkaddr11 to i64*\n  %.37.fca.5.0.load = load i64, i64* %7, align 8\n  %8 = bitcast { i8*, i8*, i64, i64, double*, [1 x i64], [1 x i64] }* %.22 to i8*\n  %sunkaddr12 = getelementptr inbounds i8, i8* %8, i64 48\n  %9 = bitcast i8* %sunkaddr12 to i64*\n  %.37.fca.6.0.load = load i64, i64* %9, align 8\n  %.44 = load i8*, i8** %.6, align 8\n  %.48 = bitcast { i8*, i8*, i64, i64, double*, [1 x i64], [1 x i64] }* %.45 to i8*\n  %10 = bitcast { i8*, i8*, i64, i64, double*, [1 x i64], [1 x i64] }* %.45 to i8*\n  call void @llvm.memset.p0i8.i64(i8* noundef nonnull align 8 dereferenceable(56) %10, i8 0, i64 56, i1 false)\n  %.49 = call i32 @NRT_adapt_ndarray_from_python(i8* %.44, i8* nonnull %.48)\n  %11 = bitcast { i8*, i8*, i64, i64, double*, [1 x i64], [1 x i64] }* %.45 to i8*\n  %sunkaddr13 = getelementptr inbounds i8, i8* %11, i64 24\n  %12 = bitcast i8* %sunkaddr13 to i64*\n  %.53 = load i64, i64* %12, align 8\n  %.54 = icmp ne i64 %.53, 8\n  %.55 = icmp ne i32 %.49, 0\n  %.56 = or i1 %.55, %.54\n  br i1 %.56, label %entry.endif.endif.endif.endif.endif.if, label %entry.endif.endif.endif.endif.endif.endif, !prof !1\n\nentry.endif.endif.endif.endif.endif.if:           ; preds = %entry.endif.endif.endif.endif\n  call void @PyErr_SetString(i8* nonnull @PyExc_TypeError, i8* getelementptr inbounds ([89 x i8], [89 x i8]* @"".const.can\'t unbox array from PyObject into native value.  The object maybe of a different type"", i64 0, i64 0))\n  call void @NRT_decref(i8* %.37.fca.0.load)\n  br label %common.ret\n\nentry.endif.endif.endif.endif.endif.endif:        ; preds = %entry.endif.endif.endif.endif\n  %13 = bitcast { i8*, i8*, i64, i64, double*, [1 x i64], [1 x i64] }* %.67 to i8**\n  %14 = bitcast { i8*, i8*, i64, i64, double*, [1 x i64], [1 x i64] }* %.45 to i8**\n  %.60.fca.0.load = load i8*, i8** %14, align 8\n  %15 = bitcast { i8*, i8*, i64, i64, double*, [1 x i64], [1 x i64] }* %.45 to i8*\n  %sunkaddr14 = getelementptr inbounds i8, i8* %15, i64 32\n  %16 = bitcast i8* %sunkaddr14 to double**\n  %.60.fca.4.load = load double*, double** %16, align 8\n  %17 = bitcast { i8*, i8*, i64, i64, double*, [1 x i64], [1 x i64] }* %.45 to i8*\n  %sunkaddr15 = getelementptr inbounds i8, i8* %17, i64 40\n  %18 = bitcast i8* %sunkaddr15 to i64*\n  %.60.fca.5.0.load = load i64, i64* %18, align 8\n  %19 = bitcast { i8*, i8*, i64, i64, double*, [1 x i64], [1 x i64] }* %.45 to i8*\n  %sunkaddr16 = getelementptr inbounds i8, i8* %19, i64 48\n  %20 = bitcast i8* %sunkaddr16 to i64*\n  %.60.fca.6.0.load = load i64, i64* %20, align 8\n  %21 = bitcast { i8*, i8*, i64, i64, double*, [1 x i64], [1 x i64] }* %.67 to i8*\n  call void @llvm.memset.p0i8.i64(i8* noundef nonnull align 8 dereferenceable(56) %21, i8 0, i64 56, i1 false)\n  %.75 = call i32 @_ZN8__main__15avx2_vectorizedB3v14B110c8tJTC_2fWQAliW1xhDEoY6EEMEUOEMISPGsAQMVj4QniQ4IXKQEMXwoMGLoQDDVsQR1NHAS2hQ9XgStYw86ABbYse0tXqICn1WqDBwFiD2AEAE5ArrayIdLi1E1A7mutable7alignedE5ArrayIdLi1E1A7mutable7alignedE({ i8*, i8*, i64, i64, double*, [1 x i64], [1 x i64] }* nonnull %.67, { i8*, i32, i8*, i8*, i32 }** nonnull %excinfo, i8* %.37.fca.0.load, i8* undef, i64 undef, i64 undef, double* %.37.fca.4.load, i64 %.37.fca.5.0.load, i64 %.37.fca.6.0.load, i8* %.60.fca.0.load, i8* undef, i64 undef, i64 undef, double* %.60.fca.4.load, i64 %.60.fca.5.0.load, i64 %.60.fca.6.0.load) #4\n  %.76 = load { i8*, i32, i8*, i8*, i32 }*, { i8*, i32, i8*, i8*, i32 }** %excinfo, align 8\n  %.83 = icmp sgt i32 %.75, 0\n  %.84 = select i1 %.83, { i8*, i32, i8*, i8*, i32 }* %.76, { i8*, i32, i8*, i8*, i32 }* undef\n  %.85.fca.0.load = load i8*, i8** %13, align 8\n  %22 = bitcast { i8*, i8*, i64, i64, double*, [1 x i64], [1 x i64] }* %.67 to i8*\n  %sunkaddr17 = getelementptr inbounds i8, i8* %22, i64 8\n  %23 = bitcast i8* %sunkaddr17 to i8**\n  %.85.fca.1.load = load i8*, i8** %23, align 8\n  %24 = bitcast { i8*, i8*, i64, i64, double*, [1 x i64], [1 x i64] }* %.67 to i8*\n  %sunkaddr18 = getelementptr inbounds i8, i8* %24, i64 16\n  %25 = bitcast i8* %sunkaddr18 to i64*\n  %.85.fca.2.load = load i64, i64* %25, align 8\n  %26 = bitcast { i8*, i8*, i64, i64, double*, [1 x i64], [1 x i64] }* %.67 to i8*\n  %sunkaddr19 = getelementptr inbounds i8, i8* %26, i64 24\n  %27 = bitcast i8* %sunkaddr19 to i64*\n  %.85.fca.3.load = load i64, i64* %27, align 8\n  %28 = bitcast { i8*, i8*, i64, i64, double*, [1 x i64], [1 x i64] }* %.67 to i8*\n  %sunkaddr20 = getelementptr inbounds i8, i8* %28, i64 32\n  %29 = bitcast i8* %sunkaddr20 to double**\n  %.85.fca.4.load = load double*, double** %29, align 8\n  %30 = bitcast { i8*, i8*, i64, i64, double*, [1 x i64], [1 x i64] }* %.67 to i8*\n  %sunkaddr21 = getelementptr inbounds i8, i8* %30, i64 40\n  %31 = bitcast i8* %sunkaddr21 to i64*\n  %.85.fca.5.0.load = load i64, i64* %31, align 8\n  %32 = bitcast { i8*, i8*, i64, i64, double*, [1 x i64], [1 x i64] }* %.67 to i8*\n  %sunkaddr22 = getelementptr inbounds i8, i8* %32, i64 48\n  %33 = bitcast i8* %sunkaddr22 to i64*\n  %.85.fca.6.0.load = load i64, i64* %33, align 8\n  call void @NRT_decref(i8* %.37.fca.0.load)\n  call void @NRT_decref(i8* %.60.fca.0.load)\n  switch i32 %.75, label %entry.endif.endif.endif.endif.endif.endif.endif [\n    i32 -2, label %entry.endif.endif.endif.endif.endif.endif.if.endif\n    i32 0, label %entry.endif.endif.endif.endif.endif.endif.if.endif\n  ]\n\nentry.endif.endif.endif.endif.endif.endif.endif:  ; preds = %entry.endif.endif.endif.endif.endif.endif\n  %34 = icmp sgt i32 %.75, 0\n  br i1 %34, label %entry.endif.endif.endif.endif.endif.endif.endif.if, label %entry.endif.endif.endif.e...endif.2.endif\n\nentry.endif.endif.endif.endif.endif.endif.if.endif: ; preds = %entry.endif.endif.endif.endif.endif.endif, %entry.endif.endif.endif.endif.endif.endif\n  %sunkaddr23 = getelementptr i8, i8* %.12, i64 24\n  %35 = bitcast i8* %sunkaddr23 to i8**\n  %.112 = load i8*, i8** %35, align 8\n  %.116.not = icmp eq i8* %.112, null\n  br i1 %.116.not, label %entry.endif.endif.endif.endif.endif.endif.if.endif.else, label %entry.endif.endif.endif.endif.endif.endif.if.endif.if\n\nentry.endif.endif.endif.endif.endif.endif.if.endif.if: ; preds = %entry.endif.endif.endif.endif.endif.endif.if.endif\n  %.118 = call i8* @PyList_GetItem(i8* nonnull %.112, i64 0)\n  br label %entry.endif.endif.endif.endif.endif.endif.if.endif.endif\n\nentry.endif.endif.endif.endif.endif.endif.if.endif.else: ; preds = %entry.endif.endif.endif.endif.endif.endif.if.endif\n  call void @PyErr_SetString(i8* nonnull @PyExc_RuntimeError, i8* getelementptr inbounds ([37 x i8], [37 x i8]* @"".const.`env.consts` is NULL in `read_const`"", i64 0, i64 0))\n  br label %entry.endif.endif.endif.endif.endif.endif.if.endif.endif\n\nentry.endif.endif.endif.endif.endif.endif.if.endif.endif: ; preds = %entry.endif.endif.endif.endif.endif.endif.if.endif.else, %entry.endif.endif.endif.endif.endif.endif.if.endif.if\n  %.113.0 = phi i8* [ %.118, %entry.endif.endif.endif.endif.endif.endif.if.endif.if ], [ null, %entry.endif.endif.endif.endif.endif.endif.if.endif.else ]\n  %36 = bitcast { i8*, i8*, i64, i64, double*, [1 x i64], [1 x i64] }* %.131 to i8**\n  %.130 = call i8* @numba_unpickle(i8* getelementptr inbounds ([32 x i8], [32 x i8]* @.const.pickledata.139583502599616, i64 0, i64 0), i32 32, i8* getelementptr inbounds ([20 x i8], [20 x i8]* @.const.pickledata.139583502599616.sha1, i64 0, i64 0))\n  store i8* %.85.fca.0.load, i8** %36, align 8\n  %37 = bitcast { i8*, i8*, i64, i64, double*, [1 x i64], [1 x i64] }* %.131 to i8*\n  %sunkaddr24 = getelementptr inbounds i8, i8* %37, i64 8\n  %38 = bitcast i8* %sunkaddr24 to i8**\n  store i8* %.85.fca.1.load, i8** %38, align 8\n  %39 = bitcast { i8*, i8*, i64, i64, double*, [1 x i64], [1 x i64] }* %.131 to i8*\n  %sunkaddr25 = getelementptr inbounds i8, i8* %39, i64 16\n  %40 = bitcast i8* %sunkaddr25 to i64*\n  store i64 %.85.fca.2.load, i64* %40, align 8\n  %41 = bitcast { i8*, i8*, i64, i64, double*, [1 x i64], [1 x i64] }* %.131 to i8*\n  %sunkaddr26 = getelementptr inbounds i8, i8* %41, i64 24\n  %42 = bitcast i8* %sunkaddr26 to i64*\n  store i64 %.85.fca.3.load, i64* %42, align 8\n  %43 = bitcast { i8*, i8*, i64, i64, double*, [1 x i64], [1 x i64] }* %.131 to i8*\n  %sunkaddr27 = getelementptr inbounds i8, i8* %43, i64 32\n  %44 = bitcast i8* %sunkaddr27 to double**\n  store double* %.85.fca.4.load, double** %44, align 8\n  %45 = bitcast { i8*, i8*, i64, i64, double*, [1 x i64], [1 x i64] }* %.131 to i8*\n  %sunkaddr28 = getelementptr inbounds i8, i8* %45, i64 40\n  %46 = bitcast i8* %sunkaddr28 to i64*\n  store i64 %.85.fca.5.0.load, i64* %46, align 8\n  %47 = bitcast { i8*, i8*, i64, i64, double*, [1 x i64], [1 x i64] }* %.131 to i8*\n  %sunkaddr29 = getelementptr inbounds i8, i8* %47, i64 48\n  %48 = bitcast i8* %sunkaddr29 to i64*\n  store i64 %.85.fca.6.0.load, i64* %48, align 8\n  %.134 = bitcast { i8*, i8*, i64, i64, double*, [1 x i64], [1 x i64] }* %.131 to i8*\n  %.135 = call i8* @NRT_adapt_ndarray_to_python_acqref(i8* nonnull %.134, i8* %.130, i32 1, i32 1, i8* %.113.0)\n  call void @NRT_decref(i8* %.85.fca.0.load)\n  br label %common.ret\n\nentry.endif.endif.endif.endif.endif.endif.endif.if: ; preds = %entry.endif.endif.endif.endif.endif.endif.endif\n  call void @PyErr_Clear()\n  %.143 = load { i8*, i32, i8*, i8*, i32 }, { i8*, i32, i8*, i8*, i32 }* %.84, align 8\n  %.144 = extractvalue { i8*, i32, i8*, i8*, i32 } %.143, 4\n  %.145 = icmp sgt i32 %.144, 0\n  %.148 = extractvalue { i8*, i32, i8*, i8*, i32 } %.143, 0\n  %.150 = extractvalue { i8*, i32, i8*, i8*, i32 } %.143, 1\n  br i1 %.145, label %entry.endif.endif.endif.endif.endif.endif.endif.if.if, label %entry.endif.endif.endif.endif.endif.endif.endif.if.else\n\nentry.endif.endif.endif.endif.endif.endif.endif.if.if: ; preds = %entry.endif.endif.endif.endif.endif.endif.endif.if\n  %.151 = sext i32 %.150 to i64\n  %.152 = call i8* @PyBytes_FromStringAndSize(i8* %.148, i64 %.151)\n  %.153 = load { i8*, i32, i8*, i8*, i32 }, { i8*, i32, i8*, i8*, i32 }* %.84, align 8\n  %.154 = extractvalue { i8*, i32, i8*, i8*, i32 } %.153, 2\n  %.156 = extractvalue { i8*, i32, i8*, i8*, i32 } %.153, 3\n  %.157 = bitcast i8* %.156 to i8* (i8*)*\n  %.158 = call i8* %.157(i8* %.154)\n  %.159 = icmp eq i8* %.158, null\n  br i1 %.159, label %entry.endif.endif.endif.e...if, label %entry.endif.endif.endif.e...endif, !prof !1\n\nentry.endif.endif.endif.endif.endif.endif.endif.if.else: ; preds = %entry.endif.endif.endif.endif.endif.endif.endif.if\n  %.172 = extractvalue { i8*, i32, i8*, i8*, i32 } %.143, 2\n  %.173 = call i8* @numba_unpickle(i8* %.148, i32 %.150, i8* %.172)\n  br label %entry.endif.endif.endif.endif.endif.endif.endif.if.endif\n\nentry.endif.endif.endif.endif.endif.endif.endif.if.endif: ; preds = %entry.endif.endif.endif.e...endif, %entry.endif.endif.endif.endif.endif.endif.endif.if.else\n  %.175 = phi i8* [ %.163, %entry.endif.endif.endif.e...endif ], [ %.173, %entry.endif.endif.endif.endif.endif.endif.endif.if.else ]\n  %.176.not = icmp eq i8* %.175, null\n  br i1 %.176.not, label %common.ret, label %entry.endif.endif.endif.e...if.1, !prof !1\n\nentry.endif.endif.endif.e...if:                   ; preds = %entry.endif.endif.endif.endif.endif.endif.endif.if.if\n  call void @PyErr_SetString(i8* nonnull @PyExc_RuntimeError, i8* getelementptr inbounds ([61 x i8], [61 x i8]* @"".const.Error creating Python tuple from runtime exception arguments"", i64 0, i64 0))\n  br label %common.ret\n\nentry.endif.endif.endif.e...endif:                ; preds = %entry.endif.endif.endif.endif.endif.endif.endif.if.if\n  %.163 = call i8* @numba_runtime_build_excinfo_struct(i8* %.152, i8* nonnull %.158)\n  %.164 = bitcast { i8*, i32, i8*, i8*, i32 }* %.84 to i8*\n  call void @NRT_Free(i8* nonnull %.164)\n  br label %entry.endif.endif.endif.endif.endif.endif.endif.if.endif\n\nentry.endif.endif.endif.e...if.1:                 ; preds = %entry.endif.endif.endif.endif.endif.endif.endif.if.endif\n  call void @numba_do_raise(i8* nonnull %.175)\n  br label %common.ret\n\nentry.endif.endif.endif.e...endif.2.endif:        ; preds = %entry.endif.endif.endif.endif.endif.endif.endif\n  call void @PyErr_SetString(i8* nonnull @PyExc_SystemError, i8* getelementptr inbounds ([43 x i8], [43 x i8]* @"".const.unknown error when calling native function"", i64 0, i64 0))\n  br label %common.ret\n}\n\ndeclare i32 @PyArg_UnpackTuple(i8*, i8*, i64, i64, ...) local_unnamed_addr\n\ndeclare void @PyErr_SetString(i8*, i8*) local_unnamed_addr\n\ndeclare i32 @NRT_adapt_ndarray_from_python(i8* nocapture, i8* nocapture) local_unnamed_addr\n\ndeclare i8* @PyList_GetItem(i8*, i64) local_unnamed_addr\n\ndeclare i8* @numba_unpickle(i8*, i32, i8*) local_unnamed_addr\n\ndeclare i8* @NRT_adapt_ndarray_to_python_acqref(i8* nocapture, i8*, i32, i32, i8*) local_unnamed_addr\n\ndeclare void @PyErr_Clear() local_unnamed_addr\n\ndeclare i8* @PyBytes_FromStringAndSize(i8*, i64) local_unnamed_addr\n\ndeclare i8* @numba_runtime_build_excinfo_struct(i8*, i8*) local_unnamed_addr\n\ndeclare void @NRT_Free(i8*) local_unnamed_addr\n\ndeclare void @numba_do_raise(i8*) local_unnamed_addr\n\ndefine { i8*, i8*, i64, i64, double*, [1 x i64], [1 x i64] } @cfunc._ZN8__main__15avx2_vectorizedB3v14B110c8tJTC_2fWQAliW1xhDEoY6EEMEUOEMISPGsAQMVj4QniQ4IXKQEMXwoMGLoQDDVsQR1NHAS2hQ9XgStYw86ABbYse0tXqICn1WqDBwFiD2AEAE5ArrayIdLi1E1A7mutable7alignedE5ArrayIdLi1E1A7mutable7alignedE({ i8*, i8*, i64, i64, double*, [1 x i64], [1 x i64] } %.1, { i8*, i8*, i64, i64, double*, [1 x i64], [1 x i64] } %.2) local_unnamed_addr {\nentry:\n  %.4 = alloca { i8*, i8*, i64, i64, double*, [1 x i64], [1 x i64] }, align 8\n  %.fca.0.gep1 = bitcast { i8*, i8*, i64, i64, double*, [1 x i64], [1 x i64] }* %.4 to i8**\n  %.fca.1.gep = getelementptr inbounds { i8*, i8*, i64, i64, double*, [1 x i64], [1 x i64] }, { i8*, i8*, i64, i64, double*, [1 x i64], [1 x i64] }* %.4, i64 0, i32 1\n  %.fca.2.gep = getelementptr inbounds { i8*, i8*, i64, i64, double*, [1 x i64], [1 x i64] }, { i8*, i8*, i64, i64, double*, [1 x i64], [1 x i64] }* %.4, i64 0, i32 2\n  %.fca.3.gep = getelementptr inbounds { i8*, i8*, i64, i64, double*, [1 x i64], [1 x i64] }, { i8*, i8*, i64, i64, double*, [1 x i64], [1 x i64] }* %.4, i64 0, i32 3\n  %.fca.4.gep = getelementptr inbounds { i8*, i8*, i64, i64, double*, [1 x i64], [1 x i64] }, { i8*, i8*, i64, i64, double*, [1 x i64], [1 x i64] }* %.4, i64 0, i32 4\n  %.fca.5.0.gep = getelementptr inbounds { i8*, i8*, i64, i64, double*, [1 x i64], [1 x i64] }, { i8*, i8*, i64, i64, double*, [1 x i64], [1 x i64] }* %.4, i64 0, i32 5, i64 0\n  %.fca.6.0.gep = getelementptr inbounds { i8*, i8*, i64, i64, double*, [1 x i64], [1 x i64] }, { i8*, i8*, i64, i64, double*, [1 x i64], [1 x i64] }* %.4, i64 0, i32 6, i64 0\n  %excinfo = alloca { i8*, i32, i8*, i8*, i32 }*, align 8\n  %0 = bitcast { i8*, i8*, i64, i64, double*, [1 x i64], [1 x i64] }* %.4 to i8*\n  call void @llvm.memset.p0i8.i64(i8* noundef nonnull align 8 dereferenceable(56) %0, i8 0, i64 56, i1 false)\n  store { i8*, i32, i8*, i8*, i32 }* null, { i8*, i32, i8*, i8*, i32 }** %excinfo, align 8\n  %extracted.meminfo = extractvalue { i8*, i8*, i64, i64, double*, [1 x i64], [1 x i64] } %.1, 0\n  %extracted.data = extractvalue { i8*, i8*, i64, i64, double*, [1 x i64], [1 x i64] } %.1, 4\n  %extracted.shape = extractvalue { i8*, i8*, i64, i64, double*, [1 x i64], [1 x i64] } %.1, 5\n  %.8 = extractvalue [1 x i64] %extracted.shape, 0\n  %extracted.strides = extractvalue { i8*, i8*, i64, i64, double*, [1 x i64], [1 x i64] } %.1, 6\n  %.9 = extractvalue [1 x i64] %extracted.strides, 0\n  %extracted.meminfo.1 = extractvalue { i8*, i8*, i64, i64, double*, [1 x i64], [1 x i64] } %.2, 0\n  %extracted.data.1 = extractvalue { i8*, i8*, i64, i64, double*, [1 x i64], [1 x i64] } %.2, 4\n  %extracted.shape.1 = extractvalue { i8*, i8*, i64, i64, double*, [1 x i64], [1 x i64] } %.2, 5\n  %.10 = extractvalue [1 x i64] %extracted.shape.1, 0\n  %extracted.strides.1 = extractvalue { i8*, i8*, i64, i64, double*, [1 x i64], [1 x i64] } %.2, 6\n  %.11 = extractvalue [1 x i64] %extracted.strides.1, 0\n  %.12 = call i32 @_ZN8__main__15avx2_vectorizedB3v14B110c8tJTC_2fWQAliW1xhDEoY6EEMEUOEMISPGsAQMVj4QniQ4IXKQEMXwoMGLoQDDVsQR1NHAS2hQ9XgStYw86ABbYse0tXqICn1WqDBwFiD2AEAE5ArrayIdLi1E1A7mutable7alignedE5ArrayIdLi1E1A7mutable7alignedE({ i8*, i8*, i64, i64, double*, [1 x i64], [1 x i64] }* nonnull %.4, { i8*, i32, i8*, i8*, i32 }** nonnull %excinfo, i8* %extracted.meminfo, i8* undef, i64 undef, i64 undef, double* %extracted.data, i64 %.8, i64 %.9, i8* %extracted.meminfo.1, i8* undef, i64 undef, i64 undef, double* %extracted.data.1, i64 %.10, i64 %.11) #4\n  %.13 = load { i8*, i32, i8*, i8*, i32 }*, { i8*, i32, i8*, i8*, i32 }** %excinfo, align 8\n  %.14.not = icmp eq i32 %.12, 0\n  %.20 = icmp sgt i32 %.12, 0\n  %.21 = select i1 %.20, { i8*, i32, i8*, i8*, i32 }* %.13, { i8*, i32, i8*, i8*, i32 }* undef\n  %.22.fca.0.load = load i8*, i8** %.fca.0.gep1, align 8\n  %.22.fca.1.load = load i8*, i8** %.fca.1.gep, align 8\n  %.22.fca.2.load = load i64, i64* %.fca.2.gep, align 8\n  %.22.fca.3.load = load i64, i64* %.fca.3.gep, align 8\n  %.22.fca.4.load = load double*, double** %.fca.4.gep, align 8\n  %.22.fca.5.0.load = load i64, i64* %.fca.5.0.gep, align 8\n  %.22.fca.6.0.load = load i64, i64* %.fca.6.0.gep, align 8\n  %.28 = insertvalue [1 x i64] poison, i64 %.22.fca.5.0.load, 0\n  %.29 = insertvalue [1 x i64] poison, i64 %.22.fca.6.0.load, 0\n  %inserted.meminfo = insertvalue { i8*, i8*, i64, i64, double*, [1 x i64], [1 x i64] } undef, i8* %.22.fca.0.load, 0\n  %inserted.parent = insertvalue { i8*, i8*, i64, i64, double*, [1 x i64], [1 x i64] } %inserted.meminfo, i8* %.22.fca.1.load, 1\n  %inserted.nitems = insertvalue { i8*, i8*, i64, i64, double*, [1 x i64], [1 x i64] } %inserted.parent, i64 %.22.fca.2.load, 2\n  %inserted.itemsize = insertvalue { i8*, i8*, i64, i64, double*, [1 x i64], [1 x i64] } %inserted.nitems, i64 %.22.fca.3.load, 3\n  %inserted.data = insertvalue { i8*, i8*, i64, i64, double*, [1 x i64], [1 x i64] } %inserted.itemsize, double* %.22.fca.4.load, 4\n  %inserted.shape = insertvalue { i8*, i8*, i64, i64, double*, [1 x i64], [1 x i64] } %inserted.data, [1 x i64] %.28, 5\n  %inserted.strides = insertvalue { i8*, i8*, i64, i64, double*, [1 x i64], [1 x i64] } %inserted.shape, [1 x i64] %.29, 6\n  %.35 = alloca i32, align 4\n  store i32 0, i32* %.35, align 4\n  br i1 %.14.not, label %common.ret, label %entry.if, !prof !13\n\nentry.if:                                         ; preds = %entry\n  %1 = icmp sgt i32 %.12, 0\n  call void @numba_gil_ensure(i32* nonnull %.35)\n  br i1 %1, label %entry.if.if, label %entry.if.endif.endif.endif\n\ncommon.ret:                                       ; preds = %entry, %.38, %entry.if.if.if.if\n  %common.ret.op = phi { i8*, i8*, i64, i64, double*, [1 x i64], [1 x i64] } [ zeroinitializer, %entry.if.if.if.if ], [ %inserted.strides, %.38 ], [ %inserted.strides, %entry ]\n  ret { i8*, i8*, i64, i64, double*, [1 x i64], [1 x i64] } %common.ret.op\n\n.38:                                              ; preds = %entry.if.if.endif, %entry.if.if.endif.if, %entry.if.endif.endif.endif\n  %.86 = call i8* @PyUnicode_FromString(i8* getelementptr inbounds ([53 x i8], [53 x i8]* @"".const.<numba.core.cpu.CPUContext object at 0x7ef27be99f60>"", i64 0, i64 0))\n  call void @PyErr_WriteUnraisable(i8* %.86)\n  call void @Py_DecRef(i8* %.86)\n  call void @numba_gil_release(i32* nonnull %.35)\n  br label %common.ret\n\nentry.if.if:                                      ; preds = %entry.if\n  call void @PyErr_Clear()\n  %.41 = load { i8*, i32, i8*, i8*, i32 }, { i8*, i32, i8*, i8*, i32 }* %.21, align 8\n  %.42 = extractvalue { i8*, i32, i8*, i8*, i32 } %.41, 4\n  %.43 = icmp sgt i32 %.42, 0\n  %.46 = extractvalue { i8*, i32, i8*, i8*, i32 } %.41, 0\n  %.48 = extractvalue { i8*, i32, i8*, i8*, i32 } %.41, 1\n  br i1 %.43, label %entry.if.if.if, label %entry.if.if.else\n\nentry.if.if.if:                                   ; preds = %entry.if.if\n  %.49 = sext i32 %.48 to i64\n  %.50 = call i8* @PyBytes_FromStringAndSize(i8* %.46, i64 %.49)\n  %.51 = load { i8*, i32, i8*, i8*, i32 }, { i8*, i32, i8*, i8*, i32 }* %.21, align 8\n  %.52 = extractvalue { i8*, i32, i8*, i8*, i32 } %.51, 2\n  %.54 = extractvalue { i8*, i32, i8*, i8*, i32 } %.51, 3\n  %.55 = bitcast i8* %.54 to i8* (i8*)*\n  %.56 = call i8* %.55(i8* %.52)\n  %.57 = icmp eq i8* %.56, null\n  br i1 %.57, label %entry.if.if.if.if, label %entry.if.if.if.endif, !prof !1\n\nentry.if.if.else:                                 ; preds = %entry.if.if\n  %.70 = extractvalue { i8*, i32, i8*, i8*, i32 } %.41, 2\n  %.71 = call i8* @numba_unpickle(i8* %.46, i32 %.48, i8* %.70)\n  br label %entry.if.if.endif\n\nentry.if.if.endif:                                ; preds = %entry.if.if.if.endif, %entry.if.if.else\n  %.73 = phi i8* [ %.61, %entry.if.if.if.endif ], [ %.71, %entry.if.if.else ]\n  %.74.not = icmp eq i8* %.73, null\n  br i1 %.74.not, label %.38, label %entry.if.if.endif.if, !prof !1\n\nentry.if.if.if.if:                                ; preds = %entry.if.if.if\n  call void @PyErr_SetString(i8* nonnull @PyExc_RuntimeError, i8* getelementptr inbounds ([61 x i8], [61 x i8]* @"".const.Error creating Python tuple from runtime exception arguments.1"", i64 0, i64 0))\n  br label %common.ret\n\nentry.if.if.if.endif:                             ; preds = %entry.if.if.if\n  %.61 = call i8* @numba_runtime_build_excinfo_struct(i8* %.50, i8* nonnull %.56)\n  %.62 = bitcast { i8*, i32, i8*, i8*, i32 }* %.21 to i8*\n  call void @NRT_Free(i8* nonnull %.62)\n  br label %entry.if.if.endif\n\nentry.if.if.endif.if:                             ; preds = %entry.if.if.endif\n  call void @numba_do_raise(i8* nonnull %.73)\n  br label %.38\n\nentry.if.endif.endif.endif:                       ; preds = %entry.if\n  call void @PyErr_SetString(i8* nonnull @PyExc_SystemError, i8* getelementptr inbounds ([43 x i8], [43 x i8]* @"".const.unknown error when calling native function.2"", i64 0, i64 0))\n  br label %.38\n}\n\ndeclare void @numba_gil_ensure(i32*) local_unnamed_addr\n\ndeclare i8* @PyUnicode_FromString(i8*) local_unnamed_addr\n\ndeclare void @PyErr_WriteUnraisable(i8*) local_unnamed_addr\n\ndeclare void @Py_DecRef(i8*) local_unnamed_addr\n\ndeclare void @numba_gil_release(i32*) local_unnamed_addr\n\n; Function Attrs: mustprogress nofree nosync nounwind readnone speculatable willreturn\ndeclare { i64, i1 } @llvm.smul.with.overflow.i64(i64, i64) #1\n\ndeclare noalias i8* @NRT_MemInfo_alloc_aligned(i64, i32) local_unnamed_addr\n\ndeclare i8* @PyEval_SaveThread() local_unnamed_addr\n\ndefine weak_odr void @__gufunc__._ZN13_3cdynamic_3e36__numba_parfor_gufunc_0x7ef28ec1b9a0B3v16B128c8tJTC_2fWQAliW1xhDEoY6EEMEUOEMISPGsAQMVj4QniQ4IXKQEMXwoMGLoQDDVsQR1NHAS2hQ9XgStYwaUTI24JtBVoBDXtb9MCvVgdJqdcC7YLGAMy5RRDjNAE_3dE5ArrayIyLi1E1C7mutable7alignedE5ArrayIdLi1E1A7mutable7alignedE5ArrayIdLi1E1A7mutable7alignedE5ArrayIdLi1E1C7mutable7alignedE(i8** %args, i64* %dims, i64* %steps, i8* %data) {\nentry:\n  %loopcount = load i64, i64* %dims, align 8\n  %.10 = load i64, i64* %steps, align 8\n  %core.step.ptr.1 = getelementptr i64, i64* %steps, i64 1\n  %.12 = load i64, i64* %core.step.ptr.1, align 8\n  %step.ptr.1 = getelementptr i64, i64* %steps, i64 5\n  %.13 = load i64, i64* %step.ptr.1, align 8\n  %core.step.ptr.2 = getelementptr i64, i64* %steps, i64 2\n  %.14 = load i64, i64* %core.step.ptr.2, align 8\n  %step.ptr.2 = getelementptr i64, i64* %steps, i64 6\n  %.15 = load i64, i64* %step.ptr.2, align 8\n  %core.step.ptr.3 = getelementptr i64, i64* %steps, i64 3\n  %.16 = load i64, i64* %core.step.ptr.3, align 8\n  %.191 = icmp sgt i64 %loopcount, 0\n  br i1 %.191, label %for.body.preheader, label %common.ret\n\nfor.body.preheader:                               ; preds = %entry\n  %data.ptr.3 = getelementptr i8*, i8** %args, i64 3\n  %data.4 = load i8*, i8** %data.ptr.3, align 8\n  %data.ptr.2 = getelementptr i8*, i8** %args, i64 2\n  %data.3 = load i8*, i8** %data.ptr.2, align 8\n  %data.ptr.1 = getelementptr i8*, i8** %args, i64 1\n  %data.2 = load i8*, i8** %data.ptr.1, align 8\n  %data.1 = load i8*, i8** %args, align 8\n  %broadcast.splatinsert.i = insertelement <4 x i64> poison, i64 %.13, i64 0\n  %broadcast.splat.i = shufflevector <4 x i64> %broadcast.splatinsert.i, <4 x i64> poison, <4 x i32> zeroinitializer\n  %broadcast.splatinsert10.i = insertelement <4 x i64> poison, i64 %.15, i64 0\n  %broadcast.splat11.i = shufflevector <4 x i64> %broadcast.splatinsert10.i, <4 x i64> poison, <4 x i32> zeroinitializer\n  br label %for.body\n\ncommon.ret:                                       ; preds = %.continue, %entry\n  ret void\n\nfor.body:                                         ; preds = %.continue, %for.body.preheader\n  %loop.index2 = phi i64 [ %.215, %.continue ], [ 0, %for.body.preheader ]\n  %0 = ptrtoint i8* %data.4 to i64\n  %1 = ptrtoint i8* %data.3 to i64\n  %2 = ptrtoint i8* %data.2 to i64\n  %3 = ptrtoint i8* %data.1 to i64\n  %.24 = mul i64 %loop.index2, %.10\n  %.26 = add i64 %.24, %3\n  %.32 = inttoptr i64 %.26 to i64*\n  %.53 = mul i64 %loop.index2, %.12\n  %.55 = add i64 %.53, %2\n  %.82 = mul i64 %loop.index2, %.14\n  %.84 = add i64 %.82, %1\n  %.111 = mul i64 %loop.index2, %.16\n  %.113 = add i64 %.111, %0\n  tail call void @llvm.experimental.noalias.scope.decl(metadata !14)\n  tail call void @llvm.experimental.noalias.scope.decl(metadata !17)\n  %.96.i = load i64, i64* %.32, align 8, !alias.scope !14, !noalias !19\n  %.134.i = getelementptr i64, i64* %.32, i64 1\n  %.135.i = load i64, i64* %.134.i, align 8, !alias.scope !14, !noalias !19\n  %.170.i = sub i64 1, %.96.i\n  %.226.i = add i64 %.170.i, %.135.i\n  %.227.inv.i = icmp sgt i64 %.226.i, 0\n  br i1 %.227.inv.i, label %B116.lr.ph.i, label %.continue\n\nB116.lr.ph.i:                                     ; preds = %for.body\n  %min.iters.check.i = icmp ult i64 %.226.i, 4\n  br i1 %min.iters.check.i, label %B116.preheader.i, label %vector.ph.i\n\nvector.ph.i:                                      ; preds = %B116.lr.ph.i\n  %n.vec.i = and i64 %.226.i, -4\n  %ind.end.i = and i64 %.226.i, 3\n  %ind.end6.i = add i64 %n.vec.i, %.96.i\n  %.splatinsert.i = insertelement <4 x i64> poison, i64 %.96.i, i64 0\n  %.splat.i = shufflevector <4 x i64> %.splatinsert.i, <4 x i64> poison, <4 x i32> zeroinitializer\n  %induction.i = add <4 x i64> %.splat.i, <i64 0, i64 1, i64 2, i64 3>\n  %broadcast.splatinsert8.i = insertelement <4 x i64> poison, i64 %.55, i64 0\n  %broadcast.splat9.i = shufflevector <4 x i64> %broadcast.splatinsert8.i, <4 x i64> poison, <4 x i32> zeroinitializer\n  %broadcast.splatinsert12.i = insertelement <4 x i64> poison, i64 %.84, i64 0\n  %broadcast.splat13.i = shufflevector <4 x i64> %broadcast.splatinsert12.i, <4 x i64> poison, <4 x i32> zeroinitializer\n  %4 = add nsw i64 %n.vec.i, -4\n  %5 = lshr exact i64 %4, 2\n  %6 = add i64 %5, 1\n  %xtraiter.i = and i64 %6, 3\n  %7 = icmp ult i64 %4, 12\n  br i1 %7, label %middle.block.unr-lcssa.i, label %vector.ph.new.i\n\nvector.ph.new.i:                                  ; preds = %vector.ph.i\n  %8 = inttoptr i64 %.113 to double*\n  %9 = add i64 %.96.i, 12\n  %scevgep26.i = getelementptr double, double* %8, i64 %9\n  %10 = lshr i64 %6, 2\n  %11 = mul nsw i64 %10, -4\n  br label %vector.body.i\n\nvector.body.i:                                    ; preds = %vector.body.i, %vector.ph.new.i\n  %lsr.iv = phi i64 [ %lsr.iv.next, %vector.body.i ], [ %11, %vector.ph.new.i ]\n  %index.i = phi i64 [ %index.next.3.i, %vector.body.i ], [ 0, %vector.ph.new.i ]\n  %vec.ind.i = phi <4 x i64> [ %vec.ind.next.3.i, %vector.body.i ], [ %induction.i, %vector.ph.new.i ]\n  %12 = mul <4 x i64> %vec.ind.i, %broadcast.splat.i\n  %13 = add <4 x i64> %12, %broadcast.splat9.i\n  %14 = inttoptr <4 x i64> %13 to <4 x double*>\n  %wide.masked.gather.i = tail call <4 x double> @llvm.masked.gather.v4f64.v4p0f64(<4 x double*> %14, i32 8, <4 x i1> <i1 true, i1 true, i1 true, i1 true>, <4 x double> undef) #8, !noalias !23\n  %15 = mul <4 x i64> %vec.ind.i, %broadcast.splat11.i\n  %16 = add <4 x i64> %15, %broadcast.splat13.i\n  %17 = inttoptr <4 x i64> %16 to <4 x double*>\n  %wide.masked.gather14.i = tail call <4 x double> @llvm.masked.gather.v4f64.v4p0f64(<4 x double*> %17, i32 8, <4 x i1> <i1 true, i1 true, i1 true, i1 true>, <4 x double> undef) #8, !noalias !23\n  %18 = fmul fast <4 x double> %wide.masked.gather14.i, %wide.masked.gather.i\n  %scevgep7 = getelementptr double, double* %scevgep26.i, i64 %index.i\n  %scevgep78 = bitcast double* %scevgep7 to <4 x double>*\n  %scevgep9 = getelementptr <4 x double>, <4 x double>* %scevgep78, i64 -3\n  store <4 x double> %18, <4 x double>* %scevgep9, align 8, !alias.scope !17, !noalias !24\n  %vec.ind.next.i = add <4 x i64> %vec.ind.i, <i64 4, i64 4, i64 4, i64 4>\n  %19 = mul <4 x i64> %vec.ind.next.i, %broadcast.splat.i\n  %20 = add <4 x i64> %19, %broadcast.splat9.i\n  %21 = inttoptr <4 x i64> %20 to <4 x double*>\n  %wide.masked.gather.1.i = tail call <4 x double> @llvm.masked.gather.v4f64.v4p0f64(<4 x double*> %21, i32 8, <4 x i1> <i1 true, i1 true, i1 true, i1 true>, <4 x double> undef) #8, !noalias !23\n  %22 = mul <4 x i64> %vec.ind.next.i, %broadcast.splat11.i\n  %23 = add <4 x i64> %22, %broadcast.splat13.i\n  %24 = inttoptr <4 x i64> %23 to <4 x double*>\n  %wide.masked.gather14.1.i = tail call <4 x double> @llvm.masked.gather.v4f64.v4p0f64(<4 x double*> %24, i32 8, <4 x i1> <i1 true, i1 true, i1 true, i1 true>, <4 x double> undef) #8, !noalias !23\n  %25 = fmul fast <4 x double> %wide.masked.gather14.1.i, %wide.masked.gather.1.i\n  %scevgep3 = getelementptr double, double* %scevgep26.i, i64 %index.i\n  %scevgep34 = bitcast double* %scevgep3 to <4 x double>*\n  %scevgep6 = getelementptr <4 x double>, <4 x double>* %scevgep34, i64 -2\n  store <4 x double> %25, <4 x double>* %scevgep6, align 8, !alias.scope !17, !noalias !24\n  %vec.ind.next.1.i = add <4 x i64> %vec.ind.i, <i64 8, i64 8, i64 8, i64 8>\n  %26 = mul <4 x i64> %vec.ind.next.1.i, %broadcast.splat.i\n  %27 = add <4 x i64> %26, %broadcast.splat9.i\n  %28 = inttoptr <4 x i64> %27 to <4 x double*>\n  %wide.masked.gather.2.i = tail call <4 x double> @llvm.masked.gather.v4f64.v4p0f64(<4 x double*> %28, i32 8, <4 x i1> <i1 true, i1 true, i1 true, i1 true>, <4 x double> undef) #8, !noalias !23\n  %29 = mul <4 x i64> %vec.ind.next.1.i, %broadcast.splat11.i\n  %30 = add <4 x i64> %29, %broadcast.splat13.i\n  %31 = inttoptr <4 x i64> %30 to <4 x double*>\n  %wide.masked.gather14.2.i = tail call <4 x double> @llvm.masked.gather.v4f64.v4p0f64(<4 x double*> %31, i32 8, <4 x i1> <i1 true, i1 true, i1 true, i1 true>, <4 x double> undef) #8, !noalias !23\n  %32 = fmul fast <4 x double> %wide.masked.gather14.2.i, %wide.masked.gather.2.i\n  %scevgep = getelementptr double, double* %scevgep26.i, i64 %index.i\n  %scevgep1 = bitcast double* %scevgep to <4 x double>*\n  %scevgep2 = getelementptr <4 x double>, <4 x double>* %scevgep1, i64 -1\n  store <4 x double> %32, <4 x double>* %scevgep2, align 8, !alias.scope !17, !noalias !24\n  %vec.ind.next.2.i = add <4 x i64> %vec.ind.i, <i64 12, i64 12, i64 12, i64 12>\n  %33 = mul <4 x i64> %vec.ind.next.2.i, %broadcast.splat.i\n  %34 = add <4 x i64> %33, %broadcast.splat9.i\n  %35 = inttoptr <4 x i64> %34 to <4 x double*>\n  %wide.masked.gather.3.i = tail call <4 x double> @llvm.masked.gather.v4f64.v4p0f64(<4 x double*> %35, i32 8, <4 x i1> <i1 true, i1 true, i1 true, i1 true>, <4 x double> undef) #8, !noalias !23\n  %36 = mul <4 x i64> %vec.ind.next.2.i, %broadcast.splat11.i\n  %37 = add <4 x i64> %36, %broadcast.splat13.i\n  %38 = inttoptr <4 x i64> %37 to <4 x double*>\n  %wide.masked.gather14.3.i = tail call <4 x double> @llvm.masked.gather.v4f64.v4p0f64(<4 x double*> %38, i32 8, <4 x i1> <i1 true, i1 true, i1 true, i1 true>, <4 x double> undef) #8, !noalias !23\n  %39 = fmul fast <4 x double> %wide.masked.gather14.3.i, %wide.masked.gather.3.i\n  %scevgep10 = getelementptr double, double* %scevgep26.i, i64 %index.i\n  %scevgep1011 = bitcast double* %scevgep10 to <4 x double>*\n  store <4 x double> %39, <4 x double>* %scevgep1011, align 8, !alias.scope !17, !noalias !24\n  %index.next.3.i = add i64 %index.i, 16\n  %vec.ind.next.3.i = add <4 x i64> %vec.ind.i, <i64 16, i64 16, i64 16, i64 16>\n  %lsr.iv.next = add i64 %lsr.iv, 4\n  %niter.ncmp.3.i = icmp eq i64 %lsr.iv.next, 0\n  br i1 %niter.ncmp.3.i, label %middle.block.unr-lcssa.i, label %vector.body.i, !llvm.loop !25\n\nmiddle.block.unr-lcssa.i:                         ; preds = %vector.body.i, %vector.ph.i\n  %index.unr.i = phi i64 [ 0, %vector.ph.i ], [ %index.next.3.i, %vector.body.i ]\n  %vec.ind.unr.i = phi <4 x i64> [ %induction.i, %vector.ph.i ], [ %vec.ind.next.3.i, %vector.body.i ]\n  %lcmp.mod.not.i = icmp eq i64 %xtraiter.i, 0\n  br i1 %lcmp.mod.not.i, label %middle.block.i, label %vector.body.epil.preheader.i\n\nvector.body.epil.preheader.i:                     ; preds = %middle.block.unr-lcssa.i\n  %40 = inttoptr i64 %.113 to double*\n  %41 = add i64 %index.unr.i, %.96.i\n  %scevgep21.i = getelementptr double, double* %40, i64 %41\n  %42 = shl i64 %xtraiter.i, 5\n  br label %vector.body.epil.i\n\nvector.body.epil.i:                               ; preds = %vector.body.epil.i, %vector.body.epil.preheader.i\n  %lsr.iv22.i = phi i64 [ 0, %vector.body.epil.preheader.i ], [ %lsr.iv.next23.i, %vector.body.epil.i ]\n  %vec.ind.epil.i = phi <4 x i64> [ %vec.ind.unr.i, %vector.body.epil.preheader.i ], [ %vec.ind.next.epil.i, %vector.body.epil.i ]\n  %43 = bitcast double* %scevgep21.i to i8*\n  %44 = mul <4 x i64> %vec.ind.epil.i, %broadcast.splat.i\n  %45 = add <4 x i64> %44, %broadcast.splat9.i\n  %46 = inttoptr <4 x i64> %45 to <4 x double*>\n  %wide.masked.gather.epil.i = tail call <4 x double> @llvm.masked.gather.v4f64.v4p0f64(<4 x double*> %46, i32 8, <4 x i1> <i1 true, i1 true, i1 true, i1 true>, <4 x double> undef) #8, !noalias !23\n  %47 = mul <4 x i64> %vec.ind.epil.i, %broadcast.splat11.i\n  %48 = add <4 x i64> %47, %broadcast.splat13.i\n  %49 = inttoptr <4 x i64> %48 to <4 x double*>\n  %wide.masked.gather14.epil.i = tail call <4 x double> @llvm.masked.gather.v4f64.v4p0f64(<4 x double*> %49, i32 8, <4 x i1> <i1 true, i1 true, i1 true, i1 true>, <4 x double> undef) #8, !noalias !23\n  %50 = fmul fast <4 x double> %wide.masked.gather14.epil.i, %wide.masked.gather.epil.i\n  %uglygep = getelementptr i8, i8* %43, i64 %lsr.iv22.i\n  %uglygep13 = bitcast i8* %uglygep to <4 x double>*\n  store <4 x double> %50, <4 x double>* %uglygep13, align 8, !alias.scope !17, !noalias !24\n  %vec.ind.next.epil.i = add <4 x i64> %vec.ind.epil.i, <i64 4, i64 4, i64 4, i64 4>\n  %lsr.iv.next23.i = add nuw nsw i64 %lsr.iv22.i, 32\n  %epil.iter.cmp.not.i = icmp eq i64 %42, %lsr.iv.next23.i\n  br i1 %epil.iter.cmp.not.i, label %middle.block.i, label %vector.body.epil.i, !llvm.loop !27\n\nmiddle.block.i:                                   ; preds = %vector.body.epil.i, %middle.block.unr-lcssa.i\n  %cmp.n.i = icmp eq i64 %.226.i, %n.vec.i\n  br i1 %cmp.n.i, label %.continue, label %B116.preheader.i\n\nB116.preheader.i:                                 ; preds = %middle.block.i, %B116.lr.ph.i\n  %.2724.ph.i = phi i64 [ %.226.i, %B116.lr.ph.i ], [ %ind.end.i, %middle.block.i ]\n  %.213.03.ph.i = phi i64 [ %.96.i, %B116.lr.ph.i ], [ %ind.end6.i, %middle.block.i ]\n  %51 = inttoptr i64 %.113 to double*\n  %52 = add i64 %.2724.ph.i, 1\n  %53 = mul i64 %.213.03.ph.i, %.15\n  %54 = add i64 %53, %.84\n  %55 = mul i64 %.213.03.ph.i, %.13\n  %56 = add i64 %55, %.55\n  %scevgep.i = getelementptr double, double* %51, i64 %.213.03.ph.i\n  br label %B116.i\n\nB116.i:                                           ; preds = %B116.i, %B116.preheader.i\n  %lsr.iv19.i = phi double* [ %scevgep.i, %B116.preheader.i ], [ %scevgep20.i, %B116.i ]\n  %lsr.iv17.i = phi i64 [ %56, %B116.preheader.i ], [ %lsr.iv.next18.i, %B116.i ]\n  %lsr.iv15.i = phi i64 [ %54, %B116.preheader.i ], [ %lsr.iv.next16.i, %B116.i ]\n  %lsr.iv.i = phi i64 [ %52, %B116.preheader.i ], [ %lsr.iv.next.i, %B116.i ]\n  %.348.i = inttoptr i64 %lsr.iv17.i to double*\n  %.349.i = load double, double* %.348.i, align 8, !noalias !23\n  %.380.i = inttoptr i64 %lsr.iv15.i to double*\n  %.381.i = load double, double* %.380.i, align 8, !noalias !23\n  %.387.i = fmul fast double %.381.i, %.349.i\n  store double %.387.i, double* %lsr.iv19.i, align 8, !alias.scope !17, !noalias !24\n  %lsr.iv.next.i = add i64 %lsr.iv.i, -1\n  %lsr.iv.next16.i = add i64 %lsr.iv15.i, %.15\n  %lsr.iv.next18.i = add i64 %lsr.iv17.i, %.13\n  %scevgep20.i = getelementptr double, double* %lsr.iv19.i, i64 1\n  %.273.i = icmp ugt i64 %lsr.iv.next.i, 1\n  br i1 %.273.i, label %B116.i, label %.continue, !llvm.loop !29\n\n.continue:                                        ; preds = %B116.i, %middle.block.i, %for.body\n  %.215 = add nuw nsw i64 %loop.index2, 1\n  %exitcond.not = icmp eq i64 %.215, %loopcount\n  br i1 %exitcond.not, label %common.ret, label %for.body\n}\n\ndeclare void @numba_parallel_for(i8*, i8*, i8*, i8*, i8*, i64, i64, i64) local_unnamed_addr\n\ndeclare void @PyEval_RestoreThread(i8*) local_unnamed_addr\n\n; Function Attrs: inaccessiblememonly mustprogress nofree nosync nounwind willreturn\ndeclare void @llvm.experimental.noalias.scope.decl(metadata) #2\n\n; Function Attrs: mustprogress nofree nosync nounwind readonly willreturn\ndeclare <4 x double> @llvm.masked.gather.v4f64.v4p0f64(<4 x double*>, i32 immarg, <4 x i1>, <4 x double>) #3\n\n; Function Attrs: noinline\ndefine linkonce_odr void @NRT_decref(i8* %.1) local_unnamed_addr #4 {\n.3:\n  %.4 = icmp eq i8* %.1, null\n  br i1 %.4, label %common.ret, label %.3.endif, !prof !1\n\ncommon.ret:                                       ; preds = %.3, %.3.endif\n  ret void\n\n.3.endif:                                         ; preds = %.3\n  fence release\n  %.8 = bitcast i8* %.1 to i64*\n  %.4.i = atomicrmw sub i64* %.8, i64 1 monotonic, align 8\n  %.10 = icmp eq i64 %.4.i, 1\n  br i1 %.10, label %.3.endif.if, label %common.ret, !prof !1\n\n.3.endif.if:                                      ; preds = %.3.endif\n  fence acquire\n  tail call void @NRT_MemInfo_call_dtor(i8* nonnull %.1)\n  ret void\n}\n\ndeclare void @NRT_MemInfo_call_dtor(i8*) local_unnamed_addr\n\n; Function Attrs: mustprogress nofree noinline norecurse nounwind willreturn\ndefine linkonce_odr void @NRT_incref(i8* %.1) local_unnamed_addr #5 {\n.3:\n  %.4 = icmp eq i8* %.1, null\n  br i1 %.4, label %common.ret, label %.3.endif, !prof !1\n\ncommon.ret:                                       ; preds = %.3.endif, %.3\n  ret void\n\n.3.endif:                                         ; preds = %.3\n  %.7 = bitcast i8* %.1 to i64*\n  %.4.i = atomicrmw add i64* %.7, i64 1 monotonic, align 8\n  br label %common.ret\n}\n\n; Function Attrs: argmemonly mustprogress nofree nosync nounwind willreturn\ndeclare void @llvm.lifetime.start.p0i8(i64 immarg, i8* nocapture) #6\n\n; Function Attrs: argmemonly mustprogress nofree nosync nounwind willreturn\ndeclare void @llvm.lifetime.end.p0i8(i64 immarg, i8* nocapture) #6\n\n; Function Attrs: argmemonly nofree nounwind willreturn writeonly\ndeclare void @llvm.memset.p0i8.i64(i8* nocapture writeonly, i8, i64, i1 immarg) #7\n\nattributes #0 = { nofree nounwind }\nattributes #1 = { mustprogress nofree nosync nounwind readnone speculatable willreturn }\nattributes #2 = { inaccessiblememonly mustprogress nofree nosync nounwind willreturn }\nattributes #3 = { mustprogress nofree nosync nounwind readonly willreturn }\nattributes #4 = { noinline }\nattributes #5 = { mustprogress nofree noinline norecurse nounwind willreturn }\nattributes #6 = { argmemonly mustprogress nofree nosync nounwind willreturn }\nattributes #7 = { argmemonly nofree nounwind willreturn writeonly }\nattributes #8 = { nounwind }\n\n!0 = !{i1 true}\n!1 = !{!""branch_weights"", i32 1, i32 99}\n!2 = !{!3, !5, !6, !8, !9, !11}\n!3 = distinct !{!3, !4, !""_ZN5numba2np8arrayobj18_ol_array_allocate12_3clocals_3e4implB2v4B42c8tJTIeFIjxB2IKSgI4CrvQClcaMQ5hEEUSJJgA_3dEN29typeref_5b_3cclass_20_27numba4core5types8npytypes14Array_27_3e_5dExj: %retptr""}\n!4 = distinct !{!4, !""_ZN5numba2np8arrayobj18_ol_array_allocate12_3clocals_3e4implB2v4B42c8tJTIeFIjxB2IKSgI4CrvQClcaMQ5hEEUSJJgA_3dEN29typeref_5b_3cclass_20_27numba4core5types8npytypes14Array_27_3e_5dExj""}\n!5 = distinct !{!5, !4, !""_ZN5numba2np8arrayobj18_ol_array_allocate12_3clocals_3e4implB2v4B42c8tJTIeFIjxB2IKSgI4CrvQClcaMQ5hEEUSJJgA_3dEN29typeref_5b_3cclass_20_27numba4core5types8npytypes14Array_27_3e_5dExj: %excinfo""}\n!6 = distinct !{!6, !7, !""_ZN5numba2np8arrayobj15_call_allocatorB2v3B42c8tJTC_2fWQA93W1AaAIYBPIqRBFCjDSZRVAJmaQIAEN29typeref_5b_3cclass_20_27numba4core5types8npytypes14Array_27_3e_5dExj: %retptr""}\n!7 = distinct !{!7, !""_ZN5numba2np8arrayobj15_call_allocatorB2v3B42c8tJTC_2fWQA93W1AaAIYBPIqRBFCjDSZRVAJmaQIAEN29typeref_5b_3cclass_20_27numba4core5types8npytypes14Array_27_3e_5dExj""}\n!8 = distinct !{!8, !7, !""_ZN5numba2np8arrayobj15_call_allocatorB2v3B42c8tJTC_2fWQA93W1AaAIYBPIqRBFCjDSZRVAJmaQIAEN29typeref_5b_3cclass_20_27numba4core5types8npytypes14Array_27_3e_5dExj: %excinfo""}\n!9 = distinct !{!9, !10, !""_ZN5numba2np8arrayobj11ol_np_empty12_3clocals_3e4implB3v15B60c8tJTIeFIjxB2IKSgI4CrvQClYb5wBbdC9XqICn1Wk1gKGLEM1QzMJzALE0AEx18class_28float64_29: %retptr""}\n!10 = distinct !{!10, !""_ZN5numba2np8arrayobj11ol_np_empty12_3clocals_3e4implB3v15B60c8tJTIeFIjxB2IKSgI4CrvQClYb5wBbdC9XqICn1Wk1gKGLEM1QzMJzALE0AEx18class_28float64_29""}\n!11 = distinct !{!11, !10, !""_ZN5numba2np8arrayobj11ol_np_empty12_3clocals_3e4implB3v15B60c8tJTIeFIjxB2IKSgI4CrvQClYb5wBbdC9XqICn1Wk1gKGLEM1QzMJzALE0AEx18class_28float64_29: %excinfo""}\n!12 = !{!9, !11}\n!13 = !{!""branch_weights"", i32 99, i32 1}\n!14 = !{!15}\n!15 = distinct !{!15, !16, !""_ZN13_3cdynamic_3e36__numba_parfor_gufunc_0x7ef28ec1b9a0B3v16B128c8tJTC_2fWQAliW1xhDEoY6EEMEUOEMISPGsAQMVj4QniQ4IXKQEMXwoMGLoQDDVsQR1NHAS2hQ9XgStYwaUTI24JtBVoBDXtb9MCvVgdJqdcC7YLGAMy5RRDjNAE_3dE5ArrayIyLi1E1C7mutable7alignedE5ArrayIdLi1E1A7mutable7alignedE5ArrayIdLi1E1A7mutable7alignedE5ArrayIdLi1E1C7mutable7alignedE: %arg.sched.4""}\n!16 = distinct !{!16, !""_ZN13_3cdynamic_3e36__numba_parfor_gufunc_0x7ef28ec1b9a0B3v16B128c8tJTC_2fWQAliW1xhDEoY6EEMEUOEMISPGsAQMVj4QniQ4IXKQEMXwoMGLoQDDVsQR1NHAS2hQ9XgStYwaUTI24JtBVoBDXtb9MCvVgdJqdcC7YLGAMy5RRDjNAE_3dE5ArrayIyLi1E1C7mutable7alignedE5ArrayIdLi1E1A7mutable7alignedE5ArrayIdLi1E1A7mutable7alignedE5ArrayIdLi1E1C7mutable7alignedE""}\n!17 = !{!18}\n!18 = distinct !{!18, !16, !""_ZN13_3cdynamic_3e36__numba_parfor_gufunc_0x7ef28ec1b9a0B3v16B128c8tJTC_2fWQAliW1xhDEoY6EEMEUOEMISPGsAQMVj4QniQ4IXKQEMXwoMGLoQDDVsQR1NHAS2hQ9XgStYwaUTI24JtBVoBDXtb9MCvVgdJqdcC7YLGAMy5RRDjNAE_3dE5ArrayIyLi1E1C7mutable7alignedE5ArrayIdLi1E1A7mutable7alignedE5ArrayIdLi1E1A7mutable7alignedE5ArrayIdLi1E1C7mutable7alignedE: %arg._binop__mul8_2.4""}\n!19 = !{!20, !21, !22, !18}\n!20 = distinct !{!20, !16, !""_ZN13_3cdynamic_3e36__numba_parfor_gufunc_0x7ef28ec1b9a0B3v16B128c8tJTC_2fWQAliW1xhDEoY6EEMEUOEMISPGsAQMVj4QniQ4IXKQEMXwoMGLoQDDVsQR1NHAS2hQ9XgStYwaUTI24JtBVoBDXtb9MCvVgdJqdcC7YLGAMy5RRDjNAE_3dE5ArrayIyLi1E1C7mutable7alignedE5ArrayIdLi1E1A7mutable7alignedE5ArrayIdLi1E1A7mutable7alignedE5ArrayIdLi1E1C7mutable7alignedE: %retptr""}\n!21 = distinct !{!21, !16, !""_ZN13_3cdynamic_3e36__numba_parfor_gufunc_0x7ef28ec1b9a0B3v16B128c8tJTC_2fWQAliW1xhDEoY6EEMEUOEMISPGsAQMVj4QniQ4IXKQEMXwoMGLoQDDVsQR1NHAS2hQ9XgStYwaUTI24JtBVoBDXtb9MCvVgdJqdcC7YLGAMy5RRDjNAE_3dE5ArrayIyLi1E1C7mutable7alignedE5ArrayIdLi1E1A7mutable7alignedE5ArrayIdLi1E1A7mutable7alignedE5ArrayIdLi1E1C7mutable7alignedE: %arg.a.4""}\n!22 = distinct !{!22, !16, !""_ZN13_3cdynamic_3e36__numba_parfor_gufunc_0x7ef28ec1b9a0B3v16B128c8tJTC_2fWQAliW1xhDEoY6EEMEUOEMISPGsAQMVj4QniQ4IXKQEMXwoMGLoQDDVsQR1NHAS2hQ9XgStYwaUTI24JtBVoBDXtb9MCvVgdJqdcC7YLGAMy5RRDjNAE_3dE5ArrayIyLi1E1C7mutable7alignedE5ArrayIdLi1E1A7mutable7alignedE5ArrayIdLi1E1A7mutable7alignedE5ArrayIdLi1E1C7mutable7alignedE: %arg.b.4""}\n!23 = !{!20, !15, !18}\n!24 = !{!20, !15, !21, !22}\n!25 = distinct !{!25, !26}\n!26 = !{!""llvm.loop.isvectorized"", i32 1}\n!27 = distinct !{!27, !28}\n!28 = !{!""llvm.loop.unroll.disable""}\n!29 = distinct !{!29, !30, !26}\n!30 = !{!""llvm.loop.unroll.runtime.disable""}\n'}"
